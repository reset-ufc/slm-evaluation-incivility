# 404: Civility Not Found? Evaluating the Effectiveness of Small Language Models in Detecting Incivility in GitHub Conversations

## üìö Context

Incivility in open-source software (OSS) platforms like GitHub can hinder collaboration, reduce contributor participation, and affect code quality. Although existing Machine Learning (ML) and Natural Language Processing (NLP) tools attempt to moderate such content, they often fail to detect subtle or implicit forms of incivility.

## üéØ Goal

This project investigates the effectiveness of Small Language Models (SLMs) in identifying both:
- **Coarse-grained incivility** (civil vs. uncivil)
- **Fine-grained incivility** (e.g., Irony, Mocking, Identity Attack)

It also evaluates how different prompting strategies affect model performance on GitHub conversations (issues and pull requests).

## üß™ Methodology

- **Models**: 10 SLMs (3B‚Äì14B parameters)
- **Prompting Strategies**: 
  - Zero-shot
  - One-shot
  - Few-shot
  - Auto Chain-of-Thought (Auto-CoT)
  - Role-based
- **Baseline Comparison**: 5 traditional ML models using Bag-of-Words and TF-IDF
- **Dataset**: 6K+ labeled GitHub conversations

## üíª Installation & Usage

To get started, follow the steps below:

### 1. Clone the repository

```bash
git clone git@github.com:reset-ufc/slm-evaluation-incivility.git
cd slm-incivility-detector
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables
Create a `.env` file in the root directory of the project with the following content:

```ini
OPENAI_API_KEY=your_openai_api_key_here
```

This key is required to use cloud-based models like `gpt-4o-mini` via the OpenAI API.
Make sure to keep this file private and never commit it to version control.

### 5. Test the setup

To confirm that everything is working, try running the following command:

```bash
python fine_granularity/get_best_model.py
```

You should see output similar to:

```
Tamanho do dicionario de melhores configs: 24 
Best config: deepseek-14b + role_based aparecendo 5 vezes 
Second Best config: gpt-4o-mini + few_shot_3 aparecendo 4 vezes
Third Best config: llama3.1_8b + auto_cot aparecendo 2 vezes
```

This indicates that the environment is set up correctly and the code is executing as expected.

> **Note**: Some scripts require access to local or cloud-based SLMs.  
> For **local execution**, ensure [Ollama](https://ollama.com) is installed and configured properly.  
> For **cloud-based execution**, you‚Äôll need an OpenAI API key, which should be set in your environment variables (e.g., `OPENAI_API_KEY`).

---

### ‚öôÔ∏è System Requirements

To run local models effectively, we used a machine with the following specifications:

- **CPU**: AMD Ryzen 7 7700 (8 cores, 16 threads)
- **RAM**: 64 GB DDR5
- **GPU**: NVIDIA RTX 4070 Ti Super (16 GB VRAM)

Cloud-based executions were performed using OpenAI's optimized infrastructure.

## üìÅ Repository Structure

- **`coarse_granularity/`**  
  - **`data/`**: Input datasets for coarse-grained analysis  
  - **`results/`**: Raw and intermediate outputs  
  - `benchmark.py`: Benchmarks traditional ML models  
  - `choice_examples.py`: Examples used for prompting in coarse analysis  
  - `generate_visualizations_and_results.py`: Visualization and results export  
  - `get_best_model.py`: Identifies best-performing model  
  - `get_ml_results.py`: Gathers outputs from traditional ML models  
  - `get_new_rq1_table.py`: Reformats RQ1 results  
  - `get_rq2_table.py`: Gathers RQ2 results  
  - `main.py`: Main script to run the coarse-grained pipeline  
  - `merge_dados.ipynb`: Notebook to merge and clean datasets  
  - `prompt.py`: Prompt management and definitions  
  - `results_without_duplicates.py`: Filters duplicate results and get compact result tables 
  - `rq3_table.py`: Handles outputs for RQ3  
  - `tokens.py`: Token counting and prompt length control  
  - `transformer_training.py`: Training logic for transformer models  
  - `verificando_resultados.py`: Results validation script  
  - `visu.ipynb/`: Visualizations and exploratory notebook  

- **`fine_granularity/`**  
  - **`data/`**: Input dataset for fine-grained analysis  
  - **`results/`**: Raw results generated by models  
  - **`results_table/`**: Results formatted into tables for reporting  
  - `benchmark.py`: Script to benchmark different ML models  
  - `choice_examples.py`: Example prompts and choices used in evaluations  
  - `excluir_erros_timeouts.py`: Filters out timeouts and erroneous results  
  - `get_best_model.py`: Retrieves best performing models  
  - `get_rq1_results.py`: Gathers results for research question 1  
  - `get_rq1_fg_table.py`: Formats fine-grained results for RQ1  
  - `get_rq2_fg_table_worst.py`: Retrieves worst-case results for RQ2  
  - `main_prompt.py`: Core prompt management script  
  - `prompts.py`: All prompt templates and configurations  
  - `summarizar_resultados.py`: Script to summarize results  
  - `transformer_training.py`: Model fine-tuning and training logic  
  - `verificando_resultados.py`: Verifies and validates result outputs  

- **`requirements.txt`**: List of required Python packages  
- **`README.md`**: Project overview and usage instructions
