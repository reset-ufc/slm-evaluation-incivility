# 404: Civility Not Found? Evaluating the Effectiveness of Small Language Models in Detecting Incivility in GitHub Conversations

## üìö Context

Incivility in open-source software (OSS) platforms like GitHub can hinder collaboration, reduce contributor participation, and affect code quality. Although existing Machine Learning (ML) and Natural Language Processing (NLP) tools attempt to moderate such content, they often fail to detect subtle or implicit forms of incivility.

## üéØ Goal

This project investigates the effectiveness of Small Language Models (SLMs) in identifying both:
- **Coarse-grained incivility** (civil vs. uncivil)
- **Fine-grained incivility** (e.g., Irony, Mocking, Identity Attack)

It also evaluates how different prompting strategies affect model performance on GitHub conversations (issues and pull requests).

## üß™ Methodology

- **Models**: 10 SLMs ranging from 3B to 14B parameters, including [**DeepSeek-R1** (8B, 14B)](https://ollama.com/library/deepseek-r1), [**Mistral** (7B)](https://ollama.com/library/mistral), [**Mistral-Nemo** (12B)](https://ollama.com/library/mistral-nemo), [**Gemma** (7B)](https://ollama.com/library/gemma), [**Gemma 2** (9B)](https://ollama.com/library/gemma2), [**LLaMA 3.1** (8B)](https://ollama.com/library/llama3.1), [**LLaMA 3.2** (3B)](https://ollama.com/library/llama3.2), [**Phi-4** (14B)](https://ollama.com/library/phi4), and **GPT-4o-mini** (with OpenAI API).
- **Prompting Strategies**: 
  - Zero-shot
  - One-shot
  - Few-shot
  - Auto Chain-of-Thought (Auto-CoT)
  - Role-based
- **Baseline Comparison**: 5 traditional ML models (**Multinomial Naive Bayes**, **Logistic Regression Classifier**, **Random Forest Classifier**, **AdaBoost Classifier**, and **DistilBERT**) using Bag-of-Words and TF-IDF
- **Dataset**: 6K+ labeled GitHub conversations

## üíª Installation & Usage

To get started, follow the steps below:

### ‚úÖ Requirements

- **Python 3.10+**
- Optional: [Ollama](https://ollama.com) (for running local SLMs)
- OpenAI API key (for cloud-based models like GPT-4o-mini)

### 1. Clone the repository

```bash
git clone git@github.com:reset-ufc/slm-evaluation-incivility.git
cd slm-incivility-detector
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables
Create a `.env` file inside either the `fine_granularity/` or `coarse_granularity/` directory, depending on which granularity you intend to run. The file should contain the following:

```ini
OPENAI_KEY=your_openai_api_key_here
```

This key is required to use cloud-based models like `gpt-4o-mini` via the OpenAI API.
Make sure to keep this file private and never commit it to version control.

### 5. Test the setup

To confirm that everything is working, try running the following command:

```bash
python fine_granularity/get_best_model.py
```

You should see output similar to:

```
Tamanho do dicionario de melhores configs: 24 
Best config: deepseek-14b + role_based aparecendo 5 vezes 
Second Best config: gpt-4o-mini + few_shot_3 aparecendo 4 vezes
Third Best config: llama3.1_8b + auto_cot aparecendo 2 vezes
```

This indicates that the environment is set up correctly and the code is executing as expected.

> **Note**: Some scripts require access to local or cloud-based SLMs.  
> For **local execution**, ensure [Ollama](https://ollama.com) is installed and configured properly.  
> For **cloud-based execution**, you‚Äôll need an OpenAI API key, which should be set in your environment variables (e.g., `OPENAI_KEY`).

---

### ‚öôÔ∏è Hardware Setup Used To Experimentation
To run local models effectively, we used a machine with the following specifications:

- **CPU**: AMD Ryzen 7 7700 (8 cores, 16 threads)
- **RAM**: 64 GB DDR5
- **GPU**: NVIDIA RTX 4070 Ti Super (16 GB VRAM)

Cloud-based executions were performed using OpenAI's optimized infrastructure.

## üìÅ Repository Structure

- **`coarse_granularity/`**  
  - **`data/`**: Input datasets for coarse-grained analysis  
  - **`results/`**: RQ tables and Raw results generated by the models
  - `benchmark.py`: Script to benchmark different ML models  
  - `choice_examples.py`: Examples used for prompting in coarse-grained incivility
  - `generate_visualizations_and_results.py`: Visualization and results export  
  - `get_best_model.py`: Identifies best-performing model  
  - `get_ml_results.py`: Gathers outputs from traditional ML models  
  - `get_rq1_cg.py`: Gathers results for research question 1  
  - `get_rq2_cg.py`: Gathers results for research question 2
  - `get_rq3_cg.py`: Gathers results for research question 3  
  - `main.py`: Main script to run the coarse-grained pipeline  
  - `merge_data.ipynb`: Notebook to merge and clean datasets  
  - `prompts.py`: All prompt templates and configurations   
  - `results_without_duplicates.py`: Filters duplicate results and get compact result tables 
  - `tokens.py`: Token counting and prompt length control  
  - `transformer_training.py`: Training logic for transformer models  
  - `verify_results.py`: Results validation script  
  - `visus.ipynb/`: Visualizations and exploratory notebook  
  - `test_coarse_grained.ipynb`: Jupyter notebook with statistical tests for coarse-grained incivility

- **`fine_granularity/`**  
  - **`data/`**: Input dataset for fine-grained analysis  
  - **`results/`**: Raw results generated by models  
  - **`results_table/`**: Results formatted into tables for reporting  
  - `benchmark.py`: Script to benchmark different ML models  
  - `choice_examples.py`: Examples used for prompting in fine-grained incivility
  - `delete_errors_timeouts.py`: Filters out timeouts and erroneous results  
  - `get_best_model.py`: Identifies best-performing model 
  - `get_ml_results.py`: Gathers outputs from traditional ML models  
  - `get_rq1_fg.py`: Gathers results for research question 1  
  - `get_rq2_fg.py`: Gathers results for research question 2
  - `get_rq3_fg.py`: Gathers results for research question 3
  - `test_fine_grained.ipynb`: Jupyter notebook with statistical tests for fine-grained incivility
  - `main.py`: Main script to run the fine-grained pipeline   
  - `prompts.py`: All prompt templates and configurations  
  - `summarize_results.py`: Script to summarize results  
  - `transformer_training.py`: Training logic for transformer models  
  - `verify_results.py`: Results validation script

- **`requirements.txt`**: List of required Python packages  
- **`README.md`**: Project overview and usage instructions
