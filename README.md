# Incivility Detection in GitHub Conversations Using Small Language Models (SLMs)

## üìö Context

Incivility in open-source software (OSS) platforms like GitHub can hinder collaboration, reduce contributor participation, and affect code quality. Although existing Machine Learning (ML) and Natural Language Processing (NLP) tools attempt to moderate such content, they often fail to detect subtle or implicit forms of incivility.

## üéØ Goal

This project investigates the effectiveness of Small Language Models (SLMs) in identifying both:
- **Coarse-grained incivility** (civil vs. uncivil)
- **Fine-grained incivility** (e.g., Irony, Mocking, Identity Attack)

It also evaluates how different prompting strategies affect model performance on GitHub conversations (issues and pull requests).

## üß™ Methodology

- **Models**: 10 SLMs (3B‚Äì14B parameters)
- **Prompting Strategies**: 
  - Zero-shot
  - One-shot
  - Few-shot
  - Auto Chain-of-Thought (Auto-CoT)
  - Role-based
- **Baseline Comparison**: 5 traditional ML models using Bag-of-Words and TF-IDF
- **Dataset**: 6K+ labeled GitHub conversations

## üìÅ Repository Structure

- **`granularidade_fina/`**  
  - **`data/`**: Input dataset for fine-grained analysis  
  - **`results/`**: Raw results generated by models  
  - **`results_df/`**: Processed results in DataFrame format  
  - **`results_table/`**: Results formatted into tables for reporting  
  - `benchmark.py`: Script to benchmark different ML models  
  - `choice_examples.py`: Example prompts and choices used in evaluations  
  - `excluir_erros_timeouts.py`: Filters out timeouts and erroneous results  
  - `get_best_model.py`: Retrieves best performing models  
  - `get_rq1_results.py`: Gathers results for research question 1  
  - `get_rq1_fg_table.py`: Formats fine-grained results for RQ1  
  - `get_rq2_fg_table_worst.py`: Retrieves worst-case results for RQ2  
  - `main_prompt.py`: Core prompt management script  
  - `prompts.py`: All prompt templates and configurations  
  - `summarizar_resultados.py`: Script to summarize results  
  - `transformer_training.py`: Model fine-tuning and training logic  
  - `verificando_resultados.py`: Verifies and validates result outputs  

- **`granularidade_grossa/`**  
  - **`data/`**: Input datasets for coarse-grained analysis  
  - **`results/`**: Raw and intermediate outputs  
  - `benchmark.py`: Benchmarks traditional ML models  
  - `choice_examples.py`: Examples used for prompting in coarse analysis  
  - `generate_visualizations_and_results.py`: Visualization and results export  
  - `get_best_model.py`: Identifies best-performing model  
  - `get_ml_results.py`: Gathers outputs from traditional ML models  
  - `get_new_rq1_table.py`: Reformats RQ1 results  
  - `get_rq2_table.py`: Gathers RQ2 results  
  - `main.py`: Main script to run the coarse-grained pipeline  
  - `merge_dados.ipynb`: Notebook to merge and clean datasets  
  - `prompt.py`: Prompt management and definitions  
  - `results_without_duplicates.py`: Filters duplicate results and get compact result tables 
  - `rq3_table.py`: Handles outputs for RQ3  
  - `tokens.py`: Token counting and prompt length control  
  - `transformer_training.py`: Training logic for transformer models  
  - `verificando_resultados.py`: Results validation script  
  - `visu.ipynb/`: Visualizations and exploratory notebook  

- **`README.md`**: Project overview and usage instructions
