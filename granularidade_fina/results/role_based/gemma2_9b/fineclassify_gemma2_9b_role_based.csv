Comments,Tbdf
"I noticed that Prepared Statements seem to be emulated client-side by escaping certain characters.
Any plans to fully support service-side Prepared Statements? This can be done via the binary protocol, but there's a slower SQL-based approach available for non-binary clients:
http://dev.mysql.com/doc/refman/5.5/en/sql-syntax-prepared-statements.html",
"Yes, prepared statements are on my todo list. I don't need them myself, so unfortunately they kind of linger at the bottom of the list unless somebody wants to sponsor some of my time to work on the feature.
That being said, the SQL based approach looks interesting as a stop-gap solution for the short term.",
"The only downside with the SQL-based approach is that you probably still end up needing to do client-side escaping. Still it does offer a little bit more structure, so it might still buy some protection. Depending on how you do it, it might also simplify the escaping part.
Unless I'm mistaken, you are already implementing the actual protocol at the lower levels of your driver. I wonder how much more you need at that level to finish?
http://dev.mysql.com/doc/internals/en/command-packet-details.html",
"Prepared statements use a range of additional packets that are currently not implemented by my driver:
- http://dev.mysql.com/doc/internals/en/prepared-statement-initialization-packet.html
- http://dev.mysql.com/doc/internals/en/parameter-packet.html
- http://dev.mysql.com/doc/internals/en/long-data-packet.html
- http://dev.mysql.com/doc/internals/en/execute-packet.html
I have not yet analyzed how much work it would be to implement them, but my gut feeling is ~5 days of work.",
"How does https://github.com/sidorares/nodejs-mysql-native handle this? Any reason not to just borrow parts of the way it's done over there?
I'm still somewhat struggling with the number of different MySQL drivers for Node.JS. I think Node makes it way too fun to write network protocol code. :P Maybe in a year or so the community will have coalesced around one or two really solid libraries.",Irony
"> How does https://github.com/sidorares/nodejs-mysql-native handle this?
It seems to implement the parts of the protocol that are required for prepared statements.
> Any reason not to just borrow parts of the way it's done over there?
Yes, I didn't have the time to work on this yet. I'm also not in the business of copying code unless it's up to my personal coding standards. So even with good inspiration like this, it will still take me some time.
> Maybe in a year or so the community will have coalesced around one or two really solid libraries.
This library is solid. It just does not implement all features.",Impatience
Couldn't we just prepare and execute statements using SQL instead of raw packets?,Impatience
"@dresende the SQL method still winds up tampering with values to make them safe (escaping quotes, etc), whereas the protocol method explicitly separates query from values so tampering is not necessary. To be fair, as long as its impossible to smuggle a query in as a value, the driver is plenty secure enough. I suppose I'm just being a nitpicky ex-PHP developer who wants everything to be conceptually elegant. :P",Irony
"I'm not sure if this is officially supported by Doctrine, since it is not documented, but it works - with an error notice.
For example, if I have documents A and B where document B has an embedded document, E, I can design the mapping as follows:
``` php
/** @Document */
class A {
/** @ReferenceMany(targetDocument=""B"", mappedBy=""foo.bar"") */
private $whatever = [];
}
/** @Document */
class B {
/** @EmbedMany(targetDocument=""E"") */
private $foo = [];
}
/** @EmbeddedDocument */
class E {
/** @ReferenceOne(targetDocument=""A"", inversedBy=""whatever"") */
private $bar;
}
```
However, reading A::$whatever generates the following notice:
> Notice: Undefined index: foo.bar in mongodb-odm/lib/Doctrine/ODM/MongoDB/Persisters/DocumentPersister.php on line 709
Despite this, the document collection is retrieved correctly.",
"This doesn't work if class `B` has a binary ID and `$bar` is a discriminator reference. I'm not sure what the culprit is here, but the symptomns are that `count(A::$whatever)` will never be > 1. That is, at most only one item is loaded into the collection on the field with an embedded document mapping.",Bitter Frustration
This issue has been automatically marked as stale because it has not had any recent activity. It will be closed in a week if no further activity occurs. Thank you for your contributions.,
Imagine thinking anyone cares about design flaws.,Mocking
Imagine thinking anyone cares about Open Source being a joint effort.,Mocking
Imagine thinking open source means other people work for you for free.,Mocking
"If you set up some task this way:
before 'deploy', 'some_task', And in some_task you simply read the ""latest_release"" variable (in a puts line for instance), then:
Current results:
a) If it's the first deploy ever in capistrano (the www folder doesn't exist), capistrano breaks completely, it cannot deploy.
b) If it's not the first deploy, then it will make capistrano change its behaviour with regards to the ""current"" symlink, because it will point to the previous release instead of the last one (after the deploy happened).
Expected results:
a) It should work.
b) It should point current to the latest release.
This is a big fuckup IMHO.",Bitter Frustration
"> This is a big fuckup IMHO.
Profanity aside, you may have a point. I'll take a patch.",Bitter Frustration
Why do you close the issue?,Impatience
"Because you don't offer a patch, and profanity really pisses me off.",Bitter Frustration
"Mutable programming pisses me off even more.
Anyway, the point of github issues is to have a bug tracker.
The contributions comes in the pull-requests tab.",Bitter Frustration
"Indeed, in short - if there's no release, you are doing something wrong if you are calling `latest_release`. It's not safe for use when it's a cold deploy. _It's that simple._
There are other variables which are safe for use throughout the deploy process.
Capistrano, the 2.0.x branch mainline is effectively on the verge of being deprecated, send a patch or things won't be fixed. And even if you do send a patch, changing the behaviour of core variable lookups is not something I approach lightly.
I'd rather invest that time into the v3 branch which will coincide with the Rails 4 release in the coming weeks, and have a stable version which has fewer insane design decisions behind it.",Bitter Frustration
"As for your second point, you may be onto something there, but it's too late to change it, it's unreasonable overhead to call back to the server to calculate that every time, the results are cached for a reason. If you populate the cache with bad data, then you can't really effectively clear it. I accept that it's confusing, but I won't change it.",Bitter Frustration
"I understand that latest_release may not have the proper value when it's queried at the wrong time... but CHANGING BEHAVIOUR???? Sorry for the caps, but really finding out this bug has put me on my nerves.
In regards to your backwards-compatibility policy: fair enough, I understand if you prefer to accept only a patch to fix this in master rather than the 2.0.x branch, but:
a) I never proposed to fix it in a branch.
b) This bug is not fixed yet on any branch, so it should be reopened. Otherwise people confused about it will not be able to find it.
This is, my friend, how bug trackers work.",Bitter Frustration
"I won't fix it, and it's been the way it is for five years without anyone running into problems. I can't afford the time to test it, fix it and make sure it's safe before releasing it. That's what it boils down to. Those variables are a source of a lot of confusion, and they are fragile, and have different meanings wherever you call them, anyway.
I'd like to be able to fix it, your profane issue report aside; but the reality is, I simply can't.",Bitter Frustration
Then leave this issue open until the end of times.,Bitter Frustration
"Otherwise it would be the first time in my entire life in which I see a bug closed as WONTFIX.
I mean, I've seen feature requests closed as WONTFIX status. But bugs? It's like denying to recognize that there is a bug.",Bitter Frustration
"Just for the sake of people being able to find the issue (and be enlightened by the work-around: ""not read a variable""), this issue should remain open.",Irony
Agreed. I've re-ragged with v2.,
"Why did you close this? People are getting here from this SO question: http://stackoverflow.com/questions/3141454/deploysymlink-on-capistrano-points-the-current-directory-to-previous-release/16043844#16043844 and if they find this issue closed, they might be misled into thinking that it is fixed.",Impatience
@knocte did you reproduce it on v3?,Impatience
"Oh, when was that released?",Impatience
"See rubygems, early september
On 2 Nov 2013 20:30, ""Andres G. Aragoneses"" notifications@github.com
wrote:
> Oh, when was that released?
> > â€”
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630493
> .",Impatience
"Ok, then I have not tested that. And I'm afraid I will not be able to test it very soon.
Just one advice: if you want to close issue in order to request feedback from users about new versions, add a comment explaining why you are closing the issue.
Thanks",
"We have more than 100 obsolete issues to close, and past experience
suggests the OPs never reapond
On 2 Nov 2013 20:36, ""Andres G. Aragoneses"" notifications@github.com
wrote:
> Ok, then I have not tested that. And I'm afraid I will not be able to test
> it very soon.
> > Just one advice: if you want to close issue in order to request feedback
> from users about new versions, add a comment explaining why you are closing
> the issue.
> Thanks
> > â€”
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630609
> .",Bitter Frustration
That doesn't respect the Robust principle ;) http://en.wikipedia.org/wiki/Robustness_principle,Irony
"People are still upvoting my stackoverflow answer linked above, which seems to hint that this bug is still present in 3.x.",Bitter Frustration
thanks for checking the so answer upvote rates @knocte,
"Running into this issue currently I think.
How does one call the current release path that is right now being built?
I have a task running at `after :updated, :build do` and I'm trying to change directories and then run composer install, but changing directories using release_path is giving me the previous release, not the one currently being built.
Is that this issue? If not I'm sorry but the stackoverflow question led me here and I'm going kind of crazy trying to figure out how to change directories and then run a command, without it doing it in the release that's about to be replaced.",Bitter Frustration
"This issue is more than five years old, and refers to a long obsolete version of Capistrano, please open a new issue.",Impatience
"Can someone link to this ""new"" issue?",Impatience
"This issue is very old and the discussion was not very productive, so I will lock it. If you are a Capistrano v3 user running into an error using the `latest_release` setting please open a new issue.",
"@batmanbury there is no ""new"" issue; please open one if you are running into this problem.",Impatience
"The only query / issue I have is the big lines it draws blue (or themed borders) around countries.
There is a method to remove them?",
"Pass the options parameter:
http://leafletjs.com/reference.html#polygon
http://leafletjs.com/reference.html#path",Impatience
"It's a shame you couldn't provide an answer. Pointing to a documentation page that has no response to the original question isn't useful at all. Equally, it's pretty useless having something that draws a map with a big blue border around every country and no explanation as to how to remove it.",Bitter Frustration
"@abrice This kind of passive-aggressiveness is disrespectful towards maintainers.
I suggest reading [""How To Ask Questions The Smart Way""](http://www.catb.org/esr/faqs/smart-questions.html) and [""How to Report Bugs Effectively""](http://www.chiark.greenend.org.uk/~sgtatham/bugs.html) so you can make good bug reports in the future.",Bitter Frustration
Because most of the projects require Composer I have a hard time installing PHP application. This is because I don't have SSH access to my shared hosting. Sadly Composer makes PHP projects for only rich people who can buy hostings with SSH access. Can you please make Composer so that it does not require SSH. http://stackoverflow.com/questions/20894518/how-do-i-install-composer-on-a-shared-hosting,Bitter Frustration
Write a PHP script that you can hit in your browser that executes the `composer.phar` on your server?,
"@milesj, any tutorial on it?",Impatience
"No tutorial really, just execute shell commands or something similar. http://php.net/shell_exec
`shell_exec('php /path/to/composer.phar install');`
Never really tried it however.",Impatience
"Unfortunately, `shell_exec` and similar functions are also disabled on most shared hosting that don't support ssh login.
The current solution is to run composer locally and upload the vendor dir with the rest of your files.
As a gift, you can use [composer as a service](http://composer.borreli.com/) by @pborreli to download a zip of all your dependencies. It must be possible to create a simple PHP page that upload your `composer.json`, download the zip and extract it.",
"@GromNaN there are plans to have it done this via a composer plugin or extension or configuration (via composer as a service future api), that hopefully soon :baby:",Impatience
@cordoval The composer plugin is not enough as the cli of composer cannot be run on these environments. A webpage is necessary. But that will be easy.,Impatience
"If you don't have a CLI access on these systems, there is a high probably than other tools used by composer are not available either. In such case, the proper workflow would indeed be to run composer locally and to upload the code including the vendor folder, a suggested by @GromNaN",
WordPress can download and modify files from internet. I'm sure it is possible to make a similar installer (dependency-manager) with user interface.,
"@ilhanyumer it is indeed possible to build an app performing the composer job, but composer itself relies on git, svn and hg (depending of the VCS being used) to install from source. And it is not always possible to install from an archive. Shared hostings don't offer them generally.
Thus, it relies on `proc_open` to call these tools, which is often disabled on these shared hostings",
"https://github.com/CurosMJ/NoConsoleComposer
This can sort this thing out, hopefully.",Impatience
"We have created a client for the Contao CMS to use composer directly from the CMS Backend.
https://github.com/contao-community-alliance/composer-client
The client can run composer in 3 modes:
- inline: directly within the web servers php process
- process: start a process with `proc_open` within the web servers php process
- detached: start and fork composer into background process",
"This isn't really an issue with composer and should be closed. > Sadly Composer makes PHP projects for only rich people who can buy hostings with SSH access.
That's entirely untrue as Digital Ocean sells VPS for $5/month and Linode sells them for $10/month.",Mocking
"Hi, im running node-mysql latest on node-latest.
Somebody using the acunetix vulnerability scanner has triggered this error:
UNKNOWN COLUMN '$acunetix' IN WHERE CLAUSE.
The query: SELECT id, email FROM accounts WHERE username = ?
How is this possible? Its very dangerous to our application, please respond quickly.",Bitter Frustration
"The problem seems to be about params that are not strings. Although I'll continue to sanitize all my user inputs (to avoid username impersonation attacks like `admÑ–n` posing as `admin`), I'd expect the query engine to convert any param to a string if it should have been one in the first place. If it already was, `String(param)` should be of low cost.",Impatience
@thekiur @mk-pmb can you post code samples?,Impatience
"We can confirm that the problem is caused by passing objects to the query call.
The objects come from the express bodyParser middleware.
We were simply passing req.body.username as the parameter for that query.
The acunetic vulnerability tester injected an object there.
We are not sure on the severity of this issue, but its unexpected to say atleast.
As we experienced, this can crash a live application in production mode if you dont expect any db errors.
There is no code to show: its as simple as passing a req.body.something to the .query call of node-mysql when using express with the bodyparser middleware. Running the vulnerability scanner against https://gist.github.com/ssafejava/9a2d77704712a8769322 causes the exception to be thrown.",Bitter Frustration
"This is not an issue with escaping with this library; this library is properly escaping all values and column names. The security issue is just with the way you are combining express and this library, such that you were expecting to get a string from express, so you were only expecting the `?` to expand according to string rules.
`req.body` properties can be anything with `bodyParser` and as such you need to at least verify what you are using is a string before passing to your query.",
"I consider prepared statements as intended to mitigate lack of input validation in the params in general. Therefor, limiting it to the case where input has already been validated as being a string, in my opinion misses the point.
Yours, MK",Bitter Frustration
"These are not prepared statements, they are done client-side and have various rules for how `?` is replaced depending on the data type, which is documented. If you want to be sure you are using the string-based `?` replacement though the API, you have to give the API a string. If you don't want to validate at all, you can use the `String()` function:
`conn.query('SELECT * FROM user WHERE username = ?', [String(req.body.username)]')`
The _purpose_ if it doing stuff different for objects is to help people who want to easily use `SET`:
`conn.query('UPDATE user SET ? WHERE id = ?', [{username: 'bob', password: '1234'}, 43])`
Please see the ""Different value types are escaped differently, here is how:"" section in https://github.com/felixge/node-mysql#escaping-query-values",
"I see. Looks like an unlucky case of embrace and extend. I wish you had opted for something like `??` in that case. Probably too late to change the interface?
Edit: Not really embrace and extend, as you wrote they aren't prepared statements. Rather just a pitfall for people who learn from tutorials and conclude topical similarity from visual similarity.
Edit 2: I see, `??` is already used for column names.",
I can't see how it's the type system's fault when programmers assume that a mechanism that looks like prepared statements will defuse any data they pass in. Let's at least blame it at the programmers for trusting visual similarity instead of reading the manual thoroughly.,Bitter Frustration
"@mk-pmb sure, though this module only has a small Readme, which has all the `?` stuff explained (https://github.com/felixge/node-mysql#escaping-query-values), so it's not even some weird hidden feature. Unfortunately if people on the Internet are writing tutorials about this module and giving incomplete or wrong information, it's hard for us to even try to police that.",Bitter Frustration
"@mk-pmb it's the programmers role to understand the libraries he/she is using at least to the extend they are documented before including them in any production environment. If the library isn't fully documented, that's on the creator, but since this is an open-source world you can't really blame somebody for dedicating their time towards creating something for free.
Inferring functionality from syntax is useful, but think rationally: if the `?` operator accepts strings, would it only accepts strings? What if it accepted other data types? Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.
Libraries and languages that make it easier to start developing are extremely useful, but I fear it gives a novice developer a misplaced sense of confidence. It's easy to build a small application, and when it ""just works"" assume nothing could possibly go wrong.",Bitter Frustration
"> Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.
I agree with that. And still, lots of people do it. So for all software that I manage, I'll try and have it be compatible with everyday flawed humans, in hopes to lessen the risk and impact of errors in software based on mine, written by fallible humans.
BOfH would ship a GNU/Linux distro where the default shell acts fully like bash, just that on every line starting with an uppercase letter, the meaning of `&&` and `||` is swapped. Might even document it properly. You'd read the manual and probably wouldn't use it. However, if the next day a toy drone crashes into your car because it's pilot didn't read the manual as thoroughly as you did, your expectations of how humans should act had much less impact than how they really do act. And I'd still partially blame that BOfH.
Update: Thanks for making it opt-in.",Bitter Frustration
"Please, this issue doesn't need any more comments. It is still open as a tracking issue for me. There are coming changes that will affects this module and even things like `express` which will make any kind of ""shoot yourself in the foot"" operations opt-in. As an example, for this module `?` really should strictly only result in a single entry in the SQL (i.e. numbers, strings, null, etc.). Anything over that should be opt-in (on the connection-level or one-off on the query level to reduce accidental exposure.
These are changes that are coming I listed, not speculation. Please just know that this issue is taken seriously.",
"Are there any circumstances where this would lead to an injection attack?
As far as I can work out so far this appears to only ever result in syntax errors.",
"@SystemParadox: I don't think so. The report seems to be badly explained and seems to be related to constructing SQL based on user input without any check.
Good usage:
``` js
db.query(""SELECT * FROM users WHERE id = ?"", [Â +req.params.id ], next);
```
No harm on that, casting forces it to be a number. Even if it wasn't a number and the `+` was omitted, it's just fine (or else you would have problems when UPDATing columns with binary data - there's tests for that).
The problem here seems to be with something more like:
``` js
// BAD! BAD!
db.query(""SELECT * FROM "" + req.params.table + "" WHERE ...."", next);
```",
@SystemParadox yeah I just took a look at the formatting and escaping code. I don't see any way that passing unvalidated data to be interpolated into the query could result in an injection vulnerability. Without validation you can easily get a syntax error.,
"Could you please implement xBR shader or xBRZ filter or both in GSDX plugin. It would be very beneficial for both PS2 and PSX 2D and sprite-based games.
xBR and xBRZ are pixel art scaling algorithms ,they give best results in 2D/sprite based games with low resolution textures and games with pre-rendered backgrounds which dont upscale well with higher internal resolutions but they also give good results in 3D games. xBR/xBRZ are already used with good results in emulators like Retroarch , Higan, Desmume and PPSSPP.
Here is explanation:http://code.google.com/p/2dimagefilter/wiki/ImageScaling#xBR
http://www.vogons.org/viewtopic.php?t=34125
Here is newest xBR source code including hybrid variants: https://github.com/libretro/common-shaders
Source code for xBRZ is in source code of HqMAME : https://sourceforge.net/projects/hqmame/ ,http://sourceforge.net/projects/hqmame/files/xBRZ.zip/download, Spline36: http://code.google.com/p/remote-joy-lite-fix/source/browse/trunk/RemoteJoyLite_pc/spline36.psh
https://github.com/xbmc/xbmc/tree/master/xbmc/cores/VideoRenderers/VideoShaders
Here is comparison for 3D graphics:http://blog.metaclassofnil.com/?p=306
Here is official tutorial about xBR: http://www.libretro.com/forums/viewtopic.php?f=6&t=134
http://forum.zdoom.org/viewtopic.php?f=19&t=37373&sid=57269f5e32514a88a5d5252839c9ff6a&start=45
Some 2D graphics of old version of xBR: http://imgur.com/a/ZZiiH
I also found interesting algorithm Libdepixelize: http://bazaar.launchpad.net/~vinipsmaker/libdepixelize/trunk/revision/184
http://vinipsmaker.wordpress.com/tag/libdepixelize/
https://sourceforge.net/projects/inkscape/
There is also ''Ours'' but I cant find source code for it anywhere: http://research.microsoft.com/en-us/um/people/kopf/pixelart/supplementary/
But Libdepixelize and Ours both use Kopf-Lischinski algorithm so they should have similiar effects.
http://www.mediafire.com/download/22o6ahnchkbzhef/Shaders.rar
http://www.mediafire.com/download/86bo6bl66cnwv2j/chromaNEDI.rar
https://github.com/jpsdr/NNEDI3
http://forum.doom9.org/showthread.php?t=170727",
"It would work pretty well as a texture scaler too.
PPSSPP already does this.",
I think best option would be to use xBR in hardware mode and xBRZ can work in both software and hardware modes.,
"Instead of asking, try submitting a patch with your desired changes.",
Unfortunately I am not programmer and I am not skilled enough to implement it myself.,
"This is something i would like to see in the future, the way its implemented in PPSSPP is great. Although really not very necessary for PS2 emulation just a nice extra.",
It would be great for textures.,
"It wouldn't actually... since its design/function is for sprites, not textures.",
Its possible to use it for textures as well.,
PPSSPP is using xBRZ for textures and it looks very good.,
yes but one more option that the team don't wan't to do is mipmap at hardware mode in gsdx but i think the mipmap are better than xBRZ,
"Mipmapping would be useful but is pretty basic xBRZ may be better. Techniques which are clearly better are Tessellation, Displacement Mapping and Parallax Occlusion Mapping.",
"xBRZ makes textures and sprites ugly, I rather have Nearest.",Bitter Frustration
"xBR/xBRZ looks ugly only in your subjective opinion. Nearest-neighbor is primitive technique which looks blurry and pixelated,",Mocking
"xBR/xBRZ are ugly because computers are not artists, they ruin the artwork of games.",Bitter Frustration
"I am not sure what you mean but not everyone wants to use emulators with ugly native graphics.
PCSX2 already has option to increase internal resolution but xBR/xBRZ would be very useful for 2D sprites, pre-rendered backgrounds and textures.",
yes that's true ratchet and clank games have this problem and are very ugly only works in software mode and it's freaking me out with a amd fx 8350 Â¬Â¬ a points of fps of 20 or 30,Bitter Frustration
"You mean Native graphics, most of which created professionally by artist then you want to ruin it with an over-exaggerated interpolation.",Bitter Frustration
"Ratchet & Clank was my first PS2 game I ever owned, i have being waiting years for it to be fixed in PCSX2",Impatience
Developers wanted games to look like they look in high resolution but were limited by underpowered hardware.,
Developers certainly not want to apply a silly image interpolation like xBR to ruin all their artwork,Mocking
Prove they dont.,Impatience
"If you take a SNES Game like Super Mario World, the developers created all the artwork pixel by pixel.. xBR simply adds unwanted pixels everywhere, ruining the sharp-crisp sprites, it's a silly gimmicky interpolation .",Bitter Frustration
Snes games have huge blocky pixels because hardware was primitive and not because developers intended it to look that way. xBR is advanced upscaling algorithm not ''silly gimmicky interpolation'' show little respect to shader/filter developers.,Bitter Frustration
"xBR is silly gimmicky interpolation because computers are not artists, you need an Artist to recreate the sprites and textures.",Mocking
You are obsessed about computers not being artists but they dont need to be. Shader developers being artists is enough. Artists already created xBR/xBRZ.,
xBR destroys Pixel art,Mocking
Pixel art is term invented by nostalgia fetishists no such thing exists chunky pixels are result of low resolution forced by weak hardware.,Mocking
"You continue to prove that you have no idea what you're talking about Monochrome,
xBR is not a texture filter, it is a sprite filter.",Bitter Frustration
At least xBRZ can be used for textures. PPSSPP is using it for textures so it can be done.,
"I prefer Nearest over xBRZ, I don't like my games looking like playdoh",Bitter Frustration
Nearest-Neighbor is primitive garbage you may like what you want but I prefer games to not look like blurry blobs.,Bitter Frustration
"This would be an interesting enhancement, in certain instances it can help reduce scaling artifacts in some PSP games, so it could possibly help games like FFX.
https://i.imgur.com/as4jG1S.jpg
https://i.imgur.com/8p76qGp.jpg
Just a few things about this issue thread:
1. Ratchet and Clank HD collection uses a similar smoothing filter for textures:
https://imgur.com/a/Kc52r
2. Squall, you're - like always - wrong, it can be used to filter textures - or any pixel based medium.
Whether or not xBR texture filtering looks better is subjective, so there's no point arguing either way. Adding this would be nice for those who like it, it could help scaling artifacts, and implementing a way to add xBR filtering could mean other texture filters could be added too, so people could have a variety and go with what they like.",Mocking
"SonofUgly,
You're wrong for one key reason.
xBR and xBRZ are per pixel filters, they can only operate on the inside of sprite if the renderer operates AT the pixel level.
In the case of PS2 sprites, or textures, it will only filter the edges and no more.",
"What are you on about. Are you talking specifically about how GSdx currently handles textures, now? Your previous condescending remarks sounded like you didn't think xBRZ could be used on textures at all - which you do realize PPSSPP uses it, right?
""In the case of PS2 sprites, or textures, it will only filter the edges and no more.""
http://i.imgur.com/p6OEJSp.png",Bitter Frustration
"Arguing aside, PPSSPP has it i think it works well (that's IMO). Something to consider if there is enough of a demand for it. Also i believe PPSSPP De-posterizes the textures as well which helps with the colour banding.",
"Your argument about XBRZ being good or not is irrelevant. In the case of a github feature request, nobody cares about your points about XBRZ being wrong because it's not a forum discussion thread. The fact is that there is a huge amount of emulation scene fans that really enjoy XBRZ texture scalling... ...and the context ends here. With PPSSPP leading the way and lots of other emulators now following these steps (PS1, N64...) it's safe to say that everybody would turn to see if PCSX2 would follow on a such well recieved feature. It's not a leage of who likes it or not, making points etc, it's a polite-asking feature request and it's actually possible. Now it's up to somebody with the interest and skill to pick it up. If closed source plugins like Pete's OpenGL2 could be modified to achieve such effect then I bet PCSX2/GSDx can aswell. Let's just not argue if it's possible or not.",Impatience
nobody cares,Bitter Frustration
#800,
"@LasagnaPie But older games were designed to be displayed on CRTs, where the sprites aren't sharp or crisp. :^)",Irony
"It would be great for games like metal slug, megaman, etc.",
are we there yet ? xD,Impatience
"I would like to have a deeper knowledge to implement it, unfortunately not.
However I have an idea how this goes, bilinear filtering for pixel art just like Xbrz, so if it can be implemented, it goes right here. We need an expert developer on the subject.
![image](https://user-images.githubusercontent.com/46450049/97360816-afce4e80-186c-11eb-9d76-8f4c1bc0f4fc.png)",Bitter Frustration
"Perhaps someone on PPSSPP, it works great there.
Best link other issue too #3755.",Impatience
"@MRCHEESE97 https://github.com/stenzek/duckstation/commit/6f250a4ff7ffc06c68ed9d48ee7b97f925d354d8
https://github.com/flyinghead/flycast/pull/57/files
https://github.com/stenzek/duckstation/commit/62892b02d11893bc8079523da079ac04960f79f6
https://github.com/stenzek/duckstation/commit/5635ee1d7c521e1a1cd590bf804287d30c901948
https://github.com/stenzek/duckstation/commit/ae1e4b1b8fd52690ecda600232141e6ed1df1d4d",Impatience
"Just wanted to say that I am very much looking forward to this feature being implemented and not having to choose between blocky or blurry 2D assets.
Also for those that argue that ""developers did not intended for the graphics to look that way"" well, they did not intend for their ps2 games to be played on a PC either, but here we are...",Irony
then I would like to add bicubic,
"the reason why xbrz is highly desired is because it looks great and is the
closest thing to high rez textures. it has to be internally coded for it to
work properly. all the other shaders can be acquired thru 3rd party means.
xbrz is technically a scaler.
On Tue, Feb 16, 2021, 12:34 PM Papermanzero <notifications@github.com>
wrote:
> then I would like to add bicubic
>
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/PCSX2/pcsx2/issues/10#issuecomment-779999104>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AA6LIM2IELRTWRRL2CMK2PTS7KUBPANCNFSM4ANWZOUQ>
> .
>",
"![2021-04-06_17h16_08](https://user-images.githubusercontent.com/42075250/113734722-dafe7900-96fb-11eb-91a4-aeb572ca14c1.png)
Which is better ? Xbr or nearest neighbor ?",
biliear and/or bicubic ðŸ˜…,Mocking
"> ![2021-04-06_17h16_08](https://user-images.githubusercontent.com/42075250/113734722-dafe7900-96fb-11eb-91a4-aeb572ca14c1.png)
> Which is better ? Xbr or nearest neighbor ?
ðŸ˜»ðŸ˜»ðŸ˜»â¤â¤â¤",Mocking
"> xbr or nearest neighbor ?
nearest neighbor. your mistaking the smooth look of xbr with an HD upscale. what it is, is a smoothed over approximation of nearest neighbor. i don't think any game studio would have intentionally made a game that looks like the xbr. it some how looks blockier and chunkier than nearest neighbor.",Bitter Frustration
"Xbrz forever, I really love this filter in all emulators, and for those who don't want this feature implemented, nobody is forcing you to enable this option in case someone implements the filter on the pcsx2.",
"> > xbr or nearest neighbor ?
> > nearest neighbor. your mistaking the smooth look of xbr with an HD upscale. what it is, is a smoothed over approximation of nearest neighbor. i don't think any game studio would have intentionally made a game that looks like the xbr. it some how looks blockier and chunkier than nearest neighbor.
It literally does not matter what game studios would or would not have their games look like.
xBR is not perfect, but it is the closest we are going to get in real time to high resolution 2D assests (short of texture replacement, wich not all games will have; and it fills disc space, wich might not be ideal to some people, depending on the amout of games they own).
If you prefer nearest neighbor, nice, luckily that option is already available to you.
This is not a discussion of wich is better (wich is highly subjective); this is about having the option to use a feature that is already available in other emulators, that a number of people enjoy.",Impatience
"Locking as the discussion here hasn't been very productive.
If you're interested in implementing these shaders please open a pull request.
If you're wondering about the status of this feature request, check if there is a linked pull request.",
"Closing as low priority, and what fobes said.",Mocking
"```
SPL: Loaded module v0.6.3-1
ZFS: Loaded module v0.6.3-1, ZFS pool version 5000, ZFS filesystem version 5
```
doing nothing but some basic rsync's with moderate sizes 4-10G result always in having approx 1MB/sec throughtput (very slow) on a up-date 16G RAM HP server - CentOS 6 with OpenVZ and selfcompiled modules for ZFS - dmesg:
```
INFO: task rsync:6517 blocked for more than 120 seconds.
Tainted: P --------------- 2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
rsync D ffff8804029bafb0 0 6517 6516 0 0x00000080
ffff88001e55da18 0000000000000086 0000000000000003 00000003d2dad320
000000000001ec80 0000000000000001 ffff88001e55dab8 0000000000000082
ffffc900105a3428 ffff8803fe837d60 ffff8804029bb578 000000000001ec80
Call Trace:
[<ffffffff810a1dfe>] ? prepare_to_wait_exclusive+0x4e/0x80
[<ffffffffa019fb35>] cv_wait_common+0x105/0x1c0 [spl]
[<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
[<ffffffffa019fc45>] __cv_wait+0x15/0x20 [spl]
[<ffffffffa02ae1fb>] txg_wait_open+0x8b/0x110 [zfs]
[<ffffffffa027194e>] dmu_tx_wait+0x29e/0x2b0 [zfs]
[<ffffffff81530bfe>] ? mutex_lock+0x1e/0x50
[<ffffffffa0271a41>] dmu_tx_assign+0x91/0x490 [zfs]
[<ffffffffa027fab7>] ? dsl_dataset_block_freeable+0x27/0x60 [zfs]
[<ffffffffa02e8d3e>] zfs_write+0x43e/0xcf0 [zfs]
[<ffffffff8100bc4e>] ? apic_timer_interrupt+0xe/0x20
[<ffffffff811c5e5c>] ? core_sys_select+0x1ec/0x2d0
[<ffffffffa02fd354>] zpl_write_common+0x54/0xd0 [zfs]
[<ffffffffa02fd438>] zpl_write+0x68/0xa0 [zfs]
[<ffffffff811ac798>] vfs_write+0xb8/0x1a0
[<ffffffff811ad091>] sys_write+0x51/0x90
[<ffffffff810f4dee>] ? __audit_syscall_exit+0x25e/0x290
[<ffffffff8100b102>] system_call_fastpath+0x16/0x1b
INFO: task txg_sync:876 blocked for more than 120 seconds.
Tainted: P --------------- 2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
txg_sync D ffff880405d28640 0 876 2 0 0x00000000
ffff8803fe839b70 0000000000000046 0000000000000001 ffff880405101430
0000000000000000 0000000000000000 ffff8803fe839af0 ffffffff81060032
ffff8803fe839b40 ffffffff81054939 ffff880405d28c08 000000000001ec80
Call Trace:
[<ffffffff81060032>] ? default_wake_function+0x12/0x20
[<ffffffff81054939>] ? __wake_up_common+0x59/0x90
[<ffffffff8152f833>] io_schedule+0x73/0xc0
[<ffffffffa019fadc>] cv_wait_common+0xac/0x1c0 [spl]
[<ffffffffa02f53e0>] ? zio_execute+0x0/0x140 [zfs]
[<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
[<ffffffffa019fc08>] __cv_wait_io+0x18/0x20 [spl]
[<ffffffffa02f561b>] zio_wait+0xfb/0x1b0 [zfs]
[<ffffffffa02867e3>] dsl_pool_sync+0xb3/0x440 [zfs]
[<ffffffffa029a67b>] spa_sync+0x40b/0xae0 [zfs]
[<ffffffffa02aebb4>] txg_sync_thread+0x384/0x5e0 [zfs]
[<ffffffff8105b309>] ? set_user_nice+0xc9/0x130
[<ffffffffa02ae830>] ? txg_sync_thread+0x0/0x5e0 [zfs]
[<ffffffffa01978e8>] thread_generic_wrapper+0x68/0x80 [spl]
[<ffffffffa0197880>] ? thread_generic_wrapper+0x0/0x80 [spl]
[<ffffffff810a1596>] kthread+0x96/0xa0
[<ffffffff8100c34a>] child_rip+0xa/0x20
[<ffffffff810a1500>] ? kthread+0x0/0xa0
[<ffffffff8100c340>] ? child_rip+0x0/0x20
```",Bitter Frustration
"I hit something similar but maybe different last night. I believe it occurred while KVM was copying disk blocks from another server to this one. This ended up causing actual corruption on at least one of the zvols (as seen by the VM). ```
[381360.908047] INFO: task zvol/0:331 blocked for more than 120 seconds.
[381360.908147] Tainted: PF O 3.13.0-32-generic #57-Ubuntu
[381360.908240] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[381360.908352] zvol/0 D ffff88062fc54440 0 331 2 0x00000000
[381360.908357] ffff88060796dbe0 0000000000000002 ffff880609aa17f0 ffff88060796dfd8
[381360.908360] 0000000000014440 0000000000014440 ffff880609aa17f0 ffff880609a5eb30
[381360.908362] ffff880609a5e9f8 ffff880609a5eb38 0000000000000000 0000000000000002
[381360.908365] Call Trace:
[381360.908374] [<ffffffff817200d9>] schedule+0x29/0x70
[381360.908396] [<ffffffffa00c0485>] cv_wait_common+0x105/0x1a0 [spl]
[381360.908402] [<ffffffff810aaf00>] ? prepare_to_wait_event+0x100/0x100
[381360.908408] [<ffffffffa00c0535>] __cv_wait+0x15/0x20 [spl]
[381360.908459] [<ffffffffa0209abb>] txg_wait_open+0x8b/0x110 [zfs]
[381360.908476] [<ffffffffa01cd83b>] dmu_tx_wait+0x29b/0x2a0 [zfs]
[381360.908492] [<ffffffffa01cd8cc>] dmu_tx_assign+0x8c/0x460 [zfs]
[381360.908520] [<ffffffffa025a8c7>] zvol_write+0xa7/0x480 [zfs]
[381360.908527] [<ffffffffa00bab27>] taskq_thread+0x237/0x4b0 [spl]
[381360.908530] [<ffffffff81097508>] ? finish_task_switch+0x128/0x170
[381360.908534] [<ffffffff8109a800>] ? wake_up_state+0x20/0x20
[381360.908539] [<ffffffffa00ba8f0>] ? taskq_cancel_id+0x1f0/0x1f0 [spl]
[381360.908543] [<ffffffff8108b3d2>] kthread+0xd2/0xf0
[381360.908545] [<ffffffff8108b300>] ? kthread_create_on_node+0x1d0/0x1d0
[381360.908548] [<ffffffff8172c5bc>] ret_from_fork+0x7c/0xb0
[381360.908550] [<ffffffff8108b300>] ? kthread_create_on_node+0x1d0/0x1d0
```
This was repeated for `zvol/0` through `zvol/9`. There are 31 zvols on this system.
```
[ 4.995804] SPL: Loaded module v0.6.3-1~precise
[ 5.130074] ZFS: Loaded module v0.6.3-2~precise, ZFS pool version 5000, ZFS filesystem version 5
```
I tried migrating a VM again today and all hell broke loose but I did not get these errors. The system load was in the 100's on an 8 core system. Major I/O wait time. I killed the migration but ended up having at least 3 corrupt zvols anyways.",Bitter Frustration
"i do not have any corruption or problem - the system is stable and running - also the rsync tasks are done 100% perfect - but very slow 1M/second write performance - and while running the whole system is not responding fast - but no errors and no corruption - these ""blocked for more than 120 seconds"" dmesg's come in pairs for rsync and txg_sync once a day",Bitter Frustration
"not similar but related message:
Aug 29 05:37:06 morpheus kernel: [46185.239554] ata6.00: configured for UDMA/133
Aug 29 05:37:06 morpheus kernel: [46185.239562] ata6: EH complete
Aug 29 05:53:40 morpheus kernel: [47179.890587] INFO: task txg_sync:1462 blocked for more than 180 seconds.
Aug 29 05:53:40 morpheus kernel: [47179.890589] Tainted: P O 3.16.0_ck1-smtnice6_BFQ_integra_intel #1
Aug 29 05:53:40 morpheus kernel: [47179.890590] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Aug 29 05:53:40 morpheus kernel: [47179.890591] txg_sync D 0000000000000006 0 1462 2 0x00000000
Aug 29 05:53:40 morpheus kernel: [47179.890594] ffff8805fbda9e40 0000000000000046 ffff88020931a9b0 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890596] 0000000000000066 ffff8807fb2a0f20 000000000000b020 0000000000013100
Aug 29 05:53:40 morpheus kernel: [47179.890597] ffff88065b6103a0 00002992b81d47dd ffff880617373fd8 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890599] Call Trace:
Aug 29 05:53:40 morpheus kernel: [47179.890605] [<ffffffff82a3eaa8>] ? io_schedule+0x88/0xd0
Aug 29 05:53:40 morpheus kernel: [47179.890613] [<ffffffffc028b536>] ? __cv_timedwait+0x96/0x110 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890616] [<ffffffff820fa7b0>] ? finish_wait+0x90/0x90
Aug 29 05:53:40 morpheus kernel: [47179.890623] [<ffffffffc03c38cb>] ? zio_wait+0xeb/0x1a0 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890631] [<ffffffffc03553da>] ? dsl_pool_sync+0xaa/0x450 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890639] [<ffffffffc036d4c3>] ? spa_sync+0x483/0xb20 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890642] [<ffffffff820f7c3d>] ? default_wake_function+0xd/0x20
Aug 29 05:53:40 morpheus kernel: [47179.890644] [<ffffffff821168bd>] ? ktime_get_ts+0x3d/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890652] [<ffffffffc037ce8a>] ? txg_sync_start+0x6ea/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890655] [<ffffffff820013ca>] ? __switch_to+0x2a/0x560
Aug 29 05:53:40 morpheus kernel: [47179.890662] [<ffffffffc037cb60>] ? txg_sync_start+0x3c0/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890664] [<ffffffffc0286f75>] ? spl_kmem_fini+0xa5/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890667] [<ffffffffc0286f00>] ? spl_kmem_fini+0x30/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890669] [<ffffffff820ea5dc>] ? kthread+0xbc/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890670] [<ffffffff82a40000>] ? __ww_mutex_lock_slowpath+0x8c/0x2cc
Aug 29 05:53:40 morpheus kernel: [47179.890672] [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 05:53:40 morpheus kernel: [47179.890674] [<ffffffff82a4242c>] ? ret_from_fork+0x7c/0xb0
Aug 29 05:53:40 morpheus kernel: [47179.890675] [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 06:28:01 morpheus kernel: [49241.775276] usb 1-1.1: USB disconnect, device number 3
this seems to occur from time to time with a rather slow USB3.0 powered 4TB hdd (Touro Desk 3.0, HGST5K4000) in an external case during rsync & transferring of large files (several GiB)",
"clarification for above:
I had this happen with the above mentioned drive - of which I still don't know what causes it (but I suspect it could be related either to the chipset in the external harddrive enclosure where the drive sits in or powersaving features of the XHCI driver & hardware, which I already had issues with in the past)
another drive showed this behavior (a seagate ST3000DM001) which likely underwent a headcrash and did reallocate several sectors (<10). It had been placed in an external enclosure by fantec [db-f8u3e with an incompatible chipset against smartctl] that had shown in the past to have a life & mind of its own: it would occasionally turn off during transfers and causing trouble with other filesystems, on ZFS, however, the files so far seemed fine. The day before yesterday I placed the drive in another external enclosure and it worked well during backups (only transferring several hundreds of MiB of data per backup job incrementally via rsync) until I decided to run a scrub and check everything: after several hours the drive again screamed and made hearable noises of a head-crash and/or sector re-allocation (had those in the past) and access wasn't possible anymore to the drive
that's where the above posted message occured again
so at least in the second case (ST3000DM001) ZFS seemingly showed indirectly that something was wrong with the hardware/harddrive
so when encountering this message - make sure to double- or triple-check that it's not a hardware-issue instead a ""software""- (ZFS-related) problem",Bitter Frustration
@wankdanker I think that your issue is separate. It might have been caused by the zvol processing occuring inside an interrupt context. Pull request #2484 might resolve it.,
"@freakout42 Would you tell us more about your pool configuration? Also, do you have data deduplication enabled on this pool?",
"```
[root@blood ~]# zpool status
pool: tank
state: ONLINE
scan: scrub repaired 0 in 0h53m with 0 errors on Tue Sep 9 12:45:13 2014
config:
NAME STATE READ WRITE CKSUM
tank ONLINE 0 0 0
mirror-0 ONLINE 0 0 0
sda4 ONLINE 0 0 0
sdb4 ONLINE 0 0 0
errors: No known data errors
[root@blood ~]# zpool get all
NAME PROPERTY VALUE SOURCE
tank size 824G -
tank capacity 34% -
tank altroot - default
tank health ONLINE -
tank guid 3198719639486948540 default
tank version - default
tank bootfs - default
tank delegation on default
tank autoreplace off default
tank cachefile - default
tank failmode wait default
tank listsnapshots off default
tank autoexpand off default
tank dedupditto 0 default
tank dedupratio 1.00x -
tank free 538G -
tank allocated 286G -
tank readonly off -
tank ashift 0 default
tank comment - default
tank expandsize 0 -
tank freeing 0 default
tank feature@async_destroy enabled local
tank feature@empty_bpobj active local
tank feature@lz4_compress enabled local
```",
"Had similar failures, which occurred during heavy rsync pulls from remote machine. I was able to make it happen very quickly by starting up the remote pull. Did this three times in a row and caused the fault every time. The symptom was that any userland zfs/zpool commands hang, but the machine was still responsive to other commands. I set the parameter spl_kmem_cache_slab_limit=0 (it had been spl_kmem_cache_slab_limit=16384), and the problem seems to be gone, or not easliy triggered.
Part of the process which triggers this includes snapshot renaming, **but no zvols** are involved in this process, although the pool has some.
The pool is a raidz1 pool, and there are no hardware issues on the server.
```
Sep 23 16:17:51 zephyr kernel: [ 27.497382] SPL: Loaded module v0.6.3-1~precise
Sep 23 16:17:51 zephyr kernel: [ 27.515752] ZFS: Loaded module v0.6.3-2~precise, ZFS pool version 5000, ZFS filesystem version 5
Sep 26 00:11:23 zephyr kernel: [200583.071397] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Sep 26 00:11:23 zephyr kernel: [200583.093284] txg_sync D ffffffff8180cbe0 0 4178 2 0x00000000
Sep 26 00:11:23 zephyr kernel: [200583.093291] ffff8807d9fd1aa0 0000000000000046 00000000000000ff ffffffffa026c280
Sep 26 00:11:23 zephyr kernel: [200583.093297] ffff8807d9fd1fd8 ffff8807d9fd1fd8 ffff8807d9fd1fd8 00000000000139c0
Sep 26 00:11:23 zephyr kernel: [200583.093303] ffff8807f3d6ae20 ffff8807e0189710 00000000ffffffff ffffffffa026c280
Sep 26 00:11:23 zephyr kernel: [200583.093309] Call Trace:
Sep 26 00:11:23 zephyr kernel: [200583.093333] [<ffffffff8169f099>] schedule+0x29/0x70
Sep 26 00:11:23 zephyr kernel: [200583.093338] [<ffffffff8169f35e>] schedule_preempt_disabled+0xe/0x10
Sep 26 00:11:23 zephyr kernel: [200583.093343] [<ffffffff8169df77>] __mutex_lock_slowpath+0xd7/0x150
Sep 26 00:11:23 zephyr kernel: [200583.093350] [<ffffffff8169db8a>] mutex_lock+0x2a/0x50
Sep 26 00:11:23 zephyr kernel: [200583.093395] [<ffffffffa0230f19>] zvol_rename_minors+0x79/0x180 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093432] [<ffffffffa01ad349>] dsl_dataset_rename_snapshot_sync_impl+0x189/0x2c0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093460] [<ffffffffa01ad52f>] dsl_dataset_rename_snapshot_sync+0xaf/0x190 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093491] [<ffffffffa01bde52>] dsl_sync_task_sync+0xf2/0x100 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093521] [<ffffffffa01b63b3>] dsl_pool_sync+0x2f3/0x420 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093554] [<ffffffffa01cc4f4>] spa_sync+0x414/0xb20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093590] [<ffffffffa01db021>] ? spa_txg_history_set+0x21/0xd0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093625] [<ffffffffa01de335>] txg_sync_thread+0x385/0x5d0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093659] [<ffffffffa01ddfb0>] ? txg_init+0x260/0x260 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093672] [<ffffffffa00d3fc8>] thread_generic_wrapper+0x78/0x90 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.093682] [<ffffffffa00d3f50>] ? __thread_create+0x300/0x300 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.093688] [<ffffffff81077ee3>] kthread+0x93/0xa0
Sep 26 00:11:23 zephyr kernel: [200583.093694] [<ffffffff816a9b24>] kernel_thread_helper+0x4/0x10
Sep 26 00:11:23 zephyr kernel: [200583.093700] [<ffffffff81077e50>] ? flush_kthread_worker+0xb0/0xb0
Sep 26 00:11:23 zephyr kernel: [200583.093704] [<ffffffff816a9b20>] ? gs_change+0x13/0x13
Sep 26 00:11:23 zephyr kernel: [200583.093737] INFO: task zfs:22581 blocked for more than 120 seconds.
Sep 26 00:11:23 zephyr kernel: [200583.104867] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Sep 26 00:11:23 zephyr kernel: [200583.126712] zfs D ffffffff8180cbe0 0 22581 22253 0x00000000
Sep 26 00:11:23 zephyr kernel: [200583.126717] ffff880182f2f958 0000000000000082 ffff8807eee9c530 ffff8807deda2550
Sep 26 00:11:23 zephyr kernel: [200583.126722] ffff880182f2ffd8 ffff880182f2ffd8 ffff880182f2ffd8 00000000000139c0
Sep 26 00:11:23 zephyr kernel: [200583.126728] ffff8807f3f04530 ffff8807eee9c530 0000000000000292 ffff8807deda2550
Sep 26 00:11:23 zephyr kernel: [200583.126733] Call Trace:
Sep 26 00:11:23 zephyr kernel: [200583.126740] [<ffffffff8169f099>] schedule+0x29/0x70
Sep 26 00:11:23 zephyr kernel: [200583.126753] [<ffffffffa00dbd8d>] cv_wait_common+0xfd/0x1b0 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.126768] [<ffffffff810787c0>] ? add_wait_queue+0x60/0x60
Sep 26 00:11:23 zephyr kernel: [200583.126779] [<ffffffffa00dbe95>] __cv_wait+0x15/0x20 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.126813] [<ffffffffa01c4aec>] rrw_enter_read+0x3c/0x130 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126849] [<ffffffffa01c4c70>] rrw_enter+0x20/0x30 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126878] [<ffffffffa01b51cd>] dsl_pool_config_enter+0x1d/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126915] [<ffffffffa01b709a>] dsl_pool_hold+0x4a/0x60 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126927] [<ffffffffa019957b>] dmu_objset_hold+0x2b/0xb0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126929] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.126942] [<ffffffffa01b8a6f>] dsl_prop_get+0x3f/0x90 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126944] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.126956] [<ffffffffa01b8ade>] dsl_prop_get_integer+0x1e/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126970] [<ffffffffa022ef6f>] __zvol_create_minor+0xbf/0x630 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126985] [<ffffffffa0230bc7>] zvol_create_minor+0x27/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126998] [<ffffffffa0230bee>] zvol_create_minors_cb+0xe/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127010] [<ffffffffa0197e6e>] dmu_objset_find_impl+0x37e/0x3f0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127023] [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127034] [<ffffffffa0197cae>] dmu_objset_find_impl+0x1be/0x3f0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127048] [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127061] [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127072] [<ffffffffa0197f32>] dmu_objset_find+0x52/0x80 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127074] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.127088] [<ffffffffa0230d53>] zvol_create_minors+0x33/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127102] [<ffffffffa0202a69>] zfs_ioc_snapshot+0x259/0x2a0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127116] [<ffffffffa0206530>] zfsdev_ioctl+0x180/0x500 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127118] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.127131] [<ffffffff8119ae9a>] do_vfs_ioctl+0x8a/0x340
Sep 26 00:11:23 zephyr kernel: [200583.127135] [<ffffffff8119b1e1>] sys_ioctl+0x91/0xa0
Sep 26 00:11:23 zephyr kernel: [200583.127141] [<ffffffff816a8829>] system_call_fastpath+0x16/0x1b
```",
"@ColdCanuck Your comments regarding `spl_kmem_cache_slab_limit` piqued my interest in this issue which until now, I've not had time to follow. In fact, a quick skim of this whole issue makes me wonder how related each of the problem reports are and whether there's too much issue creep.
Back to the point at hand: I'm posting this followup because there have been a disturbing number of seemingly otherwise unrelated problems sporadically seemingly caused by using the Linux slab. Although I've not been able to spend the time on it I've wanted to, I've been rather knee deep investigating the series of issues related to Posix ACLs and SA xattrs and have seen at least one report (#2701) and, more interestingly #2725 which makes me think there may be a tie-in to our use of the Linux slab for <= 16KiB objects. I don't have any other brilliant observations offer at the moment other than to raise concern there may be problems realted to using the Linux slab and to ask @behlendorf, @ryao et al. what your thoughts are on this (particularly given the last few comments in #2725).",
"just posting what comes to mind:
could scheduling a regular cronjob which compacts memory via
echo 1 > /proc/sys/vm/compact_memory
change things (provided slab issues and timeouts are related to memory fragmentation)",
"I'm systematically having this issue when trying to RSync when using the latest ZFS from Arch: zfs-git 0.6.3_r170_gd958324f_3.18.2_2-1
```
[46181.967521] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
[79468.027144] INFO: task txg_sync:583 blocked for more than 120 seconds.
[79468.027287] Tainted: P O 3.18.2-2-ARCH #1
[79468.027363] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[79468.027468] txg_sync D 0000000000000000 0 583 2 0x00000000
[79468.027476] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[79468.027483] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[79468.027489] ffff8801591955b8 ffffffff00000000 ffff880159195570 00000000025e92cf
[79468.027494] Call Trace:
[79468.027509] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[79468.027518] [<ffffffff81550b59>] schedule+0x29/0x70
[79468.027524] [<ffffffff81550e38>] io_schedule+0x98/0x100
[79468.027547] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[79468.027554] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[79468.027565] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[79468.027588] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[79468.027616] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[79468.027625] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[79468.027656] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[79468.027663] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[79468.027694] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[79468.027724] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[79468.027734] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[79468.027742] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[79468.027749] [<ffffffff81090e0a>] kthread+0xea/0x100
[79468.027755] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[79468.027761] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[79468.027767] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120097] INFO: task txg_sync:583 blocked for more than 120 seconds.
[82589.120252] Tainted: P O 3.18.2-2-ARCH #1
[82589.120347] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[82589.120479] txg_sync D 0000000000000001 0 583 2 0x00000000
[82589.120489] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[82589.120497] ffff88021ce03fd8 0000000000013640 ffff8802240ceeb0 ffff88021cd5eeb0
[82589.120505] ffff88022363f7b0 ffff88022363f798 0000000000000000 0000000000000003
[82589.120512] Call Trace:
[82589.120529] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[82589.120540] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[82589.120549] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[82589.120558] [<ffffffff81550b59>] schedule+0x29/0x70
[82589.120564] [<ffffffff81550e38>] io_schedule+0x98/0x100
[82589.120591] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[82589.120599] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[82589.120613] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[82589.120641] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[82589.120676] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[82589.120689] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[82589.120727] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[82589.120735] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[82589.120774] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[82589.120811] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[82589.120823] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[82589.120834] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[82589.120842] [<ffffffff81090e0a>] kthread+0xea/0x100
[82589.120849] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120858] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[82589.120864] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.468460] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89311.468644] Tainted: P O 3.18.2-2-ARCH #1
[89311.468741] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89311.468872] txg_sync D 0000000000000000 0 583 2 0x00000000
[89311.468882] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89311.468890] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89311.468897] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89311.468905] Call Trace:
[89311.468922] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89311.468933] [<ffffffff81550b59>] schedule+0x29/0x70
[89311.468940] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89311.468968] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89311.468976] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89311.468989] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89311.469017] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89311.469052] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89311.469065] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89311.469103] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89311.469112] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89311.469150] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89311.469187] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89311.469199] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89311.469211] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89311.469218] [<ffffffff81090e0a>] kthread+0xea/0x100
[89311.469226] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.469234] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89311.469241] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
```
```
hinoki% free -h
total used free shared buff/cache available
Mem: 7.8G 4.3G 533M 1.1M 3.0G 702M
Swap: 4.0G 0B 4.0G
```",
"Still trying to run rsync:
```
[89431.510429] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89431.510571] Tainted: P O 3.18.2-2-ARCH #1
[89431.510647] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89431.510753] txg_sync D 0000000000000000 0 583 2 0x00000000
[89431.510762] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89431.510769] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89431.510774] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89431.510780] Call Trace:
[89431.510796] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89431.510805] [<ffffffff81550b59>] schedule+0x29/0x70
[89431.510811] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89431.510836] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89431.510842] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89431.510853] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89431.510877] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89431.510906] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89431.510915] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89431.510946] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89431.510953] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89431.510984] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89431.511014] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89431.511024] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89431.511033] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89431.511039] [<ffffffff81090e0a>] kthread+0xea/0x100
[89431.511045] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89431.511052] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89431.511058] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.552404] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89551.552565] Tainted: P O 3.18.2-2-ARCH #1
[89551.552660] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89551.552792] txg_sync D 0000000000000000 0 583 2 0x00000000
[89551.552803] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89551.552812] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89551.552819] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89551.552826] Call Trace:
[89551.552844] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89551.552854] [<ffffffff81550b59>] schedule+0x29/0x70
[89551.552862] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89551.552890] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89551.552898] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89551.552911] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89551.552940] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89551.552975] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89551.552987] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89551.553025] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89551.553034] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89551.553073] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89551.553110] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89551.553122] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89551.553133] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89551.553140] [<ffffffff81090e0a>] kthread+0xea/0x100
[89551.553148] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.553156] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89551.553163] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.594371] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89671.594507] Tainted: P O 3.18.2-2-ARCH #1
[89671.594605] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89671.594711] txg_sync D 0000000000000000 0 583 2 0x00000000
[89671.594720] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89671.594727] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89671.594733] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89671.594738] Call Trace:
[89671.594753] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89671.594762] [<ffffffff81550b59>] schedule+0x29/0x70
[89671.594768] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89671.594792] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89671.594798] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89671.594809] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89671.594832] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89671.594860] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89671.594870] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89671.594901] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89671.594908] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89671.594939] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89671.594969] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89671.594979] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89671.594988] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89671.594994] [<ffffffff81090e0a>] kthread+0xea/0x100
[89671.595000] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.595007] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89671.595013] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636364] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89791.636507] Tainted: P O 3.18.2-2-ARCH #1
[89791.636583] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89791.636688] txg_sync D 0000000000000000 0 583 2 0x00000000
[89791.636697] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89791.636703] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89791.636709] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89791.636715] Call Trace:
[89791.636729] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89791.636738] [<ffffffff81550b59>] schedule+0x29/0x70
[89791.636744] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89791.636767] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89791.636774] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89791.636785] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89791.636808] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89791.636836] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89791.636852] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89791.636883] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89791.636890] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89791.636921] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89791.636951] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89791.636960] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89791.636969] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89791.636975] [<ffffffff81090e0a>] kthread+0xea/0x100
[89791.636981] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636989] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89791.636994] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90105.367103] systemd[1]: systemd-journald.service stop-sigabrt timed out. Terminating.
[90105.490377] systemd[1]: Listening on Journal Audit Socket.
[90105.490436] systemd[1]: Starting Journal Service...
[90151.762415] INFO: task kswapd0:31 blocked for more than 120 seconds.
[90151.762569] Tainted: P O 3.18.2-2-ARCH #1
[90151.762663] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.762795] kswapd0 D 0000000000000000 0 31 2 0x00000000
[90151.762805] ffff8802235a3908 0000000000000046 ffff880224225a90 0000000000013640
[90151.762814] ffff8802235a3fd8 0000000000013640 ffffffff81818540 ffff880224225a90
[90151.762821] ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.762828] Call Trace:
[90151.762845] [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.762856] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.762865] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.762893] [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.762902] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.762915] [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.762956] [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.762984] [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763011] [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763040] [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763049] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763075] [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763083] [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763090] [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763097] [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763106] [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763115] [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763123] [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763131] [<ffffffff8116e0e9>] kswapd_shrink_zone+0x129/0x1d0
[90151.763138] [<ffffffff8116eb7a>] kswapd+0x54a/0x8f0
[90151.763147] [<ffffffff8116e630>] ? mem_cgroup_shrink_node_zone+0x1c0/0x1c0
[90151.763155] [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.763162] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763171] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.763178] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763190] INFO: task systemd-journal:138 blocked for more than 120 seconds.
[90151.763340] Tainted: P O 3.18.2-2-ARCH #1
[90151.763433] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.763563] systemd-journal D 0000000000000000 0 138 1 0x00000004
[90151.763571] ffff88022310b128 0000000000000082 ffff880222c25080 0000000000013640
[90151.763577] ffff88022310bfd8 0000000000013640 ffff88021cd5e4a0 ffff880222c25080
[90151.763583] ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.763590] Call Trace:
[90151.763599] [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.763607] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.763614] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.763627] [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.763635] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.763647] [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.763683] [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.763710] [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763737] [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763765] [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763774] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763799] [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763805] [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763812] [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763819] [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763827] [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763834] [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763842] [<ffffffff811c1523>] ? mem_cgroup_iter+0x2f3/0x4d0
[90151.763850] [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763858] [<ffffffff8116da45>] do_try_to_free_pages+0x365/0x4e0
[90151.763866] [<ffffffff8116dc71>] try_to_free_pages+0xb1/0x1a0
[90151.763873] [<ffffffff81160ca7>] __alloc_pages_nodemask+0x697/0xb50
[90151.763884] [<ffffffff811a6e9c>] alloc_pages_current+0x9c/0x120
[90151.763891] [<ffffffff811afba5>] new_slab+0x305/0x370
[90151.763899] [<ffffffff811b2175>] __slab_alloc.isra.51+0x545/0x650
[90151.763907] [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763914] [<ffffffff814380f4>] ? raw_pci_write+0x24/0x50
[90151.763923] [<ffffffff812e8be6>] ? pci_bus_write_config_word+0x66/0x80
[90151.763949] [<ffffffffa004ed7f>] ? ata_bmdma_start+0x2f/0x40 [libata]
[90151.763960] [<ffffffffa01121ad>] ? atiixp_bmdma_start+0x9d/0xe0 [pata_atiixp]
[90151.763969] [<ffffffff811b5445>] __kmalloc_node_track_caller+0xa5/0x240
[90151.763976] [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763984] [<ffffffff81445a11>] __kmalloc_reserve.isra.38+0x31/0x90
[90151.763990] [<ffffffff81445acb>] ? __alloc_skb+0x5b/0x210
[90151.763997] [<ffffffff81445af9>] __alloc_skb+0x89/0x210
[90151.764004] [<ffffffff81445de4>] alloc_skb_with_frags+0x64/0x1e0
[90151.764011] [<ffffffff8143efb9>] sock_alloc_send_pskb+0x219/0x290
[90151.764020] [<ffffffff810136fb>] ? __switch_to+0x1fb/0x600
[90151.764029] [<ffffffff814f886d>] unix_dgram_sendmsg+0x18d/0x690
[90151.764037] [<ffffffff8143ac39>] sock_sendmsg+0x79/0xb0
[90151.764045] [<ffffffff810986da>] ? finish_task_switch+0x4a/0xf0
[90151.764051] [<ffffffff815504c8>] ? __schedule+0x3e8/0xa50
[90151.764059] [<ffffffff8143c76c>] ? move_addr_to_kernel+0x2c/0x50
[90151.764066] [<ffffffff8144a3c7>] ? verify_iovec+0x47/0xd0
[90151.764074] [<ffffffff8143b928>] ___sys_sendmsg+0x408/0x420
[90151.764083] [<ffffffff812149d0>] ? ep_read_events_proc+0xe0/0xe0
[90151.764089] [<ffffffff81440030>] ? sk_prot_alloc.isra.33+0x30/0x130
[90151.764097] [<ffffffff811b291a>] ? kmem_cache_alloc+0x16a/0x170
[90151.764104] [<ffffffff811d100c>] ? get_empty_filp+0x5c/0x1c0
[90151.764112] [<ffffffff8126e496>] ? security_file_alloc+0x16/0x20
[90151.764118] [<ffffffff811d1084>] ? get_empty_filp+0xd4/0x1c0
[90151.764126] [<ffffffff811d118f>] ? alloc_file+0x1f/0xb0
[90151.764134] [<ffffffff8143d651>] __sys_sendmsg+0x51/0x90
[90151.764142] [<ffffffff8143d6a2>] SyS_sendmsg+0x12/0x20
[90151.764149] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[90151.764186] INFO: task txg_sync:583 blocked for more than 120 seconds.
[90151.764325] Tainted: P O 3.18.2-2-ARCH #1
[90151.764418] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.764547] txg_sync D 0000000000000000 0 583 2 0x00000000
[90151.764554] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[90151.764560] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[90151.764566] ffff88018de384b8 ffffffff00000000 ffff88018de38490 00000000025e92cf
[90151.764572] Call Trace:
[90151.764581] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[90151.764588] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.764594] [<ffffffff81550e38>] io_schedule+0x98/0x100
[90151.764607] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[90151.764615] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.764627] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[90151.764651] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[90151.764686] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[90151.764698] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[90151.764735] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[90151.764744] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[90151.764781] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[90151.764818] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[90151.764830] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[90151.764841] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[90151.764847] [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.764855] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.764862] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.764869] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90195.648689] systemd[1]: systemd-journald.service stop-sigterm timed out. Killing.
[90195.649942] systemd[1]: Starting Journal Service...
```",
"This might be caused by #2523, can you verify you have the fix to the SPL applied: zfsonlinux/spl@a3c1eb77721a0d511b4fe7111bb2314686570c4b",
"More of the same
```
[ 1320.575152] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1320.575263] Tainted: P O 3.18.2-2-ARCH #1
[ 1320.575314] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1320.575384] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1320.575390] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1320.575395] ffff88021ba43fd8 0000000000013640 ffff88021dee2840 ffff8800cbfc4670
[ 1320.575399] ffff880131dcc138 ffffffff00000000 ffff880131dcc130 00000000b3f1aac7
[ 1320.575402] Call Trace:
[ 1320.575414] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1320.575420] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1320.575424] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1320.575442] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1320.575447] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1320.575453] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1320.575470] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1320.575490] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1320.575496] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1320.575517] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1320.575522] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1320.575543] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1320.575563] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1320.575569] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1320.575575] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1320.575579] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1320.575583] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1320.575588] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1320.575592] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.661841] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1560.661995] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.662090] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662221] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1560.662232] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1560.662240] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1560.662247] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1560.662255] Call Trace:
[ 1560.662272] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1560.662283] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1560.662292] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1560.662301] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.662307] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1560.662335] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1560.662343] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.662355] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1560.662383] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1560.662418] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1560.662430] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1560.662468] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1560.662477] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1560.662516] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1560.662552] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1560.662564] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1560.662575] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1560.662583] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1560.662590] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662598] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1560.662605] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662617] INFO: task java:1218 blocked for more than 120 seconds.
[ 1560.662753] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.662846] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662975] java D 0000000000000001 0 1218 655 0x00000000
[ 1560.662983] ffff8800c17f7ac8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1560.662990] ffff8800c17f7fd8 0000000000013640 ffff8802240ceeb0 ffff8800c33e1420
[ 1560.662997] 0000008000000000 ffff88002041b7a0 0000000000000000 ffff88002041b778
[ 1560.663003] Call Trace:
[ 1560.663027] [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1560.663049] [<ffffffffa0261eca>] ? dbuf_read+0x8da/0xf20 [zfs]
[ 1560.663061] [<ffffffffa01c8901>] ? kmem_asprintf+0x51/0x80 [spl]
[ 1560.663068] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663080] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663088] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663099] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663126] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663153] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663161] [<ffffffff810e1006>] ? getrawmonotonic+0x36/0xd0
[ 1560.663186] [<ffffffffa0268e1c>] dmu_free_long_range+0x1ac/0x280 [zfs]
[ 1560.663217] [<ffffffffa02daf1c>] zfs_rmnode+0x6c/0x340 [zfs]
[ 1560.663244] [<ffffffffa02fe141>] zfs_zinactive+0xc1/0x1d0 [zfs]
[ 1560.663273] [<ffffffffa02f6974>] zfs_inactive+0x64/0x200 [zfs]
[ 1560.663281] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[ 1560.663307] [<ffffffffa030ef18>] zpl_vap_init+0x838/0xa10 [zfs]
[ 1560.663315] [<ffffffff811eab68>] evict+0xb8/0x1b0
[ 1560.663322] [<ffffffff811eb405>] iput+0xf5/0x1a0
[ 1560.663330] [<ffffffff811df7f2>] do_unlinkat+0x1e2/0x350
[ 1560.663337] [<ffffffff811d4839>] ? SyS_newstat+0x39/0x60
[ 1560.663345] [<ffffffff811e02c6>] SyS_unlink+0x16/0x20
[ 1560.663353] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1560.663371] INFO: task sshd:1381 blocked for more than 120 seconds.
[ 1560.663508] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.663601] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.663791] sshd D 0000000000000001 0 1381 764 0x00000004
[ 1560.663798] ffff880113ad7b38 0000000000000086 ffff8800a7356eb0 0000000000013640
[ 1560.663804] ffff880113ad7fd8 0000000000013640 ffff8802240ceeb0 ffff8800a7356eb0
[ 1560.663810] ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000280 0000000000000001
[ 1560.663816] Call Trace:
[ 1560.663825] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663839] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663847] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663858] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663885] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663912] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663940] [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1560.663951] [<ffffffffa01c8956>] ? kmem_free_debug+0x16/0x20 [spl]
[ 1560.663962] [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1560.663972] [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1560.663998] [<ffffffffa02fe288>] ? zfs_tstamp_update_setup+0x38/0x1c0 [zfs]
[ 1560.664026] [<ffffffffa02f0cfe>] ? zfs_read+0x39e/0x460 [zfs]
[ 1560.664049] [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1560.664056] [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1560.664082] [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1560.664106] [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1560.664114] [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1560.664122] [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1560.664128] [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1560.664137] [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1560.664145] [<ffffffff81554f20>] int_signal+0x12/0x17
[ 1680.702704] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1680.702864] Tainted: P O 3.18.2-2-ARCH #1
[ 1680.702958] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1680.703089] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1680.703100] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1680.703108] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1680.703116] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1680.703123] Call Trace:
[ 1680.703141] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1680.703152] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1680.703161] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1680.703169] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1680.703176] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1680.703203] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1680.703211] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1680.703224] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1680.703252] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1680.703287] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1680.703299] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1680.703338] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1680.703346] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1680.703385] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1680.703422] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1680.703433] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1680.703444] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1680.703452] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1680.703459] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1680.703467] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1680.703474] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.742712] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1800.742869] Tainted: P O 3.18.2-2-ARCH #1
[ 1800.742964] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1800.743095] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1800.743106] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1800.743114] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1800.743121] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1800.743129] Call Trace:
[ 1800.743146] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1800.743157] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1800.743165] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1800.743174] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1800.743180] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1800.743206] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1800.743214] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1800.743227] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1800.743256] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1800.743290] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1800.743302] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1800.743341] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1800.743349] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1800.743388] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1800.743425] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1800.743437] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1800.743448] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1800.743455] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1800.743462] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.743471] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1800.743477] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782195] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1920.782354] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.782449] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.782580] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1920.782590] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1920.782599] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1920.782606] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1920.782614] Call Trace:
[ 1920.782631] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1920.782641] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1920.782650] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1920.782658] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.782665] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1920.782693] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1920.782701] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.782714] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1920.782742] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1920.782777] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1920.782789] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1920.782827] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1920.782836] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1920.782874] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1920.782911] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1920.782923] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1920.782934] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1920.782942] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1920.782949] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782957] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1920.782964] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782976] INFO: task java:1218 blocked for more than 120 seconds.
[ 1920.783136] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.783236] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.783366] java D 0000000000000000 0 1218 655 0x00000000
[ 1920.783373] ffff8800c17f7ab8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1920.783381] ffff8800c17f7fd8 0000000000013640 ffffffff81818540 ffff8800c33e1420
[ 1920.783387] ffff8800c17f7a08 ffffffffa025a872 0000000000000000 ffff8800235be4a0
[ 1920.783393] Call Trace:
[ 1920.783416] [<ffffffffa025a872>] ? arc_buf_eviction_needed+0x82/0xc0 [zfs]
[ 1920.783439] [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1920.783476] [<ffffffffa02b634d>] ? bp_get_dsize+0xad/0xf0 [zfs]
[ 1920.783503] [<ffffffffa02770e4>] ? dmu_tx_callback_register+0x324/0xab0 [zfs]
[ 1920.783512] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.783524] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.783532] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.783544] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.783570] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.783597] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.783627] [<ffffffffa02810e5>] ? dsl_dataset_block_freeable+0x45/0x1d0 [zfs]
[ 1920.783653] [<ffffffffa0276fb9>] ? dmu_tx_callback_register+0x1f9/0xab0 [zfs]
[ 1920.783681] [<ffffffffa02f74a0>] zfs_write+0x3c0/0xbf0 [zfs]
[ 1920.783688] [<ffffffff810a9dee>] ? enqueue_entity+0x24e/0xaa0
[ 1920.783696] [<ffffffff8109a1f0>] ? resched_curr+0xd0/0xe0
[ 1920.783705] [<ffffffff810ec497>] ? wake_futex+0x67/0x90
[ 1920.783711] [<ffffffff810ef856>] ? do_futex+0x8f6/0xae0
[ 1920.783736] [<ffffffffa030d2cb>] zpl_putpage+0x21b/0xd50 [zfs]
[ 1920.783744] [<ffffffff811cf1d7>] vfs_write+0xb7/0x200
[ 1920.783752] [<ffffffff811cfd29>] SyS_write+0x59/0xd0
[ 1920.783760] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1920.783778] INFO: task imap:1389 blocked for more than 120 seconds.
[ 1920.783916] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.784009] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.784138] imap D 0000000000000000 0 1389 770 0x00000000
[ 1920.784145] ffff88000f97fb38 0000000000000082 ffff880222d3bc60 0000000000013640
[ 1920.784151] ffff88000f97ffd8 0000000000013640 ffff88009d290a10 ffff880222d3bc60
[ 1920.784161] ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000fd8 0000000000000001
[ 1920.784168] Call Trace:
[ 1920.784176] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.784190] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.784197] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.784209] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.784236] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.784263] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.784292] [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1920.784303] [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1920.784314] [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1920.784338] [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1920.784345] [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1920.784372] [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1920.784395] [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1920.784403] [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1920.784411] [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1920.784417] [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1920.784426] [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1920.784434] [<ffffffff81554f20>] int_signal+0x12/0x17
[ 2280.901736] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 2280.901897] Tainted: P O 3.18.2-2-ARCH #1
[ 2280.901992] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 2280.902124] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 2280.902134] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 2280.902143] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 2280.902150] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 2280.902157] Call Trace:
[ 2280.902175] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 2280.902186] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 2280.902194] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 2280.902203] [<ffffffff81550b59>] schedule+0x29/0x70
[ 2280.902210] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 2280.902237] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 2280.902245] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 2280.902258] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 2280.902286] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 2280.902321] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 2280.902333] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 2280.902371] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 2280.902380] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 2280.902419] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 2280.902455] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 2280.902467] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 2280.902478] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 2280.902486] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 2280.902493] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 2280.902502] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 2280.902508] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[38275.738573] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
```
Running spl versions from arch:
```
hinoki% pacman -Q | grep spl
spl-git 0.6.3_r54_g03a7835_3.18.2_2-1
spl-utils-git 0.6.3_r54_g03a7835_3.18.2_2-1
```",
"Free memory:
```
hinoki% free -h
total used free shared buff/cache available
Mem: 7.8G 3.6G 691M 1.0M 3.6G 872M
Swap: 4.0G 0B 4.0G
```",
I'm getting this every couple of days now since upgrading to the latest Debian build (which according to the linked bug has the fix in it). Oops messages more or less the same as those already posted.. The zvols lock up and load average climbs into the hundreds. I've never had an issue prior to this.,Bitter Frustration
"I'm seeing similar symptoms after running a `zfs rollback` on a SMB shared dataset. After seeing load quickly rise I've stopped the SMB service and will give the rollback command some time to see if it recovers.
From dmesg, on Ubuntu Server 14.04.2 LTS, ZoL 0.6.3-5 from the Ubuntu PPA:
```
[3764100.795021] smbd D ffff88010b433480 0 34303 60686 0x00000000
[3764100.795025] ffff8800ae491d68 0000000000000082 ffff8800cf951800 ffff8800ae491fd8
[3764100.795029] 0000000000013480 0000000000013480 ffff8800cf951800 ffff8800e48dc3d0
[3764100.795032] ffff8800e48dc3a0 ffff8800e48dc3d8 ffff8800e48dc3c8 0000000000000000
[3764100.795037] Call Trace:
[3764100.795052] [<ffffffff817251a9>] schedule+0x29/0x70
[3764100.795067] [<ffffffffa006b7b5>] cv_wait_common+0x125/0x1c0 [spl]
[3764100.795073] [<ffffffff810ab0b0>] ? prepare_to_wait_event+0x100/0x100
[3764100.795091] [<ffffffffa006b865>] __cv_wait+0x15/0x20 [spl]
[3764100.795130] [<ffffffffa017019b>] rrw_enter_read+0x3b/0x150 [zfs]
[3764100.795181] [<ffffffffa01bf65d>] zfs_getattr_fast+0x3d/0x180 [zfs]
[3764100.795230] [<ffffffffa01d81fd>] zpl_getattr+0x2d/0x50 [zfs]
[3764100.795234] [<ffffffff811c2829>] vfs_getattr_nosec+0x29/0x40
[3764100.795237] [<ffffffff811c28fd>] vfs_getattr+0x2d/0x40
[3764100.795240] [<ffffffff811c29d2>] vfs_fstatat+0x62/0xa0
[3764100.795244] [<ffffffff811c2e5f>] SYSC_newstat+0x1f/0x40
[3764100.795248] [<ffffffff811cdc99>] ? putname+0x29/0x40
[3764100.795252] [<ffffffff811bcfe8>] ? do_sys_open+0x1b8/0x280
[3764100.795256] [<ffffffff811c30ae>] SyS_newstat+0xe/0x10
[3764100.795260] [<ffffffff8173186d>] system_call_fastpath+0x1a/0x1f
[3764100.795263] INFO: task smbd:34313 blocked for more than 120 seconds.
```",
"I can trigger the same error by using rsync from two different pools. The rsync process is hanging and can not be killed.
If I won't stop the rsync from the cli, the <my target pool> will endup into a fault status. The fault status is gone after a reboot.
```
May 07 22:43:27 <my host> kernel: INFO: task txg_sync:1408 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel: Tainted: P O 4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: txg_sync D ffff880028b63a28 0 1408 2 0x00000000
May 07 22:43:27 <my host> kernel: ffff880028b63a28 ffff88018c5f3cc0 ffff880070a16540 0000000000000000
May 07 22:43:27 <my host> kernel: ffff880028b63fd8 ffff88021fc93e00 7fffffffffffffff ffff8801b1ccae08
May 07 22:43:27 <my host> kernel: 0000000000000001 ffff880028b63a48 ffffffff8156fa87 ffff88008e7bdcb0
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel: [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel: [<ffffffff8157246c>] schedule_timeout+0x1bc/0x250
May 07 22:43:27 <my host> kernel: [<ffffffff8101f599>] ? read_tsc+0x9/0x10
May 07 22:43:27 <my host> kernel: [<ffffffff810e6757>] ? ktime_get+0x37/0xb0
May 07 22:43:27 <my host> kernel: [<ffffffff8156ef9a>] io_schedule_timeout+0xaa/0x130
May 07 22:43:27 <my host> kernel: [<ffffffffa034daa0>] ? zio_taskq_member.isra.6+0x80/0x80 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa0192248>] cv_wait_common+0xb8/0x140 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel: [<ffffffffa0192328>] __cv_wait_io+0x18/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa034f943>] zio_wait+0x123/0x210 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02d6be1>] dsl_pool_sync+0xc1/0x480 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02f1f80>] spa_sync+0x480/0xbf0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b36>] ? autoremove_wake_function+0x16/0x40
May 07 22:43:27 <my host> kernel: [<ffffffffa0303e06>] txg_sync_thread+0x386/0x630 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff8156fd62>] ? preempt_schedule_common+0x22/0x40
May 07 22:43:27 <my host> kernel: [<ffffffffa0303a80>] ? txg_quiesce_thread+0x3a0/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa018d561>] thread_generic_wrapper+0x71/0x80 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa018d4f0>] ? __thread_exit+0x20/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff81093338>] kthread+0xd8/0xf0
May 07 22:43:27 <my host> kernel: [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel: [<ffffffff81573718>] ret_from_fork+0x58/0x90
May 07 22:43:27 <my host> kernel: [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel: INFO: task rsync:10064 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel: Tainted: P O 4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: rsync D ffff8801a4633a58 0 10064 1 0x00000004
May 07 22:43:27 <my host> kernel: ffff8801a4633a58 ffff880216186f60 ffff880070a56540 ffff8801a4633a68
May 07 22:43:27 <my host> kernel: ffff8801a4633fd8 ffff8800da2a4a20 ffff8800da2a4ae0 ffff8800da2a4a48
May 07 22:43:27 <my host> kernel: 0000000000000000 ffff8801a4633a78 ffffffff8156fa87 ffff8800da2a4a20
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel: [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel: [<ffffffffa019229d>] cv_wait_common+0x10d/0x140 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel: [<ffffffffa01922e5>] __cv_wait+0x15/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa030327b>] txg_wait_synced+0x8b/0xd0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c078c>] dmu_tx_wait+0x25c/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c096e>] dmu_tx_assign+0x9e/0x520 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c8e10>] ? dsl_dataset_block_freeable+0x20/0x70 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02be639>] ? dmu_tx_count_dnode+0x59/0xb0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa033f5de>] zfs_write+0x3ce/0xc50 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff81515821>] ? unix_stream_recvmsg+0x701/0x7e0
May 07 22:43:27 <my host> kernel: [<ffffffffa03558cd>] zpl_write+0xbd/0x130 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff811d56b3>] vfs_write+0xb3/0x200
May 07 22:43:27 <my host> kernel: [<ffffffff811d624c>] ? vfs_read+0x11c/0x140
May 07 22:43:27 <my host> kernel: [<ffffffff811d6399>] SyS_write+0x59/0xd0
May 07 22:43:27 <my host> kernel: [<ffffffff815737c9>] system_call_fastpath+0x12/0x17
May 07 22:44:27 <my host> kernel: WARNING: Pool '<my target pool>' has encountered an uncorrectable I/O failure and has been suspended.
May 07 22:44:27 <my host> zed[12566]: eid=2302 class=data pool=<my target pool>
May 07 22:44:27 <my host> zed[12569]: eid=2303 class=io_failure pool=<my target pool>
```
```
# zpool status -v
pool: <my target pool>
state: ONLINE
scan: scrub repaired 0 in 9h15m with 0 errors on Thu May 7 09:01:37 2015
config:
NAME STATE READ WRITE CKSUM
<my target pool> ONLINE 0 0 0
<my target pool>-crypt ONLINE 0 0 0
errors: No known data errors
pool: <my source pool>
state: ONLINE
scan: scrub repaired 0 in 6h3m with 0 errors on Thu May 7 05:49:33 2015
config:
NAME STATE READ WRITE CKSUM
<my source pool> ONLINE 0 0 0
mirror-0 ONLINE 0 0 0
crypt-<my source pool>-00 ONLINE 0 0 0
crypt-<my source pool>-01 ONLINE 0 0 0
errors: No known data errors
```
```
#modinfo zfs | head
filename: /lib/modules/4.0.1-1-ARCH/extra/zfs/zfs.ko.gz
version: 0.6.4.1-1
license: CDDL
author: OpenZFS on Linux
description: ZFS
srcversion: 8324F6AEA2A06B2B6F0A0F5
depends: spl,znvpair,zunicode,zcommon,zavl
vermagic: 4.0.1-1-ARCH SMP preempt mod_unload modversions ```
```
#modinfo spl | head
filename: /lib/modules/4.0.1-1-ARCH/extra/spl/spl.ko.gz
version: 0.6.4.1-1
license: GPL
author: OpenZFS on Linux
description: Solaris Porting Layer
srcversion: 8907748310B8940C9D0DCD2
depends: vermagic: 4.0.1-1-ARCH SMP preempt mod_unload modversions ```
```
#uname -a
Linux <my host> 4.0.1-1-ARCH #1 SMP PREEMPT Wed Apr 29 12:00:26 CEST 2015 x86_64 GNU/Linux
```
Fingers crossed I've provided good information. I'm running an arch linux with demz repo.",
"I got the same issue with high-loaded mongodb on ZFS.
```
free -m
total used free shared buffers cached
Mem: 7928 7422 506 76 192 1024
-/+ buffers/cache: 6205 1723
Swap: 0 0 0
```
```
# modinfo spl|head
filename: /lib/modules/4.0.0-1-amd64/updates/dkms/spl.ko
version: 0.6.4-2-62e2eb
license: GPL
author: OpenZFS on Linux
description: Solaris Porting Layer
srcversion: 3F1EAF06925B312A0B3F767
depends: vermagic: 4.0.0-1-amd64 SMP mod_unload modversions parm: spl_hostid:The system hostid. (ulong)
parm: spl_hostid_path:The system hostid file (/etc/hostid) (charp)
```
```
# modinfo zfs|head
filename: /lib/modules/4.0.0-1-amd64/updates/dkms/zfs.ko
version: 0.6.4-16-544f71
license: CDDL
author: OpenZFS on Linux
description: ZFS
srcversion: 29BA21B62706579B75D5974
depends: spl,znvpair,zunicode,zcommon,zavl
vermagic: 4.0.0-1-amd64 SMP mod_unload modversions parm: zvol_inhibit_dev:Do not create zvol device nodes (uint)
parm: zvol_major:Major number for zvol device (uint)
```
```
# uname -a
Linux dan-desktop 4.0.0-1-amd64 #1 SMP Debian 4.0.2-1 (2015-05-11) x86_64 GNU/Linux
```
```
# lsb_release -a
No LSB modules are available.
Distributor ID: Debian
Description: Debian GNU/Linux unstable (sid)
Release: unstable
Codename: sid
```",
"Same for me on Debian Jessie with Linux 3.16.0-4-amd64 and zfs 0.6.5.2-2.
Happened on a postgresql server, no load at that time.
Jan 22 07:04:42 db04 kernel: [5056080.684110] INFO: task txg_sync:378 blocked for more than 120 seconds.
Jan 22 07:04:43 db04 kernel: [5056080.684128] Tainted: P O 3.16.0-4-amd64 #1
Jan 22 07:04:43 db04 kernel: [5056080.684134] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Jan 22 07:04:43 db04 kernel: [5056080.684142] txg_sync D ffff8800f96bc7e8 0 378 2 0x00000000
Jan 22 07:04:43 db04 kernel: [5056080.684153] ffff8800f96bc390 0000000000000246 0000000000012f00 ffff8800f707ffd8
Jan 22 07:04:43 db04 kernel: [5056080.684165] 0000000000012f00 ffff8800f96bc390 ffff8800ff3137b0 ffff880048db7570
Jan 22 07:04:43 db04 kernel: [5056080.684176] ffff880048db75b0 0000000000000001 ffff8800f8eb9000 0000000000000000
Jan 22 07:04:43 db04 kernel: [5056080.684187] Call Trace:
Jan 22 07:04:43 db04 kernel: [5056080.684201] [<ffffffff8150e159>] ? io_schedule+0x99/0x120
Jan 22 07:04:43 db04 kernel: [5056080.684220] [<ffffffffa00f8760>] ? cv_wait_common+0x90/0x100 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684232] [<ffffffff810a7a40>] ? prepare_to_wait_event+0xf0/0xf0
Jan 22 07:04:43 db04 kernel: [5056080.684264] [<ffffffffa02b679b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684287] [<ffffffffa02447ca>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684313] [<ffffffffa025e6b6>] ? spa_sync+0x366/0xb30 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684339] [<ffffffffa0270131>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684363] [<ffffffffa026fd60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684374] [<ffffffffa00f3deb>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684388] [<ffffffffa00f3d80>] ? __thread_exit+0x20/0x20 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684398] [<ffffffff81087f7d>] ? kthread+0xbd/0xe0
Jan 22 07:04:43 db04 kernel: [5056080.684406] [<ffffffff81087ec0>] ? kthread_create_on_node+0x180/0x180
Jan 22 07:04:43 db04 kernel: [5056080.684417] [<ffffffff81511618>] ? ret_from_fork+0x58/0x90
Jan 22 07:04:43 db04 kernel: [5056080.684428] [<ffffffff81087ec0>] ? kthread_create_on_node+0x180/0x180",
"@andreas-p please update to 0.6.5.4\* if available
alternatively: you can build your own latest zfsonlinux packages:
http://zfsonlinux.org/generic-deb.html",Impatience
"Unfortunately, debian8 is DKMS still on 6.5.2, no update in the last three months. Any clue when this gets resolved?",Impatience
"@andreas-p sorry, no idea,
but there's always the option of building the packages on your own - which is some effort but you'll know that you can trust those instead of having to rely on third-party repositories, etc.",
"Got the very same problem with 0.6.5.4 on a different machine with Debian8, zfs built from source. The stack trace shows exactly the same positions as the trace from Jan 18.
Virtual machine: 6GB RAM, 1.5GB swap, 4 AMD CPU cores.
12:54 Starting an rsync from a 2.5TB xfs to 4TB zfs partition, memory rising from 2GB to 5GB within 5 minutes.
13:09 CPU load/1m steps up from 1 to 10, CPU utilization around 50%, mostly system
13:11 txg_sync hung_timeout first appearance, CPU load/1m steps to 16, committed memory drops to 0, CPU utilization 85% system.
13:15 CPU utilization 95% system, load 16, need reboot.",Bitter Frustration
"same problem here with Debian 8, ZoL 6.5.2 from the official package repository.",
"Exactly the same block with 0.6.5.5:
Mar 24 11:13:06 db04 kernel: [4746960.372343] [<ffffffff815107c9>] ? io_schedule+0x99/0x120
Mar 24 11:13:06 db04 kernel: [4746960.372358] [<ffffffffa0198572>] ? cv_wait_common+0x92/0x110 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372370] [<ffffffff810a7ba0>] ? prepare_to_wait_event+0xf0/0xf0
Mar 24 11:13:06 db04 kernel: [4746960.372400] [<ffffffffa05410ab>] ? zio_wait+0x10b/0x1e0 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372423] [<ffffffffa04ce75a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372447] [<ffffffffa04e8706>] ? spa_sync+0x366/0xb30 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372472] [<ffffffffa04fa181>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372496] [<ffffffffa04f9db0>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372507] [<ffffffffa0193cab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372518] [<ffffffffa0193c40>] ? __thread_exit+0x20/0x20 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372529] [<ffffffff8108805d>] ? kthread+0xbd/0xe0
Mar 24 11:13:06 db04 kernel: [4746960.372547] [<ffffffff81087fa0>] ? kthread_create_on_node+0x180/0x180
Mar 24 11:13:06 db04 kernel: [4746960.372558] [<ffffffff81513c58>] ? ret_from_fork+0x58/0x90
Mar 24 11:13:06 db04 kernel: [4746960.372567] [<ffffffff81087fa0>] ? kthread_create_on_node+0x180/0x180
I got two consecutive ""txg_sync blocked for more than 120 seconds"", then it went back to normal.",Bitter Frustration
"I'm suffering from the similar problem. `rsync` reads/writes are extremely slow ~3M. After struggling like this ~24h (it's a multi-million file dataset) machine gets bricked. Please advice.
---
```
# uname -a
Linux ip-172-30-0-118 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
```
```
# lsb_release -a
No LSB modules are available.
Distributor ID: Debian
Description: Debian GNU/Linux 8.5 (jessie)
Release: 8.5
Codename: jessie
```
```
# dpkg -l '*zfs*' | grep ii
ii libzfs2linux 0.6.5.7-1 amd64 OpenZFS filesystem library for Linux
ii zfs-dkms 0.6.5.7-1 all OpenZFS filesystem kernel modules for Linux
ii zfs-zed 0.6.5.7-1 amd64 OpenZFS Event Daemon
ii zfsutils-linux 0.6.5.7-1 amd64 command-line tools to manage OpenZFS filesystems
```
```
# dpkg -l '*spl*' | grep ii
ii spl 0.6.5.7-1 amd64 Solaris Porting Layer user-space utilities for Linux
ii spl-dkms 0.6.5.7-1 all Solaris Porting Layer kernel modules for Linux
```
```
# zpool status
pool: zfs-backup
state: ONLINE
scan: none requested
config:
NAME STATE READ WRITE CKSUM
zfs-backup ONLINE 0 0 0
xvdf ONLINE 0 0 0
logs
xvdg ONLINE 0 0 0
errors: No known data errors
```
```
# zpool iostat -v
capacity operations bandwidth
pool alloc free read write read write
---------- ----- ----- ----- ----- ----- -----
zfs-backup 564G 380G 538 0 1.48M 2.42K
xvdf 564G 380G 538 0 1.48M 2.11K
logs - - - - - -
xvdg 0 1.98G 0 0 471 312
---------- ----- ----- ----- ----- ----- -----
```
```
# zpool get all
NAME PROPERTY VALUE SOURCE
zfs-backup size 944G -
zfs-backup capacity 59% -
zfs-backup altroot - default
zfs-backup health ONLINE -
zfs-backup guid 2876612074418704500 default
zfs-backup version - default
zfs-backup bootfs - default
zfs-backup delegation on default
zfs-backup autoreplace off default
zfs-backup cachefile - default
zfs-backup failmode wait default
zfs-backup listsnapshots off default
zfs-backup autoexpand off default
zfs-backup dedupditto 0 default
zfs-backup dedupratio 1.00x -
zfs-backup free 380G -
zfs-backup allocated 564G -
zfs-backup readonly off -
zfs-backup ashift 0 default
zfs-backup comment - default
zfs-backup expandsize - -
zfs-backup freeing 0 default
zfs-backup fragmentation 44% -
zfs-backup leaked 0 default
zfs-backup feature@async_destroy enabled local
zfs-backup feature@empty_bpobj active local
zfs-backup feature@lz4_compress active local
zfs-backup feature@spacemap_histogram active local
zfs-backup feature@enabled_txg active local
zfs-backup feature@hole_birth active local
zfs-backup feature@extensible_dataset enabled local
zfs-backup feature@embedded_data active local
zfs-backup feature@bookmarks enabled local
zfs-backup feature@filesystem_limits enabled local
zfs-backup feature@large_blocks enabled local
```
```
Jun 15 16:55:40 ip-172-30-0-118 kernel: [113640.618167] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:55:41 ip-172-30-0-118 kernel: [113640.621686] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.625572] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.629691] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:55:44 ip-172-30-0-118 kernel: [113640.633524] Call Trace:
Jun 15 16:55:45 ip-172-30-0-118 kernel: [113640.634740] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:55:47 ip-172-30-0-118 kernel: [113640.637492] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:55:48 ip-172-30-0-118 kernel: [113640.640409] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.643375] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.646118] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:55:50 ip-172-30-0-118 kernel: [113640.649130] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:55:51 ip-172-30-0-118 kernel: [113640.651910] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.655111] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.658303] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:55:54 ip-172-30-0-118 kernel: [113640.661609] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:55:55 ip-172-30-0-118 kernel: [113640.664528] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:55:56 ip-172-30-0-118 kernel: [113640.667024] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.670190] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.672778] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:57:39 ip-172-30-0-118 kernel: [113760.681975] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:57:40 ip-172-30-0-118 kernel: [113760.685521] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:57:41 ip-172-30-0-118 kernel: [113760.689295] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:57:42 ip-172-30-0-118 kernel: [113760.693211] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:57:43 ip-172-30-0-118 kernel: [113760.697110] Call Trace:
Jun 15 16:57:44 ip-172-30-0-118 kernel: [113760.698425] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:57:45 ip-172-30-0-118 kernel: [113760.701106] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:57:47 ip-172-30-0-118 kernel: [113760.704131] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:57:48 ip-172-30-0-118 kernel: [113760.707157] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:57:49 ip-172-30-0-118 kernel: [113760.709944] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:57:51 ip-172-30-0-118 kernel: [113760.713084] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:57:52 ip-172-30-0-118 kernel: [113760.715920] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:57:54 ip-172-30-0-118 kernel: [113760.719014] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:57:55 ip-172-30-0-118 kernel: [113760.722283] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.725571] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.728485] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:57:58 ip-172-30-0-118 kernel: [113760.730974] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:58:00 ip-172-30-0-118 kernel: [113760.734102] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:58:02 ip-172-30-0-118 kernel: [113760.736819] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:38 ip-172-30-0-118 kernel: [113880.747394] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.750796] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.754658] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:59:41 ip-172-30-0-118 kernel: [113880.758412] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:59:42 ip-172-30-0-118 kernel: [113880.762234] Call Trace:
Jun 15 16:59:43 ip-172-30-0-118 kernel: [113880.763454] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:59:44 ip-172-30-0-118 kernel: [113880.766117] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.769024] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.772106] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:59:46 ip-172-30-0-118 kernel: [113880.775310] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:59:48 ip-172-30-0-118 kernel: [113880.778331] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:59:50 ip-172-30-0-118 kernel: [113880.781198] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:59:51 ip-172-30-0-118 kernel: [113880.784324] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:59:52 ip-172-30-0-118 kernel: [113880.787500] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:59:54 ip-172-30-0-118 kernel: [113880.790757] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:59:55 ip-172-30-0-118 kernel: [113880.793707] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:59:56 ip-172-30-0-118 kernel: [113880.796081] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:57 ip-172-30-0-118 kernel: [113880.799104] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:59:58 ip-172-30-0-118 kernel: [113880.801724] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:41 ip-172-30-0-118 kernel: [114000.817906] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 17:01:42 ip-172-30-0-118 kernel: [114000.822256] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 17:01:43 ip-172-30-0-118 kernel: [114000.827952] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.833092] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.837533] Call Trace:
Jun 15 17:01:47 ip-172-30-0-118 kernel: [114000.838994] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 17:01:48 ip-172-30-0-118 kernel: [114000.842069] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 17:01:49 ip-172-30-0-118 kernel: [114000.845833] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 17:01:50 ip-172-30-0-118 kernel: [114000.849362] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 17:01:51 ip-172-30-0-118 kernel: [114000.852630] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 17:01:52 ip-172-30-0-118 kernel: [114000.856037] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 17:01:53 ip-172-30-0-118 kernel: [114000.859185] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 17:01:54 ip-172-30-0-118 kernel: [114000.862633] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 17:01:55 ip-172-30-0-118 kernel: [114000.866301] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 17:01:56 ip-172-30-0-118 kernel: [114000.870041] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.873530] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.876425] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:59 ip-172-30-0-118 kernel: [114000.879915] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 17:02:00 ip-172-30-0-118 kernel: [114000.882644] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
```",
"@narunask could you please post output of ```
cat /proc/spl/kstat/zfs/arcstats
```
or meanwhile access to the box is denied (""bricked"") ?
Also please post some hardware and specific configuration data (RAM, processor, mainboard, harddrive type, lvm/cryptsetup/etc. etc.)
thanks",Impatience
"Server is on the `EC2`, currently 1 core, 2GB RAM (`t2.small`), HDD - the magnetic one, I believe it is documented [here](https://aws.amazon.com/ebs/previous-generation/).
HDD that I'm copying from is LVM based, consisting of 3 PVs. HDD are not encrypted.
Also FYI, currently `rsync` is calculating deltas and should start moving data again very soon, if that adds something to the stats.
```
# cat /proc/spl/kstat/zfs/arcstats
6 1 0x01 91 4368 35041278654 31678952268315
name type data
hits 4 42347862
misses 4 10285010
demand_data_hits 4 0
demand_data_misses 4 68
demand_metadata_hits 4 34887103
demand_metadata_misses 4 6345542
prefetch_data_hits 4 0
prefetch_data_misses 4 0
prefetch_metadata_hits 4 7460759
prefetch_metadata_misses 4 3939400
mru_hits 4 20830447
mru_ghost_hits 4 2991105
mfu_hits 4 14056656
mfu_ghost_hits 4 1782606
deleted 4 2167955
mutex_miss 4 90369
evict_skip 4 1781974161
evict_not_enough 4 15390660
evict_l2_cached 4 0
evict_l2_eligible 4 35359622656
evict_l2_ineligible 4 55754657792
evict_l2_skip 4 0
hash_elements 4 5452
hash_elements_max 4 48255
hash_collisions 4 57676
hash_chains 4 69
hash_chain_max 4 4
p 4 7007439
c 4 34933248
c_min 4 33554432
c_max 4 1053282304
size 4 105425184
hdr_size 4 1969640
data_size 4 0
metadata_size 4 30842880
other_size 4 72612664
anon_size 4 753664
anon_evictable_data 4 0
anon_evictable_metadata 4 0
mru_size 4 27006976
mru_evictable_data 4 0
mru_evictable_metadata 4 3653632
mru_ghost_size 4 6850560
mru_ghost_evictable_data 4 0
mru_ghost_evictable_metadata 4 6850560
mfu_size 4 3082240
mfu_evictable_data 4 0
mfu_evictable_metadata 4 32768
mfu_ghost_size 4 25972224
mfu_ghost_evictable_data 4 0
mfu_ghost_evictable_metadata 4 25972224
l2_hits 4 0
l2_misses 4 0
l2_feeds 4 0
l2_rw_clash 4 0
l2_read_bytes 4 0
l2_write_bytes 4 0
l2_writes_sent 4 0
l2_writes_done 4 0
l2_writes_error 4 0
l2_writes_lock_retry 4 0
l2_evict_lock_retry 4 0
l2_evict_reading 4 0
l2_evict_l1cached 4 0
l2_free_on_write 4 0
l2_abort_lowmem 4 0
l2_cksum_bad 4 0
l2_io_error 4 0
l2_size 4 0
l2_asize 4 0
l2_hdr_size 4 0
l2_compress_successes 4 0
l2_compress_zeros 4 0
l2_compress_failures 4 0
memory_throttle_count 4 0
duplicate_buffers 4 0
duplicate_buffers_size 4 0
duplicate_reads 4 0
memory_direct_count 4 947
memory_indirect_count 4 259028
arc_no_grow 4 0
arc_tempreserve 4 0
arc_loaned_bytes 4 0
arc_prune 4 3960
arc_meta_used 4 105425184
arc_meta_limit 4 789961728
arc_meta_max 4 816526072
arc_meta_min 4 16777216
arc_need_free 4 0
arc_sys_free 4 32911360
```
```
# cat /proc/cpuinfo processor : 0
vendor_id : GenuineIntel
cpu family : 6
model : 63
model name : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
stepping : 2
microcode : 0x25
cpu MHz : 2394.530
cache size : 30720 KB
physical id : 0
siblings : 1
core id : 0
cpu cores : 1
apicid : 0
initial apicid : 0
fpu : yes
fpu_exception : yes
cpuid level : 13
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm xsaveopt fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips : 4789.06
clflush size : 64
cache_alignment : 64
address sizes : 46 bits physical, 48 bits virtual
power management:
```
```
# dmidecode --type memory
# dmidecode 3.0
Scanning /dev/mem for entry point.
SMBIOS 2.4 present.
Handle 0x1000, DMI type 16, 15 bytes
Physical Memory Array
Location: Other
Use: System Memory
Error Correction Type: Multi-bit ECC
Maximum Capacity: 2 GB
Error Information Handle: Not Provided
Number Of Devices: 1
Handle 0x1100, DMI type 17, 21 bytes
Memory Device
Array Handle: 0x1000
Error Information Handle: 0x0000
Total Width: 64 bits
Data Width: 64 bits
Size: 2048 MB
Form Factor: DIMM
Set: None
Locator: DIMM 0
Bank Locator: Not Specified
Type: RAM
Type Detail: None
```
Thanks",
"I haven't seen the problem for quite a while now (seemed to have gone since 0.6.5.6), but this morning I had the very same hung task with 0.6.5.7 (single occurrance) come up again. Sigh...",Bitter Frustration
"I can confirm that I have not seen this issue happen for ages on the same hardware. I have not had it reoccur, probably within the last 3-6 months.",
"Still happening with heavy RSync backups.
```
[88234.807775] INFO: task rsync:18699 blocked for more than 120 seconds.
[88234.807929] Tainted: P O 4.8.13-1-ARCH #1
[88234.808026] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[88234.808159] rsync D ffff880124bcb9d0 0 18699 18698 0x00000000
[88234.808171] ffff880124bcb9d0 00ffffff81003016 ffff88021cd00d00 ffff880127fa6800
[88234.808179] ffff88022fc17ff0 ffff880124bcc000 ffff88021d8eaa10 ffff88021d8eab58
[88234.808185] ffff88021d8eaa38 0000000000000000 ffff880124bcb9e8 ffffffff815f40ec
[88234.808190] Call Trace:
[88234.808203] [<ffffffff815f40ec>] schedule+0x3c/0x90
[88234.808217] [<ffffffffa0171e1f>] cv_wait_common+0x10f/0x130 [spl]
[88234.808225] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[88234.808235] [<ffffffffa0171e55>] __cv_wait+0x15/0x20 [spl]
[88234.808294] [<ffffffffa03095f8>] txg_wait_open+0xa8/0xe0 [zfs]
[88234.808338] [<ffffffffa02c73db>] dmu_tx_wait+0x32b/0x340 [zfs]
[88234.808380] [<ffffffffa02c747b>] dmu_tx_assign+0x8b/0x490 [zfs]
[88234.808422] [<ffffffffa0342d09>] zfs_write+0x3f9/0xc80 [zfs]
[88234.808461] [<ffffffffa034f567>] ? zio_destroy+0xb7/0xc0 [zfs]
[88234.808500] [<ffffffffa0352c78>] ? zio_wait+0x138/0x1d0 [zfs]
[88234.808507] [<ffffffffa016ab9a>] ? spl_kmem_free+0x2a/0x40 [spl]
[88234.808551] [<ffffffffa0337510>] ? zfs_range_unlock+0x1a0/0x2c0 [zfs]
[88234.808590] [<ffffffffa035826c>] zpl_write_common_iovec+0x8c/0xe0 [zfs]
[88234.808627] [<ffffffffa0358497>] zpl_write+0x87/0xc0 [zfs]
[88234.808635] [<ffffffff81208797>] __vfs_write+0x37/0x140
[88234.808642] [<ffffffff810c7be7>] ? percpu_down_read+0x17/0x50
[88234.808648] [<ffffffff81209566>] vfs_write+0xb6/0x1a0
[88234.808652] [<ffffffff8120a9e5>] SyS_write+0x55/0xc0
[88234.808658] [<ffffffff815f8032>] entry_SYSCALL_64_fastpath+0x1a/0xa4
[88234.808664] INFO: task imap:19376 blocked for more than 120 seconds.
[88234.808803] Tainted: P O 4.8.13-1-ARCH #1
[88234.808898] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[88234.809029] imap D ffff8801b875fad8 0 19376 1488 0x00000000
[88234.809036] ffff8801b875fad8 00ff88021d8eab00 ffffffff81a0d500 ffff8801a9f95b00
[88234.809042] ffff8801b875fab8 ffff8801b8760000 ffff88021d8eaa10 ffff88021d8eab58
[88234.809048] ffff88021d8eaa38 0000000000000000 ffff8801b875faf0 ffffffff815f40ec
[88234.809053] Call Trace:
[88234.809059] [<ffffffff815f40ec>] schedule+0x3c/0x90
[88234.809067] [<ffffffffa0171e1f>] cv_wait_common+0x10f/0x130 [spl]
[88234.809073] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[88234.809081] [<ffffffffa0171e55>] __cv_wait+0x15/0x20 [spl]
[88234.809131] [<ffffffffa03095f8>] txg_wait_open+0xa8/0xe0 [zfs]
[88234.809175] [<ffffffffa02c73db>] dmu_tx_wait+0x32b/0x340 [zfs]
[88234.809217] [<ffffffffa02c747b>] dmu_tx_assign+0x8b/0x490 [zfs]
[88234.809259] [<ffffffffa0342299>] zfs_dirty_inode+0xe9/0x300 [zfs]
[88234.809265] [<ffffffff8122a3b4>] ? mntput+0x24/0x40
[88234.809272] [<ffffffff81189adb>] ? release_pages+0x2cb/0x380
[88234.809278] [<ffffffff811c763e>] ? free_pages_and_swap_cache+0x8e/0xa0
[88234.809315] [<ffffffffa035a2cc>] zpl_dirty_inode+0x2c/0x40 [zfs]
[88234.809322] [<ffffffff81237785>] __mark_inode_dirty+0x45/0x400
[88234.809362] [<ffffffffa0346fe8>] zfs_mark_inode_dirty+0x48/0x50 [zfs]
[88234.809399] [<ffffffffa0358826>] zpl_release+0x46/0x90 [zfs]
[88234.809404] [<ffffffff8120b42f>] __fput+0x9f/0x1e0
[88234.809408] [<ffffffff8120b5ae>] ____fput+0xe/0x10
[88234.809414] [<ffffffff8109a0d0>] task_work_run+0x80/0xa0
[88234.809420] [<ffffffff8100366a>] exit_to_usermode_loop+0xba/0xc0
[88234.809425] [<ffffffff81003b2e>] syscall_return_slowpath+0x4e/0x60
[88234.809430] [<ffffffff815f80ba>] entry_SYSCALL_64_fastpath+0xa2/0xa4 ```
Most of the time performance is fine.",
"So, I replaced the drive which might have been causing issues, with a new Samsung 840 PRO SSD, partitioned 50GB OS, 4GB Swap and the rest available as L2ARC.
I've set the arc max size to 4GB on a system with only 8GB memory.
Tonight, the same issue occurred, rsync and then everything ground to a halt. So, it seems unlikely to be a disk issue. Nominal read speeds are 100MB/s+ so things are humming along quite nicely.",Bitter Frustration
I can reproduce this issue fairly easily so just let me know what information you'd like me to collect and I'll try to do it.,
"```
[78889.098553] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[78889.098667] Tainted: P O 4.8.13-1-ARCH #1
[78889.098719] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[78889.098790] txg_sync D ffff88021c893ab8 0 1372 2 0x00000000
[78889.098798] ffff88021c893ab8 00ffffff810a5d2f ffff8802212b5b00 ffff88021d3ece00
[78889.098802] 0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[78889.098806] ffff880105123b60 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[78889.098809] Call Trace:
[78889.098818] [<ffffffff815f40ec>] schedule+0x3c/0x90
[78889.098821] [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[78889.098825] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[78889.098828] [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[78889.098832] [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[78889.098835] [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[78889.098843] [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[78889.098846] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[78889.098851] [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[78889.098886] [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[78889.098912] [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[78889.098939] [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[78889.098942] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[78889.098969] [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[78889.098997] [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[78889.099002] [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[78889.099006] [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[78889.099010] [<ffffffff8109be38>] kthread+0xd8/0xf0
[78889.099013] [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[78889.099016] [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[78889.099019] [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[79380.619090] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[79380.619245] Tainted: P O 4.8.13-1-ARCH #1
[79380.619342] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[79380.619475] txg_sync D ffff88021c893ab8 0 1372 2 0x00000000
[79380.619487] ffff88021c893ab8 00ffffff810a5d2f ffffffff81a0d500 ffff88021d3ece00
[79380.619494] 0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[79380.619500] ffff8801e9c0c850 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[79380.619506] Call Trace:
[79380.619519] [<ffffffff815f40ec>] schedule+0x3c/0x90
[79380.619525] [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[79380.619531] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[79380.619537] [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[79380.619543] [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[79380.619547] [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[79380.619560] [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[79380.619565] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[79380.619574] [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[79380.619623] [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[79380.619672] [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[79380.619724] [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[79380.619729] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[79380.619780] [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[79380.619830] [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[79380.619839] [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[79380.619847] [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[79380.619854] [<ffffffff8109be38>] kthread+0xd8/0xf0
[79380.619860] [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[79380.619866] [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[79380.619872] [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[81223.826929] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[81223.827091] Tainted: P O 4.8.13-1-ARCH #1
[81223.827226] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[81223.827381] txg_sync D ffff88021c893ab8 0 1372 2 0x00000000
[81223.827393] ffff88021c893ab8 00ffffff810a5d2f ffff8802157c3400 ffff88021d3ece00
[81223.827400] 0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[81223.827407] ffff88015f37d0f0 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[81223.827413] Call Trace:
[81223.827426] [<ffffffff815f40ec>] schedule+0x3c/0x90
[81223.827432] [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[81223.827439] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[81223.827444] [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[81223.827451] [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[81223.827455] [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[81223.827468] [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[81223.827473] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[81223.827482] [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[81223.827531] [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[81223.827580] [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[81223.827632] [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[81223.827636] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[81223.827687] [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[81223.827738] [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[81223.827747] [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[81223.827755] [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[81223.827762] [<ffffffff8109be38>] kthread+0xd8/0xf0
[81223.827767] [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[81223.827773] [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[81223.827779] [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[94923.528386] CE: hpet increased min_delta_ns to 20115 nsec
[102727.918958] INFO: task rsync:5577 blocked for more than 120 seconds.
[102727.919092] Tainted: P O 4.8.13-1-ARCH #1
[102727.919170] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[102727.919277] rsync D ffff8801dfe9ba10 0 5577 5576 0x00000000
[102727.919287] ffff8801dfe9ba10 00000000ffffffff ffffffff81a0d500 ffff8802157c3400
[102727.919293] ffffffffa0340567 ffff8801dfe9c000 ffff88021ed41148 ffff88021ed41180
[102727.919298] ffff88021ed41170 0000000000000000 ffff8801dfe9ba28 ffffffff815f40ec
[102727.919303] Call Trace:
[102727.919352] [<ffffffffa0340567>] ? zio_destroy+0xb7/0xc0 [zfs]
[102727.919358] [<ffffffff815f40ec>] schedule+0x3c/0x90
[102727.919367] [<ffffffffa0203e1f>] cv_wait_common+0x10f/0x130 [spl]
[102727.919373] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[102727.919380] [<ffffffffa0203e55>] __cv_wait+0x15/0x20 [spl]
[102727.919414] [<ffffffffa02b814b>] dmu_tx_wait+0x9b/0x340 [zfs]
[102727.919448] [<ffffffffa02b847b>] dmu_tx_assign+0x8b/0x490 [zfs]
[102727.919482] [<ffffffffa0333d09>] zfs_write+0x3f9/0xc80 [zfs]
[102727.919514] [<ffffffffa0340567>] ? zio_destroy+0xb7/0xc0 [zfs]
[102727.919544] [<ffffffffa0343c78>] ? zio_wait+0x138/0x1d0 [zfs]
[102727.919579] [<ffffffffa0328510>] ? zfs_range_unlock+0x1a0/0x2c0 [zfs]
[102727.919610] [<ffffffffa034926c>] zpl_write_common_iovec+0x8c/0xe0 [zfs]
[102727.919640] [<ffffffffa0349497>] zpl_write+0x87/0xc0 [zfs]
[102727.919646] [<ffffffff81208797>] __vfs_write+0x37/0x140
[102727.919652] [<ffffffff810c7be7>] ? percpu_down_read+0x17/0x50
[102727.919656] [<ffffffff81209566>] vfs_write+0xb6/0x1a0
[102727.919660] [<ffffffff8120a9e5>] SyS_write+0x55/0xc0
[102727.919665] [<ffffffff815f8032>] entry_SYSCALL_64_fastpath+0x1a/0xa4
```
Happened last night during backups.",
"Okay, so I've replaced the drive which had the high await time, and also increased the memory to 16GB, still having issues:
```
[12785.566973] CE: hpet increased min_delta_ns to 20115 nsec
[25560.317294] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25560.317451] Tainted: P O 4.9.6-1-ARCH #1
[25560.317542] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25560.317668] txg_sync D 0 1392 2 0x00000000
[25560.317678] ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25560.317686] ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25560.317692] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25560.317698] Call Trace:
[25560.317713] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25560.317719] [<ffffffff816061cd>] schedule+0x3d/0x90
[25560.317726] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25560.317781] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25560.317821] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25560.317859] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25560.317867] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25560.317873] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25560.317884] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25560.317891] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25560.317900] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25560.317938] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25560.317984] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25560.318035] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25560.318041] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25560.318091] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25560.318096] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318145] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25560.318154] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25560.318161] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25560.318167] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25560.318171] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318176] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318180] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318184] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[25683.204571] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25683.204722] Tainted: P O 4.9.6-1-ARCH #1
[25683.204814] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25683.204940] txg_sync D 0 1392 2 0x00000000
[25683.204950] ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25683.204958] ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25683.204965] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25683.204970] Call Trace:
[25683.204985] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25683.204992] [<ffffffff816061cd>] schedule+0x3d/0x90
[25683.204999] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25683.205055] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25683.205095] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25683.205134] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25683.205142] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25683.205148] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25683.205159] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25683.205166] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25683.205175] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25683.205213] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25683.205259] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25683.205310] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25683.205315] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25683.205365] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25683.205371] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205419] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25683.205428] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25683.205436] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25683.205441] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25683.205445] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205450] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205454] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205458] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[26051.866268] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[26051.866424] Tainted: P O 4.9.6-1-ARCH #1
[26051.866516] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[26051.866642] txg_sync D 0 1392 2 0x00000000
[26051.866652] ffff8803e6f2c000 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[26051.866660] ffff88041b50b400 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[26051.866667] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[26051.866673] Call Trace:
[26051.866687] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[26051.866694] [<ffffffff816061cd>] schedule+0x3d/0x90
[26051.866700] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[26051.866756] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[26051.866796] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[26051.866834] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[26051.866842] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[26051.866847] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[26051.866859] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[26051.866866] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[26051.866874] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[26051.866913] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[26051.866959] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[26051.867009] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[26051.867015] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[26051.867065] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[26051.867070] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867119] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[26051.867128] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[26051.867135] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[26051.867140] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[26051.867145] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867149] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867153] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867157] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
```",
"Running
```
% pacman -Q | egrep ""zfs|spl""
spl-linux 0.6.5.9_4.9.6_1-2
spl-utils-linux 0.6.5.9_4.9.6_1-2
zfs-linux 0.6.5.9_4.9.6_1-2
zfs-utils-linux 0.6.5.9_4.9.6_1-2
```",
"Memory available is okay:
```
% free -h
total used free shared buff/cache available
Mem: 15G 12G 2.7G 692K 630M 3.0G
Swap: 4.0G 0B 4.0G
```
iostat seem okay (sde is samsung 840 pro)
```
% iostat -mx 10
Linux 4.9.6-1-ARCH (hinoki) 06/02/17 _x86_64_	(2 CPU)
avg-cpu: %user %nice %system %iowait %steal %idle
0.69 0.00 2.26 3.18 0.00 93.88
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 4.39 13.96 0.19 0.23 47.25 0.06 3.16 11.42 0.57 2.28 4.18
sdb 0.00 0.00 4.39 13.69 0.19 0.22 46.63 0.06 3.13 11.19 0.55 2.26 4.09
sdc 0.00 0.00 4.29 13.58 0.19 0.23 47.97 0.06 3.61 12.91 0.66 2.58 4.62
sdd 0.00 0.00 4.32 13.80 0.18 0.23 46.40 0.06 3.18 11.60 0.55 2.29 4.15
sde 0.00 0.30 1.46 11.25 0.02 0.36 61.82 0.01 0.41 0.78 0.36 0.22 0.28
avg-cpu: %user %nice %system %iowait %steal %idle
10.43 0.00 47.68 28.80 0.00 13.08
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 54.20 0.80 3.27 0.01 122.01 0.31 5.70 5.77 0.88 3.10 17.06
sdb 0.00 0.00 54.60 0.80 3.09 0.01 114.60 0.43 7.70 7.80 0.75 3.62 20.07
sdc 0.00 0.00 54.20 1.00 3.15 0.01 117.16 0.39 6.98 7.11 0.00 3.80 21.00
sdd 0.00 0.00 56.30 1.00 3.08 0.01 110.27 0.43 7.57 7.70 0.30 3.87 22.17
sde 0.00 5.60 27.30 159.50 0.08 7.38 81.70 0.06 0.34 0.21 0.36 0.22 4.20
avg-cpu: %user %nice %system %iowait %steal %idle
5.69 0.00 43.65 30.28 0.00 20.38
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 43.30 0.90 2.45 0.01 113.81 0.35 7.86 8.03 0.00 4.38 19.34
sdb 0.00 0.00 40.30 0.90 2.31 0.01 114.99 0.36 8.68 8.87 0.44 4.47 18.40
sdc 0.00 0.00 41.80 0.80 2.39 0.01 115.08 0.34 8.08 8.22 0.38 4.68 19.93
sdd 0.00 0.00 41.20 0.80 2.30 0.01 112.34 0.44 10.39 10.58 0.88 4.56 19.16
sde 0.00 3.10 25.80 136.70 0.07 5.80 74.00 0.05 0.28 0.48 0.25 0.21 3.47
avg-cpu: %user %nice %system %iowait %steal %idle
3.42 0.00 38.83 28.72 0.00 29.02
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 26.70 0.90 1.58 0.01 117.86 0.26 9.37 9.63 1.78 5.51 15.20
sdb 0.00 0.00 28.50 0.90 1.53 0.01 107.54 0.25 8.59 8.86 0.00 5.04 14.83
sdc 0.00 0.00 27.40 0.90 1.41 0.01 103.12 0.27 9.34 9.65 0.00 5.49 15.53
sdd 0.00 0.00 28.60 0.90 1.43 0.01 100.18 0.24 8.21 8.47 0.00 4.84 14.27
sde 0.00 0.80 27.30 84.50 0.08 2.67 50.38 0.02 0.21 0.27 0.19 0.19 2.07
```",
"Just wanted to give a short update to my post in this issue from last year: in the mean time I have upgraded to ZFS 0.6.5.8 from Debian's backports, still using Debian 8. Unfortunately I still get the exact same timeout in the kernel logs.",Bitter Frustration
"@kpande if you're going to assert that, can you please describe what's big enough?
I have resources 4X the average described in this thread and I also see the same issues, in this case with the combination of zfs send and nfsd load.",Bitter Frustration
"> severely undersized hardware
You may be right.
I have 4x 4TB drives, and one 256 SSD as OS/Cache. 16GB memory, 2 core CPU (1.5Ghz Atom). This system is almost exclusively used for RSync backups. I do feel that this is pretty reasonable for my needs.
The ARC cache is 12GB, the OS has 4GB left over. The L2 cache, if enabled , is about 170GB.
I'll have to check how many files it is, but I'm not sure if it's multi-millions or not.",
"@kpande you are right I forgot to give any relevant infos about my underlying hardware. Sorry about that, here are hopefully all the relevant infos:
My server is a virtualization Server running Xen 4.4 with currently 6 virtual machines which all have their logical volumes (LVM) stored on a RAIDZ1 volume with 3x 2TB Seagate SATA enterprise disks (ST32000645NS). The Debian 8 OS is independent and located on two internal SATA-SSD disks of both 16 GB in RAID1 using Linux MD for mirroring. The CPU is an Intel E5-2620 v3 @ 2.40GHz with 6 cores/12 threads. Out of these 6 cores 4 vCPUs have been pinned to the host/hypervisor/dom0 using the `dom0_max_vcpus=4 dom0_vcpus_pin` Linux kernel options. The server has 64 GB of TruDDR4 ECC memory and out of this 64 GB of memory 6 GB has been reserved to the host/hypervisor/dom0 using the respective Linux kernel options `dom0_mem=6G,max:6G`. Finally I have reserved 2 GB RAM of these 6 GB for the ARC using the following `zfs_arc_max=2147483648` zfs module option. I have also disabled ZFS prefetch if that is of any relevance (`zfs_prefetch_disable=1`).
Below is the output of an actual ARC summary (server has bee rebooted 5 days ago):
```
ZFS Subsystem Report	Tue Feb 07 15:11:32 2017
ARC Summary: (HEALTHY)
Memory Throttle Count:	0
ARC Misc:
Deleted:	3.05m
Mutex Misses:	7
Evict Skips:	7
ARC Size:	77.69%	1.55	GiB
Target Size: (Adaptive)	100.00%	2.00	GiB
Min Size (Hard Limit):	1.56%	32.00	MiB
Max Size (High Water):	64:1	2.00	GiB
ARC Size Breakdown:
Recently Used Cache Size:	93.75%	1.88	GiB
Frequently Used Cache Size:	6.25%	128.00	MiB
ARC Hash Breakdown:
Elements Max:	171.83k
Elements Current:	97.97%	168.34k
Collisions:	68.60m
Chain Max:	6
Chains:	12.12k
ARC Total accesses:	102.96m
Cache Hit Ratio:	48.88%	50.32m
Cache Miss Ratio:	51.12%	52.63m
Actual Hit Ratio:	48.88%	50.32m
Data Demand Efficiency:	0.00%	49.34m
CACHE HITS BY CACHE LIST:
Most Recently Used:	63.44%	31.92m
Most Frequently Used:	36.56%	18.40m
Most Recently Used Ghost:	0.08%	42.37k
Most Frequently Used Ghost:	0.00%	0
CACHE HITS BY DATA TYPE:
Demand Data:	0.00%	167
Prefetch Data:	0.00%	0
Demand Metadata:	100.00%	50.32m
Prefetch Metadata:	0.00%	0
CACHE MISSES BY DATA TYPE:
Demand Data:	93.74%	49.34m
Prefetch Data:	0.00%	0
Demand Metadata:	6.26%	3.29m
Prefetch Metadata:	0.00%	145
ZFS Tunable:
metaslab_debug_load 0
zfs_arc_min_prefetch_lifespan 0
zfetch_max_streams 8
zfs_nopwrite_enabled 1
zfetch_min_sec_reap 2
zfs_dbgmsg_enable 0
zfs_dirty_data_max_max_percent 25
zfs_arc_p_aggressive_disable 1
spa_load_verify_data 1
zfs_zevent_cols 80
zfs_dirty_data_max_percent 10
zfs_sync_pass_dont_compress 5
l2arc_write_max 8388608
zfs_vdev_scrub_max_active 2
zfs_vdev_sync_write_min_active 10
zvol_prefetch_bytes 131072
metaslab_aliquot 524288
zfs_no_scrub_prefetch 0
zfs_arc_shrink_shift 0
zfetch_block_cap 256
zfs_txg_history 0
zfs_delay_scale 500000
zfs_vdev_async_write_active_min_dirty_percent 30
metaslab_debug_unload 0
zfs_read_history 0
zvol_max_discard_blocks 16384
zfs_recover 0
l2arc_headroom 2
zfs_deadman_synctime_ms 1000000
zfs_scan_idle 50
zfs_free_min_time_ms 1000
zfs_dirty_data_max 624856268
zfs_vdev_async_read_min_active 1
zfs_mg_noalloc_threshold 0
zfs_dedup_prefetch 0
zfs_vdev_max_active 1000
l2arc_write_boost 8388608
zfs_resilver_min_time_ms 3000
zfs_vdev_async_write_max_active 10
zil_slog_limit 1048576
zfs_prefetch_disable 1
zfs_resilver_delay 2
metaslab_lba_weighting_enabled 1
zfs_mg_fragmentation_threshold 85
l2arc_feed_again 1
zfs_zevent_console 0
zfs_immediate_write_sz 32768
zfs_dbgmsg_maxsize 4194304
zfs_free_leak_on_eio 0
zfs_deadman_enabled 1
metaslab_bias_enabled 1
zfs_arc_p_dampener_disable 1
zfs_object_mutex_size 64
zfs_metaslab_fragmentation_threshold 70
zfs_no_scrub_io 0
metaslabs_per_vdev 200
zfs_dbuf_state_index 0
zfs_vdev_sync_read_min_active 10
metaslab_fragmentation_factor_enabled 1
zvol_inhibit_dev 0
zfs_vdev_async_write_active_max_dirty_percent 60
zfs_vdev_cache_size 0
zfs_vdev_mirror_switch_us 10000
zfs_dirty_data_sync 67108864
spa_config_path /etc/zfs/zpool.cache
zfs_dirty_data_max_max 1562140672
zfs_arc_lotsfree_percent 10
zfs_zevent_len_max 64
zfs_scan_min_time_ms 1000
zfs_arc_sys_free 0
zfs_arc_meta_strategy 1
zfs_vdev_cache_bshift 16
zfs_arc_meta_adjust_restarts 4096
zfs_max_recordsize 1048576
zfs_vdev_scrub_min_active 1
zfs_vdev_read_gap_limit 32768
zfs_arc_meta_limit 0
zfs_vdev_sync_write_max_active 10
l2arc_norw 0
zfs_arc_meta_prune 10000
metaslab_preload_enabled 1
l2arc_nocompress 0
zvol_major 230
zfs_vdev_aggregation_limit 131072
zfs_flags 0
spa_asize_inflation 24
zfs_admin_snapshot 0
l2arc_feed_secs 1
zio_taskq_batch_pct 75
zfs_sync_pass_deferred_free 2
zfs_disable_dup_eviction 0
zfs_arc_grow_retry 0
zfs_read_history_hits 0
zfs_vdev_async_write_min_active 1
zfs_vdev_async_read_max_active 3
zfs_scrub_delay 4
zfs_delay_min_dirty_percent 60
zfs_free_max_blocks 100000
zfs_vdev_cache_max 16384
zio_delay_max 30000
zfs_top_maxinflight 32
ignore_hole_birth 1
spa_slop_shift 5
zfs_vdev_write_gap_limit 4096
spa_load_verify_metadata 1
spa_load_verify_maxinflight 10000
l2arc_noprefetch 1
zfs_vdev_scheduler noop
zfs_expire_snapshot 300
zfs_sync_pass_rewrite 2
zil_replay_disable 0
zfs_nocacheflush 0
zfs_arc_max 2147483648
zfs_arc_min 0
zfs_read_chunk_size 1048576
zfs_txg_timeout 5
zfs_pd_bytes_max 52428800
l2arc_headroom_boost 200
zfs_send_corrupt_data 0
l2arc_feed_min_ms 200
zfs_arc_meta_min 0
zfs_arc_average_blocksize 8192
zfetch_array_rd_sz 1048576
zfs_autoimport_disable 1
zfs_arc_p_min_shift 0
zio_requeue_io_start_cut_in_line 1
zfs_vdev_sync_read_max_active 10
zfs_mdcomp_disable 0
zfs_arc_num_sublists_per_state 4
```
Do you need any more information? and what do you think about this setup? is my hardware undersized?",
I checked and the majority of my backups are < 200k files and < 20GBytes.,
"I hope it helps:
> user@server ~ $ cat /var/log/syslog | grep zfs
> Mar 21 04:01:11 server kernel: [204939.534145] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534234] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534339] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534439] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534539] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534648] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534752] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371676] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371764] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371869] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371969] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372069] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372177] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372282] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209034] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209123] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209228] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209329] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209429] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209537] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209641] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:07:13 server kernel: [205302.045999] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046088] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046192] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046293] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046392] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046501] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046605] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:09:14 server kernel: [205422.882950] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883039] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883144] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883244] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883343] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883452] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883556] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:11:15 server kernel: [205543.719843] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:11:15 server kernel: [205543.719933] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720037] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720137] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720237] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720345] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720449] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556609] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556698] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556802] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556903] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557003] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557112] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557216] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393394] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393483] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393587] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393687] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393787] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393896] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:15:17 server kernel: [205785.394000] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:17:18 server kernel: [205906.229973] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230062] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230166] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230289] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230389] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230498] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230602] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066643] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066732] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066837] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066937] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067037] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067146] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067250] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 07:54:33 server systemd[1]: zfs-import-cache.service: Main process exited, code=exited, status=1/FAILURE
> Mar 21 07:54:33 server systemd[1]: zfs-import-cache.service: Failed with result 'exit-code'.
> user@server ~ $ cat /etc/issue
> Ubuntu Bionic Beaver (development branch) \n \l
> > user@server ~ $ uname -a
> Linux server 4.15.0-12-generic #13-Ubuntu SMP Thu Mar 8 06:24:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
> user@server ~ $ dpkg -l zfs*
> Desired=Unknown/Install/Remove/Purge/Hold
> | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
> |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
> ||/ Name Version Architecture Description
> +++-==============-============-============-=================================
> un zfs <none> <none> (no description available)
> un zfs-dkms <none> <none> (no description available)
> un zfs-dracut <none> <none> (no description available)
> un zfs-fuse <none> <none> (no description available)
> un zfs-initramfs <none> <none> (no description available)
> un zfs-modules <none> <none> (no description available)
> ii zfs-zed 0.7.5-1ubunt amd64 OpenZFS Event Daemon
> un zfsutils <none> <none> (no description available)
> ii zfsutils-linux 0.7.5-1ubunt amd64 command-line tools to manage Open
> user@server ~ $
> user@server ~ $ df -h /data
> Filesystem Size Used Avail Use% Mounted on
> data 14T 11T 3.1T 78% /data
it was mv process from one zfs to another in the same pool (actually it behaves as cp, not faster). the process hangs forever (at least few hours) and not allow to terminate with a signal. zpool reports errors-free. Now I've started `zpool scrub`, let's see what it will report.
UPDATE:
> user@server ~ $ zpool status
> pool: data
> state: ONLINE
> scan: scrub repaired 0B in 18h22m with 0 errors on Thu Mar 22 02:32:52 2018
> config:
> > NAME STATE READ WRITE CKSUM
> data ONLINE 0 0 0
> raidz2-0 ONLINE 0 0 0
> sdf ONLINE 0 0 0
> sdg ONLINE 0 0 0
> sdh ONLINE 0 0 0
> sdi ONLINE 0 0 0
> sdj ONLINE 0 0 0
> sdk ONLINE 0 0 0
> sdl ONLINE 0 0 0
> sdm ONLINE 0 0 0
> > errors: No known data errors
Let me know if you need other infos.
With best regards, Ag",Impatience
"I have two associated models.
``` ruby
class Girl < ActiveRecord::Base
belongs_to :boy
end
class Boy < ActiveRecord::Base
has_one :girl
end
```
And I define factory girl for them:
``` ruby
factory :girl do
boy
name 'saria'
end
factory :boy do
name do
puts create boy'
'leo'
end
end
```
Then i create a girl with an exsisting boy:
``` ruby
boy = create :boy
girl = create :girl, boy: boy
```
I will get following result:
``` ruby
>>>create boy
>>>create boy
```
I am wondering why the block of name of boy runs twice.
And, if i put code that can only runs once, troubles come.
Version of my factory_girl:
- factory_girl (4.4.0)
- factory_girl_rails (4.4.1)",Bitter Frustration
"I have the same issue or question.
In my case this leads to a huge amount of time spent invoking the factory twice.
Only half of the objects are actually used, because one of the two created objects are discarded anyway.
Is there any way to prevent this from happening?
It would save a lot of time in my test suite.",
"+1, same question as this makes difficult for me to test some things and this is not documented",Impatience
"Ran into a similar problem where we had two associations attempting to create unique parent instances when we wanted them to belong to the same parent. ``` ruby
Class Term < ActiveRecord::Base
has_many :candidates
has_many :periods
end
Class Period < ActiveRecord::Base
belongs_to :term
end
Class Candidate < ActiveRecord::Base
belongs_to :term
end
```
In order to avoid needing to make multiple factory calls from the spec to reach the same term we ended up defining the factories like this:
``` ruby
factory :period do
term { Term.first || association(:term) }
end
factory :candidate do
term { Term.first || association(:term) }
end
```
This way the pre-existing Term can be referred to instead of creating two instances.",
"Calling `Thing.first` introduces non-determinism in the test suite, since records can be created at various layers without the developer even being aware. I'd recommend creating a record and assigning it directly if the intent is to share the value.",
@joshuaclayton is there any chance that you could give some practical example to how to do that?,Impatience
"@scaryguy sure thing!
``` ruby
term = create(:term)
period = create(:period, term: term)
candidate = create(:candidate, term: term)
```",
Thank you @joshuaclayton !,
"**From OP:**
```ruby
class Girl < ActiveRecord::Base
belongs_to :boy
end
class Boy < ActiveRecord::Base
has_one :girl
end
```
---
For real? Girl belongs to boy didn't feel a little gross to anyone?
Let's write examples without reducing women to objects, please, and call this stuff out when we see it.",Bitter Frustration
"@Euraldius, both `Boy` and `Girl` are ""objects"", as in Ruby objects. Let's not make such a big deal out of nothing, really, it's just an example. Nobody questioned or undermined human rights or the equality of genders.",Bitter Frustration
"Bumping this, OPs question still isn't answered. In the original post, OP is already using the suggestion [provided here](https://github.com/thoughtbot/factory_girl/issues/683#issuecomment-248977879).
The only solution I can come up with is to say (in the context of OP example)
```
factory :girl do
boy { |weird_thing| somehow_inspect_weird_thing_to_check_if_boy_was_already_provided(weird_thing) }
name 'saria'
end
```
But this doesnt seem very idiomatic.
Also, at least where I live, there is a a constant, subtle, barrage of messaging coming from the TV and other sources telling us that women are the property of men. This is insidious, and FWIW, I think we should try to be conscious not to take part. cc @linkyndy.",Bitter Frustration
"just use `.where(name: 'James').first_or_create!(foo: bar)` in the block
so you could have something like
```
Class Term < ActiveRecord::Base
has_many :candidates
has_many :periods
end
Class Period < ActiveRecord::Base
belongs_to :term
end
Class Candidate < ActiveRecord::Base
belongs_to :term
end
```
Then do something like
```
factory :period do
term { Term.where(name: 'Term 1').first_or_create! }
end
```",
"@linkyndy Do you seriously think @Euraldius did not understand that those labels referred to Ruby objects? Let me slowly explain to you the point that was being made:
When engineers use labels such as ""boy""/""girl"" in ways that reinforce toxic gender stereotypes, such engineers show their ignorance and (perhaps unconscious, perhaps latent) misogyny. So, stop. Think about how you use your language. And be considerate.
Another related example: the widespread use of repulsive ""master""/""slave"" labels in software systems.
Just fucking think about it for a second... bro.",Bitter Frustration
"https://github.com/thoughtbot/factory_bot/pull/943
https://github.com/thoughtbot/factory_bot/pull/1051
Bad example or not, the issue has been handled on this and many other levels.
We still haven't really found a great solution to the original problem, but if the only submissions here are going to be in regards to the social issues involved, then the should be locked. @Euraldius @joshuaclayton",Bitter Frustration
"I solved this problem by using `:inverse_of` on the association in both models.
```
class Belonging < ActiveRecord::Base
belongs_to :having, inverse_of: :belonging
end
```
```
class Having < ActiveRecord::Base
has_one :belonging, inverse_of: :having
end
```
And then @Manifold0 I used an after-create callback on the factory for the model without the foreign key, so they look like this:
```
factory :belonging do
having
end
```
```
factory :having do
after(:create) do |having, evaluator|
having.belonging || create(:belonging, having: having)
end
end
```
In the callback, `inverse_of` allows for the associated Belonging to be returned from memory even if it isn't saved yet, as is the case when you `create :belonging`. You can do some dynamic checking, while not relying on something like `Belonging.first`.
@Kriechi this will help your test performance by not creating extra throwaway records, and also affect your actual production performance by making fewer trips to the db. Which is the actual purpose of `inverse_of`.",
"What do you think about:
```ruby
factory :singleton do
initialize_with do
Singleton.where(
name: 'unique_name'
).first_or_initialize
end
end
```
this moves the logic to the definition of your singleton factory.",
"If you're really working with a Singleton, I'm not sure you need FactoryBot at all. If it's not really a singleton, than first_or_initialize has the non-determinism problem mentioned above. Am I missing something about what you're trying to do?",
"I feel hurt by the word Singleton because it reminds me of who I think I am.
This message is not trollish intent to hurt anyone, rather a part of a philosophical debate. (cc @skatenerd)
I wholeheartedly agree that we should fight any discrimination.",
"@mib32 sry i got a bit defensive, seeing how the original namechange discussion ... turned out",Bitter Frustration
"@drewcimino The requirement in my case was that there is only one entry with a specific name in the db table. Every reference to a `:singleton` with the same name should be the same instance. `first_or_initialize` is not atomic and could lead to race conditions. My tests run single threaded so there shouldn't be any race condition. Anyway the unique condition would be enforced on database level with a unique index, but a race condition would break the test ...",Bitter Frustration
"i can see how the girl-belongs-to-boy rubs the wrong way, linkyndy. imagine a similar example, only swap the Girl model to Black and Boy model to White.",Identify Attack/Name Calling
"@adler99 So it seems your Singleton class isn't really a singleton, but does have relatively few saved records, like some sort of reference data. If the objects of this class are _that_ standardized - that is, a record with a given name always has the same associated data - I would create them at the beginning of the test suite with a seeds-file-like script, and building some class methods for accessing them.
Something like:
```
Singleton.drewcimino
=> #<Singleton id: 1, name: ""drewcimino"">
```
as opposed to:
```
Singleton.where(name: ""drewcimino"").first_or_initialize
=> #<Singleton id: 1, name: ""drewcimino""> OR #<Singleton id: nil, name: ""drewcimino"">
```",
"Using the `term`, `period` and `candidate` example, if I add another layer, say:
```
Class Bear < ActiveRecord::Base
belongs_to :candidate
belongs_to :period
end
```
Is it possible to create the `term` inside of the `bear` FactoryBot beforehand and then get candidate and period to use the `bear` to create internally?
Probably a wrong example here but is it possible to do it inside (sometimes an object belongs to an object that belongs to an object, and if we were to manually create each object from the top and reference the next one outside of the factorybot file then it would be a mess everywhere), something like this:
```
# this is wrong but you get the idea
FactoryBot.define do
factory :bear do
# Run this first so everytime a bear gets created, a term for this bear gets created first to be used later
before(:create) { create(:term }
# Using the term created before this
association :period, factory: :period, term: term
association :candidate, factory: :candidate, term: term
end
end
```",
"@CyberMew If you want to complete the period and candidate with Bear#term after Bear/Term are created, I think an `after` block is the prescription.
```
FactoryBot.define do
factory :bear do
association :term
association :period
association :candidate
after(:build) do |bear|
bear.period.term ||= bear.term
bear.candidate.term ||= bear.term
end
end
end
```
I use `after(:build)` instead of `after(:create)` and AR methods instead of passing ids around, so this setup will work for both build(:bear) and create(:bear). The associations follow the build strategy (build/create) of the parent, so if you create they'll assign first and then automatically populate all the ids on save.
You can force build strategies if you want to, but I don't recommend it as you can easily end up with some funky saved/not-saved ""associated"" records. But there's a bit on that here: https://github.com/thoughtbot/factory_bot/blob/master/GETTING_STARTED.md#associations",
"I came here for code and got distracted by social issues. I don't have a problem with discussing our society. In fact, I'd like to know, who decides which social issues are worth discussing while working? Is it as simple as me labeling my issue an injustice - then I proceed to make my case? Currently, I have access to this comment box, so I suppose I have an equal right to say - but can someone point me to the rules?
I opted for `before(:create)`
```
FactoryBot.define do
factory :album do
name { Faker::Music::album }
year { rand(1887..Time.new.year).to_i }
before(:create) do |album|
album.artist = create(:artist)
album.genre = create(:genre)
end
end
end
```",Impatience
I believe we need `many-to-many` association between `Boy` and `Girl` classes.,
"Came here because of a frustrating engineering problem, disappointed to see people are discussing irrelevant social issues.
* Yes the social issues are important
* No github issues is probably not the right place to discuss/debate them
* Call the classes `Yob` and `Lrig`, I don't care, I need a solution to the inter-dependent factory objects",Bitter Frustration
"As a reminder, anybody participating here agrees to follow our [Code of Conduct](https://thoughtbot.com/open-source-code-of-conduct). A code example where a girl belongs to a boy does not follow our Code of Conduct and I appreciate the folks who called that out.
I'm going to lock this issue. If somebody would be willing to open a new issue with a better example and a summary of the problem and potential solutions offered so far I would be most grateful.",
"Hi all,
I am very new in scikit-learn. My questions is: how to download my own dataset (csv file).
I will be highly appreciated any answers.
Thanks.
Martin",
The documentation of sklearn is really very useful and should answer your question: http://scikit-learn.org (basically you have to put your data in numpy arrays),
"This is something that could have a bit more documentation than is in there
currently. You might find Pandas useful.
On 29 October 2014 09:45, Alexander Fabisch notifications@github.com
wrote:
> The documentation of sklearn is really very useful and should answer
> your question: http://scikit-learn.org (basically you have to put your
> data in numpy arrays)
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60845063
> .",
"@jnothman Should we reopen this issue and add a new section in the documentation? For example in this section: http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading your own data"").",
"see #2801
On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
wrote:
> @jnothman https://github.com/jnothman Should we reopen this issue and
> add a new section in the documentation? For example in this section:
> http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> your own data"").
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069
> .",
"My own dataset means the dataset that I have collected by my self, not the
standard dataset that all machine learning have in their depositories (e.g.
iris or diabetes).
I have a simple csv file and I on my desktop and I want to load it inside
scikit-learn. That will allow me to use scikit-learn properly and introduce
it to my colleges to serve our community.
I need a very simple and easy way to do so.
I will be highly appreciated any useful advice.
On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> see #2801
> > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> wrote:
> > > @jnothman https://github.com/jnothman Should we reopen this issue and
> > add a new section in the documentation? For example in this section:
> > http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> > your own data"").
> > > > â€”
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > .
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212
> .",
"See http://pandas.pydata.org/pandas-docs/stable/io.html
On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> My own dataset means the dataset that I have collected by my self, not
> the
> standard dataset that all machine learning have in their depositories
> (e.g.
> iris or diabetes).
> > I have a simple csv file and I on my desktop and I want to load it inside
> scikit-learn. That will allow me to use scikit-learn properly and
> introduce
> it to my colleges to serve our community.
> > I need a very simple and easy way to do so.
> > I will be highly appreciated any useful advice.
> > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > see #2801
> > > > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> > wrote:
> > > > > @jnothman https://github.com/jnothman Should we reopen this issue
> > > and
> > > add a new section in the documentation? For example in this section:
> > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> > > your own data"").
> > > > > > â€”
> > > Reply to this email directly or view it on GitHub
> > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > > .
> > > > â€”
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > > > .
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671
> .",
"Thanks for the link. I checked it out, but the process looks complicated.
Perhaps if there is a short youtube video explains the process much easier,
otherwise I do not know what to do to solve this matter.
On 29 October 2014 19:12, jnothman notifications@github.com wrote:
> See http://pandas.pydata.org/pandas-docs/stable/io.html
> > On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> > > My own dataset means the dataset that I have collected by my self, not
> > the
> > standard dataset that all machine learning have in their depositories
> > (e.g.
> > iris or diabetes).
> > > > I have a simple csv file and I on my desktop and I want to load it
> > inside
> > scikit-learn. That will allow me to use scikit-learn properly and
> > introduce
> > it to my colleges to serve our community.
> > > > I need a very simple and easy way to do so.
> > > > I will be highly appreciated any useful advice.
> > > > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > > > see #2801
> > > > > > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> > > > > > wrote:
> > > > > > > @jnothman https://github.com/jnothman Should we reopen this issue
> > > > and
> > > > add a new section in the documentation? For example in this section:
> > > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html
> > > > (""Loading
> > > > your own data"").
> > > > > > > > â€”
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > > > .
> > > > > > â€”
> > > Reply to this email directly or view it on GitHub
> > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > > > > .
> > > > â€”
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671>
> > > > .
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60906075
> .",Impatience
"It probably looks something like:
import pandas as pd
data = pd.read_csv(open('myfile.csv'))
target = data[target_column_name]
del data[target_column_name]
# Then fit a scikit-learn estimator
SVC().fit(data, target)
On 29 October 2014 23:19, MartinLion notifications@github.com wrote:
> Thanks for the link. I checked it out, but the process looks complicated.
> Perhaps if there is a short youtube video explains the process much
> easier,
> otherwise I do not know what to do to solve this matter.
> > On 29 October 2014 19:12, jnothman notifications@github.com wrote:
> > > See http://pandas.pydata.org/pandas-docs/stable/io.html
> > > > On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> > > > > My own dataset means the dataset that I have collected by my self, not
> > > the
> > > standard dataset that all machine learning have in their depositories
> > > (e.g.
> > > iris or diabetes).
> > > > > > I have a simple csv file and I on my desktop and I want to load it
> > > inside
> > > scikit-learn. That will allow me to use scikit-learn properly and
> > > introduce
> > > it to my colleges to serve our community.
> > > > > > I need a very simple and easy way to do so.
> > > > > > I will be highly appreciated any useful advice.
> > > > > > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > > > > > see #2801
> > > > > > > > On 29 October 2014 18:07, Alexander Fabisch <
> > > > notifications@github.com>
> > > > > > > > wrote:
> > > > > > > > > @jnothman https://github.com/jnothman Should we reopen this
> > > > > issue
> > > > > and
> > > > > add a new section in the documentation? For example in this
> > > > > section:
> > > > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html
> > > > > (""Loading
> > > > > your own data"").
> > > > > > > > > > â€”
> > > > > Reply to this email directly or view it on GitHub
> > > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > > > > .
> > > > > > > > â€”
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > > > > > .
> > > > > > â€”
> > > Reply to this email directly or view it on GitHub
> > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671>
> > > > > .
> > > > â€”
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60906075>
> > > > .
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60913843
> .",Impatience
You could also have a look at `np.genfromtxt` . Might be useful.,
"Hi jnothman, Thank you so much for your help, I really appreciate your cooperation.
I tried applying your code. Thus, once I interned (import pandas as pd). Directly I had the following message in red color:
import pandas as pd
No module named 'dateutil'
Traceback (most recent call last):
File ""<pyshell#0>"", line 1, in <module>
import pandas as pd
File ""C:\Python34\lib\site-packages\pandas__init__.py"", line 7, in <module>
from . import hashtable, tslib, lib
File ""pandas\tslib.pyx"", line 37, in init pandas.tslib (pandas\tslib.c:76813)
ImportError: No module named 'dateutil'
What should I do? Thanks a lot",
"It just means you do not have the dateutil module installed. You can install it by doing ```
sudo apt-get install python-dateutil
```
hth",
"You can have a look at this for more details, http://stackoverflow.com/questions/20853474/importerror-no-module-named-dateutil-parser",
"Thanks MechCoder for your contribution. I tried ""sudo apt-get install python-dateutil"", but it is not clear to me at what stage should indicate this code?
Do you think that there is an easy way to load my (excel or csv) file suing any simple ways such as open folder (regular way). There is another matter also which how to determine the class label that I want to predict form my dataset using scikit-learn. But anyway this step supposed to be after loading the file itself. Not easy process at all.
Is there any youtube tutorial about loading dataset (not iris which is everywhere or other famous. stuff). Video is easy than links",Bitter Frustration
"HI all,
I wrote the following code:
import pandas as pd
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
and i got this error:
File ""<ipython-input-10-dd0ba70fe93f>"", line 2
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
^
SyntaxError: invalid syntax
could you plz guide me.",
"We could tell you what the problem is but I think in this case you will learn more from it if you find it on your own. You should read the error message carefully. It is a Python syntax error.
```
File """", line 2
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
^
SyntaxError: invalid syntax
```",
"On 22 Aug 2015 08:33, ""samira afzal"" notifications@github.com wrote:
> HI all,
> I wrote the following code:
> > import pandas as pd
> data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > and i got this error:
> File """", line 2
> data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> ^
> SyntaxError: invalid syntax
> > could you plz guide me.
I recommend you finding another tool where you can work with easily without
headache yourself with this particular one. This is what I did myself.
Good luck
Martin
> â€”
> Reply to this email directly or view it on GitHub.",Mocking
"To be clear, these previous posters are saying that being somewhat
comfortable with the Python language is a prerequisite to using
scikit-learn. You have missed some quotes around a string. This shows great
unfamiliarity with Python (and a characteristic of most programming
languages), and scikit-learn is probably not the best place to start.
On 22 August 2015 at 18:04, MartinLion notifications@github.com wrote:
> On 22 Aug 2015 08:33, ""samira afzal"" notifications@github.com wrote:
> > > HI all,
> > I wrote the following code:
> > > > import pandas as pd
> > data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > > > and i got this error:
> > File """", line 2
> > data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > ^
> > SyntaxError: invalid syntax
> > > > could you plz guide me.
> > I recommend you finding another tool where you can work with easily without
> headache yourself with this particular one. This is what I did myself.
> > Good luck
> Martin
> > > â€”
> > Reply to this email directly or view it on GitHub.
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-133650026
> .",Mocking
"Just want to support @MartinLion --- I am a scikit-learn newbie and have just have spent a frustrating time going thought the docs, and I can't find anywhere how to read my own data (and not a prepared toy dataset), and what the python format of data is.",Bitter Frustration
"Kindly refer - - [How do I load my data to work with scikit-learn?](http://stackoverflow.com/q/21492726/3109769)
- [How to load data from CSV file?](http://stackoverflow.com/q/11023411/3109769)",
"> â€¢ How do I load my data to work with scikit-learn?
> â€¢ How to load data from CSV file?
We should add these in the FAQ.",Impatience
"should we instead add as a section in the tutorial below/above [""Loading an example dataset""](http://scikit-learn.org/stable/tutorial/basic/tutorial.html#loading-an-example-dataset)?",
"Also could you tag this ""Question"", ""Documentation"" and reopen it?",Impatience
"> should we instead add as a section in the tutorial here?
We should reference it. But I don't see this as tutorial material because
it is outside the scope of scikit-learn. We can only give pointers
That's an answer that the users really don't want to hear, because there
point of view is that they have a lump of data and they want it inside
scikit-learn. The answer is: this is not a problem that scikit-learn
solves, go see pandas if you have CSV, scikit-image if you have images,
database connectors (SQLAlchemy?) if you work on databases...
I guess that we should have a sentence like this in the tutorial, where
you reference, with pointers.
As a side note, the kind of errors hit by the users on the thread of this
issue (lack of basic knowledge of Python for instance) tells me that we
cannot solve their problem. They need to go to entry-level tutorials on
Python, and get a bigger picture. Maybe we should make sure that we give
pointers to these in the right spots, eg early on in the tutorial.",Bitter Frustration
"> > should we instead add as a section in the tutorial here?
> > We should reference it. But I don't see this as tutorial material because
> it is outside the scope of scikit-learn. We can only give pointers
> > That's an answer that the users really don't want to hear, because there
> point of view is that they have a lump of data and they want it inside
> scikit-learn. The answer is: this is not a problem that scikit-learn
> solves, go see pandas if you have CSV, scikit-image if you have images,
> database connectors (SQLAlchemy?) if you work on databases...
> > I guess that we should have a sentence like this in the tutorial, where
> you reference, with pointers.
> > As a side note, the kind of errors hit by the users on the thread of this
> issue (lack of basic knowledge of Python for instance) tells me that we
> cannot solve their problem. They need to go to entry-level tutorials on
> Python, and get a bigger picture. Maybe we should make sure that we give
> pointers to these in the right spots, eg early on in the tutorial.
Well, take it easy!!!
I don't know whether you are one of scikit-learn staff or not, but I need
to say that your way of talking is harming both scikit-learn staff and
users (us), due to the two reasons:
First reason, criticizing people (like what you did) and assuming that they
are novice in Python so they don't know how to work with scikit-learn,
means you or the staff are trying to blind their eyes to the truth that
scikit-learn staff are not able to create a clear tutorial to allow loading
the real data, at least. In addition, pretending that the tutorial of
scikit-learn is perfect in spite all the questions regarding loading the
real data (not the toy data as it is too easy to be imported comparing to
the real data) is something needs to be reconsidered, and this means that
scikit-learn staff don't care about the name of scikit-learn at all.
Second, we can understand from your unsuitable way of talking that you
already forgot that scikit-learn is a product, and we as users are
customers, so either you or the staff of scikit-learn should respect all of
us and thank us for any comment or bug fixing. This is the professional way
of behavior. So I recommend you to think of your words before saying them. If
you are knowing the way of loading the real data and you'd like to help,
don't only say go see pandas, better you answer people's question nicely
rather than hurting them with your words, but if you're simply not able to
do that, so keep quite.
On the other hand, regarding the question ""should we instead add as a
section in the tutorial here?"", I would like to say ""_YES_"", you or
scikit-learn staff should add a section in the official tutorial about how
to load your own data either CSV, or ARFF or text or whatever, as users are
interested to load their own data, this is very critical issue should be
considered in the tutorial (not to be ignored). *If you rely on the user,
then what is your work? *
Nevertheless, for those who are still struggling with scikit-learn, I would
like to say, this is not the end of the world, and as I mentioned
previously, find another to tool make your life much easier. For this
reason, and in order to save your time, I would like to recommend some
tools to assist you in data mining procedures. For instance, Waikato
Environment for Knowledge Analysis (WEKA),
http://www.cs.waikato.ac.nz/ml/weka/, last version is WEKA 3-7-13, is a
collection of machine learning algorithms for data mining tasks. WEKA
allows you to use its schemes either from GUI or writing Java code, so its
very easy for non-programmers. Additional to WEKA, R is also an excellent
tool for data mining stuff, you can also perform tasks of R from WEKA or
vice versa. However, if you have a patience to design a prediction process
manually (drag/drop), RapidMiner is a great tool for this propose where you
can design a very nice flow to achieve your target.
Thanks David van Leeuwen for your support.
Good luck in your analysis.
Cheers,
Martin
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-160153930
> .",Bitter Frustration
"Hey Martin,
Kindly don't be offended.
He did not criticize :) He, being one of the top contributors to scikit-learn has to make tough decisions as to what will go into our codebase and what will not, as a more verbose documentation or tutorial might not be preferable for a lot of people. Gael has in fact contributed a lot of user guides himself to scikit learn to help users.
The reason why he was opposing that addition to the tutorial was that there are multitude of ways in which users have their data stored and such a user guide on how to get the input data from all of them (a text file/csv file/database/zipped archive), is indeed out of scope for scikit learn, which is a machine learning library.
The most important thing to note here is that **it is very clearly explained by documentations of libraries which handle data, like numpy or pandas.**
It is expected from the user that he or she knows this! Since it seems to not be very clear, he suggests that we add a FAQ, pointing the user to such userguides, which are more elaborate than we could possibly get :)
It may appear that our tutorial could be a bit more elaborate on how the inputs are obtained. But the thing, in general, with userguides is that, it could _always_ be a little bit more elaborate, which makes us set a hard limit on how detailed our userguides can get, to help contain the userguide in a maintainable format :) If you think from that perspective, you yourself would understand our situation.
As this issue is open someone will indeed send a PR soon adding a nice FAQ entry and an example, maybe, which could help clarify your (or any other new user's) doubts on input formats.
Cheers!",
"Hello @MartinLion ,
we understand your eagerness to solve your problem, and your frustration when it is not solved.
However, you seem quite misinformed about what is scikit-learn, how it works, and how the project is developed. Therefore, I would like to make some points clear for you. As you can see from
http://scikit-learn.org/stable/about.html, scikit-learn is a community effort that is developed by a team of volunteers, mostly on their free time. GaÃ«l is one of the creators of the project and its current leader:
scikit-learn would certainly not be the same without his contribution (the same for other volunteers), and he certainly did not deserve your dismissive words.
What I would like to emphasize is that there is no such thing as a scikit-learn ""product"", or scikit-learn ""staff"" (only a handful of people have worked full time on the project). You mention ""we as users are
customers"", but how much are you paying for using scikit-learn? Despite the important development cost, users get scikit-learn for free (and of course that's how it's intended to be). In fact, the development of the project relies on a fragile alchemy: users' needs being a top priority for developers, and users reporting bugs and concerns in the most positive way. The kind of ""ranting"" that you wrote can be very discouraging for developers, who contribute their free time and their expertise just because they believe that scikit-learn is a useful tool for the community. Some prominent developers stopped contributing to open-source software precisely because of such ""customer-like attitude"" of a few people underlining only shortcomings, and dismissing the huge development efforts. Please try to see the bright side as well: you received advice and comments from a lot of people, I'm sure that there was something for you to learn out of it, even if it did not solve your problem. Also, although users' needs are indeed a top priority of scikit-learn (it has an amazing documentation, of which most scientific Python packages can be jealous!), each software addresses a well-targeted niche of users, and it is just normal that scikit-learn cannot fit all users. For example, it is preferable to use scikit-learn with already a good knowledge of Scientific Python. So, I'm really glad that you found a
package that suited your needs better, but please also acknowledge the time and good will that people gave away when answering you.
So, folks, let's all show some good will and keep a constructive dialog.
That's how the project we love will keep on rocking!",
"> For this reason, and in order to save your time, I would like to recommend some tools to assist you in data mining procedures. For instance, Waikato Environment for Knowledge Analysis (WEKA), http://www.cs.waikato.ac.nz/ml/weka/, last version is WEKA 3-7-13, is a collection of machine learning algorithms for data mining tasks. WEKA allows you to use its schemes either from GUI or writing Java code, so its very easy for non-programmers. Additional to WEKA, R is also an excellent tool for data mining stuff, you can also perform tasks of R from WEKA or vice versa. However, if you have a patience to design a prediction process manually (drag/drop), RapidMiner is a great tool for this propose where you can design a very nice flow to achieve your target.
Maybe we should make clear that scikit-learn is a Python **library**. It does not have the same scope as WEKA or RapidMiner. It fits perfectly into the [scientific Python ecosystem](http://www.scipy-lectures.org/) but you should be willing to write code if you want to use it.",
"Perhaps I should elaborate on my original frustration, to give you some context. I've been programming in Python almost exclusively for a year now (I am a late convert), and am fairly familiar with the ecosystem---I've done lot's of webservice related things, but also manipulation of resources related to automatic speech recognition. I do my scientific work in [Julia](http://julialang.org/) since a couple of years, and before that, in R, octave, c++/c (some 30 years in total). The Julia ecosystem is quite dynamic, and it is all very exciting, but Python just has this very large ecosystem and very clean coding, which makes it very attractive to use for little side experiments. This time I had to do some topic classification of (single sentence) text documents. Now there is an abundant choice of language technology tools in Python, and I believe that via [lda](https://pythonhosted.org/lda/index.html) I got to scikit-learn. Great tutorials, lovely datasets and all, but I found it very difficult to find out how to organize my own data so that I could load this in. Just now, I browsed through the user guide again to find the docs for ""load_files"", but I could't find an entry. So a google search for ""sklearn.datasets.load_files"" got me there just now, and I happened to remember the particular module path from more painstaking searches yesterday (it is mentioned somewhere in a tutorial). For me, the essential information would have been: ""Organize your data one document per file, one directory per class""---more or less what's under the documentation for [load_files](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html). This all makes perfect sense, but I come from a community where usual formats are ""one datapoint per line"", often with the class label on that line. But having said all this, I am pretty impressed how the Python (text) community has standardized data representation, from what I've seen so far. But perhaps because of the widely used standard data representation, this aspect has naturally less attention in documentation. As a final note, whenever I try to teach students how to use some scientific tool set or another, I have to spend quite some time on ""how to import your data"". Nobody likes to do it, it can be a lot of effort for what you potentially use only once, and is therefore always a difficult threshold.",Bitter Frustration
"@davidavdav I agree that loading data is a difficult and important thing. However, it is a domain-specific problem. You have a particular type of data. I have another. My data is medical images of brain activity. I can tell you how I organize my data and load them. I can even tell you that we have written a whole package about this, with its own documentation. But that will probably not help you.
What you want is something that tells you how to organize and load _your_ data. Now, it may be that your data is something fairly classic, that many people have; for instance tabular data most often stored in CSV files. In which case there is a need for a package doing this data loading. I don't believe that it should be in scikit-learn. It needs to be in a package that is specialized for this data. For instance, we are not going to put image processing in scikit-learn. For tabular data, the dedicated package is pandas; as I mentioned in my reply we need to point to it. We, the scikit-learn team, want to make plugin pandas into scikit-learn easier. But it is not as easy as it may seem and it takes time (one of our core devs is prototyping something).
I realize rereading your post that your data is most likely text documents. So my two examples of data (medical images and tabular data) were both wrong :$. Maybe the documentation on processing text with scikit-learn could indeed be improved and touch a bit on data organization. I don't know, I very seldom process text. But if you want to do add a few words on this, you are most welcomed to do a pull request. Anyhow, this illustrate my point about the diversity of the data: this whole thread is mostly about loading CSV files, as can be seen from earlier comments (before the thread exploded into a rant). The important thing is not the ""CSV"", which is the container, but the data model that underlies a CSV file. This data model is that of columns of data with different nature. It's a very different data model than processing text documents.
And finally, you are unhappy that teaching people ""how to import your data"" is time consuming. I don't think that there is an easy fix for this, even in a specific domain. The reason being that data meaning (ie data semantics) is still very much an open area. It's intrinsically hard to describe what the data means and how it's organized. You can try a simple experience: grab a dataset from someone you don't know, about an experiment you don't know, and try understanding it. Not even loading it, just understanding it. I am sure that it will take time. What takes a human time tends to be very difficult for a computer.",
Hm I don't think we added pointers to the FAQ yet. It's certainly a FAQ.,Irony
I wrote a short tutorial on how to get the dataset from a text format to a pandas DataFrame for use by sklearn here http://cis399-he.tumblr.com/post/151024047044/load-your-data-to-scikit-learn,
fixed in #7516,
"Well you could add some information about common errors that happen when loading own data. For example I have bumped on `Unknown label type: 'continuous-multioutput'` and not a hint, anywhere, what that could mean. I've tried passing:
- a numpy array created from csv reader
- a numpy array created from pandas
- regular array parsed line by line
- pandas dataframe
- a numpy array created from csv reader converted to regular array
- etc...
I'd recommend adding section summarizing assumptions about data, for example it was sheer luck that I found information that data sets may not contain NaN's Nulls etc. Also, what should be the parameters of a numpy array and how should pandas dataframe look like.",Bitter Frustration
"Perhaps that error message could be clearer, but I think passing regression targets to a classifier (as in your #7801) is a usage error nothing to do with loading your own dataset.",Impatience
"@MartinLion @MartinLion Thank you to brought up this issue.
I am facing your problem now. I want to upload my IRIS like dataset. Which is 500 rows and 16 columns.
and
Thank you @chenhe95 ,
I have seen your tutorial titled ""Can I go to the bathroom"" , that is a real work , real help to users like us.
And Thank you all developers,
You worked free (or almost free) to deliver this library to people. I appreciate that. @MartinLion has spirit of learning, I praise that.
Actually stubbornness is a virtue in academic.",
"@tursunwali,
Thanks for the kind words.
Please let me know if you still have any issues with your data or learning
process.
Regards,
Martin
On 20 November 2016 at 12:47, tursunwali notifications@github.com wrote:
> @MartinLion https://github.com/MartinLion @MartinLion
> https://github.com/MartinLion
> > Thank you to brought up this issue.
> I am facing your problem now. I want to upload my IRIS like dataset. Which
> is 500 rows and 16 columns.
> and
> Thank you @chenhe95 https://github.com/chenhe95 ,
> I have seen your tutorial titled ""Can I go to the bathroom"" , that is a
> real work , real help to users like us.
> > And Thank you all developers,
> You worked free (or almost free) to deliver this library to people. I
> appreciate that.
> > @MartinLion https://github.com/MartinLion has spirit of learning, I
> praise that.
> Actually stubbornness is a virtue in academic.
> > â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-261758848,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AI_3_7E0UU8Os42HXUC5n2eLVYW6hqe6ks5q_9DngaJpZM4Cz90f
> .",
"When relying on vagrant to download a box I frequently see connection speeds like this:
```
default: Downloading: http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
default: Progress: 20% (Rate: 179k/s, Estimated time remaining: 0:41:37)
```
(Rate: **179k/s**)
Yet when I use wget to the same URL:
```
wget http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
--2015-02-10 09:52:12-- http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
Resolving boxes.example.com... 10.1.0.17
Connecting to boxes.example.com|10.1.0.17|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 830674320 (792M) [text/plain]
Saving to: 'packer_c6_2.5.2_virtualbox.box'
packer_c6_2.5.2_virtualbox.bo 0%[ ] 7.12M 696KB/s eta 19m 50s
```
(Rate: **696KB/s**) or often higher.
This particular example was pulled when on Wifi and connected to an IPSEC VPN.",Bitter Frustration
"Hi @spkane Some boxes are hosted on Atlas and sometimes Atlas is just acting as a proxy to a user-hosted box. If you give more information on the specific box(es) you're downloading, we can do some research.",
@sethvargo This box is actually a box I built using packer and it is hosted on a remote server. I'm trying to understand why the download in significantly slower using vagrant then using wget to the exact same URL.,Impatience
"@spkane sorry - I misread your original issue.
I would suspect (and maybe @mitchellh could elaborate more) a few things:
1. Ruby is slow and somehow throttling the subprocess
2. Wget is faster than curl (which is what Vagrant is using)
3. Vagrant is also allocating time to unpack the box
4. Wget is allowing for some type of compressed download
It would be helpful if you could benchmark this with curl for reference.",
"I really can't explain this. Vagrant doesn't do anything during the subprocess Ruby-wise: it subprocesses to `curl`. It doesn't even do the download in Ruby. Perhaps wget is using multiple connections to download multiple parts? I really don't know, but unless we get more information I have to assume that Vagrant is fine here. Is `curl` just as slow? Vagrant is just subprocessing to curl until it completes.",Bitter Frustration
"I'm experience the same slow experience. Anyone can try aria - http://aria2.sourceforge.net/ and http://stackoverflow.com/questions/3430810/wget-download-with-multiple-simultaneous-connections
It's seems a little bit faster, but, man, you can set this up using default vagrant download mechanism and take a walk or make yourself a sandwich. Get way from screen for a little bit.",Impatience
"Having the same problem here:
1. Upload a box manually to atlas
2. Create a new Vagrantfile with just `vm_cfg.vm.box_url = <user>/box-name`
3. `vagrant up` - box downloads slowly
4. wget box url from atlas (see `vagrant up` output) - box downloads lightening fast",Impatience
"I wish there was just a +1 for this. Me too. Same connection for all 3 attempts. VPN turned off.
- `vagrant up` took 25+ minutes.
- `wget` took 3 minutes.
- `curl` took 4 minutes.",Bitter Frustration
"Ubuntu vivid64 is downloading at ~56kbps. I'm on a 100mbit symmetric connection.
edit: it timed out before it could finish.
edit2: I can confirm that https://atlas.hashicorp.com/ubuntu/boxes/vivid64/versions/20160128.0.0/providers/virtualbox.box downloads dramatically faster over wget than via ""vagrant up"".",Impatience
"I'm trying to download the scotch/box and current download speeds using vagrant are less than 10kbps.
default: Progress: 0% (Rate: 2603/s, Estimated time remaining: 33:17:38)
However just as bad using wget.",Impatience
ditto; some popular boxes are very slow to download - i'm updating ubuntu/trusty64 as we speak and it's dropping below 1Kb/s. Been seeing this for a couple wks now.,Bitter Frustration
+1 -- exact same as last comment,Impatience
"Same here:
```
$ vagrant box add lazygray/heroku-cedar-14
==> box: Loading metadata for box 'lazygray/heroku-cedar-14'
box: URL: https://atlas.hashicorp.com/lazygray/heroku-cedar-14
==> box: Adding box 'lazygray/heroku-cedar-14' (v1.0.6) for provider: virtualbox
box: Downloading: https://atlas.hashicorp.com/lazygray/boxes/heroku-cedar-14/versions/1.0.6/providers/virtualbox.box
==> box: Box download is resuming from prior download progress
box: Progress: 3% (Rate: 281k/s, ...
```",
"same here
```
vagrant box update
==> default: Checking for updates to 'laravel/homestead'
default: Latest installed version: 0.4.1
default: Version constraints: >= 0
default: Provider: vmware_desktop
==> default: Updating 'laravel/homestead' with provider 'vmware_desktop' from version
==> default: '0.4.1' to '0.4.2'...
==> default: Loading metadata for box 'https://atlas.hashicorp.com/laravel/homestead'
==> default: Adding box 'laravel/homestead' (v0.4.2) for provider: vmware_desktop
default: Downloading: https://atlas.hashicorp.com/laravel/boxes/homestead/versions/0.4.2/providers/vmware_desktop.box
default: Progress: 0% (Rate: 42210/s, Estimated time remaining: 6:10:54))
```",
Is there any way to use something like axel to stream downloads in quicker?,Impatience
"I guess there's nothing preventing people from sharing boxes via torrent. For example, below is a magnet link for the heroku-cedar-14 box:
> magnet:?xt=urn:btih:5bb1480d5316f229bb71be55b56b06278de41a67&dn=heroku-cedar-14.box&tr=http%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=http%3A%2F%2Fannounce.torrentsmd.com%3A6969%2Fannounce&tr=http%3A%2F%2Fbt.careland.com.cn%3A6969%2Fannounce&tr=http%3A%2F%2Fexplodie.org%3A6969%2Fannounce&tr=http%3A%2F%2Fmgtracker.org%3A2710%2Fannounce&tr=http%3A%2F%2Ftracker.tfile.me%2Fannounce&tr=http%3A%2F%2Ftracker.torrenty.org%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.trackerfix.com%2Fannounce&tr=http%3A%2F%2Fwww.mvgroup.org%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.me%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2710%2Fannounce&tr=udp%3A%2F%2Fcoppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Fexodus.desync.com%3A6969%2Fannounce&tr=udp%3A%2F%2Fglotorrents.pw%3A6969%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.glotorrents.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker4.piratux.com%3A6969%2Fannounce
Anyone know a good website where one can search for torrents of vagrant boxes?",
"@wkretzsch - I personally don't know at the moment about any torrent sites - but for me I wouldn't want to trust torrent links as the source for my infrastructure testing. It's a possible option but security is also important. For me official vagrant boxes from folks like puppetlabs hosted on Atlas are so slow to download at times that I wish this issue could be resolved. For internal vagrant boxes that I build for my company we have the option to host on S3 or Artifactory or private Atlas org.
@mitchellh - yes - curl is just as slow (for me). I don't think it is a Vagrant issue - but a backed server hosting issue. Granted - not a Vagrant issue per se.",Bitter Frustration
"![screenshot from 2016-03-17 14-04-58](https://cloud.githubusercontent.com/assets/7142025/13838783/486408ba-ec49-11e5-9903-19cc1e031395.png)
Yes, this is because curl can only use one of my 3 connections at the same time. No, that's not the connection's rated speed. The rated speed is 45mbps. Yes, bittorrent does perform better. Just sayin-- your rationale for not supporting bittorrent is kinda thin here.",Bitter Frustration
@tehmaspc surely there must be a way for a website to publish the hash of their box along with a torrent link?,Impatience
"I wish in general, there was a way to have incremental images, like docker images, with vagrant boxes. For the provisioners, which bootstrap (cfengine, chef, salt, puppet, docker, etc) by downloading their platform, I wish there was a way to download a packaged up installer, so that other fresh images that use that provisioner, e.g. ubuntu + docker, would not need to download the goods again. Box updates and provisioner downloads were already painful, but recently, have been beyond notoriously slow.",Bitter Frustration
"Just went to update my box for the first time (trusty64 - noticed the warning on my vagrant up command output), and it's going to take my 1.5 hours on a 150MBps connection - pathetic. It's 2016 - I don't know the specifics of what's going on here, but surely we can fix this, like, by the end of next week? The tech that goes into modern technologies like vagrant is amazing, something this basic should be overcome in mere hours.",Bitter Frustration
"Amen, Matt, Amen. This is about UX.
There should be a recognition that line speed != line speed and practical
steps can be taken to overcome the daunting issue of line speed != line
speed.
Jacob Gadikian
E-mail: faddat@gmail.com
SKYPE: faddat
Phone/SMS: +84 167 789 6421
On Sun, Apr 10, 2016 at 6:32 AM, Matt Porter notifications@github.com
wrote:
> Just went to update my box for the first time (noticed the warning on my
> vagrant up command output), and it's going to take my 1.5 hours on a
> 150MBps connection - pathetic. It's 2016 - I don't know the specifics of
> what's going on here, but surely we can fix this, like, by the end of next
> week? The tech that goes into modern technologies like vagrant is amazing,
> something this basic should be overcome in mere hours.
> > â€”
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/mitchellh/vagrant/issues/5319#issuecomment-207881297",
"I just tried asking Vagrant to download ubuntu/trusty64, and was getting speeds of <= 5 KiB/sec. I killed it and tried again using the exact same command, and got 29 MiB/sec. I think @mitchellh is correct in that this doesn't really seem like a Vagrant issue. If anything, it seems more like an Atlas issue (so possibly the ELB and/or whatever's sitting behind it). I highly doubt it has anything to do with the routes or hops between end-users and the ELB VIPs -- you wouldn't typically see such a polarizing set of speeds in that case, especially considering both VIPs terminate in us-east-1.
If for no other reason, it'd be highly desirable to see these made available through a CDN rather than a centrally-located ELB. Then again, I'm just one guy (who isn't paying for this service), so take that for what it's worth. Pretty thankful it's there either way.",Bitter Frustration
"It's not just an Atlas issue. I have boxes and metadata.json on S3, with a Fastly CDN in front and regularly have the exact same issue: sometimes vagrant downloads at 100kbps and sometimes it downloads at > 5mbps. You can cancel a slow download and half the time a retry gets you the faster speeds.",Bitter Frustration
"I contacted support about this around the same time I chimed in here initially. Their response is that Vagrant uses curl to download things so they don't see this as a Vagrant problem. IMO that's an unprofessional cop-out because they chose to use curl, know that there are problems and aren't considering swapping out with an alternative to eliminate the problem for their users.",Bitter Frustration
"I can confirm that this is still an issue. All my peers also report times of >1h, while the connection here for other connections is around 200MB/s.
```
vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'ubuntu/trusty32' could not be found. Attempting to find and install...
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'ubuntu/trusty32'
default: URL: https://atlas.hashicorp.com/ubuntu/trusty32
==> default: Adding box 'ubuntu/trusty32' (v20160406.0.0) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty32/versions/20160406.0.0/providers/virtualbox.box
default: Progress: 11% (Rate: 43801/s, Estimated time remaining: 1:36:50)
```",Impatience
"While I am unsure of the origin of the problem, I really do wish that Hashicorp would get back to its unrelenting focus on user experience with this one. **Muli-hour downloads (that should take 1-10 minutes)==bad ux.**",Bitter Frustration
"Currently downloading an image for the 5th time (@13Xk/s, even with `wget`). Keep disconnecting me while around 50-90%. But it ALWAYS downloads at full speed either early morning / late night EST. Assuming it is a traffic issue, but regardless very bad UX.
```
box: Progress: 47% (Rate: 106k/s, Estimated time remaining: 0:14:50)
```",Bitter Frustration
I have been trying for 2 day's now and still can not get it to download... its a shame.. it is really not impressing new comers to laravel .. i can only get 34ks speed.........,Bitter Frustration
"Speeds ok from the UK:
```
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'bento/centos-7.2' could not be found. Attempting to find and install...
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'bento/centos-7.2'
default: URL: https://atlas.hashicorp.com/bento/centos-7.2
==> default: Adding box 'bento/centos-7.2' (v2.2.6) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/bento/boxes/centos-7.2/versions/2.2.6/providers/virtualbox.box
default: Progress: 11% (Rate: 7728k/s, Estimated time remaining: 0:01:19)
```
What is your location?",Impatience
"Also, https://atlas.hashicorp.com/ URL's are delivered from Amazon Web Services (atlas-frontend-atlas-230110478.us-east-1.elb.amazonaws.com) so I doubt they are tight for bandwidth ;-)
Are the slow downloads being made from locations a long distance away from the AWS us-east-1 DC, perhaps thats the root cause of the issue?
Maybe the AWS CDN could be used to cache files around the world?",Irony
"I'm located in Vermont, which is pretty us-east-1 last I checked :dart:",Irony
I am in Brazil.... got it to download.... 10m connection here took 4.6 hours!!!!! my wife just gave birth to our 8th little girl.. It only took her 40 minutes !!!!! lol There is a big problem with there download!!!!!,Bitter Frustration
"Same here, I have a 50Mbps connection...
`
default: Progress: 44% (Rate: 102k/s, Estimated time remaining: 0:15:01))
`",Impatience
"Sign me upp here, 100mb symmetric connection (Fiber) sloooow as shit, doing 150kb/s",Bitter Frustration
"after looking at the years of complaints of slow download with no effort of resolving the issue,,, i think its time to start emailing Laravel to stop endorsing homestead until the issue is resolved..... maybe that will get their attention!!!! this is a real problem... 15 retries and then 4.6 hours to download a file is irresponsible on their part........",Bitter Frustration
"I bet it gets closed, but if you don't ask you don't get:
https://github.com/mitchellh/vagrant/issues/7307",Impatience
Just snagged a box at 85mbit. You all fix something recently? Much better than it used to be.,Impatience
"This is painful to do anything on any more - On 100mbps synchronous connection and getting 168kb, either overloaded servers or throttling",Bitter Frustration
"In looking to debug curl being slow -- I found a [stackoverflow post ](http://stackoverflow.com/questions/30984641/debugging-slow-download-with-curl) that suggests that --trace-ascii /dev/null makes your curl go at the speed you'd expect. For me, I'm trying to download [CentosOS 7](http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box) and here are my results:
NO --trace-ascii option:
```
$ curl http://cloud.centos.org/centos/nt/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
0 483M 0 489k 0 0 77724 0 1:48:44 0:00:06 1:48:38 69721
```
With trace-ascii the first time:
```
$ curl http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box --trace-ascii /dev/null
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
5 483M 5 24.5M 0 0 4259k 0 0:01:56 0:00:05 0:01:51 4599k
```
Does anyone else see the same behavior?",
"The download is extremely slow on my end too. I'm trying vagrant for the very first time. Might ditch this software and go back to my native apache2 instead.
<img width=""913"" alt=""screen shot 2016-08-17 at 12 31 40 pm"" src=""https://cloud.githubusercontent.com/assets/4960876/17724673/926a8510-6476-11e6-82ab-fb7276c448c4.png"">",Bitter Frustration
Help. I have same problem. I can't wait 3 hours! Very slow! Stupid!,Bitter Frustration
i gave up a year ago..download to slow.. problems after down load.. have to download for 3 hours again.... Vagrant will not fix the problem that has been there for several years now.. you would think that after 3 or 4 years of this problem they would address the issue.....,Bitter Frustration
8 hours to download! I hate you all!,Bitter Frustration
"Lol, I hate you too @daryn-k :)",Mocking
Guys why is this issue closed? This is still an outstanding issue and needs to be addressed ASAP. I am experiencing the same issue.,Impatience
Wow! Downloading boxes is painful please fix this. PLEASE?,Impatience
"I think it really has to do with time of day, traffic, alignment of the planets, etc. I haven't had slow speeds in a while. It seems very hit or miss. In fact, if it is slow you and start and stop it with the chance of getting a better connection. I'm not sure this is really the fault of the vagrant framework as much as it is the nature of large bottlenecked downloads.",
"@mitchellh These errant speed symptoms from hashicorp's servers could be indicative of bumping up against AWS's IOPS credits for GP2 filesystems. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#IOcredit
We had some testing infrastructure on drupal.org that would run fine for a long time, then suddenly drop to a crawl because we had ""spent"" all of our IO credits. It could be possible that hashicorps' servers are bumping up against the same limit. `sar -b` could give some insight as to whether or not this explains the random performance drops.",
It could be an idea for Hashicorp to move the images for download into S3 and use that for downloads... that would save on running instances specifically for downloads.,
"Downloading boxes used to be quick, now it's so slow it makes vagrant a no-go for quick and simple developer environments.",Bitter Frustration
"Trying to download ubuntu/xenial64. Download speed maxes out at 150 KB/s on a 1 Gbps symmetrical fiber connection. WTF. Remaining time 1 hour? I could probably download the ISO, read the guide on how to set up my own box, and finish earlier.
EDIT: Interestingly, speed went up by factor 10 when I tried to download the same box in the browser simultaneously.",Bitter Frustration
">use latest devops tools to speed things up
>spend days watching max 420k/s download speeds",Bitter Frustration
"Same here, I have a 30Mbps connection
> default: Adding box 'ubuntu/trusty64' (v20170313.0.5) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty64/versions/20170313.0.5/providers/virtualbox.box
default: Box download is resuming from prior download progress
default: Progress: 0% (Rate: 80568/s, Estimated time remaining: 1:26:22)",Impatience
"@DeadlySystem We have the same experience, when I download the same box (url) using `curl` from the commandline (during the `vagrant up`)",
"Any update on this, fetching box from Hashicorp is painfully slow.
![screenshot_2017-04-02_21-08-15](https://cloud.githubusercontent.com/assets/1684989/24587412/8f0329d8-17e8-11e7-9743-f793458b7daf.png)",Bitter Frustration
"Hey, quick thought:
If this uses curl (not libcurl) through some sort of ruby-controlled, bash-mediated process, why not just remove curl for one of:
* ipfs
* aria2
Both would do the job better than curl.",
"It honestly looks like they dont give a shit, rules this out as an option
for me!
On 3 Apr 2017 8:15 AM, ""Jacob Gadikian"" <notifications@github.com> wrote:
> Hey, quick thought:
>
> If this uses curl (not libcurl) through some sort of ruby-controlled,
> bash-mediated process, why not just remove curl for one of:
>
> - ipfs
> - aria2
>
> Both would do the job better than curl.
>
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mitchellh/vagrant/issues/5319#issuecomment-291019093>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABBAihR9ng4t2Jq1XTmAjMyMCnlEFtxRks5rsB4WgaJpZM4Deq5d>
> .
>",Bitter Frustration
"I'm on a 150Mbps line.
vagrant up = HOURS
vagrant box add = HOURS
browser download /wget = HOURS
May not be a vagrant issue per se, BUT IT IS. If your infrastructure can't handle it then your product is broken.
BAD UX",Bitter Frustration
"I opened a ticket about customizing the download tool, but it got rejected as too complicated to impliment ;-(
That said, my recent download speeds have been ok from the UK for a while. This example downloaded just now:
```
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'bento/ubuntu-16.04'
default: URL: https://atlas.hashicorp.com/bento/ubuntu-16.04
==> default: Adding box 'bento/ubuntu-16.04' (v2.3.4) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/bento/boxes/ubuntu-16.04/versions/2.3.4/providers/virtualbox.box
default: Progress: 12% (Rate: 8940k/s, Estimated time remaining: 0:01:06)
```
Maybe you guys are just too far from their AWS instances for a good download speed?
All their download servers look to be in New York with no CDN to distribute content.
```
$ dig atlas.hashicorp.com
; <<>> DiG 9.10.3-P4-Ubuntu <<>> atlas.hashicorp.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 44169
;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1
;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4000
;; QUESTION SECTION:
;atlas.hashicorp.com. IN A
;; ANSWER SECTION:
atlas.hashicorp.com. 120 IN CNAME atlas.hashi.co.
atlas.hashi.co. 60 IN A 52.206.86.0
atlas.hashi.co. 60 IN A 52.200.255.5
atlas.hashi.co. 60 IN A 52.55.203.197
;; Query time: 42 msec
;; SERVER: 10.1.1.14#53(10.1.1.14)
;; WHEN: Mon Apr 24 09:20:24 DST 2017
;; MSG SIZE rcvd: 124
```
http://geoiplookup.net/ip/52.55.203.197
http://geoiplookup.net/ip/52.206.86.0
http://geoiplookup.net/ip/52.200.255.5
Maybe its worth the people getting slow downloads detailing what ISP they are using? Maybe you are all on an ISP with a high contention ratio?",Impatience
https://github.com/mitchellh/vagrant/issues/8434#issuecomment-291979521,Bitter Frustration
"> Maybe its worth the people getting slow downloads detailing what ISP they are using?
Any ISP, in any state I've traveled to in the last couple years.
This problem is squarely on whatever Hashicorp is doing for hosting, and that's where it needs to be fixed. If they're unable or unwilling to fix it, a torrent solution would certainly help without taking their resources aside from development for adding a torrent client to the tool. If all this stuff is on S3 anyway, AWS provides torrent seeding out of the box.",Bitter Frustration
"I was getting painfully slow speeds and so I decided to see if upgrading Vagrant would change anything. Before I updated I was getting speeds of around 50-200kbps. After the update I was using the full 70mbps of my connection.
So for those of you that have slow speeds, try updating to v1.9.4 if you aren't already running it.",
"Hi, I'm on the latest Vagrant 1.9.4 and the download speed is so slow that keeps disconnect. No way to download latest laravel homestead 2.1.0 box...
When I downloaded the 2.0 with Vagrant 1.9.3 (or previous, can't remember) it was flawless",Bitter Frustration
"same issue here, very slow download rates
Rate: 35033/s (keeps jumping between 20000 and 60000...)
Estimated time remaining: 9:26:13 :(
(on vagrant 1.9.5)",Impatience
Now it's fast again... really depends on time of day and luck (as stated above...).,Impatience
"I'm having this same problem when trying to download Bento boxes. I'm using Vagrant 1.9.5 with 5.1.22. I've tried several times, during various days. My normal download speed is 1.7mb~ but when dowloading boxes I can't get pass 40kb.
If I try downloading with wget I get the same low speed.",Bitter Frustration
"Maybe the move will help:
https://www.vagrantup.com/docs/vagrant-cloud/vagrant-cloud-migration.html",
"Gonna try again tonight to see if the migration helped. Anyway I noted that with a different ISP the download rates were fine, so I guess it's not a problem with Vagrant itself (like many users already reported).",Impatience
"Well, nothing has changed, it still downloads at a snail's pace given that i am on a 125 Mbps connection!",Bitter Frustration
Sooooooooooooooo slowlyï¼ï¼ï¼ï¼:(,Impatience
"create a new issue, this has been closed for ages",Impatience
"i just downloaded latest scotch-box vagrant box, spent ~1hour for that, and then i found an issue which could be fixed only by downgrading to previous version of box, so i'm here again downloading second box for next hour of my working time. So.. it's good since i got per-time payment, but on the other hand probably i'll have some problems explaining why i spen last day downloading stuff from the internet",Bitter Frustration
I'm also facing the slow download issue.,Impatience
"Try downloading from a wired connection, it's exponentially faster!
On 4 September 2017 at 17:02, Omar Tariq <notifications@github.com> wrote:
> I'm also facing the slow download issue.
>
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mitchellh/vagrant/issues/5319#issuecomment-326939886>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABmbHxWHtAnCnEw34FB4oXX8lTQrOsNRks5se9-_gaJpZM4Deq5d>
> .
>
-- Best regards,
Shivaprabhu Wali.",
"Typical issue, been around for ages, everybody's moaning about it, and nothing is being done.",Bitter Frustration
"Came here via Google and this might help some people:
When I run `vagrant init [box] && vagrant up`, it loads super slow.
But when I run `vagrant box add [box]`, it's faster.
Going from 120 kb/s to >1000 kb/s.",Impatience
@PascalAOMS Thanks for the tip. At least it worked for me.,
"Today I've tried [aria2](https://aria2.github.io/) with 16 simultaneous connections to download laravel/homestead managed to get up to 9.6MiB
```
Download Results:
gid |stat|avg speed |path/URI
======+====+===========+=======================================================
80fa38|ERR | 7.8MiB/s|C:/development/homestead/47dce273-9892-4691-a746-c4f351ae44a5
```",
From last two days i'm trying to download vigrant box but every time i try to download it gives me error and downloading spedd is very slow. I also tried to install it manually but it also didn't worked.For newbie to Laravel it pretty frustrating and i'm still not able to download it.Don't know what should i do now.,Bitter Frustration
"I'm seeing the same problem. Running latest Vagrant 2.0.0. When downloading a box (configured using external URL on Vagrant Cloud, so it's actually redirecting to Rackspace Cloud Files) it's painfully slow. If I use cURL or Chrome to download the same URL (https://vagrantcloud.com/joomlatools/boxes/box/versions/1.5.0/providers/virtualbox.box) from the same machine it only takes a couple of seconds to complete the download. What could be the issue here?",Impatience
"Using terminal to add a box is extremely slow, more than 1 hour with 100kb/s. If I use a browser to directly download the box it is much faster, just a few minutes with 600kb/s. What's the reason for having such a huge difference? Seems as adding the box through terminal won't use all the available bandwidth.",Impatience
"I am trying to download boxes more quickly, and following the advice of some of the above comments, I used [aria2](https://aria2.github.io/) through [homebrew](http://brewformulas.org/Aria2) to download much faster in parallel. Sample command (url was from original box add attempt from Vagrant):
$ aria2c -x16 https://vagrantcloud.com/ubuntu/boxes/xenial64/versions/20171118.0.0/providers/virtualbox.box
My speed was around 100k/s with Vagrant's download, up to 1 MB/s with aria2.
Then when you finish downloading that, you [can add the box using](https://stackoverflow.com/a/26655618/532524):
$ vagrant box add ubuntu/xenial64 ../xenial64.box
I think you can remove the downloaded box after this since it will be copied to [the standard box storage path](https://stackoverflow.com/a/10226134/532524) after adding it. Hope this helps save someone else some time.",
"@panozzaj's comment above doesn't quite work if a `Vagrantfile` expects a specific version of a box (that command will add a box without a version). Instead, you can do the below.
# Steps to get a specific box at a specific version without using `vagrant` to perform the download
I'm using laravel/homestead in this example because it's what I was trying to get.
1. Download the box using a better download client (e.g. your browser, curl, wget, whatever).
2. Create a new json file (anywhere), add this to it (note, tweak the `name`, `version` and `url` keys to match the box you want, don't worry about the `checksum` key yet. For `url`, use the path to the file you just downloaded, example below):
```
{
""name"": ""laravel/homestead"",
""description"": ""Whatever you want"",
""versions"": [{
""version"": ""4.0.0"",
""providers"": [{
""name"": ""virtualbox"",
""url"": ""file://c:/users/madmatt/Downloads/47dce273-9892-4691-a746-c4f351ae44a5"",
""checksum_type"": ""sha1"",
""checksum"": ""abc123""
}]
}]
}
```
3. In your `Vagrantfile`, add the following lines `vm.box` and `vm.box_url` keys):
```
Vagrant.configure(""2"") do |config|
config.vm.box = ""laravel/homestead""
config.vm.box_url = ""file://c:/users/madmatt/path/to/the/json-file-you-created-in-step-2.json""
end
```
4. Run `vagrant up` as normal.
5. `vagrant` will complain that the sha hash doesn't match (`The checksum of the downloaded box did not match...`). Take the string that appears next to 'Expected', copy and paste that into the `checksum` key of the json file you created in step 2.
6. Run `vagrant up` again, this time it should load from the local file, store it as the correct name and version, and successfully run it.
Hopefully someone finds these steps useful... funnily enough I have done all this research (having never used vagrant before), have tested it a bunch of times, and the original `vagrant box add laravel/homestead` command that I started running 3 hours ago is still only 8% complete, even though in that time I've downloaded the box file 8 times outside of vagrant.
The rest of this is just my experience, no need to read further ;)
# My experience (aka. why is vagrant so slow to download boxes)
Was getting 420b/sec (yes, that's bytes per second) on a gigabit connection downloading https://vagrantcloud.com/laravel/boxes/homestead/versions/4.0.0/providers/virtualbox.box.
I downloaded the same file via browser, curl and wget with speeds varying between 12 and 27MB/sec. I then tried doing both at the same time - I was able to download via Chrome, Firefox, curl and wget before ```vagrant box add laravel/homestead``` had downloaded 1%.
The URL I got in the browser was ```https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/47dce273-9892-4691-a746-c4f351ae44a5?X-Amz-Algorithm=<snip>&X-Amz-Credential=<snip>&X-Amz-Date=<snip>&X-Amz-Expires=<snip>&X-Amz-SignedHeaders=<snip>&X-Amz-Signature=<snip>```
I don't know what the problem here is, but I can think of a couple:
* Whatever UA vagrant uses is bad, and AWS severely limits it
* Whatever vagrant uses to download isn't following redirects, or somehow never ends up downloading from AWS infrastructure, instead using some other terribly overloaded server/proxy",Bitter Frustration
how is this closed? still a major issue,Bitter Frustration
"Perhaps it's time to fork. This project is core to a lot of development environments, leaving us all subject to the whims of HashiCorp... and on this issue we seemingly cannot even get a reasonable official response.
Vagrant is MIT-licensed. Boxes could easily be distributed via torrent, or we can even just specify URLs in our Vagrantfile. We don't need the Vagrant Cloud dependency if some basic enhancements around box handling are made. Are there any maintained forks already in existence?
Any thoughts from others?",Bitter Frustration
Definitely still an issue!,Bitter Frustration
"Are there any trusted torrent links available? I am trying to download bentoo/ubuntu16.04 and i am getting 12 kbps and upto 15kbps when i try to do wget, i am sure that my connection is fine!",Impatience
"Still an issue here, have to update my box at ~150k/s, not able to do a lot of useful work in the meanwhile..",Impatience
Still an issue.,Impatience
@bradisbell The real solution is probably a docker-based development environment. Just waiting for that ecosystem to settle down .,Impatience
ðŸº ðŸº ðŸº ðŸº ðŸº ðŸº I'm just going to leave these here for any poor sod waiting for this download.,Bitter Frustration
"I found a root cause and a solution for my situation. I increased my download speed 100x:
Root cause: Slow speed of `ubuntu.com` cloud box servers
Solution: Switch to another trusted source for Ubuntu boxes, Puppet Labs.
I've been using `ubuntu/xenial64` for developing apps that must deploy to Ubuntu 16. (Heroku) And download speeds have become ridiculously slow, or failed entirely. I used wget and watched it follow the redirects and saw that it's actually hitting `ubuntu.com`. I tried several ways to download from Ubuntu, but they all resulted in speeds around 50 KB/s. So I went back to the Vagrant box listing to see who else might have a Xenial 64 box, and if I'd trust them. Turns out Puppet does, and I trust them as much as (or even more than) Ubuntu. My `Vagrantfile` now has the config line:
```
config.vm.box = ""puppetlabs/ubuntu-16.04-64-nocm""
```
And it downloaded in just a few seconds, as opposed to possibly an hour or never.",Bitter Frustration
@dogweather Thanks for this! My download time estimate went from 3 hours to 3 minutes when I switched to a box from the list at https://app.vagrantup.com/puppetlabs/,
"@tristanmason You're welcome! Ironically: In the past week I switched to Docker Compose, and it's **AMAZING**. I can't believe I've waited this long to use it. I used this [Quickstart for Rails](https://docs.docker.com/compose/rails/) doc.
* Configuration is 1/10 the size and complexity of Vagrant for my typical Rails app
* Build and Boot times are much faster
* On my Linux desktop in particular, there is a 10x speed increase vs. running on Mac.
* There's a huge reduction in RAM required. * I'm going to save a ton of money on development computers. I.e., 8GB RAM is plenty for a Docker-based dev machine. (Not so good for Vagrant+VirtualBox though.) And a top-tier i7 isn't necessary anymore either to get good speeds.
I was able to setup a Docker-based dev environment on four computers in a fraction of the time it takes me to create a single Vagrant setup from scratch.
I'm going to write up a blog with detailed metrics - I'll post it here.",Irony
"@dogweather Vagrant and Docker seems similar but they are different things. Also, you can use a Docker provider within Vagrant.",
"@oncet Yep, very different! But for my use case, they're equivalent and commodified: Ways I can create and launch an isolated dev environment with just one or two commands. Docker Compose works great and is far simpler than Vagrant (note, this not just ""Docker"" per se, which only launches containers one by one) and I'd need to hear about a compelling reason to try a Vagrant+Docker solution, which sounds pretty damn complex. ;-)",Irony
Pain. This is a pain. I'll never be able to download the 5.0.1 I guess,Bitter Frustration
"On a 100mbps connection in Manila, with Vagrant 2.0.1, I was trying to download:
https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/20180110.0.0/providers/virtualbox.box
...and I was getting speeds of anywhere from 1k-150k via `vagrant up` or even `wget`. Total download time: **12 hours!**
Then I did one thing: **I VPN'd to California.**
Suddenly, my download took only 3 mins. So that's something worth trying possibly.",Bitter Frustration
"Same here, I thought this is a personal problem lol.
Even update box still get really poor download speed, hashicorp please fix :(
```
==> default: Updating 'ubuntu/xenial64' with provider 'virtualbox' from version
==> default: '20171221.0.0' to '20180122.0.0'...
==> default: Loading metadata for box 'https://vagrantcloud.com/ubuntu/xenial64'
==> default: Adding box 'ubuntu/xenial64' (v20180122.0.0) for provider: virtualbox
default: Downloading: https://vagrantcloud.com/ubuntu/boxes/xenial64/versions/20180122.0.0/providers/virtualbox.box
==> default: Box download is resuming from prior download progress
default: Progress: 13% (Rate: 52999/s, Estimated time remaining: 0:31:46)
```",Impatience
"Ignoring such persistent issue means something on Hashicorp's end, I believe.",Bitter Frustration
"@dqlopez from the previous comments, this is a solved issue: it's purely the speed of ubuntu's servers. The solution is to use some other org's images.",Impatience
"Confirming that when I used `bento/ubuntu-16.04` instead of `ubuntu/xenial64` ([why?](https://github.com/hashicorp/vagrant/issues/6616#issuecomment-227776489)), I got pretty good download speeds. The box downloaded in less than a minute, on a 300 Mbps fibre optic connection. (Unfortunately I didn't think to copy and paste the log.)",
"I promised I'd report back on my transition to Docker, so here goes. I spent a ton of time creating a config that delivers the one-liner `vagrant up` experience. It's `docker-compose up`. And there's only one dependency, Docker. Not two (VM _plus_ Vagrant).
My post about performance of a Docker dev environment on Mac and Linux: [medium.com](https://medium.com/@dogweather/dev-environment-performance-tests-docker-vs-native-mac-vs-linux-old-vs-new-883399d05182)
Repos with my configuration, tailored for [Ruby on Rails](https://github.com/dogweather/rails-docker-compose) and [Phoenix](https://github.com/dogweather/phoenix-docker-compose) development. Very very similar. Can be tailored for any language, I'd imagine.
cc: @fredngo",
"@dogweather Frankly, I don't see how your commentary on docker has anything to do with the issue at hand. Please stop taking this thread off-topic.",Impatience
"Docker is not an alternative to Vagrant. They work completely differently and are intended for different environments. While I'm all for a discussion on the merits of when each should be used, this thread is not really the place for it.
Edit: I love both and use them both in development and production environments.",
"Haven't read the full thread, but it was a significant difference between using PowerShell and Git Bash.",
"Well, I cant download a single box tonite. I'm in the UK.
```
The box 'puppetlabs/ubuntu-16.04-64-puppet' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: [""https://atlas.hashicorp.com/puppetlabs/ubuntu-16.04-64-puppet""]
Error: The requested URL returned error: 404 Not Found
```
> config.vm.box = ""puppetlabs/ubuntu-16.04-64-nocm""
> config.vm.box = ""bento/ubuntu-16.04""
> config.vm.box = ""ubuntu/xenial64""
> none of those work.
BUT if you download the box manually via wget, all works fine...
```
nick@TX200-S5:~/workspaces/elk-vagrant$ wget https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
--2018-03-04 00:30:45-- https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 50.17.237.77, 54.221.226.80, 54.243.175.62, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|50.17.237.77|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box [following]
--2018-03-04 00:30:46-- https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box
Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.32.82
Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.32.82|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 666915336 (636M) [application/vnd.previewsystems.box]
Saving to: â€˜virtualbox.boxâ€™
100%[==========================================================================================>] 666,915,336 2.23MB/s in 4m 48s 2018-03-04 00:35:35 (2.21 MB/s) - â€˜virtualbox.boxâ€™ saved [666915336/666915336]
nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add bento/ubuntu-16.04
The box 'bento/ubuntu-16.04' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: [""https://atlas.hashicorp.com/bento/ubuntu-16.04""]
Error: The requested URL returned error: 404 Not Found
nick@TX200-S5:~/workspaces/elk-vagrant$ wget -O bento-ubuntu-16.04 https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
--2018-03-04 00:37:41-- https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 54.243.252.123, 50.19.252.69, 54.243.137.45, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|54.243.252.123|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA [following]
--2018-03-04 00:37:41-- https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA
Resolving archivist.vagrantup.com (archivist.vagrantup.com)... 50.16.237.173, 23.21.92.233, 54.221.212.171, ...
Connecting to archivist.vagrantup.com (archivist.vagrantup.com)|50.16.237.173|:443... connected.
HTTP request sent, awaiting response... 307 Temporary Redirect
Location: https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7 [following]
--2018-03-04 00:37:42-- https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7
Resolving vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)... 52.216.164.43
Connecting to vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)|52.216.164.43|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 428203828 (408M) [binary/octet-stream]
Saving to: â€˜bento-ubuntu-16.04â€™
100%[==========================================================================================>] 428,203,828 2.09MB/s in 3m 6s 2018-03-04 00:40:48 (2.19 MB/s) - â€˜bento-ubuntu-16.04â€™ saved [428203828/428203828]
```
A direct box add works too
```
nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add ""bento/ubuntu-16.04"" https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
==> box: Box file was not detected as metadata. Adding it directly...
==> box: Adding box 'bento/ubuntu-16.04' (v0) for provider: box: Downloading: https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
box: Progress: 7% (Rate: 2247k/s, Estimated time remaining: 0:03:13)
```",
"I would like to know how can I manually add the .box file in an offline manner. It is not clear in the documents on how to do that. I think the correct place for modification is Vagrantfile. However, I can not figure out which parameter should I change.",
@mahmoodn Your question is totally unrelated to this thread. The [box documentation](https://www.vagrantup.com/docs/boxes.html) should help you with your question.,Impatience
"```
https://vagrantcloud.com/laravel/boxes/homestead/versions/5.2.0/providers/virtualbox.box
homestead-7: Progress: 36% (Rate: 95492/s, Estimated time remaining: 0:54:16)
```
99% of the time, this is what I'm dealing with. On a stable fibre connection. No WiFi. Every now and again I get a short burst of speed that pushes progress up around 5%, but overall, I'm left waiting several hours. This is insane. People have proposed work arounds and mitigations but those aren't solutions. When this issue was first closed, it was implied that `curl` might be to blame. Perhaps this is true, and if it is, then it should be switched out for a different solution.",Bitter Frustration
"I suffer the same problem also My connection on speedtest.net 6.6Mbps while the download takes only 0.6 Mbps of my bandwidth ![image](https://user-images.githubusercontent.com/33742499/38153904-5e74252a-346f-11e8-9623-e891b6bc3466.png)
![image](https://user-images.githubusercontent.com/33742499/38153877-41067678-346f-11e8-9a15-3ef259286fa9.png)",Bitter Frustration
"Hi everyone,
This issue overall is very complicated in that it is not easily reproducible and is very dependent on location, network health, and the actual download location of the target box. In many cases the box being downloaded is located on a third party system. One feature that was introduced in Vagrant a couple releases ago was the notification of redirect to show when boxes are being downloaded from a third party server.
There have also been claims that the embedded curl is downloading files slower than the system curl. I have not been successful in validating this claim and do not see a difference in download speed between my system curl and the embedded curl. However, as of Vagrant 2.0.4, the [`VAGRANT_PREFER_SYSTEM_BINS`](https://www.vagrantup.com/docs/other/environmental-variables.html#vagrant_prefer_system_bin) environment variable is available for all platforms. When enabled, Vagrant will use local system binaries if available instead of the embedded binaries. If you observe faster download speeds with your local curl binary, this provides a switch to easily enable Vagrant to use the local binary.
My apologies for the frustration this issue has caused. It is something I have investigated multiple times, but the number of variables involved makes this extremely difficult to get a valid reproduction. I have updated Vagrant's functionality to make the underlying cause more easily understandable where available (redirect notifications) or easier to work around. I am continuing to investigate our options for box downloads from Vagrant Cloud in different regions of the world (which have occasionally presented with issues). However, with box downloads from third party locations, there is little I am able to do to control their available bandwidth or any kind of throttling they may impose.
Cheers!",
"Code:
```
$ cat typetest.rs struct Foo<Elem, List:AsSlice<Elem>> {
elems: List,
}
fn main() {
}
```
Compile:
```
$ rustc typetest.rs typetest.rs:1:12: 1:16 error: parameter `Elem` is never used
typetest.rs:1:16: 1:16 help: consider removing `Elem` or using a marker such as `core::marker::PhantomData`
error: aborting due to previous error
```
I'm pretty new to rust, so forgive me if I've missed something. But I can't see a way to say ""my struct contains a type T2, which implements `SomeGenericTrait<T>`"" without my struct also being parameterized over T, as I'm doing here. But rust thinks that I'm not using T.
I tried adding:
```
dummy: Elem,
```
to my struct def, which makes it compile. So it doesn't look like this is just one error message obscuring another, rust does really think this is the only thing wrong with my code.
---
```
$ rustc --version --verbose
rustc 1.0.0-dev (2fc8b1e7c 2015-03-07) (built 2015-03-08)
binary: rustc
commit-hash: 2fc8b1e7c4ca741e59b144c331d69bf189759452
commit-date: 2015-03-07
build-date: 2015-03-08
host: x86_64-unknown-linux-gnu
release: 1.0.0-dev
```",Bitter Frustration
"This is actually intended behavior due to a [recently merged RFC](https://github.com/rust-lang/rfcs/blob/master/text/0738-variance.md). The idea here is that the compiler needs to know what constraints it can put on the type parameter `Elem`, and usage of the [`PhantomData`](http://doc.rust-lang.org/std/marker/struct.PhantomData.html) type will instruct the compiler how it can do so.
You can check out the [examples](http://doc.rust-lang.org/std/marker/struct.PhantomData.html#examples) for `PhantomData` to see some usage, and in this case you'll probably want to do something along the lines of:
1. Refactor the struct so unused type parameters aren't needed. This can often be done with associated types, something along the lines of:
``` rust
trait A { type B; }
struct C<T: A> {
inner: A::B,
}
```
2. Use `PhantomData<Elem>` to indicate that the compiler should just consider the type parameter used, considering `Foo<A, B>` as containing something of type `A` as well as `B`.
You probably don't want to store `dummy: Elem` as it will require you to store an extra element and could have other memory implications as well.",
"Thanks for the helpful explanation, @alexcrichton. You also conveniently educated me on associated types, which I was wondering about but didn't know what to search for (I was wondering ""why is `Iterator` not generic""). It turns out this is just what I needed.",
"I just stumbled upon this limitation in a context where `PhantomData` is of no help: generic enums. Consider the following piece of code:
```
enum Foo<A, B>
where A: Bar<B> + Sized
{
X(A),
Y,
Z
}
trait Bar<B> {
fn bar(&self, &B);
}
```
Edit: I'm aware of several workarounds. These include adding `PhantomData` to `X`, but this pollutes the enum (in the sense that construction of a value by the user becomes iffy).",Bitter Frustration
"Hi, I had a similar problem regarding to unused type parameter.
Here is the simplified code, I need to control the visibility for quicksort's sort function. ```rust
struct Quicksort<T>; // <---- unused parameter warning
trait Sorter<T> {
fn sort(&self,ar<T>){...}
fn join(...);
fn split(...);
}
impl<T> Quicksort<T> {
pub fn sort(&self,ar<T>){
Sorter::sort(self, ar<T>) ;
}
}
impl<T> Sorter<T> for Quicksort<T>{
fn join(...) {...}
fn split(...) {...}
}
```
Any help to resolve the problem? because struct Quicksort does not have any fields. The type parameter is solely for the sort function from trait.
Otherwise if I removed `impl<T> Quicksort<T>`, then I don't need the T for Quicksort at all.",
"For those who will look here for answers:
""In most cases you do not need additional type parameters at all"".
That's it. Just remove them.
Taking @Piping example for illustration.
Let's get rid of type parameter for `Quicksort`.
```rust
struct Quicksort;
```
Now `rustc` obviously won't complain about unused parameters since there are none.
But at some point we actually need that type-parameter.
Continuing. Type parameter can be introduced at function level.
```rust
impl Quicksort {
pub fn sort<T>(&self,ar<T>){
Sorter::sort(self, ar<T>) ;
}
}
```
Now when it comes to implementing a trait things goes smoothly again since type parameter is already exists in `Sorter` trait.
```rust
impl<T> Sorter<T> for Quicksort {
fn join(...) {...}
fn split(...) {...}
}
```
There are few places where you actually need artificial type parameter.
1. When type conceptually contains some generic type.
2. Type must implement a trait with associated type.
```rust
trait Trait {
type X;
}
```
Now either the type in question will be able to implement the `Trait` with some predefined `X`.
```rust
struct Foo;
impl Trait for Foo {
type X = u32;
}
```
Or it has to have this artificial type parameter after all.
```rust
struct Foo<T>(PhantomData<T>);
impl<T> Trait for Foo<T> {
type X = T;
}
```",
"@neithernut In case you're still interested, I had a similar problem and I solved it by creating an additional variant that stores the `PhantomData` and a never type `Empty`:
```rust
pub enum Empty {}
pub enum Foo<A, B> {
X(A),
// ...
Void(Empty, PhantomData<B>),
}
```
[playground example](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=c297fb0f024a70630ce2ffa1e6810b3e)
The good news is that it's zero-cost and does not pollute the variants you do use.
The bad news is that, until [never type exhaustive matching](http://smallcultfollowing.com/babysteps/blog/2018/08/13/never-patterns-exhaustive-matching-and-uninhabited-types-oh-my/) comes to Rust, this approach will pollute the code when you match on your enum (although you could macro your way around it):
```rust
match foo {
Foo::A(_) => (),
// ...
Foo::Void(empty, _) => match empty {},
// ^
// phantom data
}
```",
@alexcrichton I also had some issues with this and generic enums. It would be very useful if the compiler could recognize that the type parameter is only used in a trait bound for another type parameter. I don't understand the reason why this would not work now.. Can't the compiler derive the constraints from the trait the type parameter is passed to?,Bitter Frustration
"@mankinskin I agree. PhantomData in this instance just pollutes the code. It is an implementation detail that should never be exposed in a public API (which is what happens with both @AdrienChampion 's and @neithernut 's code. It's also exceedingly irritiating, adding code complexity, complicating construction (so making it harder to just pass generic enum members as closures w/o writing extra code, etc. It's a code smell in Rust itself.",Bitter Frustration
"Looking back to @alexcrichton's response and the RFC, I think the question is not so much why unused type parameters cause an error, but why the compiler considers type parameters unused in examples like the OP's.
To me, it seems like `Elem` has one usage (in `List: AsSlice<Elem>`), albeit not in the body of the struct. It seems like that should constitute a usage for the purpose of variance inference, since it implies a variance bound: `Elem` must be invariant since `AsSlice`'s parameter is invariant.
In Scala for example, the equivalent would be `class Foo[E, S <: Seq[E]](val seq: S)`. Scala doesn't infer variance, but it ""knows"" that `E` must be covariant or invariant. If we try to make it contravariant (with `-E`), we get `error: contravariant type E occurs in covariant position`.
If getting the compiler to recognize these variance bounds would be complicated, perhaps lifetime and non-lifetime parameters could be handled differently? As the RFC notes, only lifetimes have subtype relationships, so it seems like non-lifetime parameters might as well be ""inherently"" invariant, and not subject to inference. Code like `struct Thing<'a, 'b: 'a>(&'b u32);` would continue to be rejected, but I don't think there's any ""legitimate"" reason to write lifetime parameters like that.",
"I think perhaps we should reopen this one. The associated RFC doesn't seem to justify a compiler error in code like:
```rust
pub struct Foo<A> {
a: A,
}
pub struct Bar<A, F>
where
F: AsRef<Foo<A>>,
{
foo: F,
}
```",
"Changes of this magnitude would require an RFC these days, and I would encourage folks to either open an RFC or a thread on users or internals rather than continuing to comment on an issue that was closed almost six years ago.",
"But can't this issue be reopened? This is kind of the first place you end up at when googling about this, and the discussion is basically already here. I don't think there is a lot of designing left to do, it just needs to be implemented in the right way.
I can try to write up an RFC some time, but it is not high on my priority list right now and I am completely unexperienced with rustc. Opening up this issue for now would at least raise attention to it and helps resolving this faster.",Impatience
"Large scale changes to the language, of which this is one, require an RFC. I am not on the lang team, but this isn't actually a bug: this is a request for a change to how the language works. A bug being open for it is not the procedure by which this would be changed.",
"@mankinskin I agree completely, and I feel your frustration; I share it, and it's got worse as rust has become more widely used. Rust's core team also have a very narrow view of what a bug is - one that with my software craftsmanship hat on I couldn't disagree with more strongly.
Rust's development has been captured by the bureaucratically minded; the same mindset that has infested wikipedia, and parodied beautifully in [Brazil](https://www.youtube.com/watch?v=KZ-SdU53MnY).
Personally, I find it rude; it shows disinterest in the views of those outside of the main development, and the idea that anyone can have a good idea.",Bitter Frustration
"@raphaelcohn I know what you mean and we should always be wary of overdoing that, it might stagnate innovation, but I also think there is value in organization and strict review. Rust's promise is to be a secure language, that means it must takes steps carefully, which makes it more difficult to innovate. I agree that there eventually needs to be an RFC for this but I don't really see the point of restricting discussions about feature changes in these github issues. I suppose they use this as a bug tracker, and new features are not supposed to be tracked here.
In general, I think there are a lot of improvements that could be made to the entire open source workflow nowadays. The tools don't really integrate well, basically just using hyperlinks. For open source work there ought to be issue dependencies, resource budgets, integrated documentations and tests, voting on forks, funding, user permissions, all of that. That would make open source a lot more ""open"" and transparent. It has to come eventually, but there still needs some work to be done.",
"Moderation note: what @steveklabnik said is correct. The kind of unconstructive broad criticism that has taken place in the past few comments is not appropriate here. Since this discussion doesn't seem to be headed anywhere good, and the next steps if one were to make progress have been clarified, I'm going to lock this thread.",
very good feature :+1:,
+1 for this feature : ),
1,
1,
1,
1,
1,
:+1: would definitely be useful.,
Is there and ETA for this?,Impatience
Not at the moment,Impatience
1,
1,
1,
1,
+1 extreamly important,Impatience
1,
1,
1,
1,
1,
1,
1,
1,
1,
when to achieve?,Impatience
@vkarpov15 : any progress on this?,Impatience
"Since there's still no support for this, I solved this by keeping a base model and exporting another schema to a separate file and then adding properties to it as needed.
Hope that helps:
```js
/** BaseModel.js **/
const mongoose = require('mongoose');
const myModel = new mongoose.Schema({
name: {type: String, required: true},
});
module.exports = mongoose.model('Model', myModel);
/** schema.js **/
const mongoose = require('mongoose');
module.exports = function(props) {
const params = {
description: {type: String}
};
if(props) Object.assign(params, props);
return new mongoose.Schema(params);
};
/** SubModel.js **/
const mongoose = require('mongoose');
const BaseModel = require('./BaseModel');
const BaseSchema = require('./schema');
const subModel = BaseSchema({
myProp: {type: Number}
});
module.exports = BaseModel.discriminator('SubModel', subModel);
```
You create more sub models by creating more sub-schema factories.",
1,
1,
1,
1,
1,
1,
1,
1,
"I recently took the time to open source a solution to this issue, it's call [mongo-schematic-class](https://github.com/Omninox/SchematicClass). It allows you to use native ES6 classes (or function classes, if you prefer) and normal inheritance to define your schemas.",
Any updates?,Impatience
"Example list: http://www.imdb.com/list/export?list_id=ls076157636&author_id=ur58869533p (CSV)
Or RSS: http://rss.imdb.com/list/ls076157636/
We need to provide a way to set the defaults when adding a series via this mechanism, likely on a per feed basis. Trakt is also a candidate for a feature such as this, so we could automatically import a series from a trakt list.
A new series will need a root folder, a profile, monitored state, a type (standard, daily or anime) and whether it should use season folders. When the add series page allows adding tags when adding a series we should provide the same here.",
"Radarr already has this feature, so it should be easy to copy?",Impatience
"I just discovered this feature in Radarr, and would love to see it in Sonarr as well.",
"Lists works great in Radarr, and would be an amazing addition to Sonarr",
This would be an amazing addition to Sonarr. Thanks!,
"I'd love to help out building this, but I got 0 experience with this project, and can't seem to make logic of radarr's version.",
"Same here, as Radarr can import Trakt lists I'd love to see this on Sonarr :)",
I'd love to see this too!,
1,
"Works great on Radarr, would be awesome to get it at Sonarr. That would mean one place to select, and let Sonarr/Radarr read list, search, tag, download, done.",
Maybe through a Trakt integration using list feature? Analogue how it is done in Radarr.,
I started with Radarr and assumed Sonarr could parse your IMDB list for shows. I was a bit surprised to find out it couldn't so someone pointed me here to chime in and toss my hat into the vote. Though I see it has been 3 years so I am guessing it is not as easy to do. Still tossing in my vote :),
I could really do with this feature to allow new releases to sync to sonarr automatically...,Impatience
It hard to believe that this was opened three years ago.... I assumed it's not that complicated since I started with radarr. It's the only thing missing in sonarr to match radarr. Hope it will come soon.,Impatience
that would be nice,Impatience
putting my vote in for what feature i most want from sonarr that it doesn't have yet. i have discovered a lot of great movies pulling in top 50 stuff on radarr would like the same type of thing for series.,
"d love to see the feature in Sonarr, too",
For now i used [sonarr-trakt-tv]( https://github.com/preram48/sonarr-trakt-tv) to sync TV Shows to sonarr as a temporary solution. It's works great but missing some options. (For example there is a quality profile that i don't see in sonarr-trakt-tv) But it get's the job done.,
"I desperately need this feature. I have setup of Radarr and Radarr4K. Radarr would import lists from different places and the 4K versions imports them from the normal Radarr.
I'd love to be able to the same with Sonarr",Impatience
Add another person to the list,Impatience
If you don't have anything meaningful to add and you want to add your support add a ðŸ‘ to the original post and subscribe to the issue. Everyone that has commented doesn't need to receive an alert that you're also interested in the feature.,Impatience
"Please, could you implement namespacing for this library ? If anyone has to include in a project is bad practise load single file to use the library.",Impatience
"@diegomariani if you're using HHVM or any type of opcode cache, you shouldn't worry about it being a single file. at any rate, the library is over 12,000 lines long, so if you want to fork it and namespace it out its going to be a fairly massive undertaking, and even if you did the performance gain would be neglible at best.",
"Actually you don't need to use the singe-file distribution. You could clone the repository and use it directly. ReadBean is developed ""the typical way"" with one file per class, namespaces matching the directories and so on. If you use composer, you can add RedBean and get the namespaced, multi-file version. If you're confident using dev-master: https://packagist.org/packages/gabordemooij/redbean, or you could use my fork where I only added the composer.json : https://packagist.org/packages/simirimia/redbean, https://github.com/simirimia/redbean. There you get some stable versions as well.",
"Ah, update: since 4.2 is released you get stable version here as well: https://packagist.org/packages/gabordemooij/redbean -> so no more need for my fork",
@simirimia If installing via composer can we still use everything as `R` alias ??? Installing redbean via composer would be advantageous to me but i was unable to get it to work previously,Impatience
"@r3wt If you look at the [composer.json](https://github.com/gabordemooij/redbean/blob/ea3f70175178d5385d45c83a32efa51495f80b0c/composer.json#L17-L21), you will see that the namespace `RedBeanPHP\` is autoloaded. Then, if you go in this `RedBeanPHP/` folder, you will see a [R.php file](https://github.com/gabordemooij/redbean/blob/d8f9f4f5ab01b285a587dd54f5175d013c316ade/RedBeanPHP/R.php#L6-L10), which means that the `R` facade is just in the namespace `RedBeanPHP\`.",
"Personally I use the one-file solution for all my projects.
It might be bad practice but it's damn easy, and I have never heard anyone complain, nor did it cause any bugs or conflicts.",
Do you not use Composer in your project? It really is powerful to manage dependencies.,
"@tleb so how would i alias RedBeanPHP to R ?
say i have a class like
Class DB {}
and in it i reference redbean lik `\R` how can i set that alias in my class? `\RedbeanPHP\R` becomes `\R`
FYI the reason i write my code like this is so i can isolate database code from the rest of the code. this way, if needing to change the database library or even database type, i only have one class to edit. so its like an api wrapper with methods like `isEmailInuUse($email)` etc.",
"``` php
// You could call R directly
\RedbeanPHP\R::freeze(true);
// Or use `use`
use RedbeanPHP\R;
R::freeze(true);
// Or use an alias
use RedbeanPHP\R as Redbean;
Redbean::freeze(true);
// Or create a facade of the R facade
class R extends \RedbeanPHP\R {}
\R::freeze(true);
```
Also, to make sure every database library you use use the same method names, you should create [an interface](http://php.net/interface).",
"i just used `class_alias('\RedBean\R','\R');` and it worked fine.",
"I never use composer. I never even use apt-get, yum, npm or any other kind of dependency resolution. Good software has no dependencies other than an OS / VM.",Mocking
"I understand where you are coming from Gabor, but our jobs depend on is to be agile-- that is to say, we must get alot done I short periods of time. Now I could spend time wget'ing, untaring,and requiring red bean manually, but it makes more sense to just type composer install.",
"@gabordemooij Do you know that what you are saying goes against the Unix philosophy:
```
Write programs that do one thing and do it well. Write programs to work together.
```
Tools such as `composer`, or `apt-get`, or any dependency manager, try to make it simpler to link programs together, so that programs can focus on what they must do, and so that they can do it well. If we created the same thing over and over again, every single project would fail, because they would need to do everything from scratch, even though people already created those required programs. Dependency managers are tools to build upon, they are made to create good software based on other software which achieve the task they are asked to almost perfectly.",
"@gabordemooij
You should be killed. Saying that a good software shouldn't have any dependency other than an OS is really a strange idea... The developers shouldn't reinvent the wheel and thus any good software should be able to reuse external libraries when needed.
BTW: An OS is nothing more than a bunch of libraries and executable that were compiled to form a system fully usable. Does an OS shouldn't have any external dependencies?",Threat
"@tleb composer does not link programs, it links libraries. And, as such it creates huge programs that do everything - which is basically the opposite of the UNIX philosophy. Apt-get also rarely installs programs, it installs mostly lib-this, lib-that, lib-different-version etc. Programs relying on a ton of external libs are definitely not 'Unixy'.
@nuxwin I did not say developers should re-invent the wheel. Neither is this an inevitable consequence of having no dependencies. I simply believe a well designed base system would be preferable - but that's harder to build. @nuxwin Please, let's refrain from statements like 'should be killed' and keep the discussion civilized. Open source forums are already famous for their anti-social tone. We're just people and everyone is entitled to his/her own opinion. If someone disagrees there is either an opportunity to improve knowledge or exchange new ideas.
Also - I believe I accidentally sidetracked this thread with my comment about dependencies in general, sorry for that.",
"@gabordemooij
Don't get bad ;) This was just an expression.",Irony
"@nuxwin ok, no problem",
"@gabordemooij, well, I'm curious, and not sure I got the point. You believe all the libraries belong to the base system? And hence good software is impossible with bad base system?",Impatience
"I -too- would like to continue this debate but I do not think a Github issue is the best place for this. If anyone could recommend a more appropriate place for this (an IRC channel, a website ?).",
@tleb maybe there --> http://www.php-fig.org/irc/,Impatience
"Well, it's not that I was going to debate on this. I rather wanted to clarify @gabordemooij's point of view. For now it seems self-contradictory to me.",
"@x-yuri I acknowledge sometimes you can't do without dependencies. But I believe people have given up to early on the problem. Often when you install dependencies you end up with all kinds of stuff you don't want and you lose control to a certain extend. I believe that we, as a community should invest more in solid base systems. I admire Linux distros like Slackware that have a rich package repo with very few dependencies. I think we should try harder to get our 'bases right' instead of trying to fix fundamental problems with tooling.
Also, I hate the fact, I can't just grab a lib from the web anymore because some of them seem to be tied pretty closely to the whole composer workflow, they have no standalone tarballs, they need autoloader this and that... But I have to admit I am very conservative and stubborn in some matters. There's lots of other issues as well, composer uses git/github as repositories, interfering with my use of it as a version control system (preferred layouts) etc etc..
Please note that when I say things like 'Good software should this or that...' it's just my opinion, I do not expect everyone to agree. However I would like to discuss the usefulness of composer, unfortunately, as soon as you 'ask questions' or 'criticize' a popular technique you're considered to be 'a moron'. These discussions seem to end up in complete flame wars and honestly as long as that culture persists I don't feel compelled to explain every decision I make (luckily the RedBeanPHP forum is quite civilized though).",Bitter Frustration
"I've ended up using composer simply because I find it a challenge to keep my dependencies up-to-date. I don't have a lot of them, but when you have lots of projects, it starts to quickly get out of hand.",Bitter Frustration
"@sbrl don't use composer. Just don't. If you cannot manage your dependencies you have a very big problem and composer hides it. Updating bad code in zillions of libs will not magically improve the reliability and security of your code.
- get rid of all libraries that are not crucial
- remove all libs with dependecies, they are by definition created by idiots
- keep things simple, if you need that much libraries, you failed at keeping things simple",Bitter Frustration
"I don't do this usually but I had to screenshot this whole discussion. Guy doesn't use namespaces, uses single file for his projects and he bashes the practice of tracking your dependencies with - listen to this - **programs**. If anyone stumbles upon this page via google, please take time to read what @gabordemooij wrote and take note - NEVER justify your bad decisions by making stuff up.",Bitter Frustration
"> don't use composer. Just don't. If you cannot manage your dependencies you have a very big problem and composer hides it. Updating bad code in zillions of libs will not magically improve the reliability and security of your code.
As a security engineer who works primarily in PHP and contributes to open source cryptography libraries for the PHP community, I [strongly disagree and say that using Composer is a ***damn good* idea**](https://paragonie.com/blog/2017/12/2018-guide-building-secure-php-software#dependency-management).
> Updating bad code in zillions of libs will not magically improve the reliability and security of your code.
Uh, yes, it will!
Outdated third-party libraries with known (i.e. not zero-day) vulnerabilities are one of the most reliable RCE vectors when testing a system. (Aside: Thank you, projects that ship their own PHPMailer bundles, for all the easy footholds into otherwise challenging networks to penetrate.)
This isn't intuitive for most people, so I've laid out [the case for automatic security updates](https://paragonie.com/blog/2016/10/guide-automatic-security-updates-for-php-developers#why-automatic-updates) before.
Composer makes it easier for projects to keep dependencies up-to-date. This creates a measurable improvement to the security and stability of real world systems.",
"Updates often introduce new bugs and new vulnerabilities. So your just replacing old bugs with new bugs. Only right after an update 'you consider your software safe', but as time progresses it turns out the initial update was not safe at all but by then most developers pretend the software 'is now suddenly unsafe because it is old' - that's not true, it was already compromised right after the update. Besides that, dependency management systems make it worse, because you don't even know what you install anymore. There may be even some toxic packages among the dependencies:
https://news.ycombinator.com/item?id=15272394
https://news.ycombinator.com/item?id=11340510
https://sensorstechforum.com/arch-linux-aur-repository-found-contain-malware/
While I don't pretend that bug-free software can be written, a better solution might be to focus on simplicity. Dependency management systems compromise simplicity because they make it easy to solve the natural issues regarding dependencies. In a sense, they hide complexity - and paradoxically therefore they increase the risk of importing new bugs and vulnerabilities.
So, I reverted the commit that removed the Composer file because, even though I am against it, I also don't really care, I just get a bit angry when Composer related problems (like what version string to use) creep into the RedBeanPHP repository. And yes, you make screenshots all you want and laugh about RedBeanPHP or me. I don't give a shit. Have fun.
BTW, remember that this is an open source project, you can fork and nobody is forcing you to use RedBeanPHP so, if you don't like it:
https://www.doctrine-project.org/
bye bye!",Bitter Frustration
"Literally everything you just said can apply to any dependency manager including Git, Github, apt, whatever.
How are redbean releases more secure than another PHP library? They aren't. You aren't signing your releases and there's no verification system in place. The fact is humans make up the ""PHP community"" and it's up to us to verify security. And as far as human verification goes I'll trust the millions of users of a given popular package on packigist versus having to manually check the Git of a project like yours. Honestly though just looking at your source tree your ORM is straight out of PHP 4.x times. Your project has a custom unit testing schema because apparently PHPUnit isn't secure enough? You have no `spl_autoload_register` anywhere in your code even though it makes sense here to use it. You're still publishing redbean on packigist with a composer.json. So I guess ""trust me"" but no one else is enough for you. Like honestly wtf even is [this](https://github.com/gabordemooij/redbean/blob/master/p533patch.php)? You can't even generate code coverage reports because of your home baked unit testing system which I'm sure doesn't support clover.xml output for code coverage. Which is why you don't have a code coverage badge.
Yea yea packigist needs signed releases to give it some ""security"" but honestly the fact is the market has moved on.
In most PHP shops these days it's about speed & quantity of code / features over ""security"" concerns.
Why would I implement AWS myself when they give me a library to use? Why would I implement the Fitbit API manually when [someone already wrote one](https://github.com/djchen/oauth2-fitbit)? Especially when it comes with unit tests and lots of users who will call stuff out if it breaks or is compromised.
We're all idiots? Really dude? The company I work for and all the jobs under it could literally not exist without the good work put in by hundreds of developers I've never met. Because it's literally impossible for me to sit down and write an API implementation for every service we interact with. And yet we are currently running a company based on tying all these things together into a coherent service.
I think you seriously need to re-evaluate your philosophy and take a look at a lot of the [good](https://thephpleague.com/) work being done currently in the PHP space by dedicated people.
And finally. If you care sooooo much about security maybe it's time to upgrade to PHP 7.1 because security support for 5.6.x ends in 3 months.",Bitter Frustration
"> Updates often introduce new bugs and new vulnerabilities. So your just replacing old bugs with new bugs. Updates also fix bugs and old vulnerabilities. Using a dependency manager lets you lock down to a specific version of a library. I can fork a library, perform changes, tag it, point my `composer.json` to it. I don't have to do anything manually. If I want to know which *version* I use, I don't have to dig through files.
I could literally go through every single word you wrote and try to argue with you but it's pointless. You suffer from NIH syndrome, your practices are bad and as you wrote - you don't give a shit.
You are spreading dangerous blatant lies in order to justify something you wish were true. That's the problem. The second problem, which is your personal problem, is your attitude. I know there's absolutely no way we can have a discussion, your opinion is set and I can tell you'll do anything to defend your POV. Yes, this is your project, you are entitled to opinion but in your world 1 + 1 yield 75, not 2. Ego is a dangerous thing. I won't insult you, there's no merit to that, we should be civilized after all. I'll just kindly ask you, just ASSUME you might be wrong. Just for abstract thinking's sake. I really believe there's no malicious intent behind what you wrote, but could there be a possibility you don't have all the information at hand to make statements such as ""dependencies suck"" or ""dependencies in libraries suck""? What would be the point of a library then? If I deal with SAML or OAuth2, I require a library which in turn requires encryption primitives. Should I rewrite those primitives and libraries to deal with SAML and OAuth2 so I'm ""safe"" and not an idiot?",Bitter Frustration
"> Literally everything you just said can apply to any dependency manager including Git, Github, apt, whatever.
Git is a VCS, Github is a code hosting platform, so these are not DMs. And yes, it applies to apt. That's why I recommend to use Slackware, Crux or *BSD instead.
> How are redbean releases more secure than another PHP library?
1. 180000+ tests
2. 10 years of maturity
3. Careful code review process
4. Focus on eliminating code, simplicity (minimalism)
5. 0 dependencies
Updating RedBeanPHP is also easy: just copy-paste the rb.php file and done!
> You aren't signing your releases
I provide sha256 checksums on the download page:
https://redbeanphp.com/index.php?p=/download
So you can check whether the downloaded file has not been corrupted or modified - improvements can be made here.
> I'll trust the millions of users
You use Windows I suppose (millions use it!) ? Seriously. How on earth does quality improve just by USING something?!
> your ORM is straight out of PHP 4.x times
Therefore it must be bad. Because new is always better? > Why would I implement AWS myself when they give me a library to use?
AWS is a platform, not a library. You can't implement AWS with just a library.
> We're all idiots? Really dude?
I don't know about the rest in this thread but you seem to suffer from the Dunningâ€“Kruger effect:
https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect
> You suffer from NIH syndrome
To a certain extend agreed, and how do you think I that happened after 20 years of coding?
> I'll just kindly ask you, just ASSUME you might be wrong
That's all I ever do. I know I might be wrong. But to be sure I need good arguments.
> Should I rewrite those primitives and libraries to deal with SAML and OAuth2 so I'm ""safe"" Never use OAuth:
https://hueniverse.com/oauth-2-0-and-the-road-to-hell-8eec45921529
Never roll your own crypto. Use a libreSSL-based crypto solution:
https://www.libressl.org/",Mocking
"Sha256 isn't a digital signature.
https://paragonie.com/blog/2015/08/you-wouldnt-base64-a-password-cryptography-decoded
On Mon, Sep 10, 2018, 7:59 AM Gabor de Mooij <notifications@github.com>
wrote:
> Literally everything you just said can apply to any dependency manager
> including Git, Github, apt, whatever.
>
> Git is a VCS, Github is a code hosting platform, so these are not DMs. And
> yes, it applies to apt. That's why I recommend to use Slackware, Crux or
> *BSD instead.
>
> How are redbean releases more secure than another PHP library?
>
>
> 1. 180000+ tests
> 2. 10 years of maturity
> 3. Careful code review process
> 4. Focus on eliminating code, simplicity (minimalism)
> 5. 0 dependencies
>
> Updating RedBeanPHP is also easy: just copy-paste the rb.php file and done!
>
> You aren't signing your releases
> I provide sha256 checksums on the download page:
> https://redbeanphp.com/index.php?p=/download
> So you can check whether the downloaded file has not been corrupted or
> modified - improvements can be made here.
>
> I'll trust the millions of users
> You use Windows I suppose (millions use it!) ? Seriously. How on earth
> does quality improve just by USING something?!
>
> your ORM is straight out of PHP 4.x times
> Therefore it must be bad. Because new is always better?
>
> Why would I implement AWS myself when they give me a library to use?
> AWS is a platform, not a library. You can't implement AWS with just a
> library.
>
> We're all idiots? Really dude?
> I don't know about the rest in this thread but you seem to suffer from the
> Dunningâ€“Kruger effect:
> https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect
>
> You suffer from NIH syndrome
> To a certain extend agreed, and how do you think I that happened after 20
> years of coding?
>
> I'll just kindly ask you, just ASSUME you might be wrong
> That's all I ever do. I know I might be wrong. But to be sure I need good
> arguments.
>
> Should I rewrite those primitives and libraries to deal with SAML and
> OAuth2 so I'm ""safe""
>
> Never use OAuth:
> https://hueniverse.com/oauth-2-0-and-the-road-to-hell-8eec45921529
>
> Never roll your own crypto.
> Use a libreSSL-based crypto solution:
> https://www.libressl.org/
>
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/gabordemooij/redbean/issues/450#issuecomment-419887784>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ALDfXqfBpQ30trHJ2WyimuRMOIGEXff-ks5uZlQvgaJpZM4EsR4H>
> .
>",Mocking
"```
How are redbean releases more secure than another PHP library?
> 1. 180000+ tests
> 2. 10 years of maturity
> 3. Careful code review process
> 4. Focus on eliminating code, simplicity (minimalism)
> 5. 0 dependencies
```
This is seriously your answer? You demonstrate a lack of understanding of what actually makes a package manager have ""secure"" releases. A secure release is signed with a private key that only you would have. At the moment anyone can get into your Github user and commit anything they want to your project. Sure you can revert it once you get your account back but until then damage can be done.
Your hashes mean nothing. Anyone can edit your website and edit those hashes anytime they want. None of what you mentioned gives increased security.
I also have a 10+ year old ORM with hundreds of projects and more then a decade of maturity. But you know what? Nobody cares.
If you have no code coverage report your tests mean nothing.
Never use oauth? lmao. The company I work for. Literally our entire business model depends on APIs that use oauth. I have no choice. It's not like oh lemme just make my own Fitness tracker real quick.
This entire thread can be summed up as ""I made up my mind 10 years ago and I don't feel like learning anything new so leave me alone"".",Bitter Frustration
"This issue was closed 3 years ago. Why resurrect something just because someone on the internet does things differently? Apparently [someone posted](https://www.reddit.com/r/PHP/comments/9edtkx/if_you_cannot_manage_your_dependencies_you_have_a/) this issue on /r/php, what good could come out of this?
I'm a composer user but I don't go out of my way trying to hammer into people's mind that they should do things differently. There are already forks with composer support anyway. Just move on people. This is borderline harassment at this point.
Thank you for writing RedBean @gabordemooij â¤",Bitter Frustration
"There's a person here, who can obviously write code, read what others say, have a conversation and who is obviously NOT stupid. It's also a person who created an open source project that others use. Now, that same person states certain things that one of my younger colleagues caught because he reads reddit and PHP page (I'm not a reddit user). He uses RedBean in his website and asks me how come we're doing opposite of what is stated here. Why do we use a package manager? I wondered where he got that from, and I stumbled upon this issue. I'm not trying to change people's opinions, but being able to affect someone you don't know through written word means there's a responsibility you bear - and that responsibility means you have to provide FACTUALLY correct information.
The information in this small internet discussion between all parties is sadly, not factually true. I agree that picking everything apart and doing the classic ""no, you're wrong"" type of communication is basically harrassment so I'll stop.
I don't agree with what's written here, and the only reason I'm typing this is to inform future visitors not to blindly trust everything they read, I'll even accept that I'm an idiot for using package managers, OAuth2, SAML etc. - it doesn't matter. The right tool for the job should be used, regardless of what one **thinks** might be ""correct"".
Have a nice day everyone, sorry for intruding your project Gabor and I wish you all the best - I had no ill intent, I'm sure you didn't either and I hope you're not offended, world will still continue revolving regardless of this little discussion :)",
"> So, I reverted the commit that removed the Composer file because, even though I am against it, I also don't really care,
Okay, thanks for the historical context.
> I just get a bit angry when Composer related problems (like what version string to use) creep into the RedBeanPHP repository.
Oh, I can commiserate with you on this point all day long. They're frustrating, especially when you're the person who causes them and you're puzzled about how to fix them.
I don't have a horse in either the ""Does Gabor use Composer?"" or ""Does RedBean use Composer?"" races. I do care about making PHP more secure at the ecosystem level, and that obligates me to seek out and fight against misconceptions about software security.
To be clear:
* Automatic updates *may* introduce new bugs, but they'll fix **the bugs script kiddies are actively exploiting**. Simplicity isn't a panacea here, but it is helpful. (I write my code to be easily audited by third parties, and the incidence of bugs is much lower as a result.)
* Digital signatures are a little more involved than hashes.",Bitter Frustration
"Anyway, as I already admitted you're right concerning the checksums, it only protects against accidental corruption. I will use signify() to sign the releases in the future. https://man.openbsd.org/signify",
"Okay. There's little left to discuss then, it may be a good idea to lock this thread to only collaborators.",
That was a good idea. Hopefully we can now continue the discussion in a more fruitful way.,
"I just discovered signify() is OpenBSD-only, so maybe I'll use GnuPG signatures instead. The signatures will be uploaded to github and the google groups forum. That way, to fake the sigs you must hack the RedBeanPHP website, Github and Google at the same time. Do I miss something here?",
Files now contain Signify signatures. GnuPG may follow later.,
"would be great to make filter for auto skip marked extention types from download.
ex: i need only music without covers, etc. it annoying to deselect it all time (imagine 50 albums, 4Gb covers and 1 seed). so here can help filter to .jpg, .png, etc.
if need to download photos - turn off filter checkbox in file list.
you can do sets of rules, but one is enough.
<bountysource-plugin>
---
Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/23896213-file-extension-filter?utm_campaign=plugin&utm_content=tracker%2F298524&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F298524&utm_medium=issues&utm_source=github).
</bountysource-plugin>",Bitter Frustration
Sounds useful :thumbsup:,Irony
"2015? :-( This will very likely never added then, that sucks.",Bitter Frustration
"This feature could help security:
If a user downloads only known files (e.g. audio/video), it is wise to block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS, (PY?), etc.
Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi <> Matrix.avi.exe
Also, some video torrents are bundeled with trojans, e.g. ""codec.zip"" or ""driver.exe"" containing malware. Today I spotted the attached file, an LNK file, an extension that is hidden on Windows.
None of my users download software via torrent, so I'd like to block it for them, or set blocked extensions; potentially, i'd like to block the whole torrent altogether, if a potential software extension is found in one of its files.
(My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY, CPL, DLL, LNK, SCR)
In the screenshots below: Windows hides LNK extension, a trojan disguised as AVI video:
![lnk-virus](https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png)
![virus2](https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png)",
"Id like to throw my support behind this feature request. Many torrents come with useless .txt files that do nothing but clutter up a directory. Also, a popular site I use has started to include an .exe file that I now have to deselect every time I download something. It would be great to be able to have these files automatically excluded. Being able to blacklist certain file names would also be a great addition to this feature.",
Throwing my hat in..,
"I suggest a feature for a simple list of file names ```do_not_download.exe``` and extensions ```*.exe``` that get marked as **Priority** -> **Do Not Download** automatically for all torrents. List may be accessed in **Options** -> **Downloads**.
Searching for references:
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L684
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L2068
https://www.libtorrent.org/reference-Core.html find ```file_priorities``` under ```add_torrent_params``` header.
https://github.com/qbittorrent/qBittorrent/blob/2d7b833ae6cb2145465cc7e47df398628ac95651/src/base/bittorrent/session.cpp#L1949",
"Glad i found this post, really like this feature to, are we sure (i couldn't find it) that there is no such option already?",Impatience
Definitely have my vote. It will also save (not much but) some space and unnecessary Data download for Countries that charge per Mb on top of per Speed.,
"Love it. You would need to be able to override it on a per torrent bassis,
but for people who download primarily just a couple of different file types
(cough).
On Sat, Oct 6, 2018, 4:35 PM shula <notifications@github.com> wrote:
> This feature could help security:
>
> If a user downloads only known files (e.g. audio/video), it is wise to
> block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS,
> (PY?), etc.
>
> Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi <>
> Matrix.avi.exe
>
> Some video torrents are bundeled with fake ""codec.zip"" ""driver.exe""
> containing malware. Today I spotted the attached file, an LNK file, an
> extension that is hidden on Windows.
>
> None of my users are downloading software via torrent, so I'd like to
> block it for them, or set blocked extensions; potentially, i'd like to
> block the whole torrent altogether, if a potential software is found in it.
>
> My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY,
> CPL, DLL, LNK, SCR.
>
> In the screenshots below:
> How windows hides LNK extension, which is a sure malware when only
> downloading media:
>
> [image: lnk-virus]
> <https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png>
> [image: virus2]
> <https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png>
>
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-427604310>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGbY-5e2u-o8-6NhlR-Z9UlUBj9q6YGgks5uiRPzgaJpZM4FT95h>
> .
>",Mocking
"Please add this feature since lot's of trackers now put lots of ""junk files"". Yes we can use other clients but qb has a lot to offer and you can always disable this feature if it bothers you, so it's a win-win.",
"I'd love this, too.",
This please! We need this!,Impatience
Is it possible now to completely fail specific torrent if it contains not appropriate file name?,Impatience
"+1 on the feature request.
In the meantime, I wrote a short cmd script which can be referenced in the _Tools->Options->Downloads->Run external program on torrent completion_ which renames files with a suspicious extension (.exe \ .scr \ .cmd \ .bat) to prevent them from running when double clicked.
```
rem Find suspicious files in directory and rename them
rem Usage: fsus.cmd <dirname>
@echo off
SETLOCAL
set extensions=""\.lnk \.exe \.cmd \.scr \.bat""
echo looking in %1 for %extensions%
for /f %%F in ('dir %1 /s /b') do (
(echo %%F | findstr /r %extensions% > NUL) && move %%F bad_%%F.BAD && echo Renamed %%F
)
ENDLOCAL
```",
"Plugin in python3 for this:
https://gist.github.com/oltodosel/566e051191f3a58b905db2cc6980656f",
Has there been any updates if/when this feature would be added?,Impatience
Since 2015 and this still isnâ€™t added yet? Come on. This would be such a useful feature.,Impatience
"I would love to have this feature implemented. There could be a global file extension filter, or have the filter setup by category (so categories can have different filters).",
2015-2020 is not create this function. qBit - Shit!,Bitter Frustration
"Why isn't this issue considered critical?
Distracted users shouldn't run viruses so easily.",Impatience
"> Why isn't this issue considered critical?
To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3"", and not ""distracted users clicking files with hidden extensions"" - that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK. Alternatively, just use better sites and download better torrents.
This is why this isn't ""critical"".",Mocking
"> To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3""
That's definitely my motivation for wanting this feature. I use the RSS downloader and it would be nice to automatically exclude unnecessary files.",
"> > Why isn't this issue considered critical?
> > that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK.
As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.
Also, it is arguably because torrent clients aren't smart that these types of malicious torrents are still going around, and that's why I think qBittorrent should provide this feature.
IMHO certain extensions should also be skipped by default, for the same reason. It would be a great security and usability improvement.
This also allows you to skip certain extensions because you prefer so, but the security issue should be considered prominently.",Impatience
"> As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.
If you are downloading torrents with malicious .lnk files, you need a solution for a more urgent problem: don't download such torrents, use better sites/sources, or pay more attention. After all, it is the user's responsibility to not fall for phishing emails as well. Inspect URLs/files you click.
Alternatively, you could try to enable showing `.lnk` extensions: https://www.tenforums.com/customization/111886-how-show-lnk-extension.html
But because Windows is Windows, this might lead to undesirable presentation elsewhere (such as the start menu).",
"Your solution is for power users, has usability drawbacks, and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.
I think the qBittorrent team should step up and fix this. There is almost never a good reason to download certain file extensions, and users should actively check those files for download.",Bitter Frustration
"You can't expect people that execute random files to know how to use power features. That something can (and should) be done another way isn't a reason to not include a security feature.
These are the same people that download from the first torrent site that shows up in Google search. So the ""use a better site"" isn't a valid argument either.
Obviously all this is only true if your target is the mass and not just tech-savvy people.",Bitter Frustration
"> Your solution is for power users, has usability drawbacks, Fighting phishing emails is something everyone has to learn to do, no matter the occupation. I think it is reasonable to demand a certain level of proficiency and common sense.
> and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.
""Malicious users are taking advantage of distracted/careless users"" would be a more accurate statement. Do you think the possibility of receiving phishing emails is a flaw of E-mail? If so, is the possibility of hearing the voice of a scammer in real life, believing what they say, and giving them money, a flaw of your ears? Should your auditory system should autoblock certain words/sentences on its own? Perhaps it should be the brain acting on the information instead.
Furthermore, the greater issue of downloading these kinds of torrents should not be underestimated. You have to go out of your way, even when searching for illegal content, to find these kinds of torrents. And no, the `.exe` in RARBG torrents does not count as an example of this practice in a popular site; it is actually just a harmless text file with the `.exe` extension designed to prevent mirroring by software that, ironically, relies on ""file extensions"" to make assumptions about their content.
Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.",Mocking
">You can't expect people that execute random files to know how to use power features. That something can (and should) be done another way isn't a reason to not include a security feature.
These are the same people that download from the first torrent site that shows up in Google search. So the ""use a better site"" isn't a valid argument either.
>
>Obviously all this is only true if your target is the mass and not just tech-savvy people.
First of all, it is indeed a shame that the Windows default is wrong, and that the way to change it requires some knowledge to do so. But that is a Windows problem. One can post an issue on the relevant forum/issue tracker about that, not here.
Secondly, regardless of such setting, I'm not really keen on catering to this kind of ignorance/stupidity of ""I'm carelessly clicking on stuff and ignoring warnings and expecting it to work"". If someone doesn't care to learn how to properly use a saw, should their complaints to the manufacturer be taken seriously when they cut themselves?
We should strive to make things easier to use. But not to the point of bending over backwards to a level of stupidity/ignorance/carelessness that shouldn't be endorsed or excused, at the expense of time, effort, and other important things.",Bitter Frustration
"> Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.
You don't need to click through anything. Once you've clicked the .lnk file an .exe will download in the background without any warning in less than a second, and it will run again unnoticed at the next restart. There are virtually endless possibilities to the harm that can be done by these attacks.
Furthermore, they're not even detected by most antivirus software.
If the qBittorrent team needs an example, I can provide it.",Bitter Frustration
"> You don't need to click through anything. Once you've clicked the .lnk file an .exe will download in the background without any warning in less than a second, and it will run again unnoticed at the next restart. There are virtually endless possibilities to the harm that can be done by these attacks.
> Furthermore, they're not even detected by most antivirus software.
> > If the qBittorrent team needs an example, I can provide it.
There are always exceptions to the rule. I am sure there are some examples of software bypassing UAC, or just being dangerous enough without needing to do so in the first place. But this is a secondary point anyway.
I should add to https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-652597093:
Again, I'm not saying it would be bad to have this feature. It would be good for automation purposes, for example. But I don't think it is fair to consider it ""critical due to user security considerations."". Of course just by being there it could serve as an additional safety net. But that's not the main purpose and it's by no means critical for that purpose.",
"I'm sorry Francisco but I really think you are underestimating the problem.
If we followed your logic, antivirus software and antispam software should be banned, because humans should be infallible and never miss a single malicious file.
This type of attack with torrents is too common, at a certain point client software (qBittorrent) becomes complicit in this kind of exploit, which has been reported as far back as 2018 here above.
If the same arguments were used by browsers, e.g. Firefox, sandboxing bugs shouldn't be fixed because the user should only visit ""trusted"" websites.
I hope the rest of the team doesn't treat security the same way as you do. This is an overdue feature, requested since 2015.
The team has been asked before for directions on where to look in order to implement a pull request, but no attention has been given to the matter.
If not even this convinces anybody who possesses the knowledge and expertise to propose a fix, I really have nothing more to add. I have seen the developers here dedicate a lot of time and effort to ""betterment"" projects such as the new webUI and API, therefore I really am surprised such a seemingly small-to-implement but security-critical issue has not been given space in 5 years.",Bitter Frustration
"@simo1994 > If we followed your logic, antivirus software and antispam software should be banned, because humans should be infallible and never miss a single malicious file.
This is one hell of a strawman fallacy, but ok. Not to mention that you seem to be advocating in favor of antivirus/anti-spyware. In a sane computing environment, a black-box proprietary software made by a for-profit corporation running with administrative privileges (aka """"""antivirus"""""") is not considered a layer of security. The true solutions for the problems these programs claim to solve lie somewhere else in the stack - don't use Windows, use Free (as in Freedom) software, package managers with cryptographic signing, etc...
> This type of attack with torrents is too common,
These kinds of claims are worthless if not substantiated with data. You can't just claim something and ask others to disprove it. Start by defining what is ""too common"". 30% of torrents? 40%? 50%? In which sites? The burden of proof lies with you. You're welcome to open a new issue investigating this, with some pretty graphs.
> at a certain point client software (qBittorrent) becomes complicit in this kind of exploit, which has been reported as far back as 2018 here above.
- This is not an ""exploit""
- ""complicit"" is a strong word. Are email clients/servers ""complicit"" for people falling for the Nigerian prince scam?
> If the same arguments were used by browsers, e.g. Firefox, sandboxing bugs shouldn't be fixed because the user should only visit ""trusted"" websites.
Again, ridiculous strawman. Browsers need sandboxing for any kind of user due to the unfortunate way the Web has evolved. In fact, mainstream browsers don't even come secure enough out of the box IMO.
> I hope the rest of the team doesn't treat security the same way as you do.
I treat security as seriously as anyone else who really understands it. In reality, you'd be surprised how little of it _you_ actually understand, to be able to make the arguments you make and then accuse others of not treating security seriously. Hopefully one day you'll be able to look back at this and chuckle, it will mean you have evolved and learned.
>The team has been asked before for directions on where to look in order to implement a pull request, but no attention has been given to the matter.
If not even this convinces anybody who possesses the knowledge and expertise to propose a fix, I really have nothing more to add.
Anyone is welcome to submit PRs to implement the feature. There isn't any sense of urgency, because likely all of those with the ability to implement it also know that this isn't ""security critical"".
> I have seen the developers here dedicate a lot of time and effort to ""betterment"" projects such as the new webUI and API, therefore I really am surprised such a seemingly small-to-implement but security-critical issue has not been given space in 5 years.
Again, it's not ""security critical"". Stop with the fear-mongering and FUD.
> I really have nothing more to add.
Me neither. I agree that everything relevant to this feature request has been said. Now it's up to whoever wants to implement it.",Bitter Frustration
"TL;DR:
- Anyone is welcome to implement this, and I'm also convinced it won't be rejected if implemented. As mentioned above, it may be useful for certain automation scenarios.
- The default should be to not filter anything, IMO. Otherwise, it would be unexpected behavior - people would wonder why certain files don't download by default. If qBittorrent ever gets some sort of ""onboarding"" UX, this could be one of the tunables (e.g. do you want to filter ""potentially malicious files"" by default?).
- No, this is not ""security critical"" (see discussion above).",
"### Expected Behavior
All directories created by `npm install` should have the npm config setting `umask` applied to their file mode/permissions. The `umask` setting is [clearly described as](https://docs.npmjs.com/misc/config#umask) the ""value to use when setting the file creation mode on files and folders.""
### Observed Behavior
During `npm install`, if directories are created by the `tar` module as a side-effect of extracting a file, the umask setting is ignored. For example, when tar extracts `foo/bar/baz.js`, it creates the `foo` and `foo/bar` directories (if they don't exist) before writing `baz.js`. These directories do not respect the umask setting.
Instead, npm's umask setting is ignored and [`process.umask()` is used](https://github.com/npm/fstream/blob/master/lib/writer.js#L15). However, there is a further bug/unexpected behavior where the `tar` module will actually [do the equivalent of `chmod a+x`](https://github.com/npm/fstream/blob/master/lib/writer.js#L352) on directories created as a side-effects of file extraction, meaning the even `process.umask()` isn't strictly observed.
### Details
- OS: Mac OS X 10.9.5
- `npm --version`: `2.13.5`
- `node --version`: `v0.10.40`
- `npm config get umask`: `0077`
- `umask`: `0077`
An example is seen if you install `grunt-lib-phantomjs`. The directory `node_modules/phantomjs/lib` (as an example) should have a mode of `0700`; instead, it has a mode of `0711`.",
"Since I see this has been tagged as a feature request and support request, I want to be clear that this is definitely a bug in npm.
The umask setting in npm is being completely ignored for a seemingly arbitrary subset of operations during `npm install`. This is almost certainly not the intended behavior. It also explicitly disagrees with the behavior described in the documentation, and is inconsistent even within the observed behavior of npm (sometimes it respects umask, other times it doesn't, for no good reason).
The underlying cause (cited in the issue) is that the tar module isn't aware of npm's `umask` setting, and just does its own thing (using the `process.umask()` value, then overriding the execute bit on it.)
But in summary, this is 99.9% likely to be a real bug, not a new feature or user usage problem.",Bitter Frustration
"This issue still exists in npm version 3.4.0 and is most certainly a bug and not a new feature or user usage problem.
Please see https://github.com/npm/npm/issues/4197 which is correctly tagged as a bug.
#### Details
- OS: Debian GNU/Linux 7.8 (wheezy)
- `npm --version`: `3.4.0`
- `node --version`: `v4.2.2`
- `npm config get umask`: `0022`
- `umask`: `0077`",
"This is marked as a feature request in part because the current behavior that npm has is underspecified. The first step to making npm's behavior here clearer is to nail down what the current behavior is, and why. Tagging this with `footgun` gets it onto npm's road map, and the next step for the CLI team is to unearth and document the historical reasoning for how the various pieces of npm (including `node-tar`) handle permissions.",
"After discussing this as a team, we think the right thing for npm to do is to ignore whatever permissions or UIDs are set in the package tarball, and explicitly squash everything to be written with the current user's user ID and umask in all cases _except_ when npm is being run as root without `--unsafe-perm` being overwritten (you should never have files owned by nobody on your filesystem).
This is a mildly tricky bit of work because it requires good tests, and also because we need to make sure whatever API calls the CLI uses don't cause problems on Windows, but this is something we plan to address within the medium-term. If somebody else wants to treat the first paragraph as a rough spec and start working on a patch, that would be very welcome!",
"Did this issue get lost (honest question, no sarcasm)?
In my opinion it needs lot more love as the possible security implications could be quite catastrophic on a multi-user system and this issue is reported for over a year now.
The worst case scenario is that such directory results in the possibility to replace its content by a non-root user with evil code that may me executed by another user (including possibly root).
What I observed (even with newest npm 3.10.9) that it **sometimes** creates node_modules directories with permission 777. when doing the `npm install` multiple times the results vary, most of the time it results in 755 but sometimes in 777). This seems to have nothing to do with the source tarballs content (retrieved from registry.npmjs.org) but a more general issue. As mentioned, its not deterministic and the tarballs definitively don't contain any files/directories with such permissions.
This problem was observed while creating packages for a Linux distribution and boiled down to finding this ticket.",Bitter Frustration
"We're closing this issue as it has gone thirty days without activity. In our experience if an issue has gone thirty days without any activity then it's unlikely to be addressed. In the case of bug reports, often the underlying issue will be addressed but finding related issues is quite difficult and often incomplete.
If this was a bug report and it is still relevant then we encourage you to open it again as a new issue. If this was a feature request then you should feel free to open it again, or even better open a PR.
For more information about our new issue aging policies and why we've instituted them please see our [blog post](http://blog.npmjs.org/post/161832149430/npm-the-npm-github-issue-tracker-and-you).",
"@othiym23 @isaacs @iarna
can you please reopen and revisit this issue, npm 5.6.0 still randomly ends up creating node_modules directories with 777 that contain code.
PS: This npm-robot that auto-closes a security issue because nobody replied is quite damaging",Bitter Frustration
"Aug 20, 2015
O I am laffin.",Mocking
"Issue not fixed in 30 days?
Must be gone!",Impatience
"Not treating security seriously, are we?",Bitter Frustration
"Yeah, this seems to have gotten swept up -- the bot shouldn't have just closed the issue like that.
There was a step forward and a step back on this over the past year, and we haven't touched the issue since. The `patch-welcome` tag continues to apply, so if you think writing code is a more worthwhile endeavor than snarking on foss issue trackers, we super welcome your contributions!",Bitter Frustration
"The current default is to display end caps on error bars.
![errorbar_demo_features](https://cloud.githubusercontent.com/assets/2631586/9798244/bde5a546-57cd-11e5-89c8-e37e225a301c.png)
I argue that end caps are not useful as visual cues for most plots. They are seriously harmful in ""busy"" plots with many data points, where error bars start to overlap and the end caps start to cross other error bars.
Tufte teaches us that we should maximize the data-to-ink ratio. End caps do not provide any additional information over the error bar, so they should not be there by default. End caps are useful to indicate a secondary interval, like the systematic uncertainty of a point, a beautiful example is Fig. 13 in this paper:
http://arxiv.org/pdf/1409.4809v3.pdf
I recently introduced a patch to make the default length of these end caps part of the configuration, so that they can be turned off by default (by setting errorbar.capsize to 0 in matplotlibrc). Please make this the new default. :)",
"As a user (not yet a contributor), I'm not a fan of this change. I'd guess that >90% of the plots I see in astronomy have end caps, so that seems like the natural default choice.",
"fergalm's argument in favour of end caps is not a very good. Here are some counter-arguments:
1) I'd argue that the new defaults should be based on proven design principles, not on what's common. If we chose based on common use, then we should also keep jet as the default colormap, because ""everybody"" is using jet.
2) matplotlib is popular in the astronomy community. I wouldn't be surprised if endcaps are common, exactly because they are the default in matplotlib. So unless we change the default to something that is better, fergalm's argument will remain a tautology.
3) No information is lost when end-caps are removed, but visual clarity is gained in all plots, and especially in busy plots with many points and nearly overlapping error bars.",
"I would also be against removing caps as well because it isn't actually
""extra ink"". If you think about it, the lines are really the extra ink.
Those lines don't communicate anything useful except to connect the end
caps to the mean, which could easily be accomplished by having grid lines
(which is another proposal). Endcaps are also useful in projector
situations because long, thin, vertical lines aren't always very visible in
conference rooms.
On Thu, Sep 17, 2015 at 2:25 PM, Fergal Mullally notifications@github.com
wrote:
> As a user (not yet a contributor), I'm not a fan of this change. I'd guess
> that >90% of the plots I see in astronomy have end caps, so that seems like
> the natural default choice.
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/matplotlib/matplotlib/issues/5047#issuecomment-141176072
> .",
"> 1) I'd argue that the new defaults should be based on proven design principles, not on what's > common. If we chose based on common use, then we should also keep jet as the default > colormap, because ""everybody"" is using jet.
""What's common"" is a great design principle. It's also known as the principle of least surprise. I suspect most people will be surprised if they try to plot an error bar and don't get end caps by default
> 2) matplotlib is popular in the astronomy community. I wouldn't be surprised if endcaps are common, > exactly because they are the default in matplotlib. So unless we change the default to something > that is better, fergalm's argument will remain a tautology.
The current matplotlib style of drawing error bars mimics a style that pre-dates matplotlib by many years. For example, pgplot draws end caps, and it was created in 1983.
> 3) No information is lost when end-caps are removed, but visual clarity is gained in all plots, and > especially in busy plots with many points and nearly overlapping error bars.
In sparse plots the end caps draw attention to the error bars, so I think visual clarity is lost by removing them.
There's a lot of ""I think"" and ""I like"", because ultimately this is an aesthetic choice. But my vote is against.",
"Always remember the style module so if you don't like the defaults it is very easy to override the ones you don't like.
I think a 'tufte' style sheet would be a worthwhile thing to have even if it is not everyone's cup of tea.",
"> ""What's common"" is a great design principle. It's also known as the principle of least surprise. I suspect most people will be surprised if they try to plot an error bar and don't get end caps by default
The principle of least surprise is a good one, I agree, and the change I am requesting is not very surprising.
On the other hand, ""What's common"" is a terrible design principle. All progress is about change, not about doing the same thing all over. Do you have an iPhone? Think about why they are so popular. Before we got them, nobody could imagine that phones and software could be that intuitive and playful. Now we cannot imagine how we could live with the clunky interfaces we had before. iPhones were different in a better way, and that's why they took over. I don't want to be dragged into an argument about commonness and the ""majority of users"", because it is not the point.
> I would also be against removing caps as well because it isn't actually ""extra ink"". If you think about it, the lines are really the extra ink.
The command is called ""errorbar"". It draws error _bars_, you can hardly call that the ""extra ink"". If the lines are too thin, they can be made bigger. This is again not the point. I am also not against end caps in principle, as shown in the paper cited initially. The end caps alone (without a bar) are very useful to indicate a secondary uncertainty, typically a systematic uncertainty, in addition to a statistical uncertainty indicated by the error bar.
The default changes are a great chance for matplotlib to go forward. I offered my arguments, and I can continue to defend them, but I respect the decision of the higher-ups whatever they decide. As a friend and promoter of matplotlib, I am just trying to contribute in a positive way.",Bitter Frustration
"Here a direct comparison with thicker lines instead of caps:
![bla](https://cloud.githubusercontent.com/assets/189880/9970616/6a98ea80-5e56-11e5-97e3-ce2fbf53c96f.png)",
"For what it's worth (and I agree it's not worth a whole lot), I did an unscientific poll of the astronomers in my office. I asked ""If you were starting to use a new plotting package, would you prefer if the plot error bar function plotted endcaps by default or not"". There were 6 votes in favour of end caps by default, and 2 vote against. One of those two voting against changed their minds after discussing it with me, but it's probably fairer to record their initial vote.",
"In addition to the nice comparison from Tillsten, here is an example of a more busy plot. It visually compares two Gaussian distributions. I used the standard settings for the plot on the left and alpha=0.5. For the plot on the right, I used capsize=0, linewidth=2, alpha=0.5.
Plots with a high density of data points like this are typical in my area of research, high energy physics and astroparticle physics. With endcaps, the plot looks more messy and are harder to read. Our brain merges all these thin lines into a kind of blur. Look at the points near the maximum, for example. Without the end caps, the points stand out more, and the two distributions are easier to separate by eye.
![figure_1](https://cloud.githubusercontent.com/assets/2631586/10051780/4d8ec330-61f1-11e5-9986-b6aa416ae1e8.png)",
"I'm convinced. Losing the endcaps would make for a more functional as well as aesthetically pleasing default. By emphasizing the precise ends, the endcaps are visually misleading; errorbars are _rough_ estimates of a range, typically based on arbitrary cutoffs in an assumed statistical distribution. It would make more sense for them to fade out at their ends than to have them capped by lines; but that's getting too fancy. A simple line is a reasonable compromise.",
"We should probably pick a default line thickness and marker size that would
result in properly centered lines. This has actually been a long-standing
pet peeve of mine for errorbars (you can see the non-centered-ness in the
examples posted here).
On Wed, Sep 23, 2015 at 2:05 PM, Eric Firing notifications@github.com
wrote:
> I'm convinced. Losing the endcaps would make for a more functional as well
> as aesthetically pleasing default. By emphasizing the precise ends, the
> endcaps are visually misleading; errorbars are _rough_ estimates of a
> range, typically based on arbitrary cutoffs in an assumed statistical
> distribution. It would make more sense for them to fade out at their ends
> than to have them capped by lines; but that's getting too fancy. A simple
> line is a reasonable compromise.
> > â€”
> Reply to this email directly or view it on GitHub
> https://github.com/matplotlib/matplotlib/issues/5047#issuecomment-142682505
> .",Bitter Frustration
"Yes, I noticed that too. I presume it is caused by snapping to pixel boundaries. Unfortunately, I don't see how that can be solved via the defaults. Sizes are in points, and the translation to pixels depends on dpi, which can be changed at any time. We would need a way of ensuring that anything subject to snapping gets expanded or contracted to a width that is an odd number of pixels. Or something like that. attn: @mdboom",
One think i don't like about thicker lines is that this leads to more visual weight on the points with the biggest errors. Probably the best would increase the alpha with the error size :),
"what purpose do error bars have?
- for categorical x axes there are violin plots that better show distributions. another alternative would be to use vertical gradients instead of hard ends (the intensity of the color would be the probability)
- for continuous x axes, you should use a line plot instead of individual dots and correspondingly a shaded error region instead of the error bars.",
"flying-sheep: this is starting to get somewhat off-topic, but violin plots are not the ultimate answer to everything. I would argue again with Tufte: why use more ink when the same information can be represented with less?
Simple error bars are good indicators if:
- the represented probability distribution is normal and everybody knows it; then a central value (point) and a simple +/- 1 sigma interval (error bar) summarizes the information perfectly
- the represented probability distribution is not known, e.g. for systematic errors that are only estimated as a kind of upper limit
A histogram with Poisson uncertainties on the individual bins is a standard case that is well represented by errorbar.",
"I have to say that I find this change simply horrible. It removes the information where error bars end and thus renders them unusable without the caps. Consider the following example:
![without_caps](https://user-images.githubusercontent.com/22542812/45039484-04e2f100-b064-11e8-9be0-704e66d698a9.png)
![with_caps2](https://user-images.githubusercontent.com/22542812/45040016-3f995900-b065-11e8-8390-102fdadfb454.png)
The first plot indicates that the errors of line1 are too small to be seen (hidden behind the marker) and thus negligible. The plot with caps however reveals that this is certainly not true. The vertical lines are arguable only visual clutter however they are a guide to the eye to find the actual errors.
Obviously I can just change the settings (after finally finding out these things are called caps). I find this change however very dangerous, as in a less obvious case I am tricked into reading the plot wrong. This should never happen! Thus, I would argue to make the caps bigger than the actual marker by default.",Bitter Frustration
"@DerWeh I'm sorry you had this issue, however it is unlikely that we will revert this change. The use cases for Matplotlib are very broad and a stylistic choice that makes sense in one field (or with one set of data) does not make sense with another. This issue appears to happen when 1) you have discrete x values shared across all of your datasets 2) the y values are almost identical 3) the error range of the first set is strictly less than the second at all points. If any of those things were not true, we would not be having this conversation ;). You might also consider setting the alpha of each of the artists to 0.5 so you can see when they overlap. I would argue the real bug here is the interlaced z-order layering of the markers, errorbars, and caps...",Irony
"@DerWeh The standard solution to your plotting problem is to add slight offsets for two sets of points. Shift the orange points a pixel to the left and the blue points a pixel to the right. Then they are very nicely comparable. Furthermore, if you insist on caps, although they are intentionally absent in most high-quality scientific papers in my field, you can still override matplotlibrc. Just set the capsize to 3.",
This is a kind of solution: https://stackoverflow.com/a/49838455/2803344,
"I lost this argument a number of years ago, but I will say I have yet to meet anyone who thinks the default is the better choice.",Irony
"the overlay problem is just a symptom of another problem with that plot: directly overlaying errorbars is not something you should ever do. moving them slightly to the side is how that plot should have looked in the first place.
thereâ€™s quite a few people who think the new default is better in this very thread.
but to be fair, matplotlib doesnâ€™t make it easy to do the right thing. `hist` automatically staggers the bars, but you canâ€™t do the same for other kinds of plots.",
"> thereâ€™s quite a few people who think the new default is better in this very thread.
But none of them I've ever met. The people I work with who've expressed an opinion all dislike it. But I'm not going to relitigate it.",Bitter Frustration
"well, physicians and physicists also preferred the jet colormap, because it was ubiquitous in their respective fields. studies show that it however decreases their performance in interpreting plots. what people prefer is never a good argument. itâ€™s at most a hint at what *might* have proven itself through experience",Bitter Frustration
"I agree with @flying-sheep. You need objective rational arguments for your position. The new defaults were chosen so the average plot looks better. There are principles and rational arguments behind this decision, especially ""increase the data-to-ink ratio"", https://infovis-wiki.net/wiki/Data-Ink_Ratio.
You can still change the default to whatever you prefer, on a plot-by-plot basis and even permanently, by changing your `matplotlibrc` file.",
"I don't want to reopen the discussion here, I will just accept that matplotlib makes horrible choices and will always keep the overhead in my code, to always remember it.
@HDembinski The data-ink ratio is actually an argument against this choice. The cap contains the complete information, namely how big the uncertainty is. The error line itself contains no information whatsoever. It is solely a guide to the eye, to find the end more easily. What I really want to use this answer for, is to strongly emphasis that the defaults should not focus on making nice looking plots but unambiguous ones. Aesthetics is to a certain point always up to the beholder, and he is encouraged to change the design in a way he finds pleasing. However, wrong information should never be conveyed. That is why it was so important to change the default color map. This belongs to me in the same category.
The plot I showed was exactly such an example (and it is a real-life example). I tried different algorithms and suddenly my plot showed me that algorithm 1 is orders of magnitude better, as there was no more visible error in comparison to plot 2. You can now say, just change the design how you like it. But that just makes it even more dangerous for me, as I might forget about the terrible choice matplotlib does. I go to a different machine and might do the same error again and might lose a workday if I am not careful.
Thus, I just plead, focus that the plots represent the information. What you do afterwards to make the plots look better doesn't bother me. If I want a good-looking plot, I will change it anyway. But never misguide me, when I just want a quick representation of my data.",Bitter Frustration
"W/o wading through the religious war above, I think this behaviour would be an obvious one to expose via an rcParam if itâ€™s not already. If it is an rcParam already, then Iâ€™m not sure what the problem is.",
"Like I said, I will live with it. But I still consider it extremely harmful behavior. I don't care about aesthetics, first the representation has to be correct.
The real life example is shown [here](https://github.com/matplotlib/matplotlib/issues/5047#issuecomment-418405715). I compared to algorithms, took a quick look, and saw that algorithm 1 is orders of magnitude better, as the error is not visible with bare eye. Thus, I move on using algorithm 1. This is what I consider harmful default behavior, as the default plot gives me wrong information.
Of course, I can change the configuration. But this only increases the problem, I do some quick analysis on a different machine and forgot that I have to change the behavior. The result: I draw wrong conclusions and lose in the best case some hours till I realize my error. It is basically the same issue matplotlib previously had with the color map defaulting to jet.",Bitter Frustration
"This thread is getting contentious and needlessly personal.
It is the case that this behavior can be controlled both by an rcparam (for global control) and via a kwarg (for per-call control). It is also the case that we are not going to change the defaults.
Given those three things I am going to lock this thread as I do not think further discussion is going to be productive.",
"As a procedural note, was between ""resolved"" and ""too heated"" in the lock dialog. People with commit rights will still be able to comment and can unlock the thread.
If anyone has issues with me locking this and either can not (or does not want to) comment here, send me an email.",
"There's this thing with the attachment and emoticon buttons that they activate on hover (bringing up the little windows)... and to be quite honest I find that a really weird behaviour.
So I'd just like to ask the others here in the issue-land whether it's just me or they too are bothered by it.
It doesn't seem like a hard thing to change either way (I mean, to activate them on click like every other button/menuitem/whatever).",
"+1, it's VERY annoying.
Especially taking in account a wide-spread reading habit to point with a mouse onto a key areas. One tiny move over the last message timestamp and a huge popup blows in eyes.
**Add.**:
Regardless of any ""reading habits"" this ugly popup impertinently pops up during a very generic mouse operation:
1) right-click inside the input field and paste a short (not more then 1 line) text
2) move mouse to the right to click on the ""Send"" button",Bitter Frustration
"I agree with @ralesk and @VitRom . We worked for years with the point and click system, it's really annoying that hovers make the tooltip appears accidently. I was thinking maybe it's better to show the emoticons tooltip as a fixed sidebar, same as this:
![dfgfdg](https://cloud.githubusercontent.com/assets/15929497/11879979/cc1ff232-a4fe-11e5-965e-056c15f4dd5e.png)
This will prevent accident hover thing, and also increases speed in searching and choosing the sticker we want to send.",Bitter Frustration
"Still wishing for the hover popups to be turned into click-activated ones. Any feedback from developers on this, and other users?",Impatience
I still regularly bump into not wanting to open the emoji popup but managing to and ending up almost clicking an emoji.,Bitter Frustration
"After working a lot with Telegram, I get used to it and now I'm too lazy for clicking on emoji button for opening it! ðŸ˜„",Irony
"A two kind of jokes are here, one with a smile(s), and another one for those who understands that a joke's salt not in the smile(s).
Hover-activation is for the first only.
Er... :-)
PS.
Just curious HOW is this stupid hover should work on the completely click-driven systems like Android phones. AFAIK there's no any mouse nor any other source of ""hover"" events. So WTF how this ""brilliant smart design piece"" decision been placed into production at all?",Bitter Frustration
My biggest problem with all the UI bugs I reported is that nobody from Telegram bothers to respond to it.,Bitter Frustration
#1751 btw,Impatience
Thanks @stek29. Amazing response time there too.,Irony
"The user's list from a group, with less than 100 members, just pops up when I move the mouse to the Back Button when I want to go back to main screen. That's pretty annoying, because I end up viewing a user's profile if I just insta-click.
![image](https://cloud.githubusercontent.com/assets/8980291/24885070/c1ce0fa8-1e4c-11e7-9489-1388c6fec3c2.png)",Bitter Frustration
"@gottesman That's a regression, this dropdown should appear below the top bar (which leads to group profile or back), I'll fix that in the next version, thanks.",
I wonder if there's even a point of that popup. I mean going to the group profile gives you a much more convenient user list anyway.,Impatience
"@john-preston It's way better now in the new Alpha :thumbsup:
![image](https://cloud.githubusercontent.com/assets/8980291/24935824/b2229352-1f24-11e7-9eee-a8f99a557632.png)",
"@john-preston but what about this original issue with emoji toolbar? If you afraid to change UX, simply add option to switch between mouse move and click on emoji button.",Impatience
@VBelozyorov Now you can just use three column mode and not have emoji pan appear on hover event.,
"Excuse me, what is > three column mode
I've tried to search, but no success.",Impatience
@VBelozyorov You can click on the emoji icon near the send button and switch to a three column mode.,
"Hmmâ€¦
I have v1.0.29 on Linux (openSUSE Leap 42.2)
But I have no send button:
![Version and input field](https://cloud.githubusercontent.com/assets/860208/25097142/ed017dae-23ab-11e7-8d9f-3a5dcf55c85e.png)
Meanwhile emoji toolbar looks like this:
![Emoji toolbar](https://cloud.githubusercontent.com/assets/860208/25097238/62c0876a-23ac-11e7-8dd6-31b48c8ba994.png)
It's quite three-columned for me, but it's still appears on mouse hover on emoji button",
"@VBelozyorov I mean click on the emoji icon (near to send button when send button is displayed, when it is not â€” near to the record audio message button, in the bottom right corner of the screen).",
"Sorry, I do not understand - what should happen when I click on it?
What happens now: _hover_ - _toolbar appear_ - _click_ - _toolbar disappear_
And all goes the same way on next _hover_, behavior is not changes.
![vokoscreen-2017-04-17_21-10-37](https://cloud.githubusercontent.com/assets/860208/25098745/6c930ffa-23b2-11e7-8ae7-f5fc5d4ba8c7.gif)",
@VBelozyorov Looks like your screen is too small for the third column then :( Sorry. It was supposed to become like this:,Mocking
![image](https://cloud.githubusercontent.com/assets/17900494/25098912/f3e46472-23b2-11e7-83a2-8f1fc09f65ac.png),
"1920Ã—1080 isn't big enough?
Tried with maximized window, of course. (GIF with that resolution too big to upload here)",Impatience
"@VBelozyorov Ah, I forgot! It is available only in the alpha version right now, you can try it from https://desktop.telegram.org/changelog#alpha-version or wait for the next stable version update.",
"It's interesting, but not solution (for me). All those emoji overload window and make message perception more difficult.
Thanks for make this panel optional.",Bitter Frustration
"I stand with @VBelozyorov :) Glad that the emoji toolbar three panel mode is (will be) optional; it certainly doesn't solve the issue for those who use the collapsed (narrow window, single panel) mode. The hover on the bottom icons (or really, everywhere) gets in the way too much.",Bitter Frustration
P.S. Thanks for finally responding to this issue.,Impatience
"Just wanted to chime in to keep this thread active because this is actually pretty important. A setting to turn off emoji panel open-on-hover is the proper solution in my opinion. I love telegram and will continue to use it, but this happens to me dozens of times per day and gets really frustrating.",Bitter Frustration
"The smileyface is too close to the ""scroll to bottom"" thing and the hover thing always opens and instead of scroll to bottom, your cursor now points to a heart icon which is where the scroll to bottom buttom was before the hover thing appeared over it. Annoying.",Bitter Frustration
I can't count the amount of times I have to try and retry and retry clicking something because the god damned hover smileys popped up.,Bitter Frustration
"I'm here because of the stupid emoji menu that annoys the living daylight out of me. I NEVER use emoji or stickers or gifs, so the only purpose the emoji popup has for me is to drive me insane.",Bitter Frustration
"It is just unbelievable...
The only thing many people here need is a configurable hover timeout for damned emoji button. No bloody UI filosophy or UX ideology. Just one configurable timeout value. What the heck?",Bitter Frustration
How has it taken nearly 3 years to just add a simple 250ms delay on that damn emoji button?,Bitter Frustration
"@S-U-M-1 You're saying ""has taken"" as if it was done.",Mocking
@YanDoroshenko Exactly.,
@S-U-M-1 Erm... So... It's done???,Impatience
Not that I can see...,Irony
"@Kirrrr No, I'm saying ""exactly"" as in it wasn't even done, let alone ""has taken"" time to do.",Bitter Frustration
Showing on click was added to Settings > Advanced > Experimental.,
"I use RoR 4.2.5
in migration:
```
t.decimal :price, precision: 12, scale: 2
```
Convenient to use `@value.price?` to check for the value of any. Check for is not nil and not zero.
```
@value.price = nil
@value.price? #=> false
@value.price = 0
@value.price? #=> false
```
All Ok!
But!
```
@value.price = 0.01
@value.price? #=> false
@value.price = 0.9
@value.price? #=> false
@value.price = -0.9
@value.price? #=> false
@value.price = 1.0
@value.price? #=> true
@value.price = -1.0
@value.price? #=> true
```
WHY?",Bitter Frustration
"@Sega100500 Can you print what value it assigns to price when you assign ""0.9""? i.e. ``` ruby
@value.price = 0.9
p @value.price
```
and paste the output here.
I tried above code and its working. I have doubt if it assigns 0 (floors the number) when you try to assign 0.9 (decimal number).",Impatience
"@anujaware ```
0.9
```
What are you expecting something else?",Impatience
Then it should return true. I was thinking if its assigning 0 in your case. I am not able to reproduce this error.,Impatience
"@anujaware Web console:
```
>> p @product.price
=> 0.9
>> p @product.price?
=> false
```",
"@anujaware Exactly!
""Then it should return true""
But it return false",Bitter Frustration
@Sega100500 You can debug this in lib/active_record/attribute_methods/query.rb#query_attribute(attr_name) in activerecord gem.,
"@anujaware Web console:
```
>> @product.price = 0
=> 0
>> p @product.price?
=> false
>> @product.price = 1.0
=> 1.0
>> p @product.price?
=> true
>> @product.price = 0.9
=> 0.9
>> p @product.price?
=> false
```",
"@anujaware ruby -v
ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-linux]
RoR 4.2.5
May be it help ?",
"@anujaware BUG!!! in `query_attribute(attr_name)`
```
if column.nil?
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
!value.blank?
end
elsif column.number?
!value.zero?
else
!value.blank?
end
```
Steps:
```
>> @product.price = 0.9
=> 0.9
>> @product.price.zero?
=> false
>> @product.class.columns_hash['price']
=> nil
>> @product.class.columns_hash[:price]
=> nil
>> p @product[:price]
=> 0.9
>> 0.9 === Numeric
=> false
>> Numeric === 0.9 || 0.9 !~ /[^0-9]/
=> true
>> 0.9.to_i
=> 0
>> 0.9.to_i.zero?
=> true
```",Bitter Frustration
"I think this line `Numeric === 0.9 || 0.9 !~ /[^0-9]/` creating the issue. As `===` checks the case equality so when we compare `Numeric === 0.9` it returns `true` as `Numeric` is an ancestor for `Float` numbers. I think the fix would be something like this:
```
if column.nil?
if Numeric === value || value !~ /[^0-9]/
return !value.to_f.zero? if Float === value
!value.to_i.zero?
else
return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
!value.blank?
end
elsif column.number?
!value.zero?
else
!value.blank?
end
```",
"@sivagollapalli Not the most elegant solution. But even so.
PLEASE FIX it",Impatience
"@sivagollapalli ""As === checks the case equality so when we compare Numeric === 0.9 it returns true as Numeric is an ancestor for Float numbers.""
FALSE!
```
>> 0.9 === Numeric
=> false
>> 0.9 !~ /[^0-9]/
=> true
```",Mocking
Still I am not able to reproduce this issue with ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-linux] and rails 4.2.5. Before having solution. I am not getting why you are not getting that column in column_hash. And why we have Numericality check. Which database are you using?,Bitter Frustration
"@Sega100500 I am not able to reproduce this issue.
I have tried both Ruby 2.2.2 and 2.2.3. Also I double checked with Sqlite3 and Postgresql on both Ruby versions.
```
$ ruby -v
ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-darwin14]
$ rails -v
Rails 4.2.5
```
``` ruby
2.2.3 :003 > pr
=> #<Product id: nil, price: nil, created_at: nil, updated_at: nil>
2.2.3 :004 > pr.price
=> nil
2.2.3 :005 > pr.price?
=> false
2.2.3 :006 > pr.price = 0
=> 0
2.2.3 :007 > pr.price?
=> false
2.2.3 :008 > pr.price = 0.01
=> 0.01
2.2.3 :009 > pr.price?
=> true
2.2.3 :010 > pr.price = 0.09
=> 0.09
2.2.3 :011 > pr.price?
=> true
2.2.3 :012 > pr.price = -0.09
=> -0.09
2.2.3 :013 > pr.price?
=> true
```
Can you create a new Github repo to reproduce this error?",Impatience
"@anujaware ""Which database are you using?""
MySQL",
"@anujaware ""I am not getting why you are not getting that column in column_hash""
Because this SQL:
```
SELECT products.*, contents.*, products.id AS product_id FROM `contents` INNER JOIN products ON products.content_id = contents.id WHERE (contents.visible != 0) AND `contents`.`visible` = 1 AND `contents`.`furl` = 'apple-iphone-5s-16gb' AND `products`.`id` = 191 AND `products`.`category_id` = 312 GROUP BY contents.id
```
to get with related content-page
```
>> @product.class.columns_hash.keys
=> [""id"", ""parent_id"", ""level"", ""position"", ""node_type"", ""user_id"", ""group_id"", ""restrictive_group_id"", ""visible"", ""system"", ""list"", ""no_children"", ""no_images"", ""no_files"", ""key_lock"", ""key"", ""url"", ""furl"", ""alias"", ""name"", ""brief"", ""text"", ""title"", ""seo"", ""keywords"", ""description"", ""special"", ""content_parameter"", ""content_date"", ""created_at"", ""updated_at""]
```
```
>> @product.price = 0.9
=> 0.9
>> @product[:price]
=> 0.9
```
It is VALID COE! But you have not considered this possibility.
columns_hash NOT contain joined columns but access to they - it possible!",Bitter Frustration
@adityashedge Too simple example for ALL possible usages.,Impatience
"@anujaware Anyway
the code
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
It is BUG! Not correct to check Float numbers.",Bitter Frustration
"@Sega100500 Looks like there is no issue. Just now I have checked with `mysql` and `sqlite3` with `4.2.5` and `ruby-2.2.3`. All my tests are passing. If you still face issue please update below template and submit so that we can check.
Please ignore my previous comments. ```
begin
require 'bundler/inline'
rescue LoadError => e
$stderr.puts 'Bundler version 1.10 or later is required. Please update your Bundler'
raise e
end
gemfile(true) do
source 'https://rubygems.org'
gem 'rails', '4.2.5' #gem 'sqlite3'
gem 'mysql'
end
require 'active_record'
require 'minitest/autorun'
require 'logger'
ActiveRecord::Base.establish_connection(adapter: 'mysql', database: 'bug_test')
ActiveRecord::Base.logger = Logger.new(STDOUT)
ActiveRecord::Schema.define do
create_table :products do |t| t.decimal :price, precision: 12, scale: 2
end end
class Product < ActiveRecord::Base
end
class BugTest < Minitest::Test
def test_price
p = Product.new
p.price = 0.9
assert_equal true, p.price?
p.price = 0.01
assert_equal true, p.price?
p.price = -0.9
assert_equal true, p.price?
p.price = 1.0
assert_equal true, p.price?
p.price = -1.0
assert_equal true, p.price?
end
end
```",
"@sivagollapalli See above
1. joined tables
2. bug code for check Float is:
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
Also any questions?",
"@sivagollapalli Are you crazy?
```
create_table :products do |t| t.string :price
end ```
At BEGIN
```
t.decimal :price, precision: 12, scale: 2
```",Bitter Frustration
The code in `lib/active_record/attribute_methods/query.rb#query_attribute(attr_name)` is incorrect to check Float value! Why are you trying to prove the opposite in every way?,Bitter Frustration
"@Sega100500 oops, that's my bad. Now I modified column to `decimal` but it didn't change my result. Still my tests are passing. Could you please paste exact code snippets of your project so that it easy to debug?",Impatience
"@sivagollapalli
Oooooohhhh!
SEE ABOVE!!!
1. joined table
```
SELECT products.*, contents.*, products.id AS product_id FROM `contents` INNER JOIN products ON products.content_id = contents.id WHERE (contents.visible != 0) AND `contents`.`visible` = 1 AND `contents`.`furl` = 'apple-iphone-5s-16gb' AND `products`.`id` = 191 AND `products`.`category_id` = 312 GROUP BY contents.id
```
1. Then `columns_hash` not contains `price` column
2. Then code:
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
Is BUG!
This can be understood?
See the sources, Luk!
`lib/active_record/attribute_methods/query.rb#query_attribute(attr_name)`",Bitter Frustration
"@Sega100500 You can use this repo to reproduce the exact error. It does not occur for a single table(model). What I understand from your comments is that it occurs when you join multiple tables.
https://github.com/adityashedge/test-2.2.3-22424.git",
"@anujaware ""when you join multiple tables""
Not only that!
In case when
```
if column.nil?
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
!value.blank?
end
```
if `column.nil?` - is true
The code:
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
is BUG to check Float number
`(0.9).to_i.zero?`
What do you think will be the result?",Impatience
"@Sega100500 less yelling please.
http://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#create-an-executable-test-case",Impatience
"@anujaware Pure Ruby code:
```
p (0.9 === Numeric)
p (0.9 !~ /[^0-9]/)
p (0.9).to_i.zero?
```
result:
```
false
true
true
```
And now turn on the brain!",Mocking
"@matthewd No yelling.
I just point to bug.
Why do I still have to prove anything? Why do I have to write some other tests?
It seems pretty clear, I explained the situation.",Bitter Frustration
@Sega100500 https://github.com/rails/rails/issues/22424#issuecomment-160188274 is the end of this conversation.,Impatience
"@matthewd
Your point is - only you decide RoR will work correctly or incorrectly.",Bitter Frustration
"Well apart from the unrelated things, is this really an issue? Need some solid confirmation.",Impatience
"> Your point is - only you decide RoR will work correctly or incorrectly.
@Sega100500 pretty much yes. He is part of the core team and he is more than able to decide how Ruby on Rails will work.
But he didn't closed the issue because you are wrong or not. Let me explain why.
[By our licence](https://github.com/rails/rails/blob/master/railties/MIT-LICENSE):
> THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND
which means that nobody, again nobody, has obligation to fix your issues, no matter how wrong they are. We do because we want to, and you removed all the our desire to help you with comments like:
> PLEASE FIX it
> Are you crazy?
> And now turn on the brain!
At this point I may point you to our code of conduct http://rubyonrails.org/conduct. We believe you are not respecting it and we will not accept you behavior here.
@adityashedge I believe it is an issue but I don't care if it will fixed anymore. If you want to fix it please open a PR, otherwise we will fix when we think it is time.",Bitter Frustration
"@Sega100500 your report which had virtually no context and then you take people to task for not being able to reproduce the problem - even the variable name you use `@product` would tend to make people think that you were using a `Product` model. It's only 12 messages in where you provide the SQL which you have to read closely to infer that you're actually using the `Content` model and then `select`-ing the product columns onto that model. In fact you never even mention `select` anywhere in this thread. When you fail to provide the single piece of information that would allow people to reproduce the issue is it any wonder that the issue would be closed? When you were politely asked to provide exact code examples you just dismissively directed @sivagollapalli back to previous messages - at no point did you ever provide a model definition.
It's only when you provide a bug test example in #22434 that it's immediately obvious what the problem is - this is why we always suggest doing one as it helps both you and us by focussing on the core problem and removing extraneous factors like gems, etc.
On, one other thing - the code is `Numeric === value` not `value === Numeric` - you should check whether something is commutative before abusing them, e.g:
``` irb
>> Numeric === 0.9
=> true ```",Bitter Frustration
"Jira issue originally created by user johnconnor:
It seems that passing the limit to a subquery is not working
```
$subquery = $em->createQueryBuilder()->from('...')->where('...')->setMaxResults(5);
$query = $em->createQueryBuilder()->from('...')->where(
$qb->expr()->in('p.id', $subquery->getDQL())
);
$query->getQuery()->getResult();
```
The query works but the is no specified limit in the resulting SQL.
I am aware that DQL does not support the limits and offsets, so i guess there should be another way to get this working?",
"Comment created by @deeky666:
Can you please tell which database platform you are using? Because limiting results heavily depends on the platform used.",
"Comment created by johnconnor:
MySql",
"Comment created by @deeky666:
Hmmm I am not quite sure if the limit/offset is invoked for subqueries but I don't see why it shouldn't. Also I think this is not a DBAL issue because the limit/offset support for MySQL is the easiest we have on all platform. See: https://github.com/doctrine/dbal/blob/master/lib/Doctrine/DBAL/Platforms/MySqlPlatform.php#L51-L63
The query doesn't have to be modified but instead only the limit clause is appended to the query. Can you maybe provide the generated SQL for that query?",
"Comment created by johnconnor:
I think if you try to build any query with QueryBuilder, set a limit to it with setMaxResults then call getDQL method, you should see that the output contains no info about limit.
So if you look at my code example , at $qb->expr()->in('p.id', $subquery->getDQL()), then you will see that the getDQL passes to the IN expression a query which already DOES NOT have limit. So this is the place where any info about limits and offsets gets lost.
So I fail to see what it has to do with any specific db engine,however I can provide the mysql resulting query if you want,though it looked perfectly normal to me,just lacks the LIMIT part.",Bitter Frustration
You any news on this ? please maybe a hack ?,Impatience
Just to note - it's still impossible to use subqueries with limits.,Bitter Frustration
"New note 3 years later, it is still impossible to use subqueries with limits. Any workaround ?",Bitter Frustration
"@adrienpayen @vladimmi if I had a contribution graph like yours, I would refrain from complaining about a lack of contributions. You've done nothing to fix an issue that directly impacts you, so why would you expect people that are not impacted to do the work for you? Please stop the spamming and start being constructive.
You could provide a failing test case, or use a debugger to pinpoint where the issue is.",Bitter Frustration
"@greg0ire If I was such a tender snowflake, I would refrain from using public services having comments - especially bug trackers. And especially I would at least read ticket before posting anything there - because ""failing case"" was provided 7 years ago in the very first message and ""pinpointed where the issue is"" in the same day several messages later.",Mocking
"Well then you can move on to the next step: fix it!
If I had to read every thread when people ask for news, I wouldn't be able to make any progress. That being said, the first comment is not really what I meant by failing _test_ case. You can still work on that before attempting to fixing the bug. Make a PR with the code snippet above in a PHPUnit test.
> tender snowflake
Hmmmmâ€¦ I think the discussion is too heated, don't you agree?",Bitter Frustration
"Jira issue originally created by user pcnc:
Good afternoon,
When hydrating an Embeddable with nullable attributes the result is an instance of the Embeddable class , this is obviously correct and expected behavior. If all the attributes are null the hydrator will still return an instance of the class with all of its properties null , even if I persist and flush my Entity with the Embeddable being set as null . For clarification :
```
class MyEntity
{
protected $myEmbeddable;
public function setMyEmbeddable(MyEmbeddable $myEmbeddable = null)
{
$this->myEmbeddable = $myEmbeddable;
}
[...]
}
$newEntity = new MyEntity();
$newEntity->setMyEmbeddable(null);
$em->persist($newEntity);
$em->flsuh($newEntity);
```
Calling $newEntity->getMyEmbeddable() will return an instance of the MyEmbeddable object with all of it's attributes set to null .
I expected $newEntity->getMyEmbeddable() to be NULL . Can someone clarify is this is expected behaviour ? In case it is , how can I achieve what I'm looking for ? Best regards",
"Comment created by eugene-d:
See https://github.com/doctrine/doctrine2/pull/1275",
This is actually really confusing definitely when you combine it with the [docs](http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables),Impatience
I must say I find this rather a bug than an improvement... agree with @boekkooi regarding the docs...,Bitter Frustration
Can anyone confirm if this is a dupe of #4670 and #4568?,
"well this _is_ #4568; and #4670 is rather a duplicate of #1275, both actually ""features"" or ""improvements"".
This here however is a Bug because embeddables whose all properties are nullable and being null hydrate to an empty object when being stored as null. This is misleading (saving != retrieving) and not according to the docs.",Bitter Frustration
"@afoeder the order of the tickets is scrambled due to the fact that they were imported from Jira in December, heh",Irony
"Hi @Ocramius Has there been any further discussion on this topic? We've just hit into the same problem as described here on our first Doctrine project. Let me know if there's anything we can do to help â€” provide usage examples, code samples, discussions, etc.",
"@Harrisonbro as it currently stands, doctrine will not support nullable embeddables. That functionality may be implemented later, by implementing embeddables as hidden one-to-one records.",
OK. Is there any way for us to implement this on a case-by-case basis (eg. by hooking into the hydration process of an embeddable somehow) so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? Obviously we're not keen to allow value objects to be instantiated in an invalid state and then check an `isValid` method.,
"> so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? Then just use the lifecycle system to (de-)hydrate VOs on your own, no?",Impatience
"> Then just use the lifecycle system to (de-)hydrate VOs on your own, > no?
Do you mean lifecycle callbacks â€” maybe `postLoad` â€” as shown in http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/reference/events.html#lifecycle-callbacks? Looks like those only work on entities, not value objects, as far as I can see? Eg. if I used `postLoad` an embeddable will already have been hydrated with invalid data (if the data in the database is all `null`, for example). Alternatively, if I move the VO properties onto the entity directly Iâ€™ve lost the nice encapsulation that embeddable so usefully provides (eg. if I had a `Product` entity with a `SalePrice` with 2 properties, `value` and `currency` Iâ€™d have to move those 2 properties onto the entity. Whilst I could then have those properties be private and do an `is_null` check for those 2 properties before instantiating and returning the VO from `getSalePrice() : SalePrice { â€¦ }` it does rather compromise my entity.
Iâ€™m almost certainly missing something here, sorry. Rather new to Doctrine so still learning!",
"@Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.",
"> @Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.
OK, gotcha.
So in the example I gave â€” a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` â€” would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?
If so, that seems workable and means the entity is responsible for checking if the VO should be instantiated (rather than having the VO able to be â€˜invalidâ€™ and have to implement an `isValid()` method).
Example code of what I mean:
``` php
class Product
{
private $sale_price_amount;
private $sale_price_currency;
public getSalePrice() : SalePrice
{
if (
is_null($this->sale_price_currency) || is_null($this->sale_price_amount)
) {
return null;
}
return new SalePrice(
$this->sale_price_currency, $this->sale_price_amount
);
}
}
```
Is that something like what youâ€™re suggesting instead of nullable embeddables?",
"> So in the example I gave â€” a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` â€” would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?
Correct.
Basically, since this is a scenario that Doctrine can't cover right now (because of how RDBMS DDL works), you can just implement it in userland for the few times where it pops up.",
"OK great, thanks a lot for the help.",
"Thanks, I silently kept up reading your conversation :)
At the moment, my workaround is the following:
```
class Site
{
/**
* @var DomainName
* @ORM\Embedded(class=""DomainName"", columnPrefix=""domain_"")
*/
private $domainName;
public function domainName()
{
return ((string)$this->domainName === '' ? null : $this->domainName);
}
}
/**
* @ORM\Embeddable
*/
class DomainName
{
/**
* Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
*
* @var string
* @ORM\Column(nullable=true)
* @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
* @see https://github.com/doctrine/doctrine2/pull/1275 [2]
*/
private $name;
/**
* Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
*
* @var string
* @ORM\Column(name=""escaped_name"", nullable=true)
* @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
* @see https://github.com/doctrine/doctrine2/pull/1275 [2]
*/
private $escapedName;
public function __construct($name)
{
Assertion::notEmpty($name, 'The domain name must be provided.');
Assertion::regex($name, '/^(?!www\.)([\pL\pN\pS-]+\.)+[\pL]+$/u', 'The domain name ""%s"" must be a valid domain name without the www. subdomain, but might have others.');
$this->name = $name;
$this->escapedName = static::escapeDomainName($name);
}
public function containsSubdomain()
{
return substr_count($this->name, '.') >= 2;
}
public static function escapeDomainName($name)
{
return preg_replace('/\./', '-', $name);
}
public function __toString()
{
return (string)$this->name;
}
}
```",
"I like that approach, @afoeder â€” you still do an `is_null` check in the entity's getter method (`Site::domainName()` in your case) but you can still use an embeddable rather than having to hydrate your VOs yourself.
I suppose the major downside of your approach is that you do still have a VO in an inconsistent state, whereas if you don't let Doctrine hydrate the VO as an embeddable you avoid this; a bit more boilerplate & checking code, but you never have a VO in an invalid state.
Really it's just a trade-off between the 2 options. Others reading this should just be aware of the 2 options and their various merits.",
"I would have preferred to comment on #1275, but the present issue has the benefit to be still open.
My 2 cents on the sensitive subject of nullable embedded properties:
- when the Embeddable has at least one non-nullable `@Column`, and this field is null in the database, **there should be no ambiguity** and **`null` should be assigned to the embedded property**. Otherwise (currently!) you get an empty, invalid value object that has non-nullable properties set to `null`. IMHO, the current implementation is broken here.
- when all `@Column` in the Embeddable are nullable, there should be a boolean setting in the `@Embedded` annotation that controls whether or not you want an empty value object or a `null` value when all fields are `null` in the database. **Your choice**.
Finally, you only have a real problem when you have a fully nullable embeddable, **and** want to make the distinction between a `null` property and an empty object. People have suggested to add an extra column in the table, which would work, but would add a ton of complexity for what I think is an edge case. To clarify, **I think this edge case should not be supported by Doctrine**.
The previous two bullet points, however, **I would strongly suggest working on them ASAP**. I'll be happy to help, provided that lead developers are happy with the concept.",Bitter Frustration
"Thanks for that clear explanation, @benmorel. I quite agree with your suggested specification of how Doctrine _should_ behave, and that it should be worked on. This issue is the top priority I'd like to see addressed in doctrine. I too would be happy to help out with the development and testing of this, if the Doctrine team agree.",
"We came across the same issue, in our domain a `StreetAddress` is optional, but if given, it has to contain all fields. All fields on the `Embeddable` are `nullable: true`, so the DB is working. The domain is ensuring valid state. So I created that listener to make Doctrine load the objects the way the domain contains objects prior persisting: https://gist.github.com/havvg/602055f1488271f68e5bc82f9a828b4d
Well, it only requires knowledge on the embeddable itself, but easy workaround for now. I hope this helps other developers until the issue will be resolved by Doctrine.",
"@havvg How to use this workaround?
For example I have User entity with embeddable class Gender. Something like that:
https://gist.github.com/szepczynski/d3028eb9f92fd7aadd08a578c7a92ad3
I know that I need the User entity should have registered postLoad listener NullableEmbeddableListener but I have no idea how to register it. Can you provide any example? I guess that I need somewhere call addMapping?",Impatience
"@BenMorel did you by any chance work on a PR for this? I'd really hope to have this ""bug"" resolved, but I don't understand doctrine well enough to be able to write a pretty PR for this issue.",Impatience
"@Evertt Not yet, and I won't until I get the green light from lead developers. I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!",Bitter Frustration
@BenMorel that's too bad and I totally understand! It's sad that huge projects like these at some point seem to slow down to a point that it just seems frozen.,Bitter Frustration
"That's the dark side of open source: projects rely solely on the free time developers can invest in them, and at some point they're just too busy on other businesses and/or family life to carry on with developments.
I, too, feel like Doctrine is slowing down; it's just unfortunate that there aren't enough (available) lead developers to keep up the pace with pull requests: many developers are there to offer their help, but without enough consideration from project leaders, it's just wasted brain processing time.",Bitter Frustration
"@BenMorel the pace did slow down a bit last year, but the last months have been quite active :) Also see #6211",
doctrine 2.x is frozen because doctrine 3 is actively developing (I read somewhere post by @Ocramius),
@szepczynski except that there's no ETA for doctrine 3 so that could take years for all we know. If it really takes that long it would be nice for improvements to still be added to doctrine 2.,Impatience
"> If it really takes that long it would be nice for improvements to still be added to doctrine 2.
It would make it a mess to migrate these additions to something completely redesigned. From what I can see in the last dozen releases, doctrine functionality already abundantly covers the 90% of use-case scenarios, so we could even call it ""feature complete"", if it wasn't for some rough edges that you encounter when you explore more shady features.",Bitter Frustration
"@Ocramius I wouldn't call this issue right here a ""shady feature"". I think this is a very essential part of the embeddables system.",
"Right, and embeddables have barely been added in `2.5`, and are already removed in `develop` (`3.x`), as their fundamental internal working mechanisms need to be rewritten",Bitter Frustration
"@Ocramius Sorry to pollute this thread, but would the [transaction object](https://github.com/doctrine/dbal/pull/634) and [default lock mode](https://github.com/doctrine/doctrine2/pull/949) fit in 3.0?",
"@BenMorel most likely, yes",
"> Right, and embeddables have barely been added in 2.5, and are already removed in develop (3.x), as their fundamental internal working mechanisms need to be rewritten
@Ocramius I'm not sure I understand you right. Do you mean they will come back in 3.x after their internal working mechanisms have been rewritten?",Impatience
"@Evertt yes, but likely as completely rewritten/redesigned.",
"@Ocramius Are there ways we can help the development of Doctrine, either v2 or v3? It's a tool we all use so would love to,support development if a can. _(Sorry to pollute this thread but not sure where else to write.)_",
"@Harrisonbro https://github.com/doctrine/doctrine2/milestones/3.0
Let's please stop going further OT. If you have a question, make a new issue.",Impatience
FTR: there is a small 3rd party library that provides a listener for setting embedded entities that are all null to null (the gist that was discussed above): https://github.com/tarifhaus/doctrine-nullable-embeddable,
"@BenMorel With all due respect, I have to go r/quityourbullshit on you here:
> I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!
For Doctrine 2, [there are 26 pull requests from you](https://github.com/doctrine/doctrine2/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 2 Open, 2 Closed without merge (one was fixed differently, one would introduce a lot of pain with future pull requests), and 22 merged.
For DBAL, [there are 15 pull requests from you](https://github.com/doctrine/dbal/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 1 open, 2 Closed without merge, and 12 merged. A quick peek into other repositories (common, annotations, bundle, etc.) shows merged pull requests only.
Feel free to point out pull requests that you are waiting to get merged, but please don't say stuff like that without backing it up when other people sacrifice lots of free time to get you free software. Thank you.",Bitter Frustration
"@alcaeus Thanks for investing your time investigating my contributions to this repository.
You have already wonderfully pointed out my unmerged pull requests, you just forgot to mention their opening date:
- doctrine/dbal#634 : opened 3 years ago
- doctrine/doctrine2#949 : opened 3 years and 6 months ago
I did invest quite a lot of time on these two pull requests, and since 2 years I am getting next to no feedback, despite multiple attempts to draw attention from the team.
Yes, I had PRs merged as well (did I ever say I didn't?), but the lack of feedback on these last two blocked my motivation to contribute further to this project for now.
> [...] please don't say stuff like that without backing it up when other people sacrifice lots of free time to get you free software.
I hope I have backed it up to your taste, and rest assured that I know [quite well](https://github.com/BenMorel) what it's like to sacrifice some time on free software.
Cheers.",Bitter Frustration
"@BenMorel the way you made it sound was that people completely disregarded your work, which they don't. That's why I took the time to look through your contributions to see what's going on.
As for the pull request you mentioned, (please keep in mind that I'm not an ORM guy) it looks like it's a fairly large pull request that touches transaction logic in the DBAL, affecting most of what it (and thus ORM) does. Pull requests like that take time to review and evaluate. I'm not trying to make excuses for other people here, I'm just hoping you can bear with the people maintaining it. That said, there is currently a development push going for 3.0, so maybe there's an opportunity to get those pull requests merged.
Open source projects need contributors to move forward and evolve, especially when maintainers have little or in some cases no time for the project. However, writing pull requests is just a small part of that work - most of it is looking at issues, figuring out what's going on, evaluating pull requests and making sure your user base is not going to burn you at the stake because you messed up. That can be very time consuming and tiring, so unfortunately, large pull requests are often the first to stay open simply because of the effort it takes to review and merge them.",
"@BenMorel I agree with you. The Doctrine mantra I keep hearing seems to be along the line of ""Well, it kind of works in most cases and changing things is hard so let's not do it"".
They say a picture says a thousand words and I think this picture sums up my feelings about Doctrine right now: ![](https://i.imgur.com/Rl0VmKc.jpg)
Disclaimer: Owners, don't take offense - it's meant in a lighthearted way and is just my personal opinion. I know you work hard on this, and for that I thank you. I just disagree a bit with the general negativity that I personally see towards any major changes. I know you're working on 3.0 but Symfony and others are leaving you in the dust. If there's too much work, consider giving other contributors more rights or bumping the major version more often so that there can be BC breaks.",Bitter Frustration
"@ryall I think that's the nail on the coffin then.
Here's the exit:
![selection_176](https://user-images.githubusercontent.com/154256/35387238-2c99dbda-01cf-11e8-8599-5740938bbc9d.jpg)
Closing and locking.",Bitter Frustration
"i don't know how to make it work; i'm trying to install a project that requires a composer installation. when i try to install the composer i'm having the is error
```
Some settings on your machine make Composer unable to work properly.
Make sure that you fix the issues listed below and run this script again:
The json extension is missing.
Install it or recompile php without --disable-json
The phar extension is missing.
Install it or recompile php without --disable-phar
The filter extension is missing.
Install it or recompile php without --disable-filter
The hash extension is missing.
Install it or recompile php without --disable-hash
The openssl extension is missing, which means that secure HTTPS transfers are impossible.
If possible you should enable it or recompile php with --with-openssl
```
To solve the openssl extension missing problem; i've installed openssl 1.0.1q and link ssl/bin/openssl to usr/bin/openssl; meanwhile openssl is in my usr/local/src/php-5.6.13/ext.
The other missing i enabled them via the ./configure command option with, --enable-filter, --enable-hash, --enable-json, --enable-phar. To be precise you will find the configuration
./configure \
--prefix='/usr/local/src/php-5.6.13' \
--disable-cgi \
--with-libdir='/lib/x86_64-linux-gnu/' \
--with-config-file-path='/etc/php5' \
--with-zlib \
--with-pcre-regex \
--with-mysql \
--with-pdo-mysql \
--with-mysqli \
--enable-simplexml \
--enable-xml \
--enable-mysqlnd \
--enable-pcntl \
--enable-debug \
--enable-maintainer-zts \
--enable-mbstring \
--enable-bcmath \
--enable-exif \
--enable-ftp \
--enable-soap \
--enable-sockets \
--enable-opcache \
--enable-zip \
--enable-embedded-mysqli \
--with-fpm-user=www-data \
--with-fpm-group=www-data \
--enable-fpm \
--enable-session \
--enable-sysvmsg \
--enable-sysvsem \
--enable-sysvshm \
--enable-xmlreader \
--enable-xmlwriter \
--enable-phar \
--enable-json \
--enable-hash \
--enable-libxml \
--enable-filter \
--with-openssl \
--with-curl
I've done the following below before running the ./configure and when i run make && make install i'm having this underneath some long running text that stops then this message appears. the message is this ""No rule to make target `pharcmd', needed by`all'. Stop.""
rm ./configure
rm -rf autom4te.cache/
rm -rf aclocal.m4
./buildconf --force
make clean.
Please how can i solve my problem.",Bitter Frustration
You also need to load the extensions in your php.ini,
"Composer has nothing to do with how you compile PHP. It simply points out what it needs/expects. If your compiled version does not offer that, then either recompile, or use any of the common default binaries provided for Linux/OSX/Windows which do contain everything that is necessary.
This issue is more about installing and configuring PHP though, so I'm closing it.",Impatience
"Composer is garbage this kind of mistake should not happen, error unacceptable.",Bitter Frustration
"Why HTML::Document.parse(""one < two"") returns: ``` html
<html><body><p>one </p></body></html>' ```
?
This method cuts off the part of a text that goes after '<' sign. Because of this, the Ruby on Rails's #simple_format method works the same way: simple_format(""text_before < text_after"") => ""text_before """,Impatience
"Rails 4.2.4
Nokogiri 1.6.7",
"FYI, this works
``` irb
>> Nokogiri::HTML::Document.parse('one > two').text
=> ""one > two"" ```
One thing to bear in mind is that this may well be an issue in libxml2 itself",
"Hi,
Thanks for asking this question. Nokogiri uses XML parsing libraries under the hood (libxml2 for MRI, xerces/nekohtml for JRuby) and so we have very little control over how those libraries decide to ""fix"" broken markup.",
"Are you kidding me ? Your code does not work on a few different servers with different operating systems and you telling me ""We don't care, the library is to blame"". The ""less than"" sign is a common case, which can occur in any text.",Bitter Frustration
"The ""less than"" sign by itself is not valid HTML or XML markup. There's no spec on how to fix broken markup.
So yes, I'm saying that the underlying libraries, written in C or Java, which are responsible for tokenizing and parsing the documents passed into Nokogiri, and most especially for choosing how to ""fix"" that invalid markup, are beyond the control of Nokogiri.
Lastly, I just want to ask that you keep your tone constructive. I'll point you at our Code of Conduct that's in our [CONTRIBUTING.md](https://github.com/sparklemotion/nokogiri/blob/master/CONTRIBUTING.md) document. It's easy to misinterpret your previous comment as entitled, rude, and judgmental. As a result I've locked this issue down to ensure nobody else will respond to you in kind.
I'm sorry we weren't able to help you in this instance, good luck on future endeavors.",
"Final thought, explaining the behavior, is that ""one < two"" is interpreted by both parsers as the string ""one "" followed by an incomplete tag ""<two"".
If you want ""one < two"" to be treated as ""one &lt; two"" (that is, a text string) then I'd recommend escaping it first. Perhaps the Rails team, based on the context of `simple_format`, can selectively apply escaping? You'll have to take it up with them.",
"It's just not funny anymore. ![https://twitter.com/_sciwoman/status/659519813564424192](https://cloud.githubusercontent.com/assets/1756909/16565049/1dafe0a4-420b-11e6-8899-0af645ee6781.png)
![https://twitter.com/rjw1/status/2741916767](https://cloud.githubusercontent.com/assets/1756909/16565059/30da5254-420b-11e6-8c9a-585bfc90a30c.png)
![https://twitter.com/krausefx/status/659611440911773696](https://cloud.githubusercontent.com/assets/1756909/16565071/3d0c7228-420b-11e6-851e-aab8583ea8aa.png)
## The sane way - NOPE
```
$ sudo gem install nokogiri
Building native extensions. This could take a while...
ERROR: Error installing nokogiri:
ERROR: Failed to build gem native extension.
current directory: /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ext/nokogiri
/System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby -r ./siteconf20160704-86627-119yyod.rb extconf.rb
Using pkg-config version 1.1.7
checking if the C compiler accepts ... yes
checking if the C compiler accepts -Wno-error=unused-command-line-argument-hard-error-in-future... no
Building nokogiri using packaged libraries.
Using mini_portile version 2.1.0
checking for iconv.h... yes
checking for gzdopen() in -lz... yes
checking for iconv... yes
************************************************************************
IMPORTANT NOTICE:
Building Nokogiri with a packaged version of libxml2-2.9.4.
Team Nokogiri will keep on doing their best to provide security
updates in a timely manner, but if this is a concern for you and want
to use the system library instead; abort this installation process and
reinstall nokogiri as follows:
gem install nokogiri -- --use-system-libraries
[--with-xml2-config=/path/to/xml2-config]
[--with-xslt-config=/path/to/xslt-config]
If you are using Bundler, tell it to use the option:
bundle config build.nokogiri --use-system-libraries
bundle install
Note, however, that nokogiri is not fully compatible with arbitrary
versions of libxml2 provided by OS/package vendors.
************************************************************************
Extracting libxml2-2.9.4.tar.gz into tmp/x86_64-apple-darwin15/ports/libxml2/2.9.4... OK
Running 'configure' for libxml2 2.9.4... OK
Running 'compile' for libxml2 2.9.4... OK
Running 'install' for libxml2 2.9.4... OK
Activating libxml2 2.9.4 (from /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ports/x86_64-apple-darwin15/libxml2/2.9.4)...
************************************************************************
IMPORTANT NOTICE:
Building Nokogiri with a packaged version of libxslt-1.1.29.
Team Nokogiri will keep on doing their best to provide security
updates in a timely manner, but if this is a concern for you and want
to use the system library instead; abort this installation process and
reinstall nokogiri as follows:
gem install nokogiri -- --use-system-libraries
[--with-xml2-config=/path/to/xml2-config]
[--with-xslt-config=/path/to/xslt-config]
If you are using Bundler, tell it to use the option:
bundle config build.nokogiri --use-system-libraries
bundle install
************************************************************************
Extracting libxslt-1.1.29.tar.gz into tmp/x86_64-apple-darwin15/ports/libxslt/1.1.29... OK
Running 'configure' for libxslt 1.1.29... OK
Running 'compile' for libxslt 1.1.29... OK
Running 'install' for libxslt 1.1.29... OK
Activating libxslt 1.1.29 (from /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ports/x86_64-apple-darwin15/libxslt/1.1.29)...
checking for main() in -llzma... yes
checking for xmlParseDoc() in libxml/parser.h... no
checking for xmlParseDoc() in -lxml2... no
checking for xmlParseDoc() in -llibxml2... no
-----
libxml2 is missing. Please locate mkmf.log to investigate how it is failing.
-----
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of necessary
libraries and/or headers. Check the mkmf.log file for more details. You may
need configuration options.
Provided configuration options:
--with-opt-dir
--without-opt-dir
--with-opt-include
--without-opt-include=${opt-dir}/include
--with-opt-lib
--without-opt-lib=${opt-dir}/lib
--with-make-prog
--without-make-prog
--srcdir=.
--curdir
--ruby=/System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby
--help
--clean
--use-system-libraries
--enable-static
--disable-static
--with-zlib-dir
--without-zlib-dir
--with-zlib-include
--without-zlib-include=${zlib-dir}/include
--with-zlib-lib
--without-zlib-lib=${zlib-dir}/lib
--enable-cross-build
--disable-cross-build
--with-xml2lib
--without-xml2lib
--with-libxml2lib
--without-libxml2lib
To see why this extension failed to compile, please check the mkmf.log which can be found here:
/Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.8/mkmf.log
extconf failed, exit code 1
Gem files will remain installed in /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8 for inspection.
Results logged to /Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.8/gem_make.out
```
## The use-system-libraries way â€“ NOPE
```
$ sudo gem install -n /usr/local/bin nokogiri -- --with-xml2-include=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/libxml2 --use-system-libraries
$ bundle config build.nokogiri ""--use-system-libraries --with-xml2-include=/usr/local/opt/libxml2/include/libxml2""
$ bundle install
Fetching gem metadata from https://rubygems.org/
Fetching version metadata from https://rubygems.org/
Fetching dependency metadata from https://rubygems.org/
Using RedCloth 4.2.9
Using i18n 0.7.0
Using json 1.8.3
Using minitest 5.8.4
Using thread_safe 0.3.5
Using addressable 2.4.0
Using coffee-script-source 1.10.0
Using execjs 2.6.0
Using colorator 0.1
Using ffi 1.9.10
Using multipart-post 2.0.0
Using gemoji 2.1.0
Using net-dns 0.8.0
Using public_suffix 1.5.3
Using sass 3.4.21
Using rb-fsevent 0.9.7
Using kramdown 1.10.0
Using liquid 3.0.6
Using mercenary 0.3.5
Using rouge 1.10.1
Using safe_yaml 1.0.4
Using jekyll-feed 0.4.0
Using mini_portile2 2.0.0
Using jekyll-paginate 1.1.0
Using jekyll-sitemap 0.10.0
Using rdiscount 2.1.8
Using redcarpet 3.3.3
Using terminal-table 1.5.2
Using bundler 1.12.5
Using jekyll-textile-converter 0.1.0
Using tzinfo 1.2.2
Using coffee-script 2.4.1
Using ethon 0.8.1
Using rb-inotify 0.9.7
Using faraday 0.9.2
Using jekyll-sass-converter 1.3.0
Installing nokogiri 1.6.7.2 with native extensions
Errno::EACCES: Permission denied - /Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.7.2/gem_make.out
Using activesupport 4.2.6
Using jekyll-coffeescript 1.0.1
Using typhoeus 0.8.0
Using listen 3.0.6
Using sawyer 0.7.0
An error occurred while installing nokogiri (1.6.7.2), and Bundler cannot continue.
Make sure that `gem install nokogiri -v '1.6.7.2'` succeeds before bundling.
```
## The use-system-libraries way but put it in /usr/lib â€“ NOPE
```
$ sudo gem install nokogiri -v '1.6.7' -- --use-system-libraries --with-xml2-include=/usr/include/libxml2 --with-xml2-lib=/usr/lib
Building native extensions with: '--use-system-libraries --with-xml2-include=/usr/include/libxml2 --with-xml2-lib=/usr/lib'
This could take a while...
ERROR: While executing gem ... (Errno::EPERM)
Operation not permitted - /usr/bin/nokogiri
```",Bitter Frustration
I am facing the same issue. Does anybody have an idea how to solve it?,
"@arielelkin I agree that Nokogiri should ""just work"", and in fact it generally DOES just work on Linux and Windows. OSX is the outlier, and it's because of the combinatorial explosions of configuration options, along with the fact that Apple has decided to deliver xcode (and xcode update) configurations that are hostile to developers maintaining non-trivial C extensions.
I'm not sure what outcome you're hoping to achieve with an approach like the above -- rude, disrespectful, entitled -- but watch and hopefully learn as I treat you with more respect than you've treated Nokogiri maintainers.
We believe at this point that the installation tutorial:
```
http://www.nokogiri.org/tutorials/installing_nokogiri.html
```
works for nearly all known configurations of OSX. Did you go through the tutorial and follow all the steps for OSX? If so, that would be useful information to note when opening a support request. If not, help me understand how we can better point people like you at that tutorial? In particular, the xcode bit is really required for Mac users at this point.
If the tutorial doesn't address the problem you're having, then please make sure you provide both the output from `gem install` and also the `mkmf.log` file.
I'll further note that above, you've tried to install three different versions of Nokogiri. I'll also point out that you've got permissions errors even when you sudo, which makes me think you've either got a really weirdly configured system, or else you're trolling.
---
I'd like to note that the Nokogiri Code of Conduct
```
https://github.com/sparklemotion/nokogiri/blob/master/CONTRIBUTING.md
```
asks everyone to be nice and reminds you that Nokogiri maintainers are volunteers, who don't get paid to work on it, and have personal lives and families. Your approach in this issue is offensive and rude, and in all honesty represents most of what I dislike about being an open source maintainer. You expect something for nothing, and heap abuse on volunteers who have spent hundreds of thankless hours working to get Nokogiri to work on OSX.
If you've surmounted your personal communication and anger-management challenges and read this far, I'll ask that if you go through the installation tutorial and it doesn't address the problem you're having, that you open a new issue and respectfully provide enough information to help a maintainer diagnose the issue on your particular system.
I'll be locking this issue both because your request violates the CoC, but also just to preserve Yet Another Disrespectful Request for posterity, and so that when I finally get fed up and ragequit open source, I'll have plenty of interesting Github Issues to point people at.
Finally -- everyone in the Ruby community who saw this issue come in and didn't do anything to police it -- you're part of the problem, too. For every open-source maintainer who quits, y'all wring your hands and are full of regret; but these are the opportunities you miss to prevent that from happening. A community that doesn't police rudeness and disrepect is not a community I'm happy to be a part of.
---
@SUzB - sorry you got caught in the crossfire on this, but you've also made a very unconstructive post wherein the OP claims a problem and shows three very different logs for three different versions of Nokogiri, and you say, ""I am facing the same issue."" No, no you're not.
Please feel free to open a new issue providing enough information for us to diagnose your issue. At the very least, info about your OS, and if you're OSX then we'll need to know more about xcode and will need to see your logs.",Bitter Frustration
"### How to use GitHub
* Please use the ðŸ‘ [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are interested into the same feature.
* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.
* Subscribe to receive notifications on status change and new comments. **Is your feature request related to a problem? Please describe.**
I think it would be great if Nextcloud could notify the Admin via Email if there are pending upgrades. This would make it easier to deal with upgrades also on instances one is not actively using but rather just maintaining.",
"Since Nextcloud 10 we have update notifications delivered by the notifications API. When you have a sync client set up and connected, it will bring the notifications to your desktop.
However I just discussed with one of my colleges today, that it would also be cool to send a reminder email after some days to the admin when the instance is still not updated.",
why not just make it auto-update able?,Impatience
"To me this is very important since as a sysadmin I manage some Nextcloud instances that I don't use myself.
Using the desktop app to know a new upgrade is pending is thus not a solution.",
"Alternatively, how about Nextcloud instances that you do use, but not through the web interface or a Nextcloud app? Then you don't see any notifications at all.
i.e. I use Nextcloud primarily for syncing contacts/calendar. This is done through the online accounts in GNOME on my laptop and syncevolution on my phone. Neither of these tell me anything about upgrades.",
"an email on pending updates is really important. having such a widespread app, you should add such a important feature to make the web a little bit more secure.
> that it would also be cool to send a reminder email after some days
i am clearly against a delay. the email should be sent instantly, not with some days delay. if there is a security hole, the admins want to be notified (instantly). i assume that updates are only shown to the admin user, so keep in mind, that you require me to work as admin with my sync app to get such important informations.
workaround i found (german text but the code is english): https://spielwiese.la-evento.com/xelasblog/archives/75-Mailbenachrichtigung-bei-verfuegbaren-Nextcloud-Updates.html",Impatience
"Well, the thing is: if you don't use the admin account for syncing, you might also not have set an email address for it. Also this issue here is just meant to be an additional reminder.
> i assume that updates are only shown to the admin user, so keep in mind, that you require me to work as admin with my sync app to get such important informations.
You are assuming wrong. In the admin section you can actually select which groups get update notifications. By default it's admin only, yes. But if you don't sync with the admin user (like I do), you can also change it to any other group.",
"Additionally those update notifications are displayed are displayed as push notifications on android and iOS clients, and also the desktop client shows the notifications.",
"@nickvergessen I tend to disagree here. As an admin, I may only use ~~ownCloud~~ Nextcloud using the web UI or just for syncing calendars and contacts. Or I may administer an instance for other people so I don't use it myself at all. Running a client all time just to be informed about updates is a not very comfortable.
--
Edited: Oops. As a defense: This only shows how seamless the migration to Nextcloud was ðŸ˜‰",Irony
"Well if you are using ownCloud we can't help you anyway. [/joke]
But well, I do agree that this should be fixed. I'm just saying it's not as bad as people are drawing it.",Irony
"It's as bad as we're saying, for the group of people such as myself, who currently receive no notifications, because we don't use the sync app or similar.
Maybe it's only a small number of users, but for those users, it is a moderately serious security issue, as there is no obvious way to get notified promptly about upgrades. Currently, I typically find out about security updates weeks or sometimes even months after they are released, when I just happen to log in to the web interface.",Bitter Frustration
I added https://github.com/nextcloud/server/releases.atom to my rss reader. Edit: I just found https://coderelease.io (send you an email when new release on github),
"I wouldn't mind if the fix was exposing a parameter in `occ` that told you if your instance is up to date or not.
I could write a cron job or an icinga2 test against that and get notified.",
@baldurmen https://github.com/nextcloud/server/pull/10836 (unfortunately too late for nextcloud 14),Bitter Frustration
"@danielkesselberg of course these links help, but still they show all releases. normally a admin does not care for beta releases or release candidates. the admin wants to know is there a security update for my current installation. we are all flooded with messages from multiple services: emails, newsletter, github, rss feeds, social networks,... so nextcloud should add this feature to help admins to net get drowned in too many unneeded emails and only send an email if the admin should act.",Bitter Frustration
"A perspective that may not have been considered just yet: For the past year I've been wondering what's wrong with my instance that it's not sending me an email when new versions are released. It only occurred to me as I was typing up a question on the forum that I may have misunderstood the function, and to read the notification option carefully and search for it (which brought me here). Others may be letting their instance go unupdated not realising that there's numerous updates available.
The reason I haven't seen the notifications is because I rarely use the web app, like others here. The sync is done on my server, and I access the synced directory over the network.",
This would also be useful for update notifications of apps. Sure I also have the sync client and see notifications there. But the workflow of a sysadmin for these kind of things are usually via email notifications.,
"I was using the old `owncloud` client and I used to get notified of updates this way, but now that I switched to the `nextcloud` client I don't get those notifications anymore...
I've hacked some icinga2 tests to check for app updates and core updates for now, but this is getting really painful :(",Bitter Frustration
That should actually work quite fine,
"> This would also be useful for update notifications of apps.
I want to emphasize this statement, because it seems to be much more important to me. I am currently managing more than 20 Nextcloud instances which I am not using by my own. So I rarely login there. I actually do follow the Nextcloud releases stream, so I know when its time to update my instances. Some of them are even managed instances and therefor updated by another company. But for all of these instances there are different apps active, for some instances there are even other users enabling/disabling apps. So I would need to follow all these release channels and then know which apps are active in which instances. Then login manually to the concerned instances and press the update button. And thats even the case for the managed instances because normally app updates are not included in the management. That is more than inconvenient and its feeling very much unprofessional. A Mail notification with a direct link would help here, because then only the concerned instances would notify. An autoupdater for apps **as an option** (and activity notification) would make this even more handy for a lot of environments.",Bitter Frustration
"I've just seen this and I have to agree, email notification would be really useful.",
Since the addition of `occ update:check` you can run a cronjob to alert you by email when there are updates available or use the ouput in a proper monitoring system.,
that wont work if you don't have shell access. This is true for managed nextcloud setups.,
Why not simply add a checkbox-based choice of whether the specified groups (to be informed about updates) should also be informed by email?,
Agree with @ilippert,
a checkbox would be perfect!,
@fosple @joostvkempen please use [GitHub reactions](https://github.blog/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/). If you comment everyone subscribed to the issue get's a notification.,
Any progress on this issue ? Really it could be so usefull to receive these mails,Impatience
"or at least allow us to config a webhook which is triggered if an update is available, then we can send the emails by ourselfs",Impatience
I agree that an email notification is very important,
"I would really like to see this feature, too. This is just missing IMHO.",
Should be solved via https://github.com/nextcloud/notifications/issues/314 now,
"I needed a global install of NVM because I have some node based cron jobs and a few legacy applications that are rather picky about which version of node they are able to work on. For a time I worked around this by sourcing the nvm script everywhere but that seems to be a somewhat unmaintainable solution. The requirements I have:
- NVM should just work in non-interactive sessions
- Every user should be able to select a installed version of node (or use ```$ nvm exec```)
- Some users should be able to install newer versions of node
This is the solution I came up with: (shout out to @icecoldphp, for the initial version)
0. I've done this on a debian 8 machine, as the root user
1. Create a group called ""nvm"", ```# groupadd nvm```
2. Add root to the nvm group ```# usermod -aG nvm root```
3. Goto the ```/opt``` directory and create a directory called nvm - Make sure the groupd owner is nvm ```# chown :nvm ./nvm```
- Set the permissions so that the group is allowed to write in there and all file will inherit the group ```# chmod g+ws ./nvm```
4. Follow the [git install](https://github.com/creationix/nvm#git-install) steps using ```/opt/nvm``` as the directory - To make sure the group can also write aliases, cache downloads and install global packages make sure the directories exist and have the correct permissions:
```
# mkdir /opt/nvm/.cache
# mkdir /opt/nvm/versions
# mkdir /opt/nvm/alias # chmod -R g+ws /opt/nvm/.cache
# chmod -R g+ws /opt/nvm/versions
# chmod -R g+ws /opt/nvm/alias
```
5. Using the following snippet create ```/etc/profile.d/nvm.sh```:
```#/etc/profile.d/nvm.sh
#!/bin/bash
export NVM_DIR=""/opt/nvm""
[ -s ""$NVM_DIR/nvm.sh"" ] && . ""$NVM_DIR/nvm.sh""
```
6. Ensure that the script is executable ```# chmod +x /etc/profile.d/nvm.sh```
7. If you want to use nvm in non-interactive sessions as well make sure to source the nvm file in ```/etc/bash.bashrc``` before the line saying ```# If not running interactively, don't do anything``` by adding ```. /etc/profile.d/nvm.sh```.
8. For bash completion (which is inherently interactive ;) add ```[ -s ""$NVM_DIR/bash_completion"" ] && \. ""$NVM_DIR/bash_completion""``` after the section about bash completion.
Every user can select a version of node (as the permissions for public are ```r-x```) and users in the nvm group can install and remove versions of node (permissions for the group are ```rwx```).
My questions are:
- As a developer I know next to nothing about linux, could this be improved, is it bad style, etc? Any feedback is welcome.
- Should this be documented in the NVM README.md?",
"nvm is not intended to be global or system-wide - it's per-user, per-shell-session.
Thus, each user account must have its own `$NVM_DIR`. They can certainly share an `nvm.sh`, but I'd recommend they all have their own one of those too.
I would not want to document anything in the readme that encourages people to use nvm across user accounts - there's other tools for that.",
"There is of course [n](https://github.com/tj/n) (with [n-install](https://github.com/mklement0/n-install)) which with a coaching could do the same. I'll give that a shot and create a gist of the process, what are the possible other tools, other than say apt-get, brew or some system level package manager which you are usable to manager the version of node in your shell?",
"Yes, `n` is the sole system-wide node manager I'd recommend.
However, I'd suggest just installing `nvm` in the cronjob user, and invoking the cronjobs such that `nvm.sh` is sourced.",
Yes it is always good to go with what nvm is intended for. per user. Go for per user installation. I have a similar kind of a situation (I used to install node without nvm previously) and did the same.,
"> nvm is not intended to be global or system-wide - it's per-user, per-shell-session.
You know, for an util that should eliminate version discrepancies and staff it sure does increase it a lot... I mean I have server that deploys web projects on git pushes via hooks (i.e user = git). But sometimes I need to log in and redeploy the same things manually by invoking the git hooks manually (user = me). And sometimes my colleagues have to do the same (user = foo)... And then there is a process manager (PM2) that should be central for everyone, but it relies on node as well...
And everybody have it's own node&npm. Except root, so trying to sudo yields even less results, i.e ``node: command not found``. Ok, your util is the wrong one for these kind of things, but NPM in it's [official docs explicitely says to use nvm](https://docs.npmjs.com/getting-started/installing-node) to avoid permission problems that unavoidably accompany multiuser usage.",Bitter Frustration
"Yes - the hazard is ""multiuser usage"". In your use case, everyone should be running `node` as the same user.",
"I'm writing a PHP wrapper for a NodeJS binary and I have this exact same problem. PHP is running as ""www"" so it **has no home directory**.
PHP can't ""see"" the NVM environment.
I had this exact same problem with getting node running in crontab but at least there I could control the user to be anything.
---
Thank you @AndreSteenveld This is yet another example of open-source project maintainer arrogance. Your use case doesn't matter so jump through these hoops.",Bitter Frustration
"@hparadiz that's pretty hostile. I'm not being arrogant here, nor am I saying the use case doesn't matter - I'm saying that this project explicitly does not support this use case. ""Arrogance"" would be assuming that free labor on an open source project you don't pay for is obligated to support your use case.",
"Literally didn't even call anyone out by name other than the one person who helped and I THANKED them. This issue is still a problem. Hasn't been closed. @Spown makes a good point too.
Yes, it *is* a project maintainer's hubris to dismiss people's use cases as invalid. Heck, I do it all the time. Just cause something is free doesn't excuse you from doing things people don't like. So it's free? I'm not allowed to complain about a vital feature being missing? I guess that makes me hostile. But remember for everyone 1 of me who bothered to write something here there's 99 others that find this page, get super annoyed, and move on with their day deciding on one solution or another.
People want to use node, nvm, and npm **system-wide** in a **global** context. This isn't rocket science. Why do people have to install it manually for every user on a system?
Like would it really kill you make a link in /usr/sbin and check the user that ran the command?
Congrats your tool is now an instrumental part of NodeJS development. It's time to level up and make it a system-wide command.",Bitter Frustration
"@hparadiz I would prefer people with that desire and use case to use a different tool, since that's not what nvm is designed for. Just because you want to cut down a tree doesn't mean a pocket knife is the right tool for the job.",
"Hey guys,
A bit of heresy from me over here that relates to this thread.
(but let's not launch the Spanish Inquisition (tm) just yet)
We needed properly managed and stable node version on our bespoke in-team-dev machines as well as on some specific live use-cases, and for that have forged an Ansible role to do just that. You may call it a hack, or you may not, it's your free will, but we're using NVM as the drop in replacement for the distro's `node` package, accessible globally. NVM turned out to be heavens apart in terms of stability than any `apt + n` combination we tried. I appreciate the tool (kudos and big thanks to you @ljharb) so it's the solution we incorporated.
For anyone interested (hope you don't mind sharing here bro):
https://github.com/grzegorznowak/ansible-nvm-node
For anyone else, please use the proper user-scoped approach. Whatever floats your servers chaps.",
"This problem exists when running node via cron as well since cron doesn't source any bash scripts and therefore won't see node installed via nvm. The solution for now tends to be to source the nvm bootstrap before every cron command:
`* * * * * source .nvmrc; node /my/script.js`
See: https://unix.stackexchange.com/questions/67940/cron-ignores-variables-defined-in-bashrc-and-bash-profile
The _real_ solution is still to install node globally but what do I know? I'm just a silly user.",Bitter Frustration
"@hparadiz there's no need for the passive-aggressive comments. If you want to install node globally, go for it - but nvm isn't the right tool for that job.",Impatience
"@hparadiz , cron is notorious for that. Be it node or not. You would want to put in full paths to whatever binary you get to use inside a cron.
Thanks to your question, I just realized cron doesn't even see /usr/local/bin, so will update my role to use the most common of paths: `/usr/bin/node`, and if you did similarly yourself then you could use the absolute path to your gulps etc, like:
`/var/lib/nvm/versions/node/v8.11.3/bin/gulp`
and the hack would be completed, I think.",
"Your assertion that nvm is the wrong tool for the job is a slap in the face of the paradigm created by BSD systems for the past three decades. If I install a binary on a machine it's expected that the binary is available to the entire system. Not just a single user. If I install ffmpeg on a machine I expect that I can run ffmpeg from my terminal but also from any running process. Expecting a process to source a bash script for every binary is madness. If everyone followed your paradigm the *nix eco system would be way worse off.
Here's [documentation](https://www.freebsd.org/doc/handbook/dirstructure.html) on BSD directory structure. Here's a [pretty good discussion](https://unix.stackexchange.com/questions/11544/what-is-the-difference-between-opt-and-usr-local) on where user level packages should be installed. Sadly nvm follows none of these well documented installation paradigms. If you did we wouldn't be having this discussion.
Furthermore if there are actually two users on a machine that need to use nvm you are duplicating binaries on-disk for each user for absolutely no reason. You could easily store them globally and link them to the user's home directory should you choose to continue to use your existing directory structure. My nvm folder on my Macbook Pro is already 479M with only two versions of node. If I told every developer to install node via nvm for every user I'd be looking at gigabytes of files on-disk for absolutely no reason. I tend to like efficiency and this is not it.
The only reason people use nvm is because node's release schedule is very rapid and apt, yum, brew, and other built in system package managers choose to not update fast enough. For this reason you have people using nvm even though they need node installed globally on a machine.
Which brings me to my next point: node is a run-time binary for running Javascript code. A run-time _should_ be installed globally. This is how literally every single run-time works.
By default when someone claims to have a piece of software that is a ""version manager"" I would expect it to do that on my system, not just within my bash terminal.
You might as well change the description of this project to:
> Node Version Manager - Simple bash script to manage multiple active node.js versions (for the current terminal user in bash compatible shells only).
You have multiple people in this thread asking for this and you claim that nvm isn't designed for this. Which is odd since nvm is obviously downloading node to a directory on the user's machine and managing directory links and aliases for them. Having it be global is a tiny additional feature on top of what you already have. You could simply soft link the directory where node is installed to /usr/local/ and be good to go (if root).
Finally for security reasons Linux [explicitly supports users](https://www.tecmint.com/add-users-in-linux/) (section 6) without a home directory. A common one in Ubuntu is `www-data` for web servers. In this thread you are explicitly saying you won't support this completely valid use case.
I'm not trying to be mean or passive aggressive. What you're sensing is disappointment.",Bitter Frustration
"@grzegorznowak
Yes, cron is notorious but that's only because most people have no idea that the cron running environment is completely different from their terminal bash environment.
Make it tell you what's going on:
```bash
* * * * * (source ~/.bash_profile; node -v; date) >> ~/crontest.log
```
```
$ tail -f ~/crontest.log v8.4.0
Wed Sep 26 20:11:01 EDT 2018
```
You actually do *not* want full paths to the binaries in your crontab. Use [`env`](https://linux.die.net/man/1/env).
You can actually make your JS files themselves executable!
```bash
printf '#!/usr/bin/env node\nconsole.log(process);' > myscript.js
chmod +x myscript.js
./myscript.js
```
And yea... it's pretty dope.
<img width=""780"" alt=""image"" src=""https://user-images.githubusercontent.com/195216/46116560-bfc95f00-c1ca-11e8-9871-bcd1b9d7f375.png"">
tldr; make your crons executables themselves and use env",
"@hparadiz that's not actually how BSD systems work. Installing a binary to a location provides the binary *only if it's available in the PATH*, and the PATH can be set per-user, or per shell session, so in fact contextual binary usage *is* the predominant paradigm - assuming that a binary is global everywhere is a slap in the face to how BSD systems work, and shows your ignorance of the same.
That nvm may not follow some conventions is irrelevant; they're *conventions*, not requirements.
I don't agree that a runtime should be installed globally, nor that it is. Many binaries are installed per-user.
That multiple people are asking for (free labor) doesn't mean they're entitled to it; I continue to claim that if you want this feature, YOU SHOULD NOT USE NVM FOR IT. That you're unhappy with that isn't really my concern.
If you're disappointed, I'm sorry for that - but there's no need to take that out on me, or this thread.",Bitter Frustration
"Please see https://github.com/creationix/nvm/issues/1533#issuecomment-303203372 if you have any questions - `nvm` is not designed for, or intended for, multiple users, and will never support the same.
This issue remains open because there's clearly some documentation change that could be made in the readme to make this more clear. A PR to do so is welcome.",
"#### I'm opening this issue because:
- [ ] npm is crashing.
- [ ] npm is producing an incorrect install.
- [x] npm is doing something I don't understand.
- [ ] Other (_see below for feature requests_):
#### What's going wrong?
package.json
```
{
""dependencies"":{""some-module"":""~1.2.3""}
}
```
currently available version 1.3.0 & 1.2.4
when perform `npm update some-module`
npm override version in package.json to `^1.3.0`
### supporting information:
- `npm -v` prints: 5.0.0
- `node -v` prints: 8.0.0",
"Hm. So, npm4 doesn't do this either afaik (nor does any older version). Usually, the way to do this is to set `save-prefix='~'` in your local project's `.npmrc` (or in `~/.npmrc`).
Having npm autodetect this and preserve this sounds like a nice thing to have. I think it's reasonable to take a PR for it ðŸ‘ I'll double-check with @iarna who might have more context about the implications, though.",
I understand it. But when doing update it should not change range in package.json,Impatience
"Had the same issue. (if I understand the original issue correct)
I did ```diff
- ""hapi"": ""16.2.x"",
+ ""hapi"": ""16.3.x"",
```
Run `npm update` and it did
```diff
- ""hapi"": ""16.3.x"",
+ ""hapi"": ""^16.3.0"",
```
npm 4 did not do this and this behaviour is not wanted so it is a bug to me!
npm -v: 5.0.0
node -v: 8.0.0",Bitter Frustration
"Fair enough! Thanks for pointing that out, @AdriVanHoudt",
"np, point me in the right direction and I might even try to PR :P",Irony
"@AdriVanHoudt ...I should probably take off that tag huh. The more I think about it, the more I realize I've *no idea* how npm4 even did this. @iarna might know but I've no clue :s",Bitter Frustration
"I didn't even saw the label haha
I what do you mean by no idea how? npm 4 just did not edit package.json on update (don't know the internals ofc)",Bitter Frustration
"oh I thought you meant that `npm up --save pkg` would magically preserve this.
Ok yeah, this is back to feature request.
This is actually something someone might put up a PR for, here's the summary: add some logic to [the bit in `deps.js` that calculates the version spec to use](https://github.com/npm/npm/blob/latest/lib/install/deps.js#L273-L280) to default to the specific prefix used by that version, _if there was one_. Forget about the `save-exact` case, since it's expected that people _must_ use `save-exact` to get npm to save as a non-range and I don't think we're gonna change this (and if we do, it should be a separate thing with a separate discussion).
Give that a whirl, and if the patch works, put up a PR? ðŸ‘",
"Just to clarify my issue (let me know if you prefer a separate one)
I have a package.json with
```
""hapi"": 16.2.x
```
I run `npm i`
It installed 16.2.0
I update my package.json to ```
""hapi"": 16.3.x
```
I run `npm update`
it installs 16.3.0 (as it should)
it updates my package.json to ```
""hapi"": ^16.3.0
```
Why would it edit the package.json? npm 4 does all the sames things except that :O",Bitter Frustration
@AdriVanHoudt try `npm update --save` in `npm@4`. That's the difference.,Impatience
"@zkat :mindblown: ok that makes sense. ðŸ™„ I would still prefer if it kept the .x notation, is that possible?",Impatience
"@AdriVanHoudt as per twitter, if you can figure out a good, clean way to do this and you have well-defined semantics for it, we might take a patch (I haven't checked with @iarna but I defer to her if she'd rather not have this)",
"@zkat I think I will switch to just using the default notation, it is more correct and works better with the lockfile imo",
"@zkat Why does `npm update` default to `--save` at all?
I don't really see the point, since I don't need to change my semver ranges except for breaking changes (e.g. `^1.0.0` => `^2.0.0`) and shouldn't those be manual anyways? ðŸ˜• I find the commands in `yarn` to make a lot more sense in comparison:
- `yarn upgrade` will upgrade each package to the latest version specified by `package.json`, without modifing `package.json`
- `yarn upgrade package` will upgrade `package` to `latest`, and save the new version with the default range in `package.json`
- `yarn upgrade package@^2.0.0` will upgrade `package` according to the given semver range, and if needed save the new range in `package.json`
### Example of npm breaking my ranges
I have a package `a` that depends on package `b`.
`b` has three published versions: `1.0.0`, `1.0.1` and `1.1.0`.
For some reason, I do not want to use `b@1.1.0`, so I set my dependencies to `""b"": ""~1.0.0""`
I run `npm update`, thinking that I will get `b@1.0.1`, which I do. But npm will change my dependencies to `""b"": ""^1.0.1`.
The next time I run `npm update`, I will get `b@1.1.0`, which is *not* in the original range!
---
It feels like I will have to use `npm update --no-save` all the time, and also instruct everyone on my team to do the same. ðŸ˜• Or is there a better solution?",Bitter Frustration
"@Maistho If you want to stick with `~`, I recommend you set it in your _project_ config with `echo 'save-prefix=""~""' >> npmrc` (in your project dir). That'll set that to default.
As for everything else? Well, that's the result of setting `--save` as the default across the CLI. This is the behavior as it's always been if you'd set `--save`.
`update` is a command we've been intending to overhaul, and we know it's something where the warts start getting more obvious as soon as you start saving. I wish we'd had time to actually redesign it by the time npm5 came out, but we really just didn't have the bandwidth for it, and it seemed better to just get something out there asap and then do a proper redesign. The same goes for `npm outdated`, which is right now tied up directly with update.
The stuff you're saying is super useful and sounds like a really good direction to take the command in. It's good to hear about the different expectations people have of this so we can make sure to write the thing people want this to be.
If you care enough for it, it'd be great to get a more complete RFC filed that describes the `update` that you wish we had and that outlines your needs from it. If that sounds like too much, just hang tight -- we'll be getting to it soon. ðŸ‘ Thanks y'all for your commentary, and for your understanding!",
"@zkat Thanks for the detailed response :smile: Setting the save-prefix will instead change all `^` prefixes into `~`, which will just replace my problem with a different one (since I want to mix caret and tilde ranges).
I'd the more than happy to write an RFC concerning `update`, but I'm not sure how I would go about it. Do you have any example/guide on how it should be structured?",
"Landed here after noticing npm changing my package.json in ways similar to the above. In my case, `npm update some_module` changed the package declaration from `some_module: ^0.1.6` to `some_module: 0.1.8` (dropping the caret specifier). As it turns out this was because I'd added `save-prefix=` to `~/.npmrc`.
This behavior feels pretty broken to me. I added the caret to this particular module as a way of saying, ""take minor updates"". In removing it, NPM effectively reversed that decision, which is categorically wrong.
I believe the problem here is that the`save-prefix` option is given precedence over what is in `package.json`. Instead, `save-prefix` should only apply when installing a new module. For existing modules, the range specifier should not be altered.
Also, given that `package-lock.json` is now a thing, I can see a case for saying `npm update` shouldn't touch `package.json` at all. (But I'm still trying to understand how package.json and package-lock.json interact, so not gonna get on a soap box about that.)",Bitter Frustration
"touching package.json only makes sense on update if you go from `x: ^0.1.6` to `x: ^0.1.8` since then you say `0.1.8` is the new lowest version. In all other cases it feels weird and counter intuitive that update touches package.json. Since you want it to get your modules up to date that is all.
I think an upgrade command which does this (and preserves the notation in package.json) makes more sense.",Bitter Frustration
"Usually my dependency versions are carefully crafted, so I don't really see a use case for `npm update` or `npm update --save` ever, except if you don't care about versions at all.
Is there a shorter alias of `npm update --no-save` or any way I can reset the previous update behaviour?
Btw, the [update docs](https://docs.npmjs.com/cli/update) currently don't mention the npm@5 behavior.",Impatience
"I think that the changed behavior of NPM is really very dangerous. We very carefully monitor which module is being changed and we always use exact versioning for all modules. The fact that now `npm update` is changing `package.json` is totally a breaking change. It is absolutely unclear why this change was done. No docs, no arguments, ... nothing. ðŸ˜Ÿ",Bitter Frustration
"The use of Tilde is even in the docs. https://docs.npmjs.com/cli/update#tilde-dependencies
IMO this is a bug, not a feature-request...",Bitter Frustration
"> it'd be great to get a more complete RFC filed that describes the update that you wish we had and that outlines your needs from it.
(from https://github.com/npm/npm/issues/16813#issuecomment-306572698)
This is the important bit preventing this from moving forward. This is one of those things where a number of good behaviors are mixing in unexpected ways.",Bitter Frustration
"This is still a relatively major issue, but it seems the trail has gone cold for quite a few months. We want to keep very close tabs on our dependencies, where we mostly use tildes but not for everything. Every time we update, NPM is adding an unnecessary chance for human error should the developer forget to correct the change made to `package.json` before committing the changes. Sometimes it adds carats, sometimes it removes the prefix entirely. Why hasn't this gotten more attention?
`npm -v 5.7.1`",Bitter Frustration
"This was opened 1 year ago tomorrow. How can we get some traction on this?
First, I think the ""feature-request"" label needs to be removed and replaced with a ""bug"" label. This is not a feature request. This is npm update behaving in a way that produces unexpected and dangerous results.",Impatience
@zkat It looks like you removed the Bug label originally. Can you add it back?,Impatience
"This is a serious bug. `npm update` must preserve the semantics of the ranges when it updates `package.json`.
It is ok to update:
* `~1.2.3` -> `~1.2.8` * `^1.2.3` -> `^1.4.0`
But it is **not ok** to replace `~` by `^`.",
Or maybe update should have strong/weak options. Strong option would update all dependencies to latest and save them with carets in package.json. Weak option would only update to the latest on the existing ranges (and preserve existing tildes).,
Just discovered `npx npm-check -u` and `npm-merge-driver`. Looks this is not such a major issue because there are better ways to update.,
"> But it is not ok to replace ~ by ^.
Ouch, saw the same in a few of our projects. This is really bad and a regression.",Bitter Frustration
This is why we will move to yarn and pnpm.,Impatience
This is not a regression. This is the behavior npm has always had when it comes to this. The change is that it's more notable due to auto save.,Bitter Frustration
"I've locked this thread to make it clear that this is no longer the place to have feature request discussions. This is not a bug.
If anyone cares enough to see this behavior changed, please [file a formal RFC](https://github.com/npm/rfcs) with more details around expected behavior. We no longer have the time or bandwidth to discuss more informal, off-the-cuff ideas.
Furthermore, if what you want to do is discuss ideas more openly without a formal proposal, with the understanding that the npm team won't really engage with you about it, you can also post in the [ideas category in npm.community](https://npm.community/c/ideas), which is a great place to brainstorm before filing an RFC.",Impatience
"Hi,
Leaflet is a very useful component but I'm facing currently a behavior which seems very weird and I could not find a workaround for it.
I'm using Leaflet 1.0.3.
I'm using Chrome Version 58.0.3029.110 (64-bit). The same happens on mobile devices my app is ported to (android/iOS).
I'm running Windows 10 Pro 64 bit
1. I'm adding a marker to the map and I want to affect its ordering by zIndex means:
`var marker = new L.Marker(location,
{
icon: new L.Icon(
{
iconUrl: iconUrl,
iconSize: [size.width, size.height],
iconAnchor: [anchor.leftOffset, anchor.topOffset]
}),
draggable: draggable,
clickable: true,
zIndexOffset: zIndexOffset
});`
In my example I'm using zIndexOffset of 202.
2. After adding the marker to the map (marker.addTo(map)) immediately I see that the marker object changes and get a zIndex of 619. Actually each application run with the same initial zIndexOffset I get a different zIndex on marker object after adding it to the map.
3. I have an event listener on 'mousedown' event and when I inspect the marker object received by the listener zIndex is again different (477). Meaning it has changed even from the arbitrary value set in setp 2.
4. In case I zoom in and out the map the zIndex will change again.
All of this doesn't allow me to set different markers on top of others since step 2 change ruins any logic I may use in my code.
Here's the example of what happens in the [leaflet playground](http://playground-leaflet.rhcloud.com/gexa/edit?html,console,output):
How could this be fixed or maybe any workaround?
Thanks in advance",Bitter Frustration
"This is not a bug, this is behaviour as expected.
You have to keep in mind that the option name is zIndex**Offset**, and **not** `zIndex`. The `zIndex` of each marker depends on the *vertical coordinate of the marker*. Look at this screenshot from https://github.com/Leaflet/Leaflet.Icon.Glyph:
![](https://camo.githubusercontent.com/09ba35cbec1a15aaf2a599be333a85d701f69ead/68747470733a2f2f6c6561666c65742e6769746875622e696f2f4c6561666c65742e49636f6e2e476c7970682f64656d6f2e706e67)
Do you see how you can only see the tip of the bottom-most markers? That's because the `zIndex` of the markers in the bottom row is greater than in the second row, which have a `zIndex` greater than those in the third row, and so on. The `zIndex` is not arbitrary. It grows by 1 for every pixel down. It also gets recalculated on zooming and some view resets.
Thus, the zIndex**Offset** option is an **offset** which applies to the seemingly arbitrary `zIndex`. Thus, if I wanted the center marker to be on top of the rest, I'd set a `zIndexOffset` of at least the height of a marker, and it would look like:
![image](https://user-images.githubusercontent.com/1125786/27128107-a2a6bbb4-50fd-11e7-8244-12efd37cc3d3.png)
> All of this doesn't allow me to set different markers on top of others since step 2 change ruins any logic I may use in my code.
> Here's the example of what happens in the leaflet playground:
If the problem is that you can't set which marker goes on top, *show me an example of how that fails*, because that playground only logs a value, it does not show me markers failing to display on top of other markers. You have felt into the [XY problem](https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem).
If you still need fine control over things on top of things, consider [using map panes](http://leafletjs.com/examples/map-panes/), or post a more obvious example.",
"Hi,
Thanks for your quick response.
I've found this behavior right now and from my perspective it's definitely wrong...
Suppose you have only 2 markers on the map, A is above the B in pixels perspective. Then A will always appear below the B in zIndex means. But if I'll drag the A to be positioned below B in pixels perspective and leave it, after this action A will appear on top of B in zIndex meaning. Each marker dragging recalculates its zIndex. This results in incorrect behavior in our app.
What we need is to allow drawing several segments of polylines. Each segment is just several polylines connected while each tip of the segment and each connection of 2 polylines are marked with markers.
Then suppose I have 2 such segments while their markers appear wherever you like on the map in a mixed mode. But what I need is the ability to have one of the segments active and each of its markers shown on top of markers of inactive segment. But in case any marker of the active segment is very close to a marker of inactive segment and the marker of active segment appears below the marker of inactive segment in pixels perspective, it will appear under the marker of inactive segment which is wrong and I have no tool to make it work in a correct way with the logic you provide.
What can be done about it?",Bitter Frustration
"> Each marker dragging recalculates its zIndex.
...and it should definitely reapply the `zIndexOffset`, does it not?
> But what I need is the ability to have one of the segments active and each of its markers shown on top of markers of inactive segment.
> What can be done about it?
As I've already said: [map panes](http://leafletjs.com/examples/map-panes/). You can have the markers for the """"active"""" segment drawn on a different pane, which has a static `zIndex` relative to the other panes. Note that `zIndexOffset` applies to the `zIndex` of the DOM elements *inside* a pane, but you can control the `zIndex` of the parent DOM element, which is the pane.",
"Okay, I'll check the panes. It may help me solve the segments problem but then appears a problem with markers of the same segment.
In case 2 markers are close to each other and the one that should be active is below the other that is inactive the same problem will reappear..
So what is your suggestion? Use pane for each marker?",
"I don't know. Show me an example of the problematic case (playground or screenshots or anything) and we might be able to have ideas.
But please remind that this is a bug tracker, for bugs in Leaflet. Don't expect us to offer consultancy on how to build X using leaflet, that's out of the scope.",Impatience
"perhaps use some `marker.on('mouseover', function(l) { l.bringToFront() })` and `marker.on('mouseoout', function(l) { l.bringToBack() })`. I'm not 100% certain names of events are correct, you can maybe check them out.",
@themre Unfortunately this solution is not perfect also. Suppose there are 2 markers exactly at the same location from map's perspective but different coordinates. There always be only one marker on top and mouseover event won't bring the marker that's below on top of the one that's on top.,Bitter Frustration
"@IvanSanchez Look at the problem on the picture. The marker with the black arrow is the active marker.
![1](https://user-images.githubusercontent.com/14091163/27130503-297b3b54-510f-11e7-8c39-ccea05ce53d8.jpg)",
@lentyaishe And what are the `zIndexOffset`s and the computed `zIndex`es of both?,Impatience
"@IvanSanchez The active marker: zIndexOffset = 5, zIndex = 239
Inactive marker: zIndexOffset = 10, zIndex = 567
Do you want me to try artificially update the zIndexOffset of the active marker on click and check whether it will reappear on top of inactive one? I really doubt it'll happen.",Bitter Frustration
"@lentyaishe No, I want you to set the `zIndexOffset` of the ""active"" marker to a much higher value than the inactive one (which I'd suggest letting at zero). Try 50 or 100, or 500.",Impatience
"@IvanSanchez Actually it did.. I've used marker.setZIndexOffset(100) on the clicked active marker (A) that was below and it appeared on top of inactive one (B) but then the click on B made it on top and clicking A again has left it behind. This means that I'll need to recalculate all markers zIndexOffsets on the map to make it work as I need which is too much. Also in our setup we may have dozens of markers on the map. This way we'll reach enormous values for zIndex which also seems not good...
That's the point that in case I could have managed the actual zIndex of markers on the map it would solve everything since I could have just decided that all active objects may have zIndex above 500 and all others will have below that. Then I should have just change values of last active objects zIndexes below 500 and the newly activated one above etc. And zIndexes of markers within a specific segment also could have similar logic, i.e below 550 inactive, above 550 - active. Anyway then it's my problem to make zIndexes work for me as expected. And currently since the map component decides for me the actual zIndex value, it messes all my logic up.",Bitter Frustration
"> Actually it did.. I've used marker.setZIndexOffset(100) on the clicked active marker (A) that was below and it appeared on top of inactive one (B) but then the click on B made it on top and clicking A again has left it behind. This means that I'll need to recalculate all markers zIndexOffsets on the map to make it work as I need which is too much.
No. It means that you have to reset the `zIndexOffset` of a marker whenever it changes state from inactive to active, or from inactive to active.
> This way we'll reach enormous values for zIndex which also seems not good...
A `zIndex` is an integer which can go up to the millions, and we *never* had any issue with high values. We do know that some browsers have a limit on integer CSS properties of 2^23 (see https://github.com/Leaflet/Leaflet/commit/c5172f3088b820325e726f57f0b09b16b1ec498d, https://github.com/Leaflet/Leaflet/commit/17c180e1eff97932c3450447d5d17fda199f7c2c), but `zIndex`es have never been a big problem, because Leaflet recalculates them once in a while on every `viewreset`, so the values for both the CSS transforms and the `zIndex`es are within a manageable range.
In my experience, it's better to learn about the [stacking context](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/Stacking_without_z-index) than to be afraid of misusing `z-index`.
> That's the point that in case I could have managed the actual zIndex of markers on the map it would solve everything [...]
No you wouldn't, because if you're not resetting the `zIndex`es of the markers which transition from active to inactive, you would end up with the same problem.
I mean, if you want to *really* try if your logic would work without Leaflet's management of `zIndex`/`zIndexOffset`, you can manually disable it with something like
```js
L.Marker.prototype._updateZIndex = function(offset) {
// this._icon.style.zIndex = this._zIndex + offset;
}
```
So far, you've been subtly suggesting that Leaflet is the culprit and that your logic is infallible. [Programmers like myself don't react well to this kind of bug reporting](https://www.chiark.greenend.org.uk/~sgtatham/bugs.html#symptoms). I bet you a beer that the problem is in the logic in your side, and that it won't matter if you manage raw `zIndex`es or `zIndexOffset`s.
If you can *isolate* a case where Leaflet fails to raise a marker's `zIndex` when raising its `zIndexOffset`, of fails to lower it, please do share a playground or similar.",Mocking
"> you can manually disable it
Thanks for the suggestion. I'll try it out. It also gave me an insight on how you implement the layouting.
> your logic is infallible
Not at all. :) I just think that the API should allow disabling the recalculation of `zIndex`es and allow the API customer break his own head on how to implement what he needs.
> If you can _isolate_ a case where Leaflet fails...
The scenario is extremely simple. Use `zIndexOffset` for 2 markers with difference of 5. Since the zIndex that each marker gets from your algorithm is hundreds you may come up with the following:
`zIndexOffset(A) > zIndexOffset(B)` set by the code
but
`zIndex(A) < zIndex(B)` calculated by leaflet
And since `zIndex` is recalculated on each map operation: drag, move, zoom etc. there's no way to get the actual difference in `zIndexOffset` for markers to make it work correctly for `zIndex`es that will be calculated.",Bitter Frustration
"Well, finally I came up with a solution that works fine for me:
Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
Anyway, **thanks a bunch for your rapid help**!",
"@lentyaishe I was used this gist an run like a charm. Maybe this can be a Feature Request?
https://gist.github.com/up209d/4c68f2391f2302e510eb81aa8bcd4514",
"> Well, finally I came up with a solution that works fine for me:
> Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
> > Anyway, **thanks a bunch for your rapid help**!
I have the same problem as you. Marker's Z-index changes when scaling, but I don't want to change z-index. I want to know your final solution.",Impatience
"> > Well, finally I came up with a solution that works fine for me:
> > Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
> > Anyway, **thanks a bunch for your rapid help**!
> > I have the same problem as you. Marker's Z-index changes when scaling, but I don't want to change z-index. I want to know your final solution.
This is my final solution.",Impatience
"The documentation says:
```
By default, marker images zIndex is set automatically based on its latitude. Use this option if you want to put the marker on top of all others (or below), specifying a high value like 1000 (or high negative value, respectively).
```
It should mention that the ""automatic"" setting based on the latitude shall not be greater than N, instead of ""specifying a high value like 1000"".
Regardless, you need to separate them by thousands for each ""level"" or ""step"" if you want to have them show as you want.
Consider their problem, they want those with a lower latitude to show below the others, but you want them to be displayed in levels not considering the latitude.
They cannot really store all the possible offsets you set, unless you pass a list first. **Basically they should say in the documentation that we use a range of 0-N and you should set each level of z-index you wish to display regardless of location as (N+1 * level_desired). You might as well considering N to be 1000, it seems to work but obviously could change.**",Bitter Frustration
"It's not explained correctly in this issue, here's a more understandable explanation.
**zIndexOffset difference of 40000 does not place marker visibly above/on top of another marker that is 10 degrees lower vertically on the cartesian map.**
@lentyaishe use the example I provided in your opening comment of this issue so that people understand the problem.
https://plnkr.co/edit/hyWro1sGgmjbeDPk
**The Google logo should be visually above (per z-index) the Yandex logo.**",Impatience
"@andrewhodel use the `zIndexOffset` in the marker options not in the icon options:
```
var n = L.marker([30.505, 0.57], {icon: myIcon, zIndexOffset: 80000}).addTo(map);
```",
"@Falke-Design does not work.
<img width=""638"" alt=""Screen Shot 2022-02-07 at 4 22 30 PM"" src=""https://user-images.githubusercontent.com/741705/152796083-55234af0-474a-4d07-97ad-e894016f3662.png"">",Bitter Frustration
"It does work, look into the updated sampel: https://plnkr.co/edit/GcnZAG6c5vjPaXIl",
"@Falke-Design I think it's simply a sad reality of the layers and how things don't understand beyond them.
I've already confirmed it. It does not work with zIndexOffset with svg icons as shown in the screenshot.
It's all being set with marker_opts{}.zIndexOffset",Bitter Frustration
"@Falke-Design it's very simple to understand that when a DOM element has a z-index of 4231 and shows beneath a DOM element with a z-index of 4220 that something is awry.
The marker layer maximum is 600 and set to never change in leaflet.
It's really easy to read that in the code with `grep`.",Bitter Frustration
Please provide a sample that doesn't work. I already showed you in the current sample from you that it is possible.,Impatience
"There's nothing but the DOM here, really.
There's no need to explain anything else, if 600 is the containing element limit set statically in the library it doesn't matter what you set them at and that is based on how the layers are applied.
@Falke-Design",Bitter Frustration
"The fact that you would argue beyond the DOM goes to show that you aren't trying to help, only to entrap.
It's simple.
@Falke-Design",Bitter Frustration
It is not always that simple to understand what somebody else tries to explain. I hope for you that you find someone who wants to help you because I will not and I don't think that anyone is willing to help you if you keep that attitude.,Bitter Frustration
"Will you accept inline type hints for the peewee like
```
# type: () -> None
```
as PR?",Impatience
"No, type hints are, in my opinion, an abomination that has no business being part of the Python language :)",Mocking
May I at least add peewee's .pyi stubs to python/typeshed repository?,
"Anything you would like to do is totally fine by me, as the license for the software is permissive. I just do not want to merge anything into mainline that concerns type hinting.",
"@coleifer I'm expecting you will make a deal with static typing fans more and more. I'm partially agree you can do nothing in orm itself and should not, but you should try to understand `static dynamic` fans. Its not about limiting dynamism or to make new java from python, people just want `user.(name/id/group)` in IDE, they are tired to go from view to model definition, to 'google' any functions signature, to debug typo mistakes in runtime. Go has no objects and generics, and any data structure should upcast value to `interface{}`, C is just too low-level. You should check out [typeorm](http://typeorm.io/) and [diesel](http://diesel.rs/), Dart. Static typing was stigmatized long time as a way to make compilable, fast languages like java, cpp. It has different purpose in dynamic languages now.",Impatience
"> I'm expecting you will make a deal with static typing fans more and more.
No way. Python is a powerful dynamic language. If I want static typing or a strong type system I will use C or go...or hell, Cython. I enjoy coding in those languages, too, but if I'm using Python I'm going to play to it's strengths.
> IDE
Peewee's design goals are composability and consistency -- learn once, apply everywhere -- precisely so that peewee will ""do what you expect"" without needing to check the docs.",
"@coleifer I'm not a strong proponent of Static Typing, but the type annotations are extremely useful when developing to help catch bugs. I debug enough Python at runtime, the more I can do to reduce that, the better.
Mypy is optional, you can choose to put annotations on things, for clarity. If the inclusion of these things only ever produces net gain, why not apply them. It's not an either-or camp. The degree to which you add Static Types is varying.
Surely you can see the point of view of why Type annotations may be extremely useful? And in no ways detracts from the power and flexibility of Python by its inclusion.",
"Sorry to necro this thread but in my experience I've also noticed that type hints can also add a significant performance boost (sometimes >20%) since Cython can now use them for type inference. Granted, performance benefits aren't _guaranteed_ but they are likely. I see you have Cython support set up for PeeWee so it's entirely possible its performance could benefit from it.",
"Hi @coleifer,
You've done a great job with this lib. With or without type hints, it's nice to use.
But if someday you change your mind about type hints, I believe it won't hurt.",
"> No, type hints are, in my opinion, an abomination that has no business being part of the Python language :)
As a maintainer of typeshed, a big repository that is all about typing in Python, I think this is a perfectly valid opinion! I felt the same when I first learned about typing in Python. It shouldn't be something that library authors have to do, and it's optional by design.",Irony
"This issue was opened nearly 5 years ago. In that time I've had plenty of opportunities to see python's type hinting in the wild. I still believe that it misses the mark, offerning none of the iron-clad guarantees you get from a statically-typed language, while being injurious to Python's inherent dynamism, readability, and simplicity. Since this issue tends to attract drive-by comments of little value, I'm going to close discussion for now.
When I want static typing I reach for a statically-typed language. When I want expressiveness, simplicity and flexibility, I reach for Python.",Bitter Frustration
"##### ISSUE TYPE
- Feature Idea
##### COMPONENT NAME
ansible-vault
##### ANSIBLE VERSION
```
ansible 2.3.1.0
config file = configured module search path = Default w/o overrides
python version = 2.7.13 (default, Apr 23 2017, 16:50:35) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]
```
##### CONFIGURATION
n/a
##### OS / ENVIRONMENT
n/a
##### SUMMARY
ansible-vault decrypt allows the decryption of completely encrypted yaml files, but it will not decrypt vaulted variables in an unencrypted yaml file with encrypted variables.
It would be nice, for CLI purposes, to have decrypt take a partially encrypted file, and give us the decrypted text.
##### STEPS TO REPRODUCE
* create `test.yml` file with single encrypted variable encrypted by `~/.vault_pass.txt`
* ansible-vault decrypt file
```
ansible-vault decrypt test.yml --vault-password-file ~/.vault_pass.txt
```
##### EXPECTED RESULTS
* Expected plain text output with encrypted variable decrypted.
##### ACTUAL RESULTS
```
ERROR! input is not vault encrypted data for test.yml
```",
@alikins I believe you look after vault,
Would be also good if 'ansible-vault view' worked for such files.,Impatience
"This might be something that will get covered in https://github.com/ansible/ansible/blob/devel/docs/docsite/rst/roadmap/ROADMAP_2_4.rst#id25
As a user, what would you expect the decrypted file to look like?
First thought is just to replace the !vault yaml scalar with the decrypted text. That probably makes the most sense for 'view'.
For 'decrypt' and especially 'edit', I'm not sure that will be sufficient. For 'edit', the re-encrypt phrase is going to need to be able to figure out which variable values originally came from a vaulted value. Especially if the file is edited significantly (reordering lines for example, or changing the variable name). So the file presented for editing would need to include some markers indicating the text that was decrypted/should be re-encrypted. A couple of ways to do that:
1) Add comments to mark the text, and doing some text manipulation/regexes to replace it with encrypted text in place. Something like:
``` yaml
# START VAULT PLAINTEXT - my_var
my_var: my text goes here
# END VAULT PLAINTEXT - my_var
some_plain_var: blippy
```
2) Add a new yaml type indicating text to be encrypted. Something like:
``` yaml
my_var: !vault-plaintext |
my text goes here
some_plain_var: blippy
```
It would be best if we could yaml parse the input, decrypt the value, serialize the yaml to a file for editing, let user edit it, then yaml parse the results, encrypt the value, and serialize to yaml and save.
But... doing that with the available yaml parser would lose comments and ordering of maps.
So likely some in place string/text manipulations will be required.",
"Not going to happen for 2.4, so bumped to 2.5.",
"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",Impatience
"Any news when this is planned to be implemented in ansible?
We have lots of passwords as vaulted variables, hence updating\viewing them is troublesome.
I did some script (based on solution, from alikins last post) to at least parse such yml and decrypt every variable to stdout\file to see a decrypted file at once, but this is just a script that is not a complete solution (and it is decrypting only).
UPD: I ended up going thru ansible code to understand how it works with encrypted variables and wrote some tiny script that I can use in my automation jobs with Jenkins. I hope it would be useful for anyone who is waiting for this issue to be fixed.
https://github.com/andrunah/ansible-vault-variable-updater
It would be nice to have this functionality in ansible out of the box.",Impatience
"I do see this in 2.5.
```
root@ubuntu-xenial:~# ansible --version
ansible 2.5.0rc1 (stable-2.5 36566e62a7) last updated 2018/03/05 13:46:00 (GMT +000)
config file = /etc/ansible/ansible.cfg
configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
ansible python module location = /root/git/ansible/lib/ansible
executable location = /root/git/ansible/bin/ansible
python version = 2.7.12 (default, Nov 20 2017, 18:23:56) [GCC 5.4.0 20160609]
root@ubuntu-xenial:~# cat vars.yaml
ansible_ssh_pass: !vault |
$ANSIBLE_VAULT;1.2;AES256;my_user
31313064366365626535323066613234626234336664333266663161366233396365633063303539
3066363333666236666335656631666663373037643338630a303763363031373337663733326134
38336566366535373561373830386638663635363438333633313536333731646331366138383961
3331346163623661340a663862323337313562376338386539326438323562383136383832376266
31306663393532323761353761353435373432633632626365633734303335633436
nonpass: pass
root@ubuntu-xenial:~# ansible-vault view vars.yaml
Vault password:
ERROR! input is not vault encrypted datavars.yaml is not a vault encrypted file for vars.yaml
```",
"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",Impatience
"I poked at this a little yesterday and braindumped some thoughts in code comments at https://github.com/alikins/ansible/commit/603cac4a041a10ec8186617c95ef539a9ece787a
(copied/paraphrased here for discussion)
> Open a file, figure out if it is all vault or yaml with vault strings, edit.
> > if yaml with vault strings, parse the yaml with AnsibleYaml > and secret. Replace with '!vault-plaintext vault-id' and plaintext. > Save, open editor. > On save/reencrypt, reparse the file with AnsibleYaml, get the
> plaintext of the to be reencrypted vaulted string, encrypt it
> (!vault-plaintext -> !vault,
> AnsibleVaultUnencryptedUnicode -> AnsibleVaultEncryptedUnicode).
> > And then, things get complicated... we can't just AnsibleYaml.dumps()
> the data structure out:
> 1. Comments and comment placement is not preserved which > is kind of annoying
> 2. AnsibleYaml can loads things into data structures
> that it can not `dumps()` out.
> Ie, we can't serialize a bunch of stuff we can deserialize.
> > So just AnsibleYaml.dumps'ing the datastructure back
> to a file will usually either fail or do the wrong thing.
> > #2 above is unlikely to get fixed soon if ever.
> #1 is mostly a limitiation of the PyYaml yaml module ansible uses. > Other implementation like Rueyaml can do this, but it is unlikely for > ansible to change this any time soon.
> > So, since we can't just serialize to yaml, we likely need to do some > string manipulation to replace the '!vault ' blob.
> > We would need to know exactly what the before string looked like
> and where in the file it is, and what the new !vault will look like.
> But we don't really know what the new !vault-plaintext > string will look like.
> > For that matter, we don't know if it will be in the same place,
> or if it will exist at all, or if it will be at the same path in the > datastructure after the edit. > > We could limit edit to only try to work in cases where
> those aren't changed. We also have no idea what the
> plaintext will look like.
> > ideas:
> - !vault-plaintext is a compound yaml type, with fields for
> the vault id to use, and for the plaintext. Could also
> possibly include some identifying info for what the !vault
> it replaced looked like. An example:
> > some_var: !vault-plaintext:
> vault_id: 'dev'
> decrypted_from: |
> $ANSIBLE_VAULT;1.1;AES256
> 66393964663765613335633461643334393234346231666665306635323635333137306339356232
> plaintext: |
> The new plaintext to replace decrypted_from with
> > That would give vault-edit enough info to do a reliable job > of replacing the previous content.
> The downside to that approach is that it points out the limitations of the current !vault format. It may also be useful to extend !vault to support getting a data structure with info in it instead of just the plain text scalar. At the moment, I'm not sure if it could do both but it seems possible.
Or could just call the extended info version of !vault !vault-extended or similar. At that point it might be possible to make !vault-extended the default vault blob format for vaulted files as well. ie, instead of
a vaulted file being:
```
$ANSIBLE_VAULT;1.1;AES256
66393964663765613335633461643334393234346231666665306635323635333137306339356232
3533306631646431663239623762366365663137383435380a393139303161383561303336623962
35373663663036333863373666326634616532376335333133326163376136353636633763623739
3736343064326662390a306438356239386665306437646665323836393032393565666136643362
3663
```
It would be yaml something like
``` yaml
--- - !vault-extended:
vault_id: 'dev'
cipher: AES256
encrypted_on: 2018-03-14
ciphertext: |
$ANSIBLE_VAULT;1.2;AES256;dev
66393964663765613335633461643334393234346231666665306635323635333137306339356232
```
ie, more or less like https://github.com/voxpupuli/hiera-eyaml",
Would it be possible for the user to tell you which scalars to decrypt - not try to do the whole file?,Impatience
"Right now the usability of encrypted variables compared to whole encrypted files is rather poor unfortunately. Especially in cases where I quickly need to access an encrypted variable (e.g. a password) I really don't want to google for solutions like https://stackoverflow.com/questions/43467180/how-to-decrypt-string-with-ansible-vault-2-3-0.
It is also a problem for `git diff` use cases (https://stackoverflow.com/questions/29937195/how-to-diff-ansible-vault-changes). Is improving this state still on the roadmap? I didn't find it neither for 2.5 nor 2.6...",Bitter Frustration
"Hi, so I've figured out a way to do this for checking individual values, using a yaml parser, **yq** https://github.com/mikefarah/yq (there's more than one yq project, but I used this). This works with ansible 2.5.2
I have a vars file, with encrypted and unencrypted values, `all.yml`
```
unencrypted_value: 1234
encrypted_value: !vault |
$ANSIBLE_VAULT;1.1;AES256
37316535353565313063353530353539666634363834626664366263666538346131653332353932
3637363030613037316336306466656432353463383230370a396530323164353563363434663238
30336436396264656663663837346162323762333063376631326633356533376566633563386637
6531383261396366640a363339616164333630373730613564646434386364396534653063666238
6131
```
I have a password file, `vault-password`
```
password
```
Using `yq`, I'm able to decrypt the value pretty easily, by selecting the encrypted value and passing it to the decrypt function
```
$ yq read all.yml encrypted_value | ansible-vault --vault-id vault-password decrypt
Decryption successful
secretsecret
```
Hope this helps!",
"Thanks, this helps if it is possible to install additional software. I would argue that ansible-vault should also have this functionality built-in.",
Yeah definitely that'd be the best option :),Irony
"Also related, let rekey work on all encrypted variables in a file. There doesn't seem to be a good way to rekey all the encrypted variables, which makes encrypted variables super cumbersome now that we have to rekey (will end up having script this). Even if it just spits it back out to stdout that'd be a huge help instead of modifying the variables in the file directly.",Bitter Frustration
"Why is this issue still assigned to the `2.5 milestone` when `ansible 2.5` is already release a long time ago ? See #44556 for outdated milestones.
Please reassign to a current milestone, this is a really missing feature imo (especially the lack of rekeying functionality).",Impatience
It would be nice if rekey worked this way as well. Updating only the encrypted values in a mixed variable file.,
"Just giving another thumbs up on this; something like the `yq` solution above works okay and can be scripted, but having the functionality be part of `ansible-vault` itself would make management and re-keys so much simpler, and require one fewer dependency.",
"A simple but effective solution would be to keep the existing symantecs of ansible vault encrypt, decrypt and view commands, to detect and encrypt and decrypt values of complete files.
For existing users of encrypted files, it would be trivial to convert to the enrypted values. It could even be considered best practice is to keep encrypted values in files named such as secrets.yml, to make it easier to spot accidently unencrypted secrets.
During the encrypt phase, it would convert any unencrypted values to encrypted values. This would allow users to very simply add new values just by editing the ""secrets.yml"", test as required, then run the encypt command. Users would be able to enforce or check encryption by git hooks or similar.",
"That solution would be simple, but likely not enough. For example every variable can be encrypted with a different secret/vault identifier. Also encrypted and unencrypted variables can be mixed.
I'd still like to have a way at least to decrypt all variables belonging to a vault ID transparently using `ansible-vault`. Seriously, this is a usability problem since Ansible 2.3! This makes it nearly impossible for me to use vaulted variables, since being able to run `git diff` on changes is important.",Bitter Frustration
This is still a problem with Ansible 2.8... A solution would be really appreciated!,Impatience
"For others looking for a quick solution I created this script: https://gist.github.com/steffann/240d4170e45aa3cf7cf0df5e9beaf0ba
It uses [ruamel.yaml](https://yaml.readthedocs.io/), which preserves ordering, comments etc in the YAML file. Great when depending on decent git diffs etc :)",
maybe a bit unrelated but I like how [sops](https://github.com/mozilla/sops) does it.,
"Running into this issue again and it sucks. Please guys, this issue has been open for almost 2 years now and for people who really use ansible-vault, this is a major pain the butt.",Bitter Frustration
"same issue here, we need to unencrypt all values and it is a nightmare, this must be common function",Bitter Frustration
+1 for this functionality.,
"+1, really need it!",Impatience
"I solved this using debug mode. E.g.
`ansible localhost -m debug -a var='myVariable' -e ""@myFile.yml"" --ask-vault-pass`",
+1 it would be very handy!!!,Impatience
"> I solved this using debug mode. E.g.
> > `ansible localhost -m debug -a var='myVariable' -e ""@myFile.yml"" --ask-vault-pass`
Works beautifully! No need for `--ask-vault-pass` if you have the password in a file identified by the `ANSIBLE_VAULT_PASSWORD_FILE` environment variable:
https://docs.ansible.com/ansible/latest/reference_appendices/config.html#envvar-ANSIBLE_VAULT_PASSWORD_FILE",
+1 would be super helpful especially as we transition away from fully encrypted ansible-vault files to just files with encrypted variables (for ease of use/readability/ability to modify etc),
"A few expansions to @whirlwin's clever workaround:
Instead of a specific var, you can have ansible dump all vars (encrypted and plain) by specifying `-a var=""vars""`. There will be some noise in the result with a few stock vars, but it's a nice way to see everything at once.
You can also specify multiple source files by passing multiple `-e ""@...""` args.
Also, I haven't tested this personally but you can apparently avoid the implicit `localhost` warnings by setting `ANSIBLE_LOCALHOST_WARNING=false` on Ansible 2.6+.
All together:
```
ANSIBLE_LOCALHOST_WARNING=false ansible localhost -m debug -a var=""vars"" \
-e ""@file1.yml"" -e ""@path/to/file2.yml"" -e ""@path/to/file3.yml""
```
plus `--ask-vault-pass` if needed.
Obviously a built-in solution would be much better, but this makes mixed plain/encrypted var files somewhat workable.",
+1 for this functionality.,
"If you just need decryption instead of rekeying, I've written https://github.com/theblazehen/ansible_vault_decrypt_strings",
+1 for this feature,
"I've locked this to contributors for now. Adding +1 comments is too noisy. For future reference, add a reaction to the issue body, and don't comment.",
"Take a look at the `vault` and `unvault` Jinja filters that were added recently to core, might get what you need...",
"Continuing discussion from https://github.com/pallets/meta/issues/10#issuecomment-209980352
The naming is inconsistent:
- Github repo is `jinja`
- Pypi package name is `jinja2`
- Pallets project calls it ""Jinja"": https://www.palletsprojects.com/p/jinja/
- RTD namespace is jinja2.readthedocs.io
- Pocoo docs (currently the official ones) are ""Jinja"": http://jinja.pocoo.org/docs/2.9/
- file extensions are sometimes `.jinja`, `.j2`, `.jinja2`... Ansible project currently uses `.j2`
We should pick either ""Jinja"" or ""Jinja2"" and use it everywhere for consistency. I am open to either, ""Jinja"" is simpler and shorter, but ""Jinja2"" has a more distinctive ring to it and less likely to get confused with any other projects.",
"The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)
I really want to drop the ""2"" from the name. Start adding v2 builds to the ""jinja"" PyPI page. Deprecate the ""jinja2"" import and go back to the ""jinja"" namespace.",Bitter Frustration
@ThiefMaster @mitsuhiko @untitaker do you guys have opinions?,Impatience
I think we can do that but I would personally propose to align the 3.0 release with that.,
":+1: on waiting for 3.0.
---
> The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)
I may be able to fix that.
Edit: Yes, I can
> **Rename preview**
> jinja2 will be removed from 3486 questions
> jinja will be added to 3486 questions
> 5 commitments to jinja2 Documentation proposal will be moved to the jinja proposal
> A tag synonym mapping jinja2 â†’ jinja will be created.
> (these counts include deleted questions and exclude overlapping tags)",
"What is the timeline for 3.0 release?
The sooner we start giving folks a heads up the better, so what about adding a deprecation warning now on `jinja2` imports and a warning on `jinja` imports that we will soon be pushing v3 out to the `jinja` namespace?",Impatience
"@davidism are you able to move the RTD namespace over to `jinja`? Per my comment above, it's currently under `jinja2`, and IIRC, you were driving the cleanup/ownership migration of the RTD namespaces for other projects?",Impatience
In a way the last major release of Jinja2 was a massive change in the engine. Not even sure if there is more stuff we need to break :D,Irony
"Saving breaking changes and name consolidation for a Jinja v3 sounds great to me. We might as well try to find what breaking changes we can slate for it.
I'd like to remind everyone of a potential one - [allowing included block overrides](https://github.com/pallets/jinja/issues/243). That issue doesn't have to mean a breaking change, but if that's the route you all want to go, remaking/opening that issue with a v3 milestone is how I'd do it. Sorry for the tangent. :) Perhaps we can make another ticket for discussing what to break / milestone for Jinja v3.",
"nudge @davidism - per my comment above, are you able to modify the RTD namespace from jinja2 to jinja?",Impatience
"In the 2.11 release, I'm thinking of renaming the package to `jinja`, with a placholder module for `jinja2` that forwards all imports and issues a deprecation warning.
I'll still have to work out the timing of this next step, but I'd also like to try moving back to the ""Jinja"" name on PyPI. I think what I'd try to do is have a **Jinja** 2.11 build that includes the `jinja2` placeholder, and make the **Jinja2** 2.11 build just depend on `jinja>=2.11`, or have a small shim that explains installing the other name without breaking any code. I'm am willing to take on the extra effort of keeping these builds in sync for a while while we manage a transition.",
@davidism this shouldn't happen in a point release. This would break pickle and a bunch of other things.,Bitter Frustration
"Since I gave my blessings before I want to actually qualify this somewhat. I have some stomach ulcers with this change. Ultimately I don't think it's particularly useful for users (it just drops one character), introduces some backwards incompatibility concerns and it undoes a learning I made back when Jinja2 was originally released.
The reason the package renamed with 2.0 was that there was no way (and there still is no way) to have parallel installations of Python libraries that are incompatible unlike node or rust can. Because of that I think we're going to be sooner or later again in a stupid situation where Jinja 4.0 would need to be named ""Jinja4"" on pypi.
So I think while this rename is somewhat okay I generally don't think anymore that it's a good idea. I think this change would be without concerns if the Python import system were to support imports with different versions which however I gave up hoping for.",Bitter Frustration
"@coleifer I really have no idea what you're suggesting other than ""let's just revert this"". We won't release this as a patch/bugfix release, so I guess you are not happy that this will land in 2.11. Are you expecting us to release Jinja 3 for this? That would cause even more problems in a dependency tree that has multiple package dependant on Jinja.
Honestly I find your behavior completely unacceptable and hope it will have consequences.",Bitter Frustration
~fwiw we could also release a new (point) version of `jinja2` that reexports all of `jinja` (ie it is the shim). That usually works in Rust when you have multiple dependencies that depend on another package. You'd just have to update `jinja2` to make packages that depend on `jinja2` implicitly use the types from `jinja`.~ discard this. This is exactly what the shim is doing. I have no idea what the concern is.,Bitter Frustration
"@untitaker Interested in the issues you refer to with making the rename happen in Jinja 3.0 instead. Based on discussion with @ThiefMaster, it seemed that doing it in 3.0 made more sense, as it does represent a major change. We also thought about a 2.12 release for just the rename.
Jinja2 3.0 would be the shim and pull in Jinja 3.0 as a dependency.",
That would probably be fine but it would prohibit using the new `jinja` name with packages that explicitly depend on `Jinja2==2.*`. Which limits the potential usefulness of the shim.,Bitter Frustration
"Yeah, that was one of my initial reasons for going with 2.11. I guess 2.12 vs 3.0 comes down to deciding on if the rename is a major change even though jinja2 would continue to work and issue deprecation warnings. 3.0 was originally only going to be a major release because it dropped Python 3.
---
After some more discussion internally, we're reverting this. See #1131.",
"This PR is created in response to https://github.com/jbialobr/gitextensions/pull/3
It addresses some issues that bother me in https://github.com/jbialobr/gitextensions/pull/3 These are:
1. WorkingPathProvider depends on Directory, AppSettings and GitModule static methods. These are widely used across the app. I would not like to pass them to various objects through a multiparam constructor.
2. Turning static methods into instance methods: https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376 3. Incompleteness of arranging the exterior. Unconfigured calls return default(T), which may cause false positive tests results. I would prefer to be notified that unit test referenced not configured call. It does not address the separation CommandLineArgs concern https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376
This is only a sketch - there is no proper class per file and file per project separation. I am interested in discussing about the general idea of this solution.
The general idea of this solution is:
1. Wrap static methods in Gateway classes.
2. Gateway classes are singletons, they provide default implementation through public virtual members.
3. In unit tests Gateway classes are substituted in Setup method.
4. After the arrange phase unconfigured calls on substituted Gateways are not allowed.",
@jbialobr Do you have a link to the original discussion describing the underlying problem that both you and @RussKie were trying to solve?,Impatience
"Sure, I added this PR https://github.com/gitextensions/gitextensions/pull/3881
@RussKie would like to have tests for it and made a sketch PR https://github.com/jbialobr/gitextensions/pull/3 showing how he would see this done.",
"To set the context.
The current codebase features heavy use of inheritance and static methods.
OOP in general and inheritance in particular were all the rage about 10-15-20 years ago. As a concept it does have benefits of reuse, but it also carries downsides of maintenance overhead, complicated abstraction layers, difficult testability, side effects and surprises in behavioural changes... These days stateless interface-based implementations have become quite popular (especially with raise of functional languages) where each class has a single responsibility (or as close to that as possible) and gets all the dependencies injected via a constructor. This allows to write much cleaner and maintainable code, easier to unit test (as opposed to inheritance based classes where you would more likely be writing integration tests) and easier to alter behavior by substituting dependencies.
Another benefit of using interface-based architecture that you could plug in IoC container (e.g. autofac).
Over the years I have experience both sides, I was a big proponent of OOP but I have learnt the hard way that it makes it much harder to maintain such code, especially in large projects.
As I observed, a lot of bugs stem from implementations with overloaded concerns and lack of unit tests. As a result the code is quite confusing, inflexible and hard maintain.
In my opinion, one of the ways we could start reducing the tech debt and improving the codebase - start extracting separate concerns into individual classes and compose them as required. Gradually we would be able migrate most of the codebase. Static methods have their use too, but as any tool they have to be used appropriately.",
"My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces, leading to the following problems:
1. Tests are no longer executing against reality. Sometimes it's close, but it means tests are prone to both false positives (failed due to incorrect mock) and false negatives (passed even though the situation intended to be tested will fail at runtime).
2. Mocks are effectively a very clumsy DSL where logic must be manually reimplemented. Every mock necessarily reduces the long-term maintainability of the project, as well as the ability of developers to make behavior-preserving changes.
3. Mocks are not a substitute for properly covered tests. You can test code without mocks, or you can test code both with and without mocks, but mocks never save you from writing tests you would otherwise have to write.
As long as there is a clear understanding that mocks are only to be considered in the total absence of the ability to write a test using another approach, then I don't have a particular problem with either using or not using DI.",
"My early testing recommendation would be to enable codecov.io (or similar) service as soon as possible. Code coverage tooling which is tightly integrated with both the code review process on GitHub as well as the supporting CI system enables developers to write mid-level tests (against API boundaries) without sacrificing overall implementation coverage. Tests written at this level, which I call ""units of behavior tests"", have these advantages:
1. They are typically easy to understand
2. They demonstrate the intended use of APIs at module boundaries (public and internal), including edge cases
3. Failure of the test is typically easy to relate to a sequence of steps a user could take to produce incorrect behavior at runtime
4. They are typically easier to design in relation to anticipated use of the application
5. When coupled with code coverage tooling, they help reveal edge cases in API usage scenarios
Note that I consider user input/interaction, file system and network activity, and operating system interaction as API boundaries that can be tested against. APIs internal to the application can be added to this set in any number of ways.",
"> My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
I am not sure if completely I understand your point.
If each class has a single responsibility (SR), then DI should not add any ambiguity - for each use case you set dependencies to imitate boundary conditions, but you **only** exercise and validate functionality of the class under test.
Any changes to dependencies should not have any profound effect because the class is built against a contract and not a particular implementation. Certainly a degree of coupling is possible, but it typically it will be lose coupling.
Also a SR class (usually) define a limited number of methods (accessible via an interface). As a result you test the public surface. Generally there is a limited need to test internal or private methods (due to their absence).
Another benefit of smaller classes with SR - you know exactly what each class depends on. For example right now `GitModule` spans across 3,400+ lines of code carrying hundreds of methods. And we inject this monstrosity to classes which may need to access only one method from the lot...
If a class has multiple responsibilities and/or initialises its own concrete dependencies then it is virtually impossible to perform unit testing - instead you will be writing integration tests. Any changes to dependencies with high degree of probability will necessitate changes to the class, because there is a tight coupling to concrete implementations.
Wrt: code coverage - whilst it is a very useful metric, it shouldn't be viewed as an absolute. 100% CC can be very misleading.
Personally when I write tests I usually strive for 100% coverage taking in consideration various edge cases. I use NCrunch, it helps me to visualise what cases I may have missed.",
">Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
Yes, that's true. Regardless of the tools we use there is always a way to use them inappropriately.
I like to be pragmatic. For me it means: achieve maximum benefits at reasonable cost with decent safety.
Weighing those factors we have to take into account the existing code base and resources we have.
Thank you for pointing to codecov.io - it should help to find which pre-condition we lack in the test set (though 100% codecov coverage does not imply 100% pre-conditions coverage). Implementing codecov into GitExt project will show us all the most obvious pre-conditions against which should we write ""units of behavior tests"".
Then we have to write these tests, to do it, we have to weigh the factors I mentioned and decide whether to mock or to configure the exterior. I think we can't make this decision at a general level. Besides, one approach does not have to fit all concerns.
Here we have 3 dependencies, how would you see ideally written [this test](https://github.com/gitextensions/gitextensions/pull/3898/files#diff-67c45fbdcf3a92b53649d84f9f401c3eR26)?
```csharp
public void ReturnsRecentDirectory_if_RecentDirectory_IsValidGitWorkingDir()
{
//arange
DirectoryGateway.Inst.CurrentDirectory.Returns(string.Empty); <-- mocked
_ext.StartWithRecentWorkingDir = true; <-- mocked
string unitTestRecentWorkingDir = ""unitTestRecentWorkingDir"";
_ext.RecentWorkingDir = unitTestRecentWorkingDir;
GitModuleGateway.Inst.IsValidGitWorkingDir(unitTestRecentWorkingDir).
Returns(true);<-- mocked
//act
string workingDir = _workingPathProvider.GetWorkingDir(new string[0]);
//assert
workingDir.Should().Be(unitTestRecentWorkingDir);
}
```",
"@russkie. Thoughts of mef vs aurofac?
https://stackoverflow.com/questions/15572302/difference-between-mef-and-ioc-containers-like-unity-autofac-smap-ninject-w
On Sun, Aug 13, 2017, 11:47 AM Janusz BiaÅ‚obrzewski <
notifications@github.com> wrote:
> *@jbialobr* commented on this pull request.
> ------------------------------
>
> In GitExtensions/StaticDependencyInjection.cs
> <https://github.com/gitextensions/gitextensions/pull/3898#discussion_r132845408>
> :
>
> > + }
> +
> + public interface IInstanceFactory
> + {
> + T CreateInstance<T>() where T : class, new();
> + }
> +
> + public class NewInstanceFactory : IInstanceFactory
> + {
> + public T CreateInstance<T>() where T : class, new()
> + {
> + return new T();
> + }
> + }
> +
> + public class StaticDI
>
> Thanks, it is always better to be more thorough.
> It seems that I false assumed that the 3 dependencies are
> static/stateless. The all three services depends on IFileSystem. Changing
> this assumption, my concerns change as follow:
>
> 1. The dependencies should be passed down the tree through a
> multiparam constructor starting from the Main proc.
> 2. We should create a separate service GitWorkingDirService to which
> FindGitWorkingDir and IsValidGitWorkingDir should be moved.
> 3. It should be handled at the Nsubstitute level.
>
> I would not introduce the ServiceLocator as it is in conflict with 1)
>
> I will update this PR to comply with the updated assumption.
>
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#discussion_r132845408>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADdhsRmqOyDUF78xYjEAV-J2TwxGgYuxks5sXxqYgaJpZM4O1aRu>
> .
>",
">>My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
>I am not sure if completely I understand your point.
One problem I see in using mocks/spies is it often tests the implementation instead of expected behavior.
A typical unit test arranges the before state (internal and external), performs an action and asserts that the after state is equal to the expected state. Because mocked methods do not modify any external state we can not verify the after state, only asserting the expected sequence of calls on the dependencies. Changing the implementation breaks such an unit test.",
"> Thoughts of mef vs aurofac?
I haven't worked with MEF. Until you mentioned it I am not sure I was even acutely aware of it (though I may have heard the name). On paper MEF has benefits but as well it has a number of trade offs. To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry, so taking a dependency on it carries an inherent risk coupling to a technology which may become instantly obsolete (like Silverlight).
This leads to another problem - MEF requires a lot of additional ceremony (decorations). In event we want to migrate from MEF we won't be able to do so easily. If we use IoC switching containers is relatively trivial task and doesn't require re-architecture the whole app. I've just migrated our big solution from Funq to Autofac within two days and the footprint of the change was minimal. Also from what I've read MEF seems to be opaque in how it handles types, where IoC may be more transparent.",
"> One problem I see in using mocks/spies is it often tests the implementation instead of expected behavior.
> A typical unit test arranges the before state (internal and external), performs an action and asserts that the after state is equal to the expected state. Because mocked methods do not modify any external state we can not verify the after state, only asserting the expected sequence of calls on the dependencies. Changing the implementation breaks such an unit test.
This is only a problem if your implementations are stateful.
I am a big proponent of stateless implementations (""pure"" methods as they call it in functional land). In stateless world you test behaviors which do not depend on state in any way.",
"> To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry
It's been the primary container for Visual Studio since 2010. Over the years it's received a lot of love to meet the performance demands of that application, though not all of those benefits have made their way to the normal distributions of it *yet*. I would classify it as having fewer features, but within the bounds of its capabilities excels in ease of distribution and being generally well-understood.",
"Yep. I'm developing a smartsheet to TFS MVC website. The website handles
the communication with smartsheet and it uses mef plugins to react to the
webhook calls from smartsheet.
On Sun, Aug 13, 2017, 9:27 PM Sam Harwell <notifications@github.com> wrote:
> To me, one of the trade offs is that it doesn't seem to be widely embraced
> by the industry
>
> It's been the primary container for Visual Studio since 2010. Over the
> years it's received a lot of love to meet the performance demands of that
> application, though not all of those benefits have made their way to the
> normal distributions of it *yet*. I would classify it as having fewer
> features, but within the bounds of its capabilities excels in ease of
> distribution and generally well-understood.
>
> â€”
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322081292>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADdhsTGGLts65YkEkkC2d0bCYikelDQgks5sX6KCgaJpZM4O1aRu>
> .
>",
">This is only a problem if your implementations are stateful.
I am a big proponent of stateless implementations (""pure"" methods as they call it in functional land). In stateless world you test behaviors which do not depend on state in any way.
But it is a real problem:
https://github.com/gitextensions/gitextensions/pull/3865/files#diff-dce953c015443e09345315dc6cdf8452R59
https://github.com/gitextensions/gitextensions/blob/0b9a310019965c8183599e9afa3a722df03ae1f4/UnitTests/GitCommandsTests/Remote/GitRemoteControllerTests.cs#L239
https://github.com/jbialobr/gitextensions/pull/3/files#diff-7ca5aacd7a7769eef82fb4b8f4d3d9ddR46",
"I'm not sure what you perceive as a problem in attached links, I'm sorry.
The classes under tests are stateless - they do not have any settable fields or properties (outside the container) nor they access any objects besides those passed via the container. The external dependencies are interface-based, and as such they are stateless too. So there are no side effects and there are external context mutations - so I can assert the correctness of my implementation.
The unit tests exercise my class implementations and check the flow of execution by checking how far each use case progresses and what methods within the method under test were called. There is no magic here :)
Of course there is an arrangement phase, you can't not have it. But the arrangement only sets a possible return from the underlying dependency (boundary conditions). My duty as a class developer write unit tests to account for these boundary conditions and facilitate stable predictable response.
**EDIT:**
Just realised some classes may be accessing `AppSettings` which arguably belongs to the external context, but for the most part we can presume AppSettings to be immutable. They should be abstracted too.",
"> > To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry
>
> It's been the primary container for Visual Studio since 2010. Over the years it's received a lot of love to meet the performance demands of that application, though not all of those benefits have made their way to the normal distributions of it yet. I would classify it as having fewer features, but within the bounds of its capabilities excels in ease of distribution and being generally well-understood.
My point that it is still contained within Microsoft.
Given that we know how Microsoft (and other big players) drop support for their products overnight I would be very cautious investing into tech stack which is difficult to deprecate.
We can certainly consider MEF for dealing with plugins but I feel very uneasy committing to it for the core of GE.",Bitter Frustration
">I'm not sure what you perceive as a problem in attached links, I'm sorry.
""... it often tests the implementation instead of expected behavior.""
The following test tests implementation.
```csharp
public void SetRemoteState_should_call_ToggleRemoteState(string remoteName, bool remoteDisabled)
{
var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
_configFile.GetConfigSections().Returns(x => sections);
_controller.ToggleRemoteState(remoteName, remoteDisabled);
_configFile.Received(1).GetConfigSections();
_module.Received(remoteDisabled ? 1 : 0).RemoveRemote(remoteName);
_configFile.Received(remoteDisabled ? 0 : 1).RemoveConfigSection($""{GitRemoteController.DisabledSectionPrefix}{GitRemoteController.SectionRemote}.{remoteName}"");
_configFile.Received(1).AddConfigSection(sections[remoteDisabled ? 1 : 0]);
_configFile.Received(1).Save();
}
}
```
I would see the assert part along the lines of this:
```csharp
public void ToggleRemoteState_ChangesDisabledValue(string remoteName, bool remoteDisabled)
{
//arrange
var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
_configFile.GetConfigSections().Returns(x => sections);
//act
_controller.ToggleRemoteState(remoteName, remoteDisabled);
//assert
_controler.GetRemote(remoteName).Disabled.Should().Be(!remoteDisabled);
}
}
```
This way we don't care about the implementation details, we only deal with pre and post conditions.
If the implementation changes from `remove then add` to `rename`, the first test will fail while the second will not.
>The external dependencies are interface-based, and as such they are stateless too.
Apparently we have a different understanding of ""stateless"". The config file holds a state. Invoking its methods changes this state.",Bitter Frustration
"> The following test tests implementation.
Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.
As a class consumer you don't care about it - you expect by calling a method something happens and that something happens in expected predictable manner.
Any changes to my class behaviors should only really affect my class. And for a consumer of the class the changes should be transparent. For example, as a consumer by calling `_repo.GetRecord(id: 1)` I expect to get record with id=1 - whether the record is in a database, xml file or a remote service I don't care. If repo implementaion is migrated from a database to a remote service or an xml file - it is an implementation detail for the repository implementation and I shouldn't change my implementation.
On the other hand, as a repository developer in its implementation I would probably want to make sure that the database connection is established before I attempt to interact with the database. Naturally in the repository tests I would expect to see something like `_db.Received(1).Open()`.
. . .
> Apparently we have a different understanding of ""stateless"". The config file holds a state. Invoking its methods changes this state.
Indeed we are :smile: As a consumer I interact with an interface - I don't know nor expected to care about the implementation detail of the instance given to me at runtime. By virtue of this the dependency is treated as stateless.
. . .
Btw `GitRemoteController` isn't a stateless implementation, it has a bunch of side effects and it maintains the state in `Remotes` list. So it is probably not the best example to use.
To sum it up:
We probably won't be able to make everything stateless, not from the get go anyway:
- the codebase isn't ready for it yet, and
- UI layer is typically stateful.
If we go with interface-based implementations it is much easier to write smaller SR implementations which can be easily tested. It is much easier to make implementations stateless too. You get benefits of declarative composition - it is easy to see objects connections and dependencies. As well as DI and substitutions - i.e. you could have Windows- and Linux-specific implementations without random `if running on Windows do this else do that`.",
">Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.
I see it totally different. As a class developer you need to ensure the expected behavior. The order of execution and the content of execution is subject of change in contrast to the expected behavior.
When you want to optimize your implementation you change it and run unit test to see if everything is ok.
When unit tests test the implementation you get tests failure.",Bitter Frustration
"> Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.
This view makes sense. I would lean towards agreeing with itÂ¹ on paper on both technical merits and practicality merits. However, reality of developing over time tells a very different story.
This is not only not correct, but every time I have seen it followed it's had a disastrous impact on the project. The possibility of someone with this mindset working on tests is the reason I raised my original concern above.
Â¹ I was impressed with the approach joining another project that leveraged it. I only changed views after the problems were proven beyond any shadow of a doubt; fixing the tests was a long, hard road. But at the end every metric we came up with for measuring test quality was improved.",Bitter Frustration
">As a class consumer you don't care about it - you expect by calling a method something happens and that something happens in expected predictable manner.
I don't buy it. You write a method that calls:
...
dep.Proc1; ... dep.Proc2;
...
dep.Proc3; Then you write a unit test that ensures that `dep` received these calls in the exact order. This way you verify that you implemented it the way you think it should be implemented. However if the order should be Proc3, Proc2, Proc1 your unit test does not fail. Another thing is that reading a such specified assert you don't know what is the expected state after the method execution - you only know how test writer imagine it should be implemented. I think that tests should document expected behavior not implementation.",Bitter Frustration
"Ok, let consider this method https://github.com/gitextensions/gitextensions/blob/master/GitCommands/Remote/GitRemoteController.cs#L190-L205
```
public string RemoveRemote(GitRemote remote)
{
if (remote == null)
{
throw new ArgumentNullException(nameof(remote));
}
if (!remote.Disabled)
{
return _module.RemoveRemote(remote.Name);
}
var sectionName = $""{DisabledSectionPrefix}{SectionRemote}.{remote.Name}"";
_module.LocalConfigFile.RemoveConfigSection(sectionName, true);
return string.Empty;
}
```
1. Suppose there is a refactor gone wrong and a developer accidentally deletes ""!"" on line 197.
To ensure the correct handling of disabled remotes I would write a test something like: ```
[Test]
public void Ensure_disabled_remotes_dont_get_removed() {
// ...
_module.DidNotReceive().RemoveRemote(Arg.Any<string>());
}
```
2. I want to ensure the correct section name is passed to the config manager.
In the test I can expect to make a call
```
[Test]
public void Ensure_removing_correct_section() {
// ...
_module.LocalConfigFile.Received(1).RemoveConfigSection(""...."", true|false);
}
```
How would you write tests to ensure the correct behavior?",
"I would add a disabled remote to a configuration file, and then remove one of the other remotes. At the end of the test I verify that the disabled remote is still in the file.
If a test written in this form fails, there is no question about what it was testing or why the failure indicates bad behavior. There are many reasons why a test using the expected call sequence could fail. For example, if someone deletes the file because they thought they were removing the last item, the test passes even though the specific failure it was written to detect certainly occurred.",
"> I would add a disabled remote to a configuration file, and then remove one of the other remotes. At the end of the test I verify that the disabled remote is still in the file.
This won't be a unit test but a integration test since the class now needs to know how to write or read from a file. Which breaks SR principle.",Bitter Frustration
"```csharp
[Test]
public void Ensure_disabled_remotes_dont_get_removed() {
//arragne
_module.AddRemote(name);
_module.DisableRemote(name);
//act
_controller.RemoveRemote(name);
//assert
assert(_module.GetRemote(name).Disabled == true);
}
```",
"If `_module` isn't an interface you are too writing integration tests...
testing this class as well how module class works
On 14/08/2017 10:26 PM, ""Janusz BiaÅ‚obrzewski"" <notifications@github.com>
wrote:
> [Test]
> public void Ensure_disabled_remotes_dont_get_removed()
> {
> //arragne
> _module.AddRemote(name);
> _module.DisableRemote(name);
> //act
> _module.RemoveRemote(name);
> //assert
> assert(_module.GetRemote(name).Disbaled == true);
> }
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322176701>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AEMyXtMUnIlC8RAW-FInPnC9jRS9rtDeks5sYDzrgaJpZM4O1aRu>
> .
>",
"> This won't be a unit test but a integration test since the class now needs to know how to write or read from a file. Which breaks SR principle.
I prefer to describe tests as ""fast""/""slow"", or as ""stable""/""unstable"". A test as I described would be characterized as ""fast"", ""stable"", and also particularly well-suited to detecting regressions related to a specific scenario. When you start describing tests as ""unit"" or ""integration"", it's easy to lose sight of the fundamental purpose of testing.
The fundamental purpose of testing is to detect cases where the observable behavior of an application deviates from the expected behavior for some user scenario. I find this condition is reasonably relaxed to detecting changes in behavior under a theoretically-reachable scenario.
""Unit"" tests are a sub-optimal approximation of this that resulted from several limitations:
1. It is difficult for developers to reason about the line-level behavior of high-level testing.
1. Code coverage tooling was incomplete and/or slow, such that it was not practical to adopt as part of a team code review process.
1. High-level testing tends to be slow, unreliable, and/or difficult to stand up (e.g. only runs under a specific non-standard system configuration, or requires specialized software installations).
Code coverage tooling now performs very well, and eliminates the problems typically associated with the first item. We found that by moving away from ""unit tests"" as a goal, we were able to write tests that better represented reality. The focus instead was on test speed, stability, and independence from system configuration. The result was something between what would normally be classified as ""unit"" or ""integration"" tests - we used fakesÂ¹ to eliminate external dependencies, and tested primarily against well-defined module boundariesÂ². When possible, we fed inputs to the application in the same way it would actually occur for a user:
1. When testing the behavior of a subsystem, we tested using the same API that other code would be using to drive that subsystem. Code which we *couldn't* cover with this approach often meant the code was dead code under real-world conditions.
1. When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
1. When testing file system, we created files and folders and tested against them. We used ACLs or chmod to force access errors and held handles open to force sharing violations.
Â¹ Typically stub executables or our own implementations of interfaces in code
Â² We applied a `[ServiceInterface]` attribute to the boundary interface, which carried special requirements. These interfaces were required to be threading agnostic, safe for concurrent use without regard for the caller (new unknown callers could appear at will), idempotent, and only ever obtained from the single service container instance. Mocking was *forbidden* for any object which was not marked with `[ServiceInterface]`. For cases where one or more of these conditions could not be met, we used `[ServiceInterface(Restricted = true)]`, and required that the restriction (often a limitation on who is allowed to call it) be documented - use of this form was seen as technical debt, but used occasionally because the correct solution was too costly at the time.",
"> The order of execution and the content of execution is subject of change in contrast to the expected behavior.
This is a contradictory statement - the _order of execution_ is one of the key factors for _expected behaviour_.
And the _content of execution_ should not matter at all.",Bitter Frustration
"Thank you @sharwell for taking time and sharing your experience. I find your approach very pragmatic.
I think that at this point enough has been said and we should stop to write down the fundamental goals the tests should serve and the criteria they should meet. Only after that we can discuss how to achieve these goals and meet these criteria. These are mine:
**Goals:**
1. Ensure the expected behavior of the app. By saying expected behavior I mean that having given pre conditions the tested unit transforms the existing state into a state described by the post conditions. To be clear we should assert on the expected state not on the steps that lead to this state.
2. Document the expected behavior. **Criteria**
1. Independent from the environment - a test should arrange the environment in the way that makes it invariant regardless of changes made to the environment outside of the test.
2. Stable - a test should always produce the same result for the same starting conditions.
3. Easy to understand the pre and post conditions.",
">When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
@sharwell Is there any publicly available project that uses this approach - I would love to see this in action.",
"@sharwell great write up, thanks.
It looks like we come from different schools of thought and practice. Like yours my position comes from practical experience solving real engineering challenges.
I'm part of a small team building RAD framework and reusable components where pretty much every component can be substituted by a consumer. We have a number of major and minor releases concurrently powering hundreds of apps in production, and we provide full runtime backwards compatibility.
We can assume very little about consumers, their habits and requirements. In turn we must provide a great deal of flexibility but ensure stability of the framework core and consumer applications (to a degree). This is specifically important in security-related areas.
All of this dictates how we architect and engineer the framework, how we deal with dependencies, how we deploy and how we test.
Until we re-engineered the codebase to consist of interface based stateless single responsibility classes we had a number of challenges meeting the requirements (including production incidents). We also couldn't reason about stability and predictability of provided behaviors.
Now we have 1,800+ .NET unit tests, hundreds of JS tests and probably another few thousand of BDD tests. Some tests, especially security-related, test the order of execution - without that we can't reason about predictability, nor we can release new versions or provide backwards compatibility.
Smaller interface-based single responsibility implementations allow us to achieve * isolation (all dependencies are injected), * testability (all dependencies are mocked), * stability (via predictability), * flexibility (consumers only concern themselves with replacing a specific implementation instead of worrying about inter-dependencies). And as a bonus - our APIs are easy to register in an arbitrary IoC container.",
"@RussKie Your last post really resonates with me. One of the goals I had when I was pushing so hard to switch away from mocks (on a former project) was based on the type of application being shipped to customers. As you are well aware, anything considered public API surface area has special requirements related to versioning. These requirements constrain the development team substantially, but are a great asset from the customers perspective. However, in an application where the final product is the application (as opposed to a publicly-consumable API), these same requirements *burden* the development team. I didn't like the fact that when we wanted to make a change to our implementation, we kept running into pain points (leading to delays) resembling those faced by developers trying to make changes to public APIs.
Another application (this time a library) I've contributed to over time is StringTemplate. While it has a few known extension points built in, it was never designed to be arbitrarily reconfigurable for end users. Over time it's led to a frustrating experience for a small subset of users trying to do arguably normal things, just things we didn't think of from the start. For example, by failing to abstract the file system (location and reading) away from the template loader, it's not possible to use *some* features of StringTemplate when the templates are stored e.g. in a database, or stored using a compression mechanism we didn't think of from the start. Better adherence to SRP would likely improve the long term durability and flexibility of this library.
It sounds to me like GitExtensions is unlikely to suffer from most of the problems related to reusable components, and more likely to benefit from a testing approach that focuses on application behavior.
> > When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
>
> @sharwell Is there any publicly available project that uses this approach - I would love to see this in action.
Not to the extent of the one I've been referring to. I've started to add tests for UI related bug reports to PerfView, but these are quite limited and not particularly elegant currently.
When I built a system like this in the past, I started with the following:
1. Identify all mutable static state in the application. We will need a way to reset these to known default values between each test, ensuring that the outcome of any given test cannot influence the behavior of another (with a corollary that a test failure uniquely and deterministically identifies the culprit, which can then be executed in isolation to reproduce the failure):
* Mutable `static` fields
* File read and written
* Registry keys/values read/written
2. Identify all asynchronous operations. We will need a way to ensure that a test is not considered complete while one of these is executing (or scheduled for later execution, in the case of timers or delays).
3. Identify external dependencies for which the concrete implementation is likely problematic for high-speed deterministic testing. We will need a way to substitute the behavior of these items during testing. The method of substitution must guarantee that during the execution of any given test, all executing code is using the same implementation (either the original or the substituted one, but never a mix and never multiple substitutes).
* One obvious case is invocations of *git.exe* for many types of tests
* Loading resources from a network
Once you address the above, most tests are still written against module interface boundaries. However, when actually testing UI behaviors it's not particularly challenging to write those tests against the controls. Most of the problems typically associated with testing UI behavior are really just failures to address one of the above underlying concerns.",
"I would go as far as proposing moving away from statics altogether.
There are merits for extensions, but like any tool they have to be used sparingly to be used correctly.",
"Erik Dietrich of NDepend writes insightful articles discussing different aspects of code architecture and design. This is one of them along the lines of this discussion, worth reading https://blog.ndepend.com/singleton-pattern-costs/",
"The article is somewhat related, but the single-instance services I would advocate for behave very differently from the classical singleton pattern. Most of the differences were created to directly address one or more costs listed in that article.",
"Indeed, but it raises points how statics introduce side effects and hinder testabiltiy.",
">If `_module` isn't an interface you are too writing integration tests...
testing this class as well how module class works
I am happy to do that. I don't need unit tests, I need the app to be thoroughly tested. I expressed my needs here https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322384024
I don't want to say that asserting on implementation is always wrong. It can be helpful, but it should not be overused. I think it should be strictly limited and explicitly described when it can be used. I see to many corner cases in my mind when unit tests asserting on implementation go green and the app is not working as expected. There are even more cases I see when changing the implementation turns such unit tests into red ones. These of many assertions in a single test are the most likely to fail and the hardest to correct (I am ignoring here the fact that we should not have to correct tests for which the pre and post conditions have not changed). I don't mind writing tests that ensure a developer well understand description of the used interfaces and its operations. For example when resetting unstaged changes, I prefer to have a set of tests like:
1. Ensure staged files are untouched
2. Ensure ignored files are untouched
3. etc.
to have a test like:
1. Ensure the `git reset --hard` is executed.
>>The order of execution and the content of execution is subject of change in contrast to the expected behavior.
>This is a contradictory statement - the order of execution is one of the key factors for expected behaviour. And the content of execution should not matter at all.
Perhaps my English skills are too poor to express it correctly. I prefer to verify that the chosen by a developer sequence of execution leads to the expected state than to verify that the chosen by a developer sequence of execution is the sequence he chose. The order is indeed one of the key factors but there may exist many orders that lead to the expected behavior.",Bitter Frustration
"Lets look at this test: https://github.com/gitextensions/gitextensions/commit/9fb22f58727408d73baaea03ecf6cb69a66f997a#diff-3a91b3e9edc1faa5190d69a2fec75400R81
It is a copy-paste from the implementation. It is really a [test double](https://en.wikipedia.org/wiki/Test_double). It doubles the implementation, it doubles the false assumption that the path separator is `\`. While unit tests are useful, they should not replace integration tests.
Nevertheless, the architecture should allow to write both kinds of tests.",Bitter Frustration
"Locking this discussion for archival purposes, since I use it as a reference for an in-depth discussion from opposing viewpoints in other contexts.",
"Sam, I'd be interested in learning gained insights and outcomes of those discussions.",
"The word ""Caddy"" in the context of software is under a pending trademark application. By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)",
"> The word ""Caddy"" in the context of software is under a pending trademark application
So it's not a registered trademark?
> By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)
See issue #1 - this is planned ðŸ˜ƒ",
"Technically, there is an implicit trademark because of the continued use and recognition of the brand over the years, and it is legally enforceable. But we have also submitted a formal trademark request as well, which is pending. Just FYI, we have no problem with anyone forking Caddy. We enjoy open source!
Can I ask what your use case for Caddy is?",
"Pretty disingenous for you to refer to yourself as a member of Caddy's ""we"" when so far as I can tell you're not involved: https://github.com/mholt/caddy/graphs/contributors",Bitter Frustration
"@SirCmpwn I'm part owner of Light Code Labs, the legal entity that filed for the trademark.",
"I see. Well, in any case, be patient, and maybe also try contributing to your own web server?",Impatience
You got it ðŸ‘,
"Thanks for clarifying your involvement, @yroc92. It's great to hear you value open source!
In the United Kingdom, trademark violation requires a mark to be used ""in the course of trade"". I have no plans to make commercial use of this project in any form whatsoever (including but not limited to adding sponsor headers to HTTP responses and then charging for them to be removed).
With that said, I will be removing references to Caddy throughout the codebase as you have requested, though the primary benefit here in my opinion is reducing user confusion and ensuring users do not go seeking support from the wrong place.",
"@lol768 As long as you're dolling out legal advice on the Internet, you might well quote the other relevant section of the Trade Marks Act of 1994.
> A person may also infringe a registered trade mark where the sign is similar and the goods or services are similar to those for which the mark is registered and there is a likelihood of confusion on the part of the public as a result
See: section 10(2)(b) http://euipo.europa.eu/pdf/mark/nl_uk_1_en.pdf
I am not a lawyer, but I am somewhat versed in trademark law. Your dependence on ""commercial use"" may be unsupportable, but talk to an attorney for legal advice.
LCL are actually doing the right thing to defend their mark by providing notice.",
"Thanks for contributing your expertise @gonzopancho :) I do apologise for giving myself legal advice earlier.
The start of section 10 (2) states:
>A person infringes a **registered trade mark** if he uses **in the course of trade** a sign where
because:
Your quoted section (""10(2)(b)"") is indented **underneath the above**. Emphasis is mine.
----------
This is moot however, because I have already committed to change the name.",
"Just wanted to comment here and say thank you for the respect and the professional tone of the discussion. I was in class most of this morning and so Cory and I haven't had a chance to coordinate. I stand by Cory's choice to bring up the issue and I think this discussion was a good one to have.
As you know, per the Apache license, forks are required to maintain copyright notices and give credit to the original project, etc, so the name Caddy *should* be used somewhere in a visible place, I think the main concern was with the logo being used prominently. But the README was quickly updated, which we appreciate.
We're glad there are people who value Caddy enough and feel strongly enough about it to fork it with their own changes. Hopefully this won't result in a splintered effort to make the Web better and safer, instead we look forward to mutually benefitting somehow.",
"Hey @mholt,
Great to see you weigh in here and thanks for the conversation on HackerNews. It was useful to discuss things, even if I do completely disagree with the direction in which you're going with this.
> As you know, per the Apache license, forks are required to maintain copyright notices and give credit to the original project, etc, so the name Caddy should be used somewhere in a visible place, I think the main concern was with the logo being used prominently. But the README was quickly updated, which we appreciate.
Apologies for that. GitHub's fork feature is great at duplicating everything, so the README was misleading for a while. I'm happy to mention that this is a fork of Caddy in the README as required by the Apache license and will look at making this more obvious if it's not already apparent.
Are you happy for me to mention that Caddy has a paid support offering and link https://caddyserver.com/pricing in the README for users seeking commercial support? If it wasn't already clear, my intentions are to remove the code I disagree with for people who want to use Caddy _personally_ (though I'm also of the opinion that the binaries I will be distributing here shouldn't be limited). I'm not here to try and sell a commercial support offering.
> We're glad there are people who value Caddy enough and feel strongly enough about it to fork it with their own changes. Hopefully this won't result in a splintered effort to make the Web better and safer, instead we look forward to mutually benefitting somehow.
As long as you're intending on a) leaving the trademark issue alone and b) keeping the upstream repository open source (and licensed under an appropriate free software license), I'm happy to contribute any relevant fixes/improvements upstream and include the link I mentioned above.
It is unfortunate that at present you don't seem open to reversing your decision to introduce advertising into Caddy. I still dislike what you're doing with the binaries, but I don't dislike it enough to motivate me to continue maintaining this repository - it's really the header I have a problem with.
Caddy is an excellent piece of software, I would really urge you to read through the HackerNews thread again at some point and reconsider if this is the approach you want to take. Neither of us want to see more fragmentation here, I think.
Cheers,
Adam",Bitter Frustration
"> Apologies for that. GitHub's fork feature is great at duplicating everything, so the README was misleading for a while. I'm happy to mention that this is a fork of Caddy in the README as required by the Apache license and will look at making this more obvious if it's not already apparent.
Yeah, no problem. We were very quick to jump in here after the fork; I realize from my own experience that this isn't enough time. It was a very busy, fast-moving morning. :)
> Are you happy for me to mention that Caddy has a paid support offering and link https://caddyserver.com/pricing in the README for users seeking commercial support? If it wasn't already clear, my intentions are to remove the code I disagree with for people who want to use Caddy personally (though I'm also of the opinion that the binaries I will be distributing here shouldn't be limited). I'm not here to try and sell a commercial support offering.
Sure, you may certainly do that.
> As long as you're intending on a) leaving the trademark issue alone and b) keeping the upstream repository open source (and licensed under an appropriate free software license), I'm happy to contribute any relevant fixes/improvements upstream and include the link I mentioned above.
Sounds good to me. The use of our logo at the top of your fork was quickly resolved anyway.
> It is unfortunate that at present you don't seem open to reversing your decision to introduce advertising into Caddy.
Well, it's only been 7 hours. :) Let's give it some time and see how things are when the dust settles. Believe me, it's not a decision I took lightly.
> Caddy is an excellent piece of software, I would really urge you to read through the HackerNews thread again at some point and reconsider if this is the approach you want to take. Neither of us want to see more fragmentation here, I think.
Thanks - yes, I agree. (Frankly I'm not too convinced by the HN arguments -- as usual -- except for recognizing the fact that our pricing can be out of reach for small, bootstrapping startups; we encourage them to contact us about special pricing in the meantime.)",Bitter Frustration
">Frankly I'm not too convinced by the HN arguments
My main concern, which you probably saw, is that your website is deceptive. Nothing's wrong with getting paid for your work, but your website is designed in bad faith.",Bitter Frustration
"@SirCmpwn > My main concern, which you probably saw, is that your website is deceptive. Nothing's wrong with getting paid for your work, but your website is designed in bad faith.
Come again&mdash;what's deceptive?",Bitter Frustration
I wrote about it [on HN](https://news.ycombinator.com/item?id=15238969).,
"I'm confused at what this fork offers that Caddy doesn't... Is it the code that shows the headers that you removed all that's stopping you from using Caddy? You do realize that you can compile Caddy yourself without those headers in it, without forking it and maintaining a whole new project, right?
From my point of view people are upset by 2 things. 1: They can't get pre-compiled binaries without the header and 2: if they can, they have to pay for it.
If anything, this fork adds MORE work on you than what Caddy says to do to use it free of charge without the headers.",Bitter Frustration
"this trademark claim appears to be a false claim. the original caddy is hosted on github, and per the [github faq](https://help.github.com/articles/licensing-a-repository/):
> Note: If you publish your source code in a public repository on GitHub, according to the Terms of Service, other GitHub users have the right to view and fork your repository within the GitHub site. If you have already created a public repository and no longer want users to have access to it, you can make your repository private. When you convert a public repository to a private repository, existing forks or local copies created by other users will still exist. For more information, see ""Making a public repository private.""
i realize that you've already complied with the request, but based on this, it appears that the caddy team are not acting in good faith",Bitter Frustration
"> not acting in good faith
A misunderstanding or miscommunication is a far cry from ""not in good faith"". I'm gonna stand firm here and re-assert that our actions are in good faith.
The misunderstandings about the use of the logo have been resolved.",Bitter Frustration
The bad faith part comes in because so far as I can tell you are *deliberately* misleading users.,Bitter Frustration
"> I'm confused at what this fork offers that Caddy doesn't... Is it the code that shows the headers that you removed all that's stopping you from using Caddy?
Removal of adware and binaries that can be freely distributed and are not subject to an EULA.
> You do realize that you can compile Caddy yourself without those headers in it, without forking it and maintaining a whole new project, right?
Yes, I didn't overlook that :)
> From my point of view people are upset by 2 things. 1: They can't get pre-compiled binaries without the header and 2: if they can, they have to pay for it.
> If anything, this fork adds MORE work on you than what Caddy says to do to use it free of charge without the headers.
From my point of view (and from reading HackerNews) people are upset about the ads embedded in the webserver and served to the visitors. They're also upset about how the binaries are licensed.
The fork is absolutely more work for me. The point is that I care enough about this that I'm willing to create the fork, remove the code I disagree with, cross-compile it and then make free (as in freedom) binaries available to everyone - including those who may not be technically adept enough to set up their own golang workspace and compile it themselves.",
I think forking on Github does not require removing any reference to the project you forked from.,
"> The fork is absolutely more work for me. The point is that I care enough about this that I'm willing to create the fork, remove the code I disagree with, cross-compile it and then make free (as in freedom) binaries available to everyone - including those who may not be technically adept enough to set up their own golang workspace and compile it themselves.
Until you run into the issues that Caddy had where it actually costs a lot of money to maintain and use the infra to do such things in a way that is helpful.
Then we're right back where we started, you doing something in a way to help make money to support that stuff, everyone getting upset, and forking a forked project that was forked from another fork that forked from a project that started such tactics to help.
The circle of open sores, I like to call it.",Bitter Frustration
"I find it highly unlikely that's going to happen. I have infrastructure, if you ever need somewhere to host builds or tooling like caddy offers, reach out and I'll share it for free.",
"5 years later, fair to say this fork was created out of spite with no intentions to sustain it ðŸ˜‚
Grandstanding sure is cheap.",Mocking
They stopped the bad faith behavior upstream so there's no longer any reason for this fork to exist. Imagine sticking up for a company that you have nothing to do with when they go around being a trademark bully. You sure showed that petty FOSS volunteer the error of their defiance towards the honorable corporate entity!,Mocking
"> They stopped the bad faith behavior upstream so there's no longer any reason for this fork to exist. Imagine sticking up for a company that you have nothing to do with when they go around being a trademark bully. You sure showed that petty FOSS volunteer the error of their defiance towards the honorable corporate entity!
Imagine thinking that a solo developer trying to make a living from his work is some trademark bully and evil corporate entity, just because they have an LLC and premium support plans.",Mocking
"Trying to make a living does not make it okay to be deceptive and misleading or to threaten others. There are plenty of ways to make a living without being deceptive, especially for a talented programmer. Get your moral compass re-calibrated.",Bitter Frustration
"> 5 years later, fair to say this fork was created out of spite with no intentions to sustain it ðŸ˜‚
Bit of a strange thing to do, resurrecting an issue 5 years on to post .. that.
However - as Drew has already pointed out, the fork became obsolete as soon as upstream 180'd and dropped the tacky, baked-in ads. I'm glad no significant fork maintenance / investment in a build system was required in the end, given Matt took the sensible decision to merge the PR to remove the header (which had been added without even consulting his sponsors, putting some of them in an incredibly awkward position amongst the negative reception).
Most of us have now moved on. I am glad Caddy has a more sustainable funding position - I see this debacle as an illustration of the power of open source (and forking) providing a ""checks and balances"" mechanism when it comes to undesirable moves from upstream.",Mocking
"Sometimes I'm doing a PR and I want to update a specific dependency but I don't want to deal with updates of all my dependencies (aiohttp, flake8, etcâ€¦). If any breaking change was introduced in those dependencies, I want to deal with it in another PR.
As far as I know, the only way to do that would be to pin all the dependencies that I don't want to update in the Pipfile. But I find it to defeat the purpose of Pipenv in the first place :) .
So my feature request would be to be able to do something like:
```shell
$ pipenv lock --only my-awesome-dep
```
That would generate a Pipfile.lock with updates for only `my-awesome-dep` and its dependencies.
I can probably make a PR for that, but I would like to get some feedback first.",
"That could also be useful for `pipenv install`, as sometimes I want to install a new dependency without updating others.",
"There's a little thing to take into account here: Changing a single dependency could change the overall set of requirements.
Ex: Updating `foo` from 1.0 to 2.0 could require to update `bar` to >=2.0 (while it was <2.0 before), and so on.
I know that in the context of `pip-tools` itself (from which `pipenv` takes its dependency resolution algorithm), running the dependency resolution will only ""update"" the required packages when ""re-locking"" if there's an existing lock file. It does so by checking if the existing pins in the lockfile are valid candidate first when selecting candidate in the resolving. `pipenv` could probably do the same.
I think its a reasonable idea. Otherwise, if you want to update absolutely only one dependency, `pipenv` would have to have a mode to block if changing a dependency causes other changes, or else you would loose the guarantee of a valid environment.
I hope this helps!",
"Indeed, that was what I meant by:
> That would generate a Pipfile.lock with updates for only my-awesome-dep and its dependencies.",
"Agree 100% - and I'll go a bit farther: this should be the default.
That is, `pipenv install foo` should never touch anything besides `foo` and its dependencies. And `pipenv lock` should certainly never upgrade anything - it should just lock what's already installed.
AFAICT, this is how `npm`, `yarn`, `gem`, etc. work; it makes no sense to have a lockfile that doesn't actually lock packages, but trusts package authors to not break things in patch releases, and therefore upgrades them without being asked. I can see the use of allowing upgrades, but that should be opt-in, since it's more surprising than not upgrading them.
I apologize if I'm hijacking this issue for something else, but since this is so closely related to an issue I was about to create, I thought I'd start the conversation here. Feel free to tell me I should make a new one.",
"Just found this related issue as well: https://github.com/kennethreitz/pipenv/issues/418
Being able to specify `pipenv install --upgrade-strategy=only-if-needed` seems like what I'm looking for, though of course as I mentioned I think that should be the default, as it's becoming in pip 10. But being able to specify it semi-permanently via env var would be something, anyway.
I would be surprised if that change breaks anyone's workflow ([famous last words](https://xkcd.com/1172/)), since it's more conservative than `--upgrade-strategy=eager`.",
"Tried to work around this by setting `export PIP_UPGRADE_STRATEGY=only-if-needed` in my shell config. This doesn't work, and `pipenv lock` exhibits these surprising behaviors:
1. It ""upgrades"" packages that don't need to be upgraded (but...)
1. It actually doesn't upgrade the installed versions! i.e. `pip freeze` and `Pipfile.lock` show different versions!
Guessing pipenv is delegating to pip for the install, and pip respects its environment variable settings, but `pipenv lock` doesn't.",Bitter Frustration
"@k4nar What happens right now that you are finding undesirable? Because if you upgrade a dependency that has cascading requirements obviously it will have consequences for other dependencies. Are you suggesting some kind of resolver logic to determine the most current version of a specific package _in the context of the current lockfile_? I am hesitant to encourage too many hacks to resolution logic, which is already complicated and difficult to debug.
@brettdh I think I can shed some light because you have most of the pieces. `pipenv lock` doesn't install anything, and it doesn't claim to. It only generates the lockfile given your host environment, python version, and a provided `Pipfile`. If you manipulate your environment in some other way or if you use pip directly/manipulate pip settings outside of pipenv / are not using `pipenv run` or using `pip freeze` inside a pipenv subshell, it is quite easy for a lockfile to be out of sync from `pip freeze`. The two aren't really related.
To be clear:
1. `Pipfile.lock` is a strictly-pinned dependency resolution using the pip-tools resolver based on the user's `Pipfile`
2. If you want to maintain strict pins of everything while upgrading only one package, I believe you can do this by strictly pinning everything in your `Pipfile` except for the one thing you want to upgrade (correct me if I'm wrong @vphilippon)
As for your lockfile and `pip freeze` disagreeing with one another, I'd have to know more information, but I believe we have an open issue regarding our lockfile resolver when using non-system versions of python to resolve.",
"@techalchemy : If I have a Pipfile.lock with A, B and C where B is a dependency of A, I would like to be able to update A and B without updating C, or C without updating A and B.
Again of course I can pin all my dependencies & their dependencies in my Pipfile in order to do that, but that would be a burden to maintain (like most `requirements.txt` are).",Bitter Frustration
"I concur with everything @k4nar wrote. Sure, I could even just pin
everything in requirements.txt and not use pipenv. The point of pipenv is
to have one tool that makes that (and the virtualenv stuff, of course)
simpler to manage; i.e. all packages are locked by default to a version
thatâ€™s known to work, but it should be straightforward to upgrade a select
few (without unexpectedly upgrading others).
On Thu, Oct 26, 2017 at 4:28 AM Yannick PÃ‰ROUX <notifications@github.com>
wrote:
> @techalchemy <https://github.com/techalchemy> : If I have a Pipfile.lock
> with A, B and C where B is a dependency of A, I would like to be able to
> update A and B without updating C, or C without updating A and B.
> Again of course I can pin all my dependencies & their dependencies in my
> Pipfile in order to do that, but that would be a burden to maintain (like
> most requirements.txt are).
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/kennethreitz/pipenv/issues/966#issuecomment-339591307>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAFlnqUOEKARiFD8kEk3GVczF3NXBdVOks5swEKcgaJpZM4QEf-->
> .
>",
"Hm I see what you guys are saying. The premise of passing a setting to pip is not what Iâ€™m worried about, itâ€™s resolving with pip-tools that concerns me. What does this behavior look like right now?",
"@techalchemy I mentioned the `pip freeze` difference as a shorthand for ""the package versions that `pipenv install` installs differ from the package versions that `pipenv lock` saves to `Pipfile.lock`.""
True, this only happens when I've changed pip's default args via environment variable; I was just pointing out that it was surprising that pipenv delegated to pip for installation but not for version locking; i.e. rather than locking what's installed, it locks what it thinks *should* be installed, potentially with unrequested upgrades.
Could you clarify your question a bit? I think ""resolving with pip-tools"" is referring to what `pipenv lock` is doing, and the reason it's not affected when I set pip defaults? And could you be more specific about what you mean by ""this behavior""?",
"@brettdh The locking mechanism include a notion of ""dependency resolution"" that does not exist in `pip`. Its handled by `pip-tools` (or rather, a patched version of it, integrated in a special way by `pipenv` that bring a few differences with the original tool). In short, the locking mechanism reads the `Pipfile` and performs a full dependency resolution to select a full set of package that will meet *every* constraints defined by the required packages *and their dependencies*.
@techalchemy > [...] itâ€™s resolving with pip-tools that concerns me.
I'm not sure how those `--upgrade-strategy` would affect `pip-tools`, because it works on some low-level internals of `pip`. I have the feeling this would not give the expected result, as these option take into account what's installed, and that's not what's being dealt with in that mechanism. But we have another approach to this in `pip-tools` that could be done here.
The ""original"" `pip-tools` behavior is that it only updates what's is needed in the lockfile (in its context, its the requirements.txt), but this was ""lost"" in the way the resolver was integrated in `pipenv`. Let me explain why.
Pointing back to my resume of how `pip-tools` works: https://github.com/kennethreitz/pipenv/issues/875#issuecomment-337717817
Remember the ""select a candidate"" part? That's done by querying the `Repository` object.
In `pipenv`, we directly configure a `PyPIRepository` for the `Resolver`, but `pip-tools` does something else, it uses a `LocalRequirementsRepository` object, which keeps the existing pins from the previously existing `requirements.txt` (if found), and ""fallbacks"" on `PyPIRepository`.
So in `pip-tools`, the following happens when selecting a candidate:
1. Query `LocalRequirementsRepository` for a candidate that match `foobar>=1.0,<2.0`.
1. Check if an existing pin meets that requirements:
- If yes, return that pin as the candidate.
- If not, query the `proxied_repository` (`PyPIRepository`) for the candidate.
1. Use the candidate returned
Effectively, it means that existing pins are given a ""priority"" as candidate to try first.
But in `pipenv`, currently, it simply:
1. Query `PyPIRepository` (directly) for a candidate that match `foobar>=1.0,<2.0`.
1. Use the candidate returned.
So, I think the same behavior for the locking in `pipenv` could be done by parsing the `Pipfile.lock` to get the existing pins and use a `LocalRequirementsRepository`, like `pip-tools` does in its `pip-compile` command.",
@vphilippon do you have a sense of how difficult implementation on that would be?,Impatience
"@techalchemy - Parsing the `Pipfile.lock` to extract the existing pins: Haven't looked at that. Depends on how things are structured in `pipenv`. We need a set of `InstallRequirements` that represents the pins in the `Pipfile.lock`.
- Using `LocalRequirementsRepository`: Fairly easy: change our current `PyPIRepository` for a `LocalRequirementsRepository`.
---------------
But, as I'm looking into this, and following @brettdh comments, I realize a few things:
1. The current default `pipenv install` behavior doesn't match the `pipenv lock` behavior. Doing `pipenv install requests` alone won't update `requests` if a new version comes out (much like straight `pip install`). However, doing `pipenv lock` will update the `Pipfile.lock` with the latest version of `requests` that matches the `Pipfile` specifier, and the dependency constraints.
There's 2 main way to see this:
- A) The `Pipfile.lock` should stay as stable as possible by default, not changing pins unless required, in order to stay like the current environment, and only change in the event that we change the environment.
- B) The `Pipfile.lock` should get the newest versions that respect the environment constrains/dependencies in order to freely benefit from the open ranges in the `Pipfile` and lib dependencies, allowing to continuously acquire new compatible versions in your environment. You can then run `pipenv update` to benefit from the fresh lock.
IMHO, I would align the default behavior, which would be to go with A) by default. Because right now, everytime a lock is performed (i.e. after each installation), new versions can come in, which make the lockfile *drive the update of the environment*, which seems weird. But, this is arguable of course. While in development, I might want to continuously update my requirements to no get stale, like with B), so that should also be easily doable.
2. Even if we use `LocalRequirementsRepository` to avoid updating correct existing pins, and end up aligning the default behaviors, we then need to address the equivalent of `--upgrade` and `--upgrade-strategy` for the locking part. Currently, defining some environment variable (like `PIP_UPGRADE` and `PIP_UPGRADE_STRATEGY`) will affect the `pipenv install` behavior, but will not affect `pipenv lock`, as it doesn't affect the behavior of `pip-tools` (I confirmed that, as I was unsure at first).
Otherwise, there will be no way to update the environment without either deleting the `Pipfile.lock` (feels clunky, and ""all or nothing"") or *requiring* a newer version (I mean doing an explicit `pipenv install requests>2.18.4`, which requires you to *know* that a new version is out, and changes the specifier in the `Pipfile` itself, increasing the lower bound), which is wrong. As the ""original `pip-tools`"" doesn't deffer to `pip` to deal with this (as it's not related that what is currently installed), it offers an option to specify the dependencies to update in the lockfile, and simply remove the pins for these packages (or all) from the existing_pins list, effectively falling back to querying PyPI. I'm not sure how we can match the notion of ""--upgrade-strategy"" with this.
---------------
@techalchemy So while I was saying it was fairly easy to just ""align the default behavior"", I now realize that this would cause some major issue with being able to update the packages (as in: just fetch the latest version that match my current constraints).
If there's something unclear, ask away, a lot of editing went on when writing this.
(Dependency resolution is not easy. Good and practical dependency resolution is even worst ðŸ˜„ )",Bitter Frustration
"@vphilippon that's exactly what I meant. Keeping the things that pip installs in sync with the things that pip-tools resolves is non-trivial unless you drive the process backwards, using the resolved lockfile to do the installation. I'm pretty sure that was why things were designed the way they were. > B) The Pipfile.lock should get the newest versions that respect the environment constrains/dependencies in order to freely benefit from the open ranges in the Pipfile and lib dependencies, allowing to continuously acquire new compatible versions in your environment. You can then run pipenv update to benefit from the fresh lock.
This workflow can possibly work with the current configuration. You can use `pipenv lock` to generate a lockfile, but `pipenv update` will reinstall the whole environment. I'm pretty sure we can use one of our various output formats to resolve the dependency graph (we already have a json format as you know) and only reinstall things that don't align to the lockfile. This might be more sensible, but I would be curious about the input of @nateprewitt or @erinxocon before making a decision",
"@vphilippon Totally agree that A and B are desirable workflows in different situations. Some of your phrasing around B confused me a bit, though, seeming to say that `pipenv lock` might result in a lockfile that doesn't actually match the environment - I particularly heard this in that one would need to ""run `pipenv update` to benefit from the fresh lock"" - as if the lock is ""ahead"" of the environment rather than matching it.
Regardless of whether you are in an A workflow or a B workflow, a few things seem constant to me, and I think this squares with what @techalchemy is saying as well:
* The result of `pipenv lock` should always be a lockfile that matches the environment.
* The result of `pipenv install` should always be an environment that matches the lockfile.
I'm ignoring implementation details, but that's kind of the baseline behavior I expect from a package manager with a lockfile feature.
Running `pipenv update` periodically allows you to stay in B mode as long as you want everything to be fresh, and having the ability to `pipenv install --upgrade requests` would allow specific updates of one package and its dependencies, without affecting packages that don't need to be upgraded unnecessarily.
Am I missing any use cases? I can think of optimizations for B - e.g. a flag or env var that tells it to always update eagerly - but I think that covers the basics. I also know I'm retreading ground you've already covered; it's just helpful for me to make sure I understand what you're talking about. :)",
"> Some of your phrasing around B confused me a bit, though, seeming to say that pipenv lock might result in a lockfile that doesn't actually match the environment
@brettdh this is correct -- the `pip-tools` resolver we use to generate `Pipfile.lock` doesn't ask the virtualenv for a list of which packages have been installed. Instead, it compiles a list of packages that meet the criteria specified in the list of pins from the `Pipfile`. Because the resolver itself runs using the system or outer python / pipenv / pip-tools install, we are doing some supreme fuckery to convince it to resolve packages with the same version of python used in the virtualenv. The assumption would be that `pip install` would resolve things similarly, but that isn't always the case, although even I'm not 100% sure about that. But yes, `pipenv lock` is not generated based on the virtualenv, it is generated based on the `Pipfile`. It is a dependency resolution lockfile, not an environment state pin.",
"As a potential resolution to this: something that pip itself currently supports, but `pip-compile` doesn't, is the notion of a constraints file.
A constraints file differs from a requirements file, in that it says ""*If* this component is installed, then it *must* meet this version constraint"". However, if a particular package in the constraints file doesn't show up in the dependency tree anywhere, it doesn't get added to the set of packages to be installed.
This is the feature that's currently missing from `pipenv`, as the desired inputs to the `Pipfile.lock` generation are:
1. The updated `Pipfile` contents as a new requirements input file
2. The full set of existing dependencies from `Pipfile.lock` as a constraints file, excluding the packages specifically named in the current command
Constraints file support at the pip-tools resolver level would then be enough for `pipenv` to support a mode where attempted implicit upgrades of dependencies would fail as a constraint violation, allowing the user to decide whether or not they wanted to add that package to the set being updated.",
"currently not supported, thanks for the feedback",
"@kennethreitz Do you mean:
1. This behavior should be changed, but it's not currently a priority,
2. This behavior should be added as something optional, but it's not currently a priority, or
3. This behavior should not be added?
This is a sufficient inconvenience given the inconsistency with how other similar locking package managers work that it would be good to keep this open as a solicitation for PRs.
If instead it's (3), and this will not be added, then I think a number of us on the issue will need to adjust our plans for our choice of Python package management tools.",Impatience
"I mean that this is currently not supported, and I appreciate the feedback.",
I understand that it's not supported. Are you also saying that you would not accept PRs either changing this behavior or adding this as an option?,Impatience
I have no idea.,
"@k4nar still interested in doing a PR for this? Specifically, something like `pipenv install --only <dep-to-update` which prevents unrelated deps from being updated. Since @kennethreitz seems uninterested in discussing further, it seems to me that that's the only way to find out whether that behavior addition/change could be acceptable (and, by extension, whether folks like @taion and I can continue using pipenv).",Bitter Frustration
"I'm interested but I'm not sure to know how would be the best way to implement this. There are a lot of components in action (pip, pip-tools, pipfile, pipenvâ€¦) and probably a lot of possible solutions.",
"Per https://github.com/kennethreitz/pipenv/issues/966#issuecomment-339707418, it should be relatively straightforward. That dep resolution logic is largely just from pip-tools. I was planning on submitting a PR, but I can't justify spending the work if we're not willing to talk about how we want the API to look before we spend time writing code.
I'm currently looking at taking an alternative approach â€“ as Pipfile is a standard, interactions with it don't need to go through pipenv, and I'd like to work around some of the other odd semantics here like wiping existing virtualenvs per https://github.com/kennethreitz/pipenv/issues/997.",Bitter Frustration
"Sorry to comment on a closed issue, but I'd like to point out that, to my understanding, using pipenv in my projects currently requires a workflow like this:
```
pipenv install foo
vim Pipfile.lock # Manually remove all the unwanted updates
git add && git commit && git push
```
I find it really annoying having to communicate this to my team members. The alternative seems to be to pin everything to exact versions in `Pipfile`, but that defeats much of the purpose of using pipenv in the first place.
IIUC, this behavior is the equivalent of `apt` performing an implicit `apt dist-upgrade` whenever you run `apt install foo`.
This is made worse by the fact that `pipenv install` updates stuff in `Pipfile.lock`, but does not install the updates into the local virtualenv. If the developer does not carefully examine the diff of `Pipfile.lock`, they are still using the older versions locally, but once they share the code, all other environments see the surprising updates. People have a tendency to ignore the diff of `Pipfile.lock` because it's considered an auto-generated file.
I am strongly convinced that ""update everything to the latest version allowed by `Pipfile`"" should be an explicitly requested operation that is separate from ""install foo"".",Bitter Frustration
should be fixed in master,Impatience
"The behaviour is still present, I tested it in `pipenv 11.8.3`, @kennethreitz.",Impatience
"@marius92mc The ""fixed in master"" comment is referring to the `--selective-upgrade` and `--keep-outdated` options added in recent releases: https://docs.pipenv.org/#cmdoption-pipenv-install-keep-outdated
That allows folks that need or want more control over exactly when upgrades happen to opt in to that behaviour, while the default behaviour continues to respect [OWASP A9](https://www.owasp.org/index.php/Top_10-2017_A9-Using_Components_with_Known_Vulnerabilities) and push for eager upgrades at every opportunity.",
"@ncoghlan I think one thing that is needed (easy to ask for, not as easy to do) is an FAQ on _how_ those options behave (at least it's still confusing for me).
For instance: Using `--selective-upgrade` and `--keep-outdated` will still cause outdated libraries in the `Pipfile.lock` to be updated, if they're not directly related to the ""selected"" package to be updated.",
"It sounds like there may be implementation bugs, then.",
"They are intended to leave the pipfile.lock as-is, except for the new change.",
Let me know if it's helpful to provide a test Pipfile+.lock.,
I think you've provided enough information for us to investigate. I'll try to do that now.,
"Actually, your pipfile/lock would be great, if it contains outdated results.",Bitter Frustration
"@ncoghlan, thank you for providing the details. I tried again with your mentioned options and the result seems to be the same, it still updates the other packages as well, changing them in the `Pipfile.lock` file.",Impatience
@kennethreitz https://github.com/simonpercivall/pipenv-selective-upgrade-test,
"There are any updates about this issue, @kennethreitz?",Impatience
Sorry for the slow answers on this. We havenâ€™t nailed down the root cause for the regression here yet (I know I personally have been handling a data center migration this weekend so Iâ€™ve been kinda slow) but we will get this sorted in the next few days. Contributions welcome as always!,
"I think there is a missing use case that can use this same change: when I am developing an application I often need to upgrade a single dependency's version. The steps I would like to follow are:
1. Update the version restriction for the dependency in setup.py
2. Run either `pipenv lock --selective-upgrade ; pipenv sync` or `pipenv install --selective-upgrade ""-e .""`",
"@wichert If `Pipfile` has been edited in a way that increases the minimum required version beyond what's in the current lock file, then `--keep-outdated` should already cover what you need. `--selective-upgrade` is for the case where `Pipfile` *hasn't* changed, but you want to update to a new pinned version anyway.",
"@ncoghlan `Pipfile` has not changed in this scenario, only `setup.py` by changing the minimum version requirement for a dependency, typically to something more recent and currently in `Pipfile.lock`.",
@wichert pipenv doesn't capture changes to your `setup.py` automatically because it isn't setuptools. You have to run `pipenv lock` if you want that to happen.,
"What's the current status of this? On March 25th someone said they thought implementation issues would be resolved ""in the next couple days"", and other bug reports have been closed due to being tracked here; but as of 2018.7.1 I still see the bug reported by Simon Percivall (indirect dependencies are always updated) and that bug hasn't been discussed since the original report. Is the problem still being tracked?
(I'm currently living in a second-tier city in Senegal so my Internet is terrible and it would be a game changer not to blow my data cap on updating indirect dependencies if possible :P )
PS: Thanks for making Pipenv, it's awesome <3",Impatience
Yes for sure. We are rewriting the resolver to support this right now. Whether it lands in this release or next release remains to be seen,
"Iâ€™m not that confident with my coding skill to estimate when the resolver would land :p Seriously, this is a completely volunteer project, and we donâ€™t have a deadline mechanism as you would in commercial settings (we donâ€™t even have a boss or a project manager or whatever you have in your company that decides when a thing needs to be done). If you want a thing to be done in a timeframe you desire, you need to do it yourself, or at least provide real motivation for others to do it.",Impatience
"@uranusjr FWIW, I didn't see any demands for expediency in @benkuhn 's comment above - just a question about where things are at; i.e. what work has been done, so that outside observers can make their own estimates/decisions.
I understand that pipenv is a volunteer project and that non-contributors cannot ask for a thing to be done by a date without signing up to make it happen. I do wonder, whether there is room for more transparency in the project's development process, or if I'm just not looking in the right places. Usually the answer is either ""if the issue hasn't been updated, there's been no movement"" or ""look at this WIP pull request,"" but this issue in particular seems to have triggered a much larger effort, so the dots can be difficult to connect for those not directly involved.
As always, much thanks to you and everyone who gives their valuable time towards the improvement of pipenv. ðŸ‘",
"For sure, this one doesnâ€™t have activity or a work in progress PR because it is a lot more complicated than that. We are talking internally mostly about how we even want to structure this with respect to the larger project, and working iteratively to establish an approach that might even begin to work properly. Once we can sort that out we can build resolution logic. In the meantime the resolver stack in pipenv is super convoluted and I wouldnâ€™t be comfortable asking people to invest too much effort trying to untangle it for this purpose. Even the simplest use case here will take a significant refactor. Weâ€™d be happy to review / discuss any proposed refactor if someone is interested in helping tackle this, but the two things are tightly coupled. If someone has expertise in dependency resolution and sat solving we would certainly be interested in input but there just isnâ€™t a single concrete idea yet. Weâ€™ve been through several iterations that we never planned to carry forward as more than proof of concept. Not all code becomes a PR, and not all code organization decisions happen on the issue tracker. Sometimes we chat synchronously and propose and scrap ideas in real time.",
"Something I was _going_ to suggest as an alternative workflow that might address this is making it easy to pin to a specific version in the _Pipfile_ when installing.
I think it's slightly surprising but not completely unreasonable that pipenv interprets `foo = ""*""` to mean ""I just need to make sure _some_ version of foo is installed, the user doesn't care which"". To that end, having something like `pipenv install --pin foo` which results in `foo = ""==1.2.3""` instead of `foo = ""*""` in the Pipfile (where 1.2.3 is the current latest version of foo) seems like it might help.
The issue with this though is that the behavior of a lot of packages can change a lot based on their dependencies (e.g., the same version of pylint can do totally different things depending on what version of astroid it's using), and packages don't pin their own deps exactly. So I don't think this actually gets anyone very far. :/",Bitter Frustration
"(Just realised I have been commenting to the wrong issue. Sorry for the mess up, please ignore me) ðŸ˜ž",
"An actual use case that I've struggled with for some hours now: I want to measure test coverage in a Django 2.0 project. Even `pipenv install --keep-outdated --selective-upgrade --dev coverage` insists on updating the non-dev Django package to version 2.1, which because of breakage elsewhere I absolutely cannot use yet. **There really *must* be a way to change the set of installed packages without upgrading completely unrelated packages to possibly breaking versions.** The possibility of breakage in the latest version will always exist.
I'll try [@rfleschenberg's workaround](https://github.com/pypa/pipenv/issues/966#issuecomment-366333367), but I don't know whether having a presumably incorrect `_meta hash` property will break anything.",Bitter Frustration
"@l0b0 If your application genuinely cannot handle a particular version of Django, I think it makes sense to state that restriction in your Pipfile?",Impatience
@AlecBenzer That sounds like something for setup.py to me.,
"@wichert That might make sense too (I'm actually not totally clear on in what circumstances you'd want to have both a setup.py and a Pipfile), but if you have a line in your Pipfile that says:
```
Django = ""*""
```
you're telling pipenv that you want it to install _any_ version of Django. If what you really want it to do is install 2.0 or lower, tell it that instead:
```
Django = ""<=2.0.0""
```
While in this particular case pipenv is upgrading Django for no real reason, it could be that somewhere down the line you try to install a package that requires Django >2.0.0, at which point pipenv will happily install it if you haven't told it that you need <=2.0.0.",
"> If what you really want it to do is install 2.0 or lower, tell it that instead
@AlecBenzer on reflection, it now occurs to me that this is what npm/yarn do by default when you install a package; they find the latest major.minor version and specify `^major.minor.0` in package.json, which prevents unexpected major version upgrades, even when an upgrade-to-latest is explicitly requested. I wonder if Pipenv should do the same - but that would be a separate issue. Of course, their lock file also prevents accidental upgrades of even minor and patch versions, which is what's being requested here.",
"I think it's been discussed above and elsewhere, but there is a tension/tradeoff in the design space between npm/yarn and pipenv. Any package manager ostensibly has these goals, with some relative priority:
- Make it easy to install and upgrade packages
- Make it hard to accidentally break your app with an errant dependency upgrade
The trouble with pinning an exact version in the Pipfile is that it's then harder to upgrade packages; this is why [pip-tools](https://github.com/jazzband/pip-tools/) exists (though it's for requirements.txt).",
"The `--keep-outdated` flag does not seem to be working as intended, as was stated when the issue was re-opened. Whether that behavior should or should not be the default and how it aligns with other package managers is not really the central issue here. Let's fix the thing first.",Bitter Frustration
"@brettdh > on reflection, it now occurs to me that this is what npm/yarn do by default when you install a package; they find the latest major.minor version and specify ^major.minor.0 in package.json, which prevents unexpected major version upgrades, even when an upgrade-to-latest is explicitly requested. I wonder if Pipenv should do the same - but that would be a separate issue.
Yeah that's along the lines of what I was trying to suggest in https://github.com/pypa/pipenv/issues/966#issuecomment-408420493",
"Really excited to hear this is being worked on!
In the mean time, does anyone have a suggested workaround that's less laborious and error-prone than running `pipenv lock` and hand-reverting the resulting lockfile changes that I don't want to apply?",Impatience
@benkuhn Not that I know off - I do the same lock & revert dance all the time.,Bitter Frustration
"Ah ok, you can at least sometimes avoid hand-reverting:
1. `pipenv lock`
2. `git commit -m ""FIXME: revert""`
3. `pipenv install packagename`
4. `git commit -m 'Add dependency on packagename'`
5. `git rebase -i`
6. Drop the `FIXME: revert` commit
Unfortunately it's still possible to create an inconsistent `Pipfile.lock` if your `Pipfile.lock` starts out containing a version of a package that's too old to satisfy the requirements of `packagename`, but perhaps pipenv will complain about this if it happens?",
"`--keep-outdated` seems to *systematically* keep outdated only the explicit dependencies that are specified (unpinned) in Pipfile, while all the implicit dependencies are updated.",Bitter Frustration
"Am I correct that it is not possible to update/install single dependency using `pipenv==2018.7.1` without updating other dependencies? I tried different combinations of `--selective-upgrade` and `--keep-outdated` with no success.
Editing Pipfile.lock manually is no fun...",Bitter Frustration
"Same than @max-arnold , it's my first day using the tool in an existing project, and I have to say **I'm really disappointed**, before I started to use it, I checked the doc site and the video demo, it looked impressive to me, and now this: in real project, work with `pip` or `pipenv` is almost the same, i don't see the point, like many other said in the thread, if I have a lock file, why you are updating my other dependencies if there is no need to update them.
Of course, ### if the update is mandatory, it's OK to update all the necessary dependencies, but just those, not all the outdated instead.
Also the options `--selective-upgrade` and `--keep-outdated` are not clear for what are useful for, there is another issue highlighting this here #1554 , and nobody is able to respond what these options do, incredible.
But my **major disappointing** is why this package was **recommended by the Python official documentation** itself, these recommendations should be more careful conducted, I know this can be a great project in the feature, have a lot of potential, but simple things like this (we are not talking about a bug or a minor feature), make this project not eligible for production environments, but suddenly because it was recommended by the Python docs, everybody are trying to use it, instead of looking for other tools that maybe work better, or just stick with `pip`, that doesn't solve also these issues, but at least it's very minimalist and it's mostly included in any environment (does not add extra dependencies).",Bitter Frustration
"@mrsarm Thank you for your opinion. Sorry things donâ€™t work for you. I donâ€™t understand where the disappointment comes from, however. Nobody is forcing Pipenv on anyone; if it doesnâ€™t work for you, donâ€™t use it. That is how recommendations work.
Your rant also has nothing particularly related to this issue. I understand it requires a little self-control to not dumping trash on people when things donâ€™t go your way, but please show some respect, and refrain from doing so.",Insulting
"@uranusjr it's not trash, it's an opinion, and some times it's not an option, like my case, where somebody chose pipenv to create a project where I started to work now, and I have to deal with this.
But things get worst just now, and what I going to say it's not an opinion, it's a fact.
After trying to add one dependency that just I dismissed to avoid to deal with this issue (because it's a dev dependency, so I created a second environment with `pip` and the old `requirements-dev.txt` approach, just with that tool), I needed to add another dependency.
The new dependency is PyYAML, let say the latest version. If you install it in any new environment with `pip`, you will see that the library does not add any dependency, so only PyYAML is installed, is that simple in these cases with Pip. But adding the dependency with Pipenv (because a project that I didn't create is managed with Pipenv) the same issue happened, despite PyYAML doesn't have any dependency, and it's not previously installed in the project (an older version), `pipenv` updates all my dependencies in the lock file and the virtual environment, but I don't want to update the others dependencies, I just one to add a single new module without any dependency.
So the conclusion (and again an opinion, not a fact like pipenv broke all my dependencies) it's that Pipenv instead of help me to deal with the dependencies management, it turn it into hell.",Bitter Frustration
"I've followed this thread for months, and I think any real project will ultimately stumble upon this issue, because the behavior is unexpected, counter-intuitive, and yes: dangerous.
About a month ago I tried out a more-comprehensive alternative to `pipenv`, [`poetry`](https://github.com/sdispater/poetry); it solved the problems _I_ needed to solve:
1) managing one set of dependencies (setup.py, setup.cfg, pip, and pipfile -> pyproject.toml)
2) future oriented, backwards-compatible (again [pyproject.toml](https://www.python.org/dev/peps/pep-0518/))
3) fairly un-opinionated ([no i'm really not asking to install `redis`](https://github.com/pypa/pipenv/issues/1174))
4) and the solution to the classic Pipenv problem: ""Also, you have to explicitly tell it [pipenv] to not update the locked packages when you install new ones. This should be the default."" [[1](https://github.com/sdispater/poetry#what-about-pipenv)] [[2](https://github.com/pypa/pipenv/issues/966#issuecomment-339117289)]
I weighed sharing these thoughts on the `pipenv` issue, but as @uranusjr said, ""no one is forcing Pipenv on anyone"", and I'm not forcing Poetry. I like it, it works well, and it solves my problems, but I'm just sharing an alternative, more-comprehensive solution to the problem I was having. Just take all that as my 2Â¢.
* as a disclaimer, *I am not* a member of the Poetry team or affiliated with them.
p.s. I think the concern about Pipenv being the ""official"" solution is due to it's [first-class integrations](https://github.com/pypa/pipenv/blob/master/docs/advanced.rst#-community-integrations) â€“Â something that you, @uranusjr, might see it as a simple recommendation â€“ the industry at large is taking it as the ""blessed approach going forward"". Frankly, that recommendation is more authoritative in the community than certain PEPs that have been around for more than a year.",
"Nobody is forcing you to participate in our issue tracker; if you donâ€™t have a productive comment please find a forum that is not for triaging issues.
For users who are interested in trying the alternative resolver @uranusjr and myself have been implementing for several weeks now, please try out https://github.com/sarugaku/passa which will generate compatible lockfiles. Poetry does a lot of different things, but it also has limitations and issues itself, and we have a design philosophy disagreement about scope. This is a project we manage in our spare time; if you want to see something fixed or you have a better approach, we are happy to accept contributions. If you are here to simply tell us we ruined your day and your project, I will ask you only once to see yourself out. We have not forgotten or ignored this issue, we have a full implementation of a fix in the resolver linked above. Have patience, be courteous, or find somewhere else to talk. To those who have been waiting patiently for a fix, please do try the resolver mentioned aboveâ€” we are eager to see if it meets your needs. It implements proper backtracking and resolution and shouldnâ€™t handle this upgrade strategy In the shorter term I think we can get a band aid for this into pipenv if we donâ€™t wind up cutting over first.",Bitter Frustration
"@dfee I am not really sure that blurring lines between applications and libraries is the correct answer to dependency management, so I donâ€™t see poetryâ€™s approach as an advantage. I wasnâ€™t involved in whatever your issue was with the recommendation engine, but we took that out some time ago...",
"@techalchemy
> I am not really sure that blurring lines between applications and libraries is the correct answer to dependency management, so I donâ€™t see poetryâ€™s approach as an advantage.
Why though? I never understood this idea that you should manage the dependencies of a library and an application differently. The only difference between the two is the lock file which is needed for an application to ensure a reproducible environment. Other than that it's the same thing. This is the standard in most other languages and Python seems the exception here for some reason and this is bad from a user experience standpoint since this is making things more complex than they should be.
> it also has limitations and issues itself
Which ones? I am really curious about the issues or limitations you encountered while using Poetry.",Impatience
"My apologies to bean so rude. Now reading my comments I realize that despite the info I provided and some of my options are still valid (IMHO), it's wasn't appropriated the way I wrote what I wanted to say.
I understand that the issue tracker is most a place where to discuss bugs and improvements, and discuss whether this is a bug or an error by design is not clear in the thread, but again my apologies.
I thing there are two strong topics here:
- Should pipenv update all your outdated dependencies where you are trying just to install a new dependency: the ones that are not needed to update because the new package / version we are trying to install can works with the existent dependencies, and even the ones that aren't dependencies of the new package we are trying to install? Maybe this is out of scope of this ticket, but it's a really important topic to discuss.
- Do one of these parameters `--keep-outdated` `--selective-upgrade` allow us to avoid these behaviour? It's not clear what these options do, there is a lack of documentation about them, and even in the related issue (#1554) nobody is answering that.
In case it's a bug in on one of these params `--keep-outdated` `--selective-upgrade`, I still thinking that do not set whatever param solves the unnecessary update of the dependencies as default it's a really bad idea.
To compare with a similar scenario, imagine that you execute `apt-get install vim` to just install the `vim` tool in your system (and the necessary vim's dependencies or updates if apply), but imagine also that in this situation apt updates all the other dependencies of your system: python, the QT system, the Linux kernel... and so on. It's not that apt shouldn't allow us to updates other dependencies, but there is a clear command to do that: `apt-get upgrade`, while `apt-get install PACKAGE` just install / update PACKAGE and it's dependencies.",Bitter Frustration
"@sdispater the distinction is at the heart of every disagreement we've ever had and it's incredibly subtle but I'd point you at https://caremad.io/posts/2013/07/setup-vs-requirement/ or a good article for the elixir use case: http://blog.plataformatec.com.br/2016/07/understanding-deps-and-applications-in-your-mixfile/
`pyproject.toml` isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file. `setup.cfg` exists for that purpose (the actual successor to `setup.py` ) and IMHO both of those should really be supported. A library is published and intended for consumption with abstract dependencies so that they can play nice in the sandbox with others; applications are usually large, complex beasts with sometimes hundreds of direct dependencies. So one of our main divergences is that when we design and build our tooling, we take this into account also
@mrsarm For your first question, the update behavior was intentional (and was discussed extensively at the time, /cc @ncoghlan and related to OWASP security concerns). On the second point, the behavior is currently not properly implemented which is why the issue is still opened, which led us to rewriting the backing resolver behind pipenv, which I mentioned above. It simply didn't support this behavior. `--selective-upgrade` is supposed to selectively upgrade only things that are dependencies of the new package, while `--keep-outdated` would hold back anything that satisfied the dependencies required by a new package. Slightly different, but I am fairly sure neither works correctly right now.",
"> pyproject.toml isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file. setup.cfg exists for that purpose (the actual successor to setup.py ) and IMHO both of those should really be supported.
Well this is certainly off topic but it's an important discussion so I can't help myself.
There is actually no standard around `setup.cfg` right now other than the conventions established by distutils and setuptools. `pyproject.toml` is absolutely for library metadata as the successor to `setup.py` or the community would have placed build requirements in `setup.cfg` instead. `pyproject.toml` describes how to build a project (PEP 518), and part of building is describing metadata. I'm NOT saying that `pyproject.toml` needs a standard location for this metadata, but PEP 518 uses this file to install a build tool and from there it's very reasonable to expect that the build tool will use declarative configuration from somewhere else in the file to determine how to build the project.
Anyway, going back to pipenv vs poetry - there seems to be some idea floating around that applications don't need certain features that libraries get, like entry points, and this is just incorrect. It should be straightforward for an application to be a python package.
The only true difference between an application and a library in my experience with python and with other ecosystems is whether you're using a lockfile or not. Of course there's a third case where you really just want a `requirements.txt` or `Pipfile` and no actual code and that seems to be all that pipenv has focused on so far (`pipenv install -e .` falls into this category as pipenv is still afraid to try and support the package metadata). Unfortunately, while the design of pipenv is cleaner with this approach, it's also way less useful for most applications because PEP 518 decided to punt on how to install projects into editable mode so in order to continue using pipenv we will be stuck on setuptools quite a while longer as you cannot use `pyproject.toml` to switch away from setuptools and still use `pipenv install -e .`.",Bitter Frustration
"> There is actually no standard around setup.cfg right now other than the conventions established by distutils and setuptools. pyproject.toml is absolutely for library metadata as the successor to setup.py or the community would have placed build requirements in setup.cfg instead.
Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly. Not to mention it uses the standard outlined in pep 345 for metadata, among others, and can also be used to specify build requirements.
> the community would have placed build requirements in setup.cfg instead.
Do you mean the pep authors? You can ask them why they made their decision, they outline it all in the pep.
> pyproject.toml describes how to build a project (PEP 518), and part of building is describing metadata. I'm NOT saying that pyproject.toml needs a standard location for this metadata, but PEP 518 uses this file to install a build tool and from there it's very reasonable to expect that the build tool will use declarative configuration from somewhere else in the file to determine how to build the project.
This came up on the mailing list recently -- nothing anywhere has declared a standard around `pyproject.toml` other than that it will be used to declare build system requirements. Anything else is an assumption; you can call that ""library definition metadata"", but it isn't. Try only defining a build system with no additional information about your project (i.e. no pep-345 compliant metadata) and upload it to pypi and let me know how that goes.
> Anyway, going back to pipenv vs poetry - there seems to be some idea floating around that applications don't need certain features that libraries get, like entry points, and this is just incorrect. It should be straightforward for an application to be a python package.
Who is saying that applications don't require entry points? Pipenv has an entire construct to handle this.
> so in order to continue using pipenv we will be stuck on setuptools quite a while longer as you cannot use pyproject.toml to switch away from setuptools and still use pipenv install -e .
Not following here... we are not going to leave pip vendored at version 10 forever, I've literally been describing our new resolver, and the actual installer just falls back to pip directly... how does this prevent people from using editable installs?",Impatience
"> This came up on the mailing list recently -- nothing anywhere has declared a standard around `pyproject.toml`
That's correct, it is not a ""standard"", yet in that same thread recognise that by calling it `pyproject.toml` they likely asked for people to use this file for other project related settings/config.
So by the same logic you invoked here:
> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly.
`pyproject.toml` is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.",
"> Not following here... we are not going to leave pip vendored at version 10 forever, I've literally been describing our new resolver, and the actual installer just falls back to pip directly... how does this prevent people from using editable installs?
PEP 517 punted on editable installs... which means there is no standard way to install a project in editable mode if you are not using setup tools (which has a concept known as develop mode which installs the project in editable mode).",Bitter Frustration
"> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly. Not to mention it uses the standard outlined in pep 345 for metadata, among others, and can also be used to specify build requirements.
Yes, the build system is expected to output the PKG-INFO file described in PEP 345. This is a transfer format that goes in an sdist or wheel and is generated from a setup.py/setup.cfg, it is not a replacement as such for the user-facing metadata. PEP 518's usage of `pyproject.toml` is about supporting alternatives to distutils/setuptools as a build system, no one is trying to replace the sdist/wheel formats right now. Those replacement build systems need a place to put their metadata and fortunately PEP 517 reserved the `tool.` namespace for these systems to do so. It's not an assumption - **both flit and poetry have adopted this namespace for ""library definition metadata"".**
> Try only defining a build system with no additional information about your project (i.e. no pep-345 compliant metadata) and upload it to pypi and let me know how that goes.
How constructive.
> Who is saying that applications don't require entry points? Pipenv has an entire construct to handle this.
Where is this construct? I cannot even find the word ""entry"" on any page of the pipenv documentation at https://pipenv.readthedocs.io/en/latest/ so ""an entire construct"" sounds pretty far fetched? If you mean editable installs then we have reached the point I was making above - with pipenv deciding to couple itself to `pipenv install -e .` as the only way to hook into and develop an application as a package, for the foreseeable future pipenv's support here is coupled to setuptools. I think the entire controversy boils down to this point really and people (certainly me) are frustrated that we can now define libraries that don't use setuptools but can't develop on them with pipenv. To be perfectly clear this isn't strictly pipenv's fault (PEP 518 decided to punt on editable installs), but its refusal to acknowledge the issue has been frustrating in the discourse as poetry provides an alternative that does handle this issue in a way that's compliant with the `pyproject.toml` format. Pipenv keeps saying that poetry makes bad decisions but does not actually attempt to provide a path forward.",Bitter Frustration
"https://pipenv.readthedocs.io/en/latest/advanced/#custom-script-shortcuts
Please read the documentation.",Impatience
"@bertjwregeer:
> pyproject.toml is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.
Great, and we are happy to accommodate sdists and wheels built using this system and until there is a standard for editable installs we will continue to pursue using pip to build sdists and wheels and handle dependency resolution that way. Please read my responses in full. The authors and maintainers of pip, of the peps in question, and myself and @uranusjr are pretty well versed on the differences between editable installs and the implications of building them under the constraints of pep 517 and 518. So far All I'm seeing is that the peps in question didn't specifically address how to build them because they leave it up to the tooling, which for some reason everyone thinks means pipenv will never be able to use anything but setuptools? I've said already this is not correct. If you are actually interested in the implementation and having a productive conversation I'm happy to have that. If you are simply here to say that we don't know what we're doing, but not interested in first learning what it is we are doing, this is your only warning. We are volunteers with limited time and I am practicing a 0 tolerance policy for toxic engagements. I do not pretend my work is perfect and I don't pretend that pipenv is perfect. I will be happy to contribute my time and effort to these kinds of discussions; in exchange I ask that they be kept respectful, that they stick to facts, and that those who participate also be willing to learn, listen, and hear me out. If you are here just to soapbox you will have to find another platform; this is an issue tracker. I will moderate it as necessary.
This discussion is **wildly off topic**. If anyone has something constructive to say about the issue at hand, please feel free to continue that discussion. If anyone has issues or questions about our build system implementations, please open a new issue. If you have issues with our documentation, we accept many pull requests around documentation and we are aware it needs work. Please defer **all** of that discussion to new issues for those topics. And please note: the same rules will still apply -- this is not a soapbox, it is an issue tracker.",Bitter Frustration
"> https://pipenv.readthedocs.io/en/latest/advanced/#custom-script-shortcuts
> Please read the documentation.
Entry points are a more general concept than just console scripts and this link is completely erroneous in addressing those concerns. `<soapbox>`Ban away - you're not the only maintainer of large open source projects on here and none of my comments have been a personal attack on you or the project. People commenting here are doing so because they want to use pipenv and appreciate a lot of what it does. My comment was not the first off topic post on this thread, yet is the only one marked. Your snarky comments indicating that you think I don't know what I'm talking about are embarrassing and toxic.",Bitter Frustration
"In the project we maintain, we can soapbox. And yes, pip will support all compliant build systems which you both yourselves seem to full well understand will produce consumable metadata, and as pipenv uses pip as the backing tool to drive its installation process, as I described, yes, pipenv will support all compliant tooling. I already said this. So yeah, please take your toxicity somewhere else. Your attitude is not welcome here. Final warning. Persistent attempts to incite conflict wonâ€™t be tolerated.",Bitter Frustration
"There is too much to parse here and I would just be echoing the sentiment that:
> There's a little thing to take into account here: Changing a single dependency could change the overall set of requirements.
Ex: Updating foo from 1.0 to 2.0 could require to update bar to >=2.0 (while it was <2.0 before), and so on.
or that
> upgrade a dependency that has cascading requirements obviously it will have consequences for other dependencies
In fact -- we have pretty great resolution of dependencies now that will update within the constraints you specify in your `Pipfile` to what is the possible allowed latest versions where all specifiers and constraints are met, or otherwise it will error out and let you know that you cannot do that. There is also the --skip-lock flag that might be helpful for some. Anyway I am closing this one down for the history books and If there is anything relevant in this thread that is not addressed or needs a new issue -- feel free to open one, but keep in mind that the current behavior is correct with respect to dependency resolution and keeping the constraints of the `Pipfile` all happy and satisfied.",
"How I read PEP-8, it doesn't outlaw bare excepts, but merely recommends to catch more specific exceptions when possible. But what if there is no particular exception you want to catch, but when you just want to do some cleanup before propagating any exception?
```python
try:
self.connection.send(...)
except:
# We only close the connection on failure, otherwise we keep reusing it.
self.connection.close()
raise
```
Using `try...finally` isn't an option here, since we want to reuse the resource on success, and only clean up on failure. I could just explicitly catch `BaseException` instead, but there is no indication in PEP-8 that this is preferable (otherwise why would bare except be supported in the first place).
So how about suppressing E722 if there is a `raise` statement in the `except` block?",
"Using bare except for resource closing is a common behavior, especially when I implement context manager. The check of `do not use bare except` should be changed to `do not use bare except without raise`.",
The error code E722 was implemented in #592 / (#579) and I warned about it but sigmavirus24 wasn't interested and told that I did not read the whole thread....,Bitter Frustration
"To be clear, pycodestyle doesn't do look-ahead's so we cannot silence this *if* there is a `raise` in the following block. That's just not how pycodestyle has ever worked and it would require a significant rewrite to enable that kind of behaviour.
----
Quoting the PEP
```
When catching exceptions, mention specific exceptions whenever possible instead of using a bare except: clause.
For example, use:
try:
import platform_specific_module
except ImportError:
platform_specific_module = None
A bare except: clause will catch SystemExit and KeyboardInterrupt exceptions, making it harder to interrupt a program with Control-C, and can disguise other problems. If you want to catch all exceptions that signal program errors, use except Exception: (bare except is equivalent to except BaseException:).
A good rule of thumb is to limit use of bare 'except' clauses to two cases:
1. If the exception handler will be printing out or logging the traceback; at least the user will be aware that an error has occurred.
2. If the code needs to do some cleanup work, but then lets the exception propagate upwards with raise. try...finally can be a better way to handle this case.
```
As a *general* rule, this new check is good. There are specific cases where people will need to do general clean-up work, as described above. Since we cannot check for people re-raising the exception in the except block, it truly is up to the user to determine whether:
1. This check is useful to them at all or whether they feel it should be ignored globally
2. Their particular use of a bare `except` is necessary and it should be ignored in that particular case (e.g., with `# noqa` or `# noqa: E722`).
I would hazard a guess that 90% of pycodestyle's users will find this check worthwhile, useful, and helpful. We can not ever satisfy 100% of our users so I am happy to settle for 90%.",
"I have two (real world) scenarios:
1. Legacy code, initially written by novices, with inappropriate use of bare except all over the place. E722 is helpful there (somewhat since this usually isn't the only issue in such legacy code, and you end up ignoring most errors there anyway).
2. Code written by me (and similarly experienced Python developers), where bare except is used occasionally, but never without re-raising the exception. For reference, E722 is reported inappropriately three times in [this project](https://github.com/snoack/mypass) of mine, where the exception is re-raised.
If the design of pycodestyle doesn't allow considering the following block, in order to avoid inappropriate errors, my understanding is, that due to the potential of false positives this check should not be enabled by default, or such a check should rather be implemented in pyflakes which considers the AST and therefore is able to ignore bare excepts that re-raise the exception.
I'm not going to clutter my code with `#noqa` comments. IMO the purpose of a linter is to help you writing cleaner code, not to require additional boilerplate for legit practices. This isn't any better than using `except BaseException`.",Bitter Frustration
"> I'm not going to clutter my code with #noqa comments. I'm not sure if you're intentionally ignoring my suggestion to include it in the `ignore` list for your projects or if you just want to try to argue. Either way, I'm not here to argue with you. Pycodestyle is a tool used by novices and experts alike. Novices will learn from this and so will some experts. Since this is a style tool, there will always be places where some people disagree with the checks and will disable them. That's *normal*. Just because a handful of folks object to a rule doesn't mean we will put it in the `DEFAULT_IGNORE` list even if generally speaking it has value for everyone else.
As for the suggestion that pyflakes entertain a *style* check, you can make that argument to them, but that is typically entirely against their philosophy.",Bitter Frustration
"I just hit this as well. In my experience false positives can have a damaging effect. I've seen novices commit naive ""fixes"" for correct code. I believe this issue will cause novices to rewrite what I wrote
```python
try:
...
except:
destfile_tmp.unlink()
raise
```
to
```python
try:
...
except Exception:
destfile_tmp.unlink()
raise
```
which will of course leave temporary files around in the case of KeyboardInterrupt/SystemExit.
This check needs to see the raise, or be better worded, to steer people away from naive fixes.",Bitter Frustration
"That's kind of a bad example though because you should be using `finally` to clean up resources. It could lead to novices getting a ""wrong"" idea, I agree, but they're already doing it wrong in the first place.",Bitter Frustration
"No, as in @snoack's example, this is not an unconditional clean-up. In the happy path the file gets moved into a permanent location.",
"![image](https://user-images.githubusercontent.com/13496612/36892494-2c2e0a5c-1e05-11e8-94c0-8daecf633955.png)
autopep8 said use `BaseException`.",Impatience
"I'm with @sigmavirus24 on this one, but I'd take it a little further: a bare except is inherently unpythonic. Two important parts of python philosophy (`import this`) are 'Explicit is better than implicit' and 'Special cases aren't special enough to break the rules.'. I point these out because a bare except is a special, highly implicit case. Every other `except` has an 'argument', indicating the scope of the error to be handles. Even if that's 'everything', it's better to explicitly state that.
'There should be one-- and preferably only one --obvious way to do it.' also applies, since you can accomplish the same thing for the low-low price of 14 characters. That's simply not enough of a 'savings' to make it worth it.
There's also the fact that things like KeyboardInterrupt aren't caught by `Exception` for a reason. Those represent *very* exceptional cases - cases which shouldn't occur in normal (e.g. production) operation unless something has gone very wrong or the user is explicitly requesting an immediate shutdown.
In regards to @lordmauve's case: Why do you want to clean up those temporary files in every possible exception case? `except:` tells me that you don't know what exceptions may be raised. In those (hopefully rare) cases, your app will certainly crash (unless you have a bare except that silences every possible exception, but that's a whole new level of bad design), and you will probably need to investigate why. A temporary file is part of the state of the application at the point in time, and therefore will be forensically valuable.
If you know for sure that the temporary file will be forensically worthless, then `except Exception:` or `except BaseException:` makes that clear, and indicates that you're fully aware of the implications. If you're more worried about old temporary files accumulating in some way, consider having your app do cleanup as part of initialization. That idea comes from the [Crash-only software](https://lwn.net/Articles/191059/) pattern (It's a lot more reasonable than it sounds, I promise). Basically, it's a lot easier to assume that your app always terminates unexpectedly and code against that than to handle unexpected termination as a special case (i.e. Turning off the power is guaranteed to be an option, but a shut down command may fail or be unavailable).
As a last resort, there's always ` # noqa`. If you absolutely must break from best practices, then 8 characters is a very small price to pay.
edit: I r gud riter ðŸ˜œ",
"@hoylemd I appreciate the support.
> Those represent very exceptional cases - cases which shouldn't occur in normal (e.g. production) operation unless something has gone very wrong or the user is explicitly requesting an immediate shutdown.
This sounds like you're assuming all development is on continuously running applications on a remote server. Things like pycodestyle and flake8 are ""production"" applications that should handle `KeyboardInterrupt`. That said, both projects handle it explicitly. I think I understand your point, but it feels like it's imposing a false dichotomy around what is ""production"".
> Why do you want to clean up those temporary files in every possible exception case?
I can imagine a few cases where @lordmauve would want to (and should!) clean up the temporary files. Perhaps those temporary files contain some sensitive information and cleaning them up is the secure thing to do. In reality, neither of us know the details and I think we should be assuming that they know their constraints better than us. Of course having some non-sensitive temporary files lying around can be useful for debugging, but there are certainly valid cases where those shouldn't be left around no matter what exception happens.",
"@sigmavirus24 I think you're right re: imposing a false dichotomy - I'm primarily a web developer, where the dichotomy is pretty reliable.
In regards to temporary files containing sensititive information, Wouldn't it be a risk to write them to disk at all? If the power is shut off at the right moment, the data would still be there on disk, and no exception handling would have a chance to clean it up. So that makes me think that a temporary file is the wrong tool in the first place. I'd want to keep that in something volatile (like memory) so that it's sort of fail-safe.
You're more general point is a good one. - We can't know the real requirements on a project we're not a part of. But if those requirements do require a cleanup in all exception cases, I think we probably agree that it should be done explicitly.",
"> I'd want to keep that in something volatile (like memory) so that it's sort of fail-safe.
Again, there are constraints where it makes sense. I agree with you in general, but there are always exceptions to the rule.
Also we're getting off topic :smile:",
"`except BaseException` doesn't mean the same thing as `except:` in Python 2; in Python 2 you could raise old-style classes as exceptions, which do not inherit from BaseException.
Regarding ""I could never imagine a situation where I'd want to unconditionally clean up in the case of any exception"":
- In the case of temporary files, never leave them around on a production server because they can cause additional impact (eg. filling a disk volume) which can be much more serious than the original crash. Reproduce the issue in dev and diagnose.
- Just came across another example: in the case that my event loop crashes due to programming error, I must kill my subprocesses which are now in an unknown state ([code](https://github.com/lordmauve/chopsticks/blob/master/chopsticks/tunnel.py#L130)).",Bitter Frustration
"@lordmauve I'll admit that there are some cases where `except:` is the right call, but they're cases that pycodestyle isn't intended to handle out of the box. Marking those with `# noqa` might seem a little cluttery, but it's much more helpful to those same junior devs who would be confused by the error. It explicitly indicates a code smell that is a necessary evil. Ideally, you'd document why it is necessary as well so that they understand why the exception had to be made. So to go back to your initial comment, Pycodestyle isn't designed in a way that it can detect the raise, so that's a non-starter. As for steering novices away from naive changes, that's the purpose of code review, not linting. A better solution is to prevent them from even knowing about it in the first place though, hence `# noqa` and documentation.",
"> Pycodestyle isn't designed in a way that it can detect the raise, so that's a non-starter
Or evidence that this check needs to be moved to Pyflakes, which does consider the AST, in order to eliminate this false positive.
> As for steering novices away from naive changes, that's the purpose of code review, not linting. In a large organisation that is impossible. Junior people are often two or three steps removed from people who could confidently tell you what good practice looks like. We rely on linters to do this, and both the false positive and false negative rates are important. I'm not saying I'm not worried about the false negative. I'm arguing that this creates an undesirable false positive.
> A better solution is to prevent [novices] from even knowing about it in the first place though [by putting #noqa on that line]
You're assuming I'm writing code now for future novices to read. I'm concerned about the novices now maintaining the code I wrote 5 years ago when this check wasn't a thing.",Bitter Frustration
"> Or evidence that this check needs to be moved to Pyflakes, which does consider the AST, in order to eliminate this false positive.
PyFlakes wouldn't accept this for the very reason that it's so controversial. They only accept obvious problems, e.g., unused imports. If this isn't satisfactory here, a different stylistic option for having it would be via a Flake8 AST plugin. Luckily, `flake8-bugbear` already provides this check using the AST but a quick check of the source indicates that it's as strict as this check is. I'd suggest either collaborating with [bugbear](/pycqa/flake8-bugbear) or creating a new plugin that meets the specific needs.
As it stands, this discussion seems to be getting heated. I'm going to lock the thread because it's devolved from constructive conversation to far less productive conversation.",
"Trying to perform `brew bundle dump` and getting the following error: > sh: line 1: 18965 Segmentation fault: 11 mas list 2> /dev/null
Independently running `mas list` correctly outputs installed MAS apps.",Bitter Frustration
I can't reproduce this locally I'm afraid. Try `brew reinstall mas`.,
"Aww snap. Yea that fixed it. It looks like it was the High Sierra installer causing the issue as it doesn't have an ID. After updating mas it now correctly lists the installer (just minus an ID). Actually, this might be a different issue. Look at the line it adds to the Brewfile in this case:
`mas ""macOS High Sierra"", id: Install`
When I do `mas list`, this is how it appears within some other items:
```
595191960 CopyClip (1.9)
Install macOS High Sierra (13105)
803453959 Slack (2.9.0)
```",
@jamesdh Ok. Would you consider submitting a PR to address that case?,
@MikeMcQuaid will do,
"Decided not to bother because A) the OSX installers aren't typically something you'd want to keep around B) am unable to find anything else that recreates this issue. Either way, thanks for the response @MikeMcQuaid!",
"@jamesdh Cool, thanks for letting us know!",
"For clarity in posterity, see mas-cli/mas#82 regarding the seg fault.",
@paulp I cannot reproduce this locally with the installer installed. We will accept a pull request to resolve this (likely just not including anything in a dump missing an ID).,
"Re #321, it's a different bug - as I said, ""the bug in bundle being masked by the bug in mas"" - regardless of your agreement, it's childish and hostile to immediately close and lock the ticket. Sorry to have bothered you with my effort.",Bitter Frustration
"@paulp Calling someone who maintains software you use ""childish and hostile"" because they don't run their issue tracker in the way that you would like (not keeping issues open that we don't plan to work on) is rude. I've invested significantly more effort into this project (and Homebrew itself) over a long period of time than you or, at this point, frankly anyone. Would you consider apologising?",Bitter Frustration
"@paulp I'll take that as a no, then.",Impatience
Anyone else who hits this: we'll consider accepting a PR for fixing the case where `mas` does not output IDs.,Impatience
"##### ISSUE TYPE
- Feature Idea
##### COMPONENT NAME
import_playbook
##### ANSIBLE VERSION
<!--- Paste verbatim output from ""ansible --version"" between quotes below -->
```
2.4.0.0
```
##### CONFIGURATION
<!---
If using Ansible 2.4 or above, paste the results of ""ansible-config dump --only-changed""
Otherwise, mention any settings you have changed/added/removed in ansible.cfg
(or using the ANSIBLE_* environment variables).
-->
n/a
##### OS / ENVIRONMENT
CentOS 6
##### SUMMARY
I need to conditionally import a playbook, which isn't possible. The keyword I'm missing is ""include_playbook"" which would allow a ""when"" to apply to it. Why?
I have a playbook that performs some maintenance, OS, Kernel, package, and firmware upgrades. For Major upgrades, we track progress in a ticketing system. So, during those runs, I'd like to conditionally import a playbook that updates the ticket information. In order for that to work, I need to pass the user's password to the ticketing system, so there's a `vars_prompt` in the ticket update playbook. If the user specifies which ticket they're working to the maintenance playbook, I'd like the ticket update playbook to be called after the maintenance is performed.
If there's another way for this to work, I'm open to alternate ideas, but I think conditionally playbook imports would be generically useful. I'm not understanding why tasks could be included dynamically, but a playbook wouldn't be.
##### STEPS TO REPRODUCE
<!---
For bugs, show exactly how to reproduce the problem, using a minimal test-case.
For new features, show how the feature would be used.
-->
<!--- Paste example playbooks or commands between quotes below -->
```yaml
# Maintenance Play Runs first, then conditionally import a second playbook
- import_playbook: update-ticket.yaml
when: ticket_id is defined
```
<!--- You can also paste gist.github.com links for larger files -->
##### EXPECTED RESULTS
<!--- What did you expect to happen when running the steps above? -->
I expect `update-ticket.yaml` to only import if the `ticket_id` is defined.
##### ACTUAL RESULTS
<!--- What actually happened? If possible run with extra verbosity (-vvvv) -->
no parse error, and playbook `update-ticket.yaml` is imported 100% of the time.",Bitter Frustration
"Files identified in the description:
* [lib/ansible/modules/utilities/logic/import_playbook.py](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/utilities/logic/import_playbook.py)
If these files are inaccurate, please update the `component name` section of the description or use the `!component` bot command.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",
"We have similar need. We have a main playbook that prepares the system, but we allow the user to provide some extra steps by creating a playbook files in a well known directories. We do not know what files will be present there in advance. For this we would really like to have include_playbook that supports with_fileglob (or with_items).",
"Just to explain why include_tasks is not enough. The system is clustered and the tasks talk to localhost, other physical hosts and virtual machines. We could in theory use delegate_to if it supported host groups.",
"I also have this need in order to import different playbooks to configure Vagrant guests differently depending on the active hypervisor.
Alternate to `when` is something like (this is also not supported):
```YAML
- import_playbook: ""install_{{ 'virtualbox' if ansible_product_name == 'VirtualBox' else 'vmware' if ansible_product_name = 'VMware Virtual Platform' else 'noop' }}_extensions.yml""
```",
"Is this going to be fixed soon? I find it as a serious source of problems because the optional playbook code ca be huge when it comes to number of tasks, causing over **extensive console/log verbosity** of tasks that are never supposed to be loaded.
The `skip_reason"": ""Conditional result was False""` is not of much help either because the user will not see any condition on those tasks, the condition being few nested includes/imports away in another file.
It helps nobody that Ansible will list hundreds of lines of files that were never supposed to be run, making much harder to investigate them.",Bitter Frustration
"As a workaround until this is possible, if the use case is to support different host groups it is possible to use an include_task with a conditional on group names, i.e. a task include for a group `foo` is included when a file `tasks_directory/foo.yml` is provided:
```
- include_tasks: ""{{item}}""
with_fileglob: ""tasks_directory/*.yml""
vars:
file_host_group: ""{{ (item | basename | splitext)[0]}}""
when: ""file_host_group in group_names""
```
The playbook including this construct has to run for all hosts.",
"Is this still being worked at all? I'm kind of implementing something like an `ansible-galaxy` style method of ""installing"" playbooks into a playbooks sub-directory and then I want to `include_playbook` a playbook that was just ""installed"". Here is what I've got so far:
```yaml
- name: PLAY | Install other required playbooks
hosts: localhost
connection: local
tasks:
- name: INCLUDE_VARS | include variables to discover other needed playbooks
include_vars:
dir: playbooks/
files_matching: requirements.yml
depth: 1
- name: GIT | Clone playbooks
git:
repo: ""{{ item.src }}""
dest: ""playbooks/{{ item.src.split('/')[-1] }}""
version: ""{{ item.version }}""
loop: ""{{ elk_required_playbooks }}""
- name: SHELL | Install included playbooks roles
shell: ansible-galaxy install -r roles/requirements.yml -p roles/
args:
chdir: ""playbooks/{{ item.src.split('/')[-1] }}""
loop: ""{{ elk_required_playbooks }}""
- name: PLAY | Run the installed helloWorld playbook
import_playbook: ""playbooks/ap_hello_world/helloWorld.yml""
```
If I run this as-is, I get an import error because the playbook to be imported isn't there yet.
```bash
ERROR! Unable to retrieve file contents
Could not find or access '/path/to/playbooks/ap_hello_world/helloWorld.yml'
```
If I comment out the `import_playbook` play, the functionality above that works nearly like `ansible-galaxy` and ""installs"" the playbooks I need (and the roles they need). ```bash
PLAY [PLAY | Install other required playbooks] *************************************************************************************************************************************
TASK [Gathering Facts] *************************************************************************************************************************************************************
Friday 31 August 2018 13:54:13 -0500 (0:00:00.239) 0:00:00.240 ********* Friday 31 August 2018 13:54:13 -0500 (0:00:00.237) 0:00:00.237 ********* ok: [localhost]
TASK [INCLUDE_VARS | include variables to discover other needed playbooks] *********************************************************************************************************
Friday 31 August 2018 13:54:15 -0500 (0:00:01.366) 0:00:01.606 ********* Friday 31 August 2018 13:54:15 -0500 (0:00:01.366) 0:00:01.604 ********* ok: [localhost]
TASK [GIT | Clone playbooks] *******************************************************************************************************************************************************
Friday 31 August 2018 13:54:15 -0500 (0:00:00.124) 0:00:01.730 ********* Friday 31 August 2018 13:54:15 -0500 (0:00:00.124) 0:00:01.728 ********* changed: [localhost] => (item={u'src': u'<gir_url>/ap_hello_world', u'version': u'v0.3.0'})
TASK [SHELL | Install included playbooks roles] ************************************************************************************************************************************
Friday 31 August 2018 13:54:17 -0500 (0:00:02.591) 0:00:04.322 ********* Friday 31 August 2018 13:54:17 -0500 (0:00:02.591) 0:00:04.319 ********* changed: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})
PLAY RECAP *************************************************************************************************************************************************************************
localhost : ok=4 changed=2 unreachable=0 failed=0 ```
And now I can run the same playbook again, this time with the `import_playbook` play not commented and it works as I desire:
```bash
PLAY [PLAY | Install other required playbooks] *************************************************************************************************************************************
TASK [Gathering Facts] *************************************************************************************************************************************************************
Friday 31 August 2018 14:00:30 -0500 (0:00:00.244) 0:00:00.244 ********* Friday 31 August 2018 14:00:30 -0500 (0:00:00.241) 0:00:00.241 ********* ok: [localhost]
TASK [INCLUDE_VARS | include variables to discover other needed playbooks] *********************************************************************************************************
Friday 31 August 2018 14:00:32 -0500 (0:00:01.471) 0:00:01.716 ********* Friday 31 August 2018 14:00:32 -0500 (0:00:01.471) 0:00:01.713 ********* ok: [localhost]
TASK [GIT | Clone playbooks] *******************************************************************************************************************************************************
Friday 31 August 2018 14:00:32 -0500 (0:00:00.138) 0:00:01.854 ********* Friday 31 August 2018 14:00:32 -0500 (0:00:00.138) 0:00:01.852 ********* ok: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})
TASK [SHELL | Install included playbooks roles] ************************************************************************************************************************************
Friday 31 August 2018 14:00:35 -0500 (0:00:02.821) 0:00:04.675 ********* Friday 31 August 2018 14:00:35 -0500 (0:00:02.821) 0:00:04.673 ********* changed: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})
PLAY [PLAY | BEGIN Setup & Timing] *************************************************************************************************************************************************
TASK [set_fact] ********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:36 -0500 (0:00:01.323) 0:00:05.999 ********* Friday 31 August 2018 14:00:36 -0500 (0:00:01.323) 0:00:05.997 ********* ok: [localhost]
TASK [debug] ***********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:36 -0500 (0:00:00.155) 0:00:06.154 ********* Friday 31 August 2018 14:00:36 -0500 (0:00:00.155) 0:00:06.152 ********* ok: [localhost] => {
""msg"": ""Start Time - 2018-08-31 14:00:36""
}
PLAY [PLAY | Say Hello to My Little Friend] ****************************************************************************************************************************************
TASK [SHELL | echo something] ******************************************************************************************************************************************************
Friday 31 August 2018 14:00:36 -0500 (0:00:00.096) 0:00:06.251 ********* Friday 31 August 2018 14:00:36 -0500 (0:00:00.096) 0:00:06.249 ********* ok: [knebawils001]
TASK [DEBUG | debug host's standard output] ****************************************************************************************************************************************
Friday 31 August 2018 14:00:37 -0500 (0:00:00.714) 0:00:06.965 ********* Friday 31 August 2018 14:00:37 -0500 (0:00:00.714) 0:00:06.963 ********* skipping: [knebawils001]
PLAY [PLAY | Say Hello via an Ansible Role] ****************************************************************************************************************************************
TASK [ar_hello_world : SHELL | echo role's message on host] ************************************************************************************************************************
Friday 31 August 2018 14:00:37 -0500 (0:00:00.151) 0:00:07.117 ********* Friday 31 August 2018 14:00:37 -0500 (0:00:00.151) 0:00:07.115 ********* ok: [knebawils001]
TASK [ar_hello_world : DEBUG | debug host shell standard output] *******************************************************************************************************************
Friday 31 August 2018 14:00:37 -0500 (0:00:00.342) 0:00:07.459 ********* Friday 31 August 2018 14:00:37 -0500 (0:00:00.342) 0:00:07.457 ********* skipping: [knebawils001]
PLAY [PLAYBOOK | END Setup & Timing] ***********************************************************************************************************************************************
TASK [set_fact] ********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:38 -0500 (0:00:00.212) 0:00:07.672 ********* Friday 31 August 2018 14:00:38 -0500 (0:00:00.212) 0:00:07.669 ********* ok: [localhost]
TASK [debug] ***********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:38 -0500 (0:00:00.179) 0:00:07.851 ********* Friday 31 August 2018 14:00:38 -0500 (0:00:00.178) 0:00:07.848 ********* ok: [localhost] => {
""msg"": ""Start Time - 2018-08-31 14:00:36, End Time - 2018-08-31 14:00:38, Elapsed Time - 0:00:02""
}
PLAY RECAP *************************************************************************************************************************************************************************
knebawils001 : ok=2 changed=0 unreachable=0 failed=0 localhost : ok=8 changed=1 unreachable=0 failed=0
```
I suppose this could be split up into two (2) separate playbooks in the same Git repo. The first would be called `prepare.yml` or maybe `prerequisites.yml` to ""install"" the other needed playbooks and the second main playbook (`playbook.yml`) will do the necessary imports, etc. I was really wanting to make this a ""one-shot"" playbook.",Impatience
"Include conditionals are very useful for creating branches in our playbooks. Will it be ensured that import_playbook will support conditionals in the next release citing this issue?
If not, you are losing a lot of power and will end up creating a lot of hacks. Not to mention breaking a ton of include playbook conditionals in end user plays.",Bitter Frustration
"+1 on implementing a ""when"" conditional.",
soon-ish we approaching 1 y since this request was open and no progress so far ... any chance this get some attention @bcoca ? much thanks !,Impatience
I would like to vote a '+1'as well.,
1,
"+1 as well, would love to see this feature",
Please use the ðŸ‘ button to let the maintainers know you need this feature. Getting a ton of emails from these +1s.,Bitter Frustration
1,
"If someone is interested how to use play-level variables for conditional playbook-import:
1. Set up that variable as a fact in the play
2. Use `when` with `import_playbook` to check this variable (with full path, `hostvars.hostname.a_variable`)
If someone is interested, I managed to make import_playbook be conditional on `--limit` in the command line:
https://medium.com/opsops/import-playbook-with-play-level-condition-775122fe78ff
An example:
```
- hosts: all,localhost
gather_facts: no
run_once: True
tasks:
- set_fact:
full_run: '{{ play_hosts == groups.all }}'
delegate_to: localhost
delegate_facts: yes
- import_playbook: test.yaml
when: hostvars.localhost.full_run
```",
is this issue/request still up to date in a more current version of ansible? Iam using ansible 2.6.1 and the when condition doesn't seem to work when I use the import_playbook function.,
"@brotaxt You need to initialize variables before doing `when`. Just add some random task to random host (before doing first 'import_playbook'). F.e., do set_fact on localhost, as in example above.",
"@amarao many thanks for the quick response. :+1: Unfortunately it doesn't seem to work for me. Even if answer the prompt with ""no"" the playbook ""vmware_createsnap.yml"" gets invoked. The other tasks are working as expected. What I am doing wrong? My playbook: ```
---
-
hosts: all
gather_facts: true
vars_prompt:
- name: ""snapshots_required""
prompt: ""Do you want to automatically create VMWare Snapshots? [yes/no]""
private: no
name: ""Install all available Updates""
tasks:
- name: Check for Updates
include: checkforupdates.yml
- name: setting fact for hosts which have outstanding updates
set_fact:
updates_available: ""yes""
when: ""yumoutput.changed or zypperoutput.changed""
- name: setting fact for hosts which have no outstanding updates
set_fact:
updates_available: ""false""
when: updates_available is not defined
- import_playbook: vmware_createsnap.yml
when: snapshots_required = ""yes""
```",Impatience
"@brotaxt this is a feature request, you currently CANNOT conditionally import playbooks, the conditions above happen to skip all the tasks in one, but this is not a supported behaviour and not guaranteed to work across versions of Ansible.",
"just to be clear:
import or include? because I think the current implementation or naming is actually wrong, based on the definition in https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_includes.html
Does import_playbook actually ""lazy load"" or does it get loaded & parsed with the yaml file?",Impatience
"@Kriechi neither, it gets loaded at 'playbook compile time' which is before execution but not on file load
there is no include_playbook, that is the whole purpose of this feature request, to add one",
"@bcoca mhm that sounds even more wrong - or am I missing the big picture here?
I would have expected that `import_task` and `include_task` have an `*_playbook` sibling...",Bitter Frustration
"@Kriechi the engine never supported that, why `include:` was very misleading and we had to separate it into the different include_X/import_X options and make each behaviour explicit. So include_X is dynamic aka runtime, while import_X is 'static' aka 'compile time'.",Bitter Frustration
"ok - so `include_playbook` would be a feature request? Or can we track it here?
E.g., I'm running a git-checkout task on localhost, and then want to `include_playbook: some/repo/foo.yml`
This should include (lazy-load) the updated playbook from that repository, AFTER pulling the latest commit from the remote. Currently, `import_playbook` imports the ""old"" playbook, then pulls, and then runs the outdated playbook.",
"@Kriechi ... please read the subject of this ticket, that is EXACTLY what we are tracking here",Impatience
"true - the part the confused me is ""conditionally import..."".
`import_playbook` and `include_playbook` are the feature we want.",
"I'm trying to do what [watsonb](https://github.com/ansible/ansible/issues/34281#issuecomment-417762220) was, using ansible-galaxy to install roles, and then using the roles. I found a decent workaround for the all-in-one playbook, which was to place the playbooks, in the order that you want them to execute, on the command line. In Watsonb's case, that would look like
ansible-playbook ... prepare.yml playbook.yml
Any variables that you set on the command line are passed to the playbooks, sequentially.
This might also solve [MarSik's](https://github.com/ansible/ansible/issues/34281#issuecomment-361277446) problem, also, using file globbing on the command line instead of in the playbook.",
"@decet It could, but that would basically mean using a top level bash script as the entrypoint and spliting the main ansible playbook into multiple stage files. Not too horrible, just ugly.",Bitter Frustration
"I've overcome this in my own way as follows. First, my typical playbook directory structure:
```bash
.
â”œâ”€â”€ .ansible-lint
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .yamllint
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ callback_plugins
â”‚Â Â  â”œâ”€â”€ junit.py
â”‚Â Â  â”œâ”€â”€ log_plays.py
â”‚Â Â  â”œâ”€â”€ profile_roles.py
â”‚Â Â  â”œâ”€â”€ profile_tasks.py
â”‚Â Â  â”œâ”€â”€ timer.py
â”œâ”€â”€ check_ansible_lint.sh
â”œâ”€â”€ check_syntax.sh
â”œâ”€â”€ check_yaml_lint.sh
â”œâ”€â”€ create.yml
â”œâ”€â”€ destroy.yml
â”œâ”€â”€ Jenkinsfile
â”œâ”€â”€ localhost_inventory.yml
â”œâ”€â”€ playbooks
â”‚Â Â  â”œâ”€â”€ ap_linux_instance
â”‚Â Â  â””â”€â”€ requirements.yml
â”œâ”€â”€ prerequisites.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ reports
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ roles
â”‚Â Â  â”œâ”€â”€ ar_linux_ansible_venv
â”‚Â Â  â”œâ”€â”€ ar_linux_cname
â”‚Â Â  â”œâ”€â”€ config_encoder_filters
â”‚Â Â  â””â”€â”€ requirements.yml
â””â”€â”€ VERSION.md
```
My .gitignore ignores most sane OS/language/IDE things, but also ignores everything in the `roles/` and `playbooks/` folders except for the `requirements.yml` files in each folder. The requirements.yml file within the playbooks folder is similar to your Galaxy-style requirements.yml, but rather than calling out dependent playbooks by Galaxy owner.name, I specify the full Git source (Galaxy supports this of course). The requirements.yml within the roles/ folder is just your traditional Galaxy-style requirements.
I have this `prerequisites.yml` playbook, that looks like this:
```yaml
---
- name: PLAY | Install other required playbooks
hosts: localhost
connection: local
tasks:
- name: INCLUDE_VARS | include variables to discover other needed playbooks
include_vars:
dir: playbooks/
files_matching: requirements.yml
depth: 1
- name: GIT | Clone playbooks
git:
repo: ""{{ item.src }}""
dest: ""playbooks/{{ item.src.split('/')[-1] }}""
version: ""{{ item.version }}""
loop: ""{{ required_playbooks }}""
- name: SHELL | Install included playbooks roles
shell: ansible-galaxy install -r roles/requirements.yml -p roles/ --force
args:
chdir: ""playbooks/{{ item.src.split('/')[-1] }}""
loop: ""{{ required_playbooks }}""
when: item.galaxy
changed_when: false
tags: [ skip_ansible_lint ]
```
And, assuming that my ""big bang"" create.yml depends on a playbook and its roles from another playbook project, I import it like this:
```yaml
# ~~~~~~~~~~
# Ensure that all of the host VMs in the inventory are up and running
# either on-prem or in Azure as specified in the inventory
#
- name: Ensure inventory hosts are present
import_playbook: ""playbooks/ap_linux_instance/create.yml""
tags: [ base_server, hosts ]
```
And so, the work-flow to run my ""big bang"" (e.g. create.yml) is a 3-liner:
```bash
ansible-playbook prerequisites.yml
ansible-galaxy install -r roles/requirements -p roles/
ansible-playbook create.yml -i <path_to_inventory>
```
You could, of course, wrap the above 3-liner in a `create.sh` shell script for convenience. This method has served me well for some fairly complex playbook projects that depend on other playbook projects. This forces us to keep roles and playbooks fairly self-contained and re-usable and factor variables out into their own inventory projects. When performed with discipline, it makes it really easy to migrate unaltered roles/playbooks to other environments, then just update inventory variables that are unique to that environment.
**This doesn't solve the conditional import problem**, mind you, but does help me use the `import_playbook` statement for something that may not exist just yet. It kind of gets around a conditional in my very specific use-case. I make use of tagging on the `import_playbook` to leverage the command-line `--tags` and `--skip-tags` features if I need scalpel-like precision at run-time. But if you had to make an import decision based on some other conditional logic (e.g., OS family), well, we still need that as a language feature I think. For now, I just handle those cases with sub-playbooks and chain them together ensuring I target the appropriate hosts/groups that should or should not be targeted based on how I've setup my inventory (yes, it can get messy).
This is all pretty wild and requires a high degree if what I commonly refer to as ""4th dimensional thinking"", especially when you consider branches/versions of things and running them from CI/CD platforms like Jenkins or even AWX. But I still find Ansible fascinating and use it daily.
HTH,
Ben",
"I see that this issue now has a ""has_pr"" label. I searched a lot, but I can't find the PR. Can someone link it here?",Impatience
"The bot added that label, and it looks like it was a false positive. No work is being done on this feature, nor are there any plans to work on it currently.",
any plans for this one ?,Impatience
"I'd just like to mention that there is a bit in the documentation that insinuates that the feature requested already exists. See:
https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#applying-when-to-roles-imports-and-includes
Specifically, the note mentions that Ansible allows `when` to work with playbook includes since version 2.0. Perhaps this documentation should also be amended.",
"If only unfixed issues aged like fine wine instead of like bait fish....
But seriously is this ever going to either be fixed or closed as won't fix? The `has_pr` label typically indicates at fix is being gestated, but in this case there is no hint as to where we should be looking for the fix.",Bitter Frustration
My previous comment still stands as an answer to these questions. https://github.com/ansible/ansible/issues/34281#issuecomment-517328598,
1,
"+1
This is a frustrating ""missing feature"" Since there is no plan to allow for conditional vars_prompt, the only other way to conditionally get user input at start of playbook would be with conditional ""import_playbook"" based on whether a tag was sent in on commandline.
`- import_playbook: playbook_choice_1.yml
when: ""'specific_tag' in ansible_run_tags""`
I understand that we want a playbook to require no user input, but this is just not a realistic scenario. When you have several users of a given playbook, then you need to ask for passwords. The other option is having dozens of ""ansible vault"" files (nightmare), or by implementing hashicorp vault (first get everything working as desired then implement another level of integration)...",Bitter Frustration
"I (still) agree with @subcan that this feature should exist. This would be a great thing to get implemented while we're all on COVID-19 lockdown! ;)
As to options for managing multiple configurations, there is another option in between Ansible vault files and full-blown Hashicorp Vault deployment. I wrote a tool I named ""python_secrets"" and it works well with Ansible to manage multiple sets of variables outside of a Git repository (including one with Ansible playbooks). I document how to use it this way and have described it in several public talks listed on my home page (my talk at WSLConf from earlier this month will be added as soon as the videos are released).
https://pypi.org/project/python-secrets/
https://youtu.be/WD2Oqy2oc3A",
"Files identified in the description:
* [`lib/ansible/modules/import_playbook.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/import_playbook.py)
If these files are incorrect, please update the `component name` section of the description or use the `!component` bot command.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",
1,
1,
"I've locked this to contributors for now. Adding `+1` comments is too noisy. For future reference, add a reaction to the issue body, and don't comment.",
"Thank you very much for your submission to Ansible. It means a lot to us that you've taken time to contribute.
Unfortunately, this issue has been open for some time while waiting for a contributor to take it up but there does not seem to have been anyone that did so. So we are going to close this issue to clear up the queues and make it easier for contributors to browse possible implementation targets.
However, we're absolutely always up for discussion. Because this project is very active, we're unlikely to see comments made on closed tickets and we lock them after some time. If you or anyone else has any further questions, please let us know by using any of the communication methods listed in the page below:
* https://docs.ansible.com/ansible/latest/community/communication.html
In the future, sometimes starting a discussion on the development list prior to proposing or implementing a feature can make getting things included a little easier, but it's not always necessary.
Thank you once again for this and your interest in Ansible!
[click here for bot help](https://github.com/ansible/ansibullbot/blob/devel/ISSUE_HELP.md)
<!--- boilerplate: waiting_on_contributor_close --->",
"## Description
RetroArch builds debug builds by default after commit https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d
This is very bad and breaks ./configure && make which would explicitly need `DEBUG=0`.
## Related Issues
Debug support should not be enabled if `DEBUG` is undefined.
## Related Pull Requests
https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d
## Reviewers
@twinaphex, @alcaro, @bparker06",Bitter Frustration
"Release builds are impossible to debug properly, therefore they're the broken ones. Debug builds work fine, they're a bit slower but are not broken by any plausible definition.
Fix your buildbots instead. They're probably already setting a dozen variables, it's easy to add another one.",Bitter Frustration
"No, I'm not going to fix every distro package that follows correct behavior of not setting `DEBUG` for release builds.
You should just make this script.
```
#!/bin/sh
./configure && make DEBUG=1
```",Bitter Frustration
"Can you please untag my PR with that ridiculous bs? Changes will not be made because this is the correct behavior and you just broke it, seriously why can't you just accept that you are wrong?",Bitter Frustration
"For windows we even ship debug builds on every nightly.
I guess adding DEBUG=0 is feasible but I don't think it should be the default behavior",
"I will admit I'm wrong when (if) I'm convinced I'm wrong. Right now, I believe you're just as wrong as you believe I am.
Who says not setting debug is correct? Especially considering we defaulted to debug=yes a while ago, this is a regression (though probably an ancient one).
Either you change your build scripts once and ignore it, or I have to remember RetroArch being _**SPECIAL**_ every single time I want to do anything. I don't see any advantages whatsoever to your approach.",Bitter Frustration
"For the record, this probably breaks 95% of build scripts including the buildbot. Pretty much any build script that didn't set `DEBUG=0` explicitly.",Bitter Frustration
"Debug behavior should be opt in, not opt out. This the standard pretty much everywhere for a damn good reason. The fact this has to be spelled out for you is not only insulting, its incredibly inane.",Bitter Frustration
"If there is a ""damn good reason"" to default to undebuggable programs, post said reason and I'll consider it. Repeating variants of ""because I said so"" or ""because this person I'm conveniently not naming said so"" is not a valid argument.",Bitter Frustration
"I'm merging this, it's the right thing to do. If you have a problem with it, take it up with @twinaphex privately, NOT HERE.",Bitter Frustration
"## If you're having trouble installing Nokogiri ...
**Have you tried following [the installation tutorial][tutorial]?**
yes
**What is the output of `gem install`?**
```
PS C:\Users\replaced> gem install nokogiri -v 1.8.1
ERROR: Error installing nokogiri:
The last version of nokogiri (= 1.8.1) to support your Ruby & RubyGems was 1.8.1. Try installing it with `gem install nokogiri -v 1.8.1`
nokogiri requires Ruby version < 2.5, >= 2.2. The current ruby version is 2.5.0.
```
**What are the contents of the `mkmf.log` file?**
Didnt get so far
**What operating system are you using?**
Windows 10 x64",Impatience
Same OS and same problem...,Bitter Frustration
"Same here, and I really would like it if downgrading is not my only option",Bitter Frustration
I am currently having the same issue,
Same problem,Impatience
"Windows 7 x64 same error msg.
```
>gem install nokogiri -v 1.8.1
ERROR: Error installing nokogiri:
The last version of nokogiri (= 1.8.1) to support your Ruby & RubyGems w
as 1.8.1. Try installing it with `gem install nokogiri -v 1.8.1`
nokogiri requires Ruby version < 2.5, >= 2.2. The current ruby version i
s 2.5.0.
```",Impatience
"The fix is already merged, although a new release is pending. See: https://github.com/sparklemotion/nokogiri/pull/1704 & https://github.com/sparklemotion/nokogiri/issues/1706
Until then, it's better to stick with Ruby 2.4.2 on Windows.
The Nokogiri team removed the GEMSPEC from sources, to disallow people building from master branch, since it is unstable and probably would contain bugs. You can read more about it here:
https://github.com/sparklemotion/nokogiri/blob/master/Y_U_NO_GEMSPEC.md",
Any word on support for 2.5.0 on OS X?,Impatience
@leehanslim OSX should use the normal gems that also work on *nix. I had no problems with rbenv to install nokogiri on OSX.,Impatience
Same problem.,Impatience
any news?,Impatience
I am wondering how this happens. Its kind of a tradition ruby makes a point release around christmas every year. the dev version was around for weeks. Ignoring Windows blames the users who are bound to this OS for what ever reason,Bitter Frustration
"@SimonHoenscheid nokogiri-core is always welcoming of contributors who would like to help us support Windows. We've asked for help repeatedly over the years, as we've all got jobs that distract us from supporting open source software, despite our best efforts and intentions.
I'd like to ask that you pay attention to the tone of your message above. ""Ignoring Windows"" is clearly an untrue statement, given the number of hours I've personally spent, the number of hours @larskanis has spent, the amount of time we've spent providing [automated testing tooling][1] for [Nokogiri on Windows][2], etc., etc.
Windows support in Nokogiri is first class. We provide precompiled DLLs for users who want a fast, simple installation; and we provide support for users who want to compile Nokogiri and libxml using DevKit. Saying that we're ignoring Windows users is absurd and a little insulting.
Currently I'm blocking a release that supports Ruby 2.5 on windows on getting Ruby 2.5 supported in our test pipelines. I accept some fault for not being more transparent about this as the blocking factor. But now I'm trying to make amends by communicating exactly what's going on behind the scenes to properly support you and the platform that you use. Not ignoring you or blaming you, as you claim.
I'm going to lock this issue because I really don't want to do a whole back-and-forth. We'll release support for Ruby 2.5 on Windows as soon as we can, just as we always do, and we appreciate your patience and empathy as we work.
[1]: https://github.com/flavorjones/windows-ruby-dev-tools-release
[2]: https://ci.nokogiri.org/teams/nokogiri-core/pipelines/nokogiri/jobs/win-ruby-2.4-devkit/builds/10",Bitter Frustration
"Remaining work to be done on our CI pipeline at https://ci.nokogiri.org/
* [x] [upgrade concourse to 3.8.0](https://concourse.ci/downloads.html#v380)
* [x] test Ruby 2.5 support just added to [windows-ruby-dev-tools-release](https://github.com/flavorjones/windows-ruby-dev-tools-release/tree/flavorjones-add-ruby-25) and cut a release
* [x] add a Ruby 2.5 test job to the pipeline (updating [concourse-gem)](https://github.com/flavorjones/concourse-gem/blob/master/lib/concourse.rb#L10)
* [x] get Ruby 2.5 tests to pass
* [ ] cut a Nokogiri release that adds Ruby 2.5
This is probably a good time to remind watchers that Ruby 2.2 is approaching EOL on 2018-03-31, and so the first release after that will likely remove Ruby 2.2 binaries from the fat gem.",
"Windows Ruby 2.5 build is up and running:
> https://ci.nokogiri.org/teams/nokogiri-core/pipelines/nokogiri/jobs/win-ruby-2.5-devkit/builds/1",
Note that commit bf94cf5 was added to master to make windows tests less brittle.,
"Nokogiri 1.8.2 has shipped with Ruby 2.5 support in the ""fat binary"" windows gems. Thanks everyone for your patience.",
"We need a written code of conduct. It should reference the relevant IETF code, and include anything we feel that we need here.
In particular, we need some thing around roles:
* Definition of what it means to be a project Member, and what behavioral standards Members are held to
* Definition of what it means to be a project Collaborator, and what behavioral standards Collaborators are held to
* What behavioral standards are expected of all participants
And some things around discussions:
* What sort of requirements exist for staying on-topic in an issue
* How to bring an end to discussions where it is clear that there will not be consensus (the IETF sets the goal of ""rough consensus"", acknowledging that sometimes you can't get everyone's buy-in)
I'm going to start this issue off locked until we figure out the best / least likely to explode way to take feedback on it.",
Closing in favor of json-schema-org/community#162,
"SS4.0
```php
TextField::create(<name>, <value>); // OK
HiddenField::create(<name>, <value>); // NOT OK. HiddenField::create(<name>)->setValue(<value>);
```
There is not reason to trip up new developers like this",Bitter Frustration
"@worikgh Can you provide more info on what specifically you expect to happen vs whatâ€™s actually happening?
The second argument should be the title (that is, the â€œlabelâ€ used when the field is displayed) - the third argument is for value - is that whatâ€™s causing your issue? It is a little strange to push a title to a `HiddenField` when that title will never be displayed, but it keeps that API (slightly) more consistent with other form field types :)",
"Your examples are wrong. `FormField` objects are:
`FormField::create(<name>, <label>, <value>)`
see: https://github.com/silverstripe/silverstripe-framework/blob/4.0/src/Forms/FormField.php#L325",
"No. FormField from http://api.silverstripe.org/4/SilverStripe/Forms/FormField.html#method_castingHelper
```
__construct(string $name, null|string $title = null, mixed $value = null)
Creates a new field.
```
Using that pattern for a hiddeen field does not work
```
$hftest = HiddenField::create(""HFieldTest"", ""Value"");
$hftest2 = HiddenField::create(""HFieldTest2"")->setValue(""Value"");
```
in the template as:
```
$Fields.fieldByName(HFieldTest)
$Fields.fieldByName(HFieldTest2)
```
Results in:
```
<input type=""hidden"" name=""HFieldTest"" class=""hidden"" id=""Form_ProfileEditForm_HFieldTest"" />
<input type=""hidden"" name=""HFieldTest2"" value=""Value"" class=""hidden"" id=""Form_ProfileEditForm_HFieldTest2"" />```
(edited for clarity)",
"@worikgh that all looks as expected...
if you add:
```php
$hftest3 = HiddenField::create(""HFieldTest3"", ""Value"", ""actual value"");
```
and
```
$Fields.fieldByName('HFieldTest3')
```
you'll get output:
```html
<input type=""hidden"" name=""HFieldTest3"" value=""actual value"" class=""hidden"" id=""Form_ProfileEditForm_HFieldTest3"" />
```",
The bug is that the interface for a text field and a hidden field are different. IMO They should be the same.,
"can you explain in what way they are different, please?
---
edit:
to create a TextField: `TextField::create(<name>, <label>, <value>)`
to create a HiddenField: `HiddenField::create(<name>, <label>, <value>)`
they seem the same to me.",Impatience
"```
TextField::create(<name>, <value>); // OK
HiddenField::create(<name>, <value>); // NOT OK. ````",
"@worikgh - I'm sorry, you're not being clear.
both of those work, don't result in error, output expected HTML/fields.
From what I can tell you actually are asking for these APIs to be different, not the same... You'd like `HiddenField`'s second constructor argument to be the value and not the label, is that right?",
"They produce different output.
See above where I created an example and cut and pasted resulting HTML.",
"That is expected, the two examples you're giving are fundamentally different:
```php
$hftest = HiddenField::create(""HFieldTest"", ""Value"");
$hftest2 = HiddenField::create(""HFieldTest2"")->setValue(""Value"");
```
In the above code, `$hftest` has no value set and just a name and label, `$hftest2` has a name and no label and a value *is* set. Therefore you'd expect different output and the output you've shown looks expected. `$hftest` has no value and `$hftest2` does.",
"No.
```
TextField::create(<name>, <value>);
```
produces
```
<input name=""<name>"" value=""<value>"" ..... type=""text"">
```
```
HiddenField::create(<name>, <value>);
```
produces
```
<input name=""<name>"" value="""" ..... type=""hidden"">
```
Those are different.",
"Worik, sorry mate... I think you're confusing Title (ie. html `<label>Title`) and Value (ie. html attribute `value=""Value""`). This constructor format is true for virtually every form field.
Perhaps if you can provide a link to the documentation you're reading specifically, and what it is that has you turned around from it? ie. suggestions on how it could be clearer? Maybe the wording is a bit confusing.",
"I am showing you code I have been implementing. On close examinatuion the second argument for TextField constructor chould be 'Title'
BUT, having missed that, I passed a value as the second argument to create(..) and hey presto it is the value. Doing *exactly* the same pattern for HiddenField resulte in a different output.
That is a bug. I have reproduced it Ad nauseam Perhaps the TextFiled interface is buggy, I do not know,. it is the way it is used in the lessons, perhaps they are buggy too. I do not know.
But I do know this: The interfaces to the HiddenField and TextField's create method are different when they should be the same.
I have made that clear in examples above. It is bnot a hypothesis but an observation.
It may not seem like a bug to have idiosyncratic interfaces, but it makes life very difficult for new comers to the platform",Bitter Frustration
"# Test 1
```php
TextField::create('Test Name', 'Test Value')
```
Creates:
```html
<input type=""text"" name=""Test Name"" class=""text"" id=""Form_ItemEditForm_Test_Name"">
```
# Test 2
```php
TextField::create('Test Name', 'Test Label', 'Test Value'); // Shift 2nd param to 3rd
```
Creates:
```html
<input type=""text"" name=""Test Name"" value=""Test Value"" class=""text"" id=""Form_ItemEditForm_Test_Name"">
```",
"@worikgh Is this SilverStripe 4.0?
Lets step through it together :)
`TextField` and `HiddenField` both inherit directly from `FormField`.
[`HiddenField`](https://github.com/silverstripe/silverstripe-framework/blob/4.0/src/Forms/HiddenField.php) does not define a constructor, thus uses `FormField`'s directly - ie. `($name, $title, value)` - this is 1. unique identification Name, 2. human Readable Title, 3. a preset Value.
While it is true that `TextField` implements a constructor, it simply extends `FormFields` to add two new parameters - at the _end_ of the list (maximum length and the form it belongs to):
`public function __construct($name, $title = null, $value = '', $maxLength = null, $form = null)`
Once these two extra parameters are set as fields on the object, the parent (ie. `FormField`) constructor is called to exact the same results as that of `HiddenField`.
You can see this here:
https://github.com/silverstripe/silverstripe-framework/blob/4.0/src/Forms/TextField.php#L40
This means that `TextField` also calls `FormField::__construct` with the parameter format `($name, $title, $value)`.",
"Whatever. That is all nice theory. But the facts, here are that the interfaces differ. SS4.0",Bitter Frustration
"I'm going to lock this topic, since it looks as if the original question has been answered adequately and the discussion seems to be degrading.
If you'd like to change the API of the FormFields, it might be worth raising an RFC issue for it with some proposals and reasons for the change so it can be discussed detail with the community and the core team. We can't change any APIs for SilverStripe 4 though without breaking semantic versioning.
Please keep in mind the [SilverStripe code of conduct](https://docs.silverstripe.org/en/contributing/code_of_conduct/) when contributing and responding to bug reports. Thanks =)",
@worikgh how about we catch up on slack :),
"> we have fixed the Marketplace instructions since a while
I've only recently noticed the change (from `ext-name` to `owner.ext-name`) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past?
> Since we now have URL handlers for Mac and Windows, those instructions became almost irrelevant
I would have replaced those `ext install (...)` instructions with URL handlers, but I've tried them once and after that, for a few days/weeks, a ""Do you want to install (...)"" message kept popping up every time I opened a new window. By the time that was fixed I guess I had forgotten about them.
> what if we showed in quick open a list of extensions which match that name? Why providing 2 different interfaces for discovering extensions?
Shouldn't searching ""just work""?",Impatience
"> I've only recently noticed the change (from ext-name to owner.ext-name) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past?
I guess it wasn't, since it was a Markeplace change... Sorry about that.
> Shouldn't searching ""just work""?
It should. We've notified the Marketplace of the search issues.",Impatience
"Just to reiterate on how bad the search engine works:
- I've just published an extension named `Open in node_modules`
- Searching for `open in node modules` (without the underscore) won't even show said extension within the first 60 results ðŸ¤·â€â™‚ï¸",Bitter Frustration
"@fabiospampinato, regarding the issue of searching your new extension:
While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching. Among the string matching, exact string matches carry a higher weight than prefix match. Since `node_modules` is one word in your extension name, word `node` in search text gets prefix match here. However, there are lot of extensions on Marketplace which have exact word 'node' in their extension name. So they carry a higher weight for matching the word 'node'. Additionally, since your extension is new, download count and ratings of other extensions are way higher, pushing their total score to the top.
One way to improve your extension rank is to break word `node_modules` into separate words `node modules`. This will increase the string matching score and push your extension up in the result.",
"@gaurav42 Well, I would still consider whatever algorithm you guys are using broken because if I search for ""open in node modules"" I get these search results (some samples ~~randomly picked):
| Rank | Ext. Name | Ext. Description | My comments |
|--------|-------------|-----------------|----------------|
| `#6` | [Node Exec](https://marketplace.visualstudio.com/items?itemName=miramac.vscode-exec-node) | Execute the current file or your selected code with node.js. | 80k downloads, but no mention of ""open"" or ""module(s)"" anywhere.
| `#10` | [CSS Modules](https://marketplace.visualstudio.com/items?itemName=clinyong.vscode-css-modules) | Visual Studio Code extension for CSS Modules | 22k downloads, but no mention of ""open"" or ""node"" anywhere.
| `#17` | [Node TDD](https://marketplace.visualstudio.com/items?itemName=prashaantt.node-tdd) | Ease test-driven development in Node and JavaScript | 9k downloads, no mention of ""module(s)"", but at least it has a few ""opened"" in its readme.
| `#64` | [Open in Vim](https://marketplace.visualstudio.com/items?itemName=jonsmithers.open-in-vim) | Opens current file in vim | 2k downloads, this is not even mentioning ""node"" anywhere, let alone ""module(s)"". But at least it has ""open"" and ""in"" in it's title.
| `#65` | [Open in node_modules](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-open-in-node-modules) | Open the current selection or arbitrary string in node_modules. | 2 downloads, it matches all the provided keywords, **in its title**.
If I had to guess what's wrong with your approach I'd say:
- Lack of stopwords, kind of meaningless keywords like ""vscode"" or ""in"" are given too much weight.
- Poor query parsing, if keywords are joined in some sort or another the whole thing falls apart (vscode-foo-bar, foo_bar, GitLens etc.)
- Maybe you're giving too much weight to downloads and ratings, the first thing to sort for is relevancy.
> One way to improve your extension rank is to break word node_modules into separate words node modules. This will increase the string matching score and push your extension up in the result.
I'm not going to rename the extension to something wrong (`node_modules` is a folder, I'm not talking about `node modules`, as in ""NPM packages"", here) just to work around this.",Bitter Frustration
"I agree, the search ranking seems wierd.
Some examples (when searching for ""python""):
Py Files generator is above autoDocString, despite having about 50k less installs and 0 reviews
Same thing with ladieratheme - it is above autoDocString despite having about 49k less installs and 3 less reviews
Trustcode odo snippets and kvlang is above docker linter, despite having 47k less installs
Python paste and indent has 3 stars but apparently it's 2k extra downloads trumps Python (pydev)'s 5 star rating. Seems like download count is weighted more heavily than rating in the ranking (or maybe python paste and indent has better keywords)
Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython (with 742 _thousand_ downloads and 3.5 stars) and several other extensions with far more downloads / good ratings. So you could have a very popular or well-rated extension but if you don't have the right keywords you will still be ranked down.
Wait..,. but looking at the Python Coding Conventions package.json, it doesn't even have any keywords!
https://github.com/harip/python-coding-conventions/blob/master/package.json ðŸ¤”ðŸ¤”ðŸ¤”
> While searching an extension, we also take into account the community inputs like number of downloads, You mean the download count that is _also_ the update count? This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased.
The update count should not effect the search ranking.
> number of ratings and average rating That's good, but what algorithim are you using to calculate the weighted rating? Not sure if that is open source but hopefully it isn't something like RatingA - RatingB or RatingA/RatingB
http://www.evanmiller.org/how-not-to-sort-by-average-rating.html",Bitter Frustration
"> Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython This probably happens because of this:
> Poor query parsing, if keywords are joined in some sort of another the whole thing falls apart (vscode-foo-bar, foo_bar, GitLens etc.)
> You mean the download count that is also the update count? This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased.
The update count should not effect the search ranking.
Yeah that's another major problem, one could automatically push a new update every day and downloads will go through the roof even though the same number of people are using it.
I guess technically those users are _downloading_ the update, but once this things are counted the downloads counter becomes less meaningful. The download counter for extensions on Chrome's store decreases when somebody uninstalls your extension and I think it doesn't increase just because the extension gets updated.",Bitter Frustration
"Hey guys, I want to thank you for all the feedback. Please keep them coming. We are discussing this issue internally and I'll update this thread as soon as we have something to share.",
"Here there's another weird one, searching for ""monokai"" and ordering by downloads returns those language packs at the top. The only place where those language packs mention ""monokai"" is inside their `package.json` as the value of some `contributes.localizations[0].translations[x].id` keys, why are you guys even indexing those fields?
<img width=""1193"" alt=""screen shot 2018-07-18 at 13 09 36"" src=""https://user-images.githubusercontent.com/1812093/42878075-112f643c-8a8c-11e8-93a2-1c5f54f57589.png"">",Bitter Frustration
"I've just realized that the readmes aren't indexed at all. I searched for ""cyclomatic"" in the marketplace, and I've got this result:
<img width=""674"" alt=""screen shot 2018-09-20 at 18 27 56"" src=""https://user-images.githubusercontent.com/1812093/45832685-ebe46c00-bd02-11e8-85ec-3cf9dc21ad82.png"">
Then I tried searching for ""marketplace cyclomatic"" on Google, and this is the result:
<img width=""846"" alt=""screen shot 2018-09-20 at 18 25 18"" src=""https://user-images.githubusercontent.com/1812093/45832583-95772d80-bd02-11e8-875b-473a56d814a1.png"">
This is a bit ridiculous. I'm not saying that the search functionalities in the marketplace should be as good as Google's under **all** circumstances, but that's a full-word match in the readme, this query just can't return 0 results.
@pkuma-msft do you have any updates for us?",Bitter Frustration
"@fabiospampinato Yes, unfortunately we do not index readme.md.. sorry about that. We use SQL FTS in our backend today and that's kind of the limiting factor. There's only so much custom logic we can run over the results returned by FTS to make it 'more' relevant - and more importantly it doesn't scale in the long run.
We are exploring moving our search platform to Azure Search or Bing which are techologies that are being actively developed and should provide us with more features and capabilities.
I'm afraid we are not going to invest more in trying to optimize search with FTS.",
@pkuma-msft thanks for the update. Any timeframe for when the move will happen?,Impatience
"Holy crap changing my extension name to ""AREPL for python"" moved it to the *third* position in the search rankings when searching for python. Thanks for the tip @eamodio and @gaurav42 !",
"@Almenon Sorry we are still in early exploration stages, cannot comment on a timeline yet.",Impatience
"From @octref > Previously: https://github.com/Microsoft/vscode/issues/24511
> > Now if you search for Vue, Vetur doesn't even show up in the first page despite being the most popular Vue extension. The install count of the 24 Vue extensions in first page combined is not even half of Vetur's install count.
> > ![image](https://user-images.githubusercontent.com/4033249/50652857-a6a81b00-0f3c-11e9-95e2-e2838082acfc.png)
> > A new Vue user coming to VS Code have a hard time finding Vetur by organic search. He might be misled to install a lot of the random extensions in Marketplace and believe VS Code has poor support for Vue files.",Bitter Frustration
"~~@kesane-msft~~
~~Which~~
Oops, misclick.",
"> our search algorithm gives more weightage to matches in the extension's display name. If you add the ""vue"" term in your display name your extension would start showing as the top result.
@kesane-msft Which I would refuse. What do I call it? `Vetur Vue`?
Another example: Searching for ""Golang""
![image](https://user-images.githubusercontent.com/4033249/50705744-a2e5c880-1010-11e9-941e-5a1d68cc6848.png)
What should [Go extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.Go) do? Renaming it to Golang? After which the query `Go` wouldn't return it as top result?
Maybe it's better to start a community curation of good extensions for each language/framework. http://howistart.org/ is a good example. At least it wouldn't be impossible for new users to find which extension to use, because the search results are not ranked helpfully.",Bitter Frustration
"Maybe there could be a keyword (maybe the first one?), that is ranked as if it was part of the title. So that way GitLens could have _git_, Vetur could have _vue_, Go could have _golang_, etc.",
"@eamodio Already did that, just need marketplace to index the `keywords` in package.json:
![image](https://user-images.githubusercontent.com/4033249/50706819-cc542380-1013-11e9-8786-e286bb014c2a.png)",Impatience
"@octref yup, exactly",
"> Maybe there could be a keyword (maybe the first one?), that is ranked as if it was part of the title. So that way GitLens could have git, Vetur could have vue, Go could have golang, etc.
This sounds too specific to me, IMHO the solution is transitioning from full-text search to a real search engine. Readmes, titles, descriptions and keywords should be properly indexed and the search query should be properly parsed too.",Impatience
@fabiospampinato While I don't disagree -- I was just trying to offer an alternative that would hopefully be easy to implement but improves things somewhat and therefore likely to happen ðŸ˜‰,Irony
"Just wanted to open this as an issue. I often read `search for [xxx] and install the first result`.
I search the exact name of an extension and the one I'm looking for never is the first result :(
![image](https://user-images.githubusercontent.com/802702/53523036-073a7b00-3adc-11e9-8000-2b2b802ba1e5.png)",Bitter Frustration
"I can confirm this is still a bad issue. ### This isn't a 'bad' search algorithm, something is broken here.
When searching for ""line endings"" (picture below)<br>
The following 4 circles are all similar extensions. The red circles have MORE downloads, installs, and better ratings, and I checked the `package.json` all of them have ""line endings"" in the title and as keywords.<br>
How can 'Line Note' (4th result) which
- doesn't have 'endings' in the title
- has 203 downloads
- no reviews
- doesn't even have 'endings' in the package.json or readme Beat out both 'line-endings' and 'code-eol (2019) Line Endings'
- both contain the full search term in the title
- they have 4K and 6K downloads respectively
- they have 4 stars and 5 stars respectively
- and have 'line endings', 'line', 'endings' in the keywords of the package.json and readme
And it's not like 'Line Note' is some anomaly, there are 25 just as bad results that come before the relevant one. <img width=""1221"" alt=""Screen Shot 2019-03-31 at 5 02 32 AM"" src=""https://user-images.githubusercontent.com/17692058/55287759-c059d280-5372-11e9-8c99-21549a98d33f.png"">",Bitter Frustration
"My only theory to how these are being ranked, is that the marketplace has grabbed 200 results and accidentally sorted them by trending-ness instead of search-query relevance.
Since this is an upstream problem, is there a more direct repo we can reach out to? @auchenberg. If not then I agree with @octref, its been 2 months, it's probably time for the community to start building their own solution. I don't understand how something this critical can fail without tests catching it and a rollback being issued.",Bitter Frustration
"@jeff-hykin , I am from the Marketplace team. Thanks for bringing this issue to our notice. We are looking into it and will get it fixed.",
"Any updates after half a month?
If it hasn't been looked into, an estimate on when it will be looked into would be nice.
If it has been looked into, an update on what was found/not found would also be nice.
If the Marketplace team has higher priority tasks that's fine, I completely understand, I think we would just like to know that.",Impatience
"@jeff-hykin , we would have the fix for the issue you reported rolled out by next week or so.",
@kesane-msft any details on what changes are being made exactly?,Impatience
"@Almenon , we are tweaking our index analyzers.",
"@jeff-hykin we have rolled out the change. You should see the better search results.
Thanks for bringing it to our notice and keep the feedback coming in!",
"It seems that some issues have been addressed, but some things got even worse:
- Now searching for both `vscode-todo-plus` and `fabiospampinato.vscode-todo-plus` returns 0 results, IMHO that's unacceptable (and this was the original problem reported in this issue).
- Sometimes if I type `open in browsers` and then hit enter the query becomes `open in browser`, deleting the last character ðŸ¤·â€â™‚ï¸.
- The marketplace doesn't search automatically until I hit enter, _unless_ I delete the query, IMHO that's pretty confusing (and that's also not what's happening in the homepage).
- If I clear the input field then the cursor disappears for a while until the results are returned.
- There's probably no need to wait for those results to be returned in the first place, the top 55 extensions by downloads won't change very often, I'd just cache them (refreshing the cache once a day or so) and just ship them along with the rest of the page to the browser.",Bitter Frustration
"Yep things still aren't working right. Earlier gaurav42 said that ""While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching"". However, when I search for ""python"" ""python extended"" is above my extension despite having literally 0 reviews. In terms of download count we are very close (130k vs. 150k) so if the reviews were taken into account I would expect AREPL to be ranked higher. I would even go so far as to say that reviews should be _more_ important than download count in influencing search ranking (assuming you have enough reviews to be statistically valid). As a user when I search for things I want to find the best extensions, not the most downloaded extensions, but my extension has good reviews so I am biased on that front :P
@kesane-msft can you explain the change in more detail please? I don't know what ""tweaking the index analyzers"" means.",Bitter Frustration
"Oh and here's a really weird one - python (pydev) is ranked above python test explorer, despite having the same number of reviews, python mentioned in description of both, worse reviews on average, and ~45k less downloads.",Bitter Frustration
"I'm not sure what algorithm is being used but there's some existing ones that may be helpful:
http://www.evanmiller.org/how-not-to-sort-by-average-rating.html
https://stackoverflow.com/questions/1411199/what-is-a-better-way-to-sort-by-a-5-star-rating",
"Thanks @kesane-msft, I appreciate it.
The ""Line endings"" search seems to be fixed so I'm pretty happy about that.
After a few searches I do agree with @Almenon the search is sadly still pretty broken.
**For me, the biggest issue is its unclear why it is broken (both three months ago, and now)**
Is there some kind of AI returning the results? A handcrafted weighting system? Some transparency about how the search works would go a long way.
I know the code probably can't be made open source, but if the marketplace team made an API that would allow for exact-name matches and regex matches I'm pretty confident the VS Code community could fix this permanently within 48 hours.
Here's an example of the search still being pretty messed up. It's an improvement from last time; at least the relevant results are somewhere in the top 10, and the other results are popular. But on the other hand it is pretty sad that most students in my undergrad Info Storage and Retrieval class wrote better ranking systems in less than a month.
<img width=""382"" alt=""Screen Shot 2019-04-25 at 3 36 20 PM"" src=""https://user-images.githubusercontent.com/17692058/56767016-64ffe200-6770-11e9-970e-2efb90a2e104.png"">",Bitter Frustration
"Marketplace team has recently updated their search strategy to use Azure based search and which will expect to fix quiet a number of search related issues. There could be still some gaps which might exist and can be fixed. Since this issue is overwhelmed with lot of feedback and it is getting hard for us to understand what is fixed and what is not, I am locking this issue and ask users to create separate issues if they see any new bugs. At the same time, I would go through this complete issue and extract those which are not yet fixed into new issues which will help us and Marketplace team to better track and fix them. Thanks for the feedback provided here.",
"Took me quite a while to test, but nothing reported from this issue was fixed and search by author ID, extension ID, or combination of both are broken.
| Query | Marketplace Search | Expected | Actual | Regression |
|---|---|---|---|---|
|`fabiospampinato.vscode-todo-plus`| [Link](https://marketplace.visualstudio.com/search?term=fabiospampinato.vscode-todo-plus&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Todo+](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-todo-plus) and top-ranked | 0 result | N/A |
|`vscode-todo-plus` | [Link](https://marketplace.visualstudio.com/search?term=vscode-todo-plus&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Todo+](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-todo-plus) and top-ranked | 0 result | Used to at least hit but not top-ranked |
|`vscode-open-in-browsers` | [Link](https://marketplace.visualstudio.com/search?term=vscode-open-in-browsers&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit and top-ranked | 0 result | Used to at least hit but not top-ranked |
|`vue`| [Link](https://marketplace.visualstudio.com/search?term=vue&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Vetur](https://marketplace.visualstudio.com/items?itemName=octref.vetur) and top-ranked | Hit but not top-ranked | N/A |
|`golang`| [Link](https://marketplace.visualstudio.com/search?term=golang&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Go](https://marketplace.visualstudio.com/items?itemName=ms-vscode.Go) and top-ranked | NOT in result | N/A |
| `octref` (publisher id) | [Link](https://marketplace.visualstudio.com/search?term=octref&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit all extensions published by [octref](https://marketplace.visualstudio.com/publishers/octref) | 0 result | Used to hit all extensions published by [octref](https://marketplace.visualstudio.com/publishers/octref) |",Bitter Frustration
"@octref Thanks for consolidating already (I was planning to do it this week). These issues were already raised with Marketplace team and here is their explanation
### Searching by id: `GitHub.vscode-pull-request-github-insiders`
We donâ€™t support search on id of the extension(Not supported in SqlFTS too). When we search for GitHub.vscode-pull-request-github-insiders, then we search for two words â€œGitHubâ€ and â€œvscode-pull-request-github-insideâ€. Thatâ€™s why all the extensionâ€™s containing Github are coming at the TOP and results are looking better than SqlFTS.
**Tip**: You can search by extension id using the phrase `@id:GitHub.vscode-pull-request-github-insiders`. Issue: https://github.com/Microsoft/vscode/issues/69350. Let's track it there.
### Searching by publisher id: `dias`
Azure search provides us custom analyzer to control search relevance. We had done stemming during indexing. So when user searches for â€œdiasâ€, it returns all the extensions containing â€œdiaâ€ or â€œdiasâ€ both. Here DialogScript is coming at the top, because it contains â€œdiaâ€ , while â€œdiasâ€ is matched in publisher display name of â€œOpen Folder Context Menus for VS Codeâ€ extension. Extension display name prefix match has more weight than publisher display name. Thatâ€™s why â€œDialogScriptâ€ is coming at the top.
We have fixed this issue. We are not doing stemming now. So if you will search for â€œdiasâ€, it will return the extensionâ€™s containing â€œdiasâ€ only.
**Tip**: You can search by publisher using the phrase `publisher:""Pine Wu""`. You can also click on Publisher name in the extension editor. I think this is a good solution for searching by publisher.
### Vue
We do have weights on tags too, but itâ€™s very less (1/100) in compare to extensionâ€™s display name. Thatâ€™s why Vetur extensionâ€™s is not coming at the top, when user searches for â€˜Vueâ€™. â€˜Vueâ€™ search term is a special case, where we expect Vetur extension as top result, because itâ€™s a popular extension of â€˜Vueâ€™ language. We were unbale to solve this problem using SqlFTS, but we can solve this using AzureSearch. We can add Synonym Maps property to the index field definition in AzureSearch.
@kesane-msft Please add any details if missing. After consolidating, one of the improvements we would like to see from you is with respect to `vue` phrase in which case giving weightage to tags might help.",
Upstream issue https://github.com/microsoft/vsmarketplace/issues/154,
"This is fixed in latest [VS Code Insiders](https://code.visualstudio.com/insiders/). Please try it out and let us know if there are still examples where you think that search is not working.
We expect the fixed search to be in VS Code Stable start of December.
Looking forward to feedback - thanks!",
"This simplifies the logic a bit and gets rid of the main loop that looks
for where the key is located.
Signed-off-by: Matthew Thode <mthode@mthode.org>
Related: https://github.com/zfsonlinux/zfs/pull/7189
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [x] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux code style requirements.
- [x] I have updated the documentation accordingly.
- [x] I have read the **CONTRIBUTING** document.
- [ ] I have added tests to cover my changes.
- [x] All new and existing tests passed.
- [x] All commit messages are properly formatted and contain `Signed-off-by`.
- [ ] Change has been approved by a ZFS on Linux member.",
@tcaputi @kpande @bunder2015 your review here would be nice :D,Impatience
"I'm still not an expert in bash or dracut, but the approach looks correct to me.",
"That would be changing one `if` out for another. I'm not sure what would be gained or fixed doing so either (the logic here is straight forward, which I like).",
"# [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=h1) Report
> Merging [#7194](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=desc) into [master](https://codecov.io/gh/zfsonlinux/zfs/commit/e921f6508b212c61fcedd0eeb2f9cf9da1abc4d1?src=pr&el=desc) will **decrease** coverage by `0.23%`.
> The diff coverage is `n/a`.
[![Impacted file tree graph](https://codecov.io/gh/zfsonlinux/zfs/pull/7194/graphs/tree.svg?width=650&height=150&src=pr&token=NGfxvvG2io)](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=tree)
```diff
@@ Coverage Diff @@
## master #7194 +/- ##
==========================================
- Coverage 76.39% 76.16% -0.24% ==========================================
Files 327 327 Lines 103768 103768 ==========================================
- Hits 79278 79037 -241 - Misses 24490 24731 +241
```
| Flag | Coverage Î” | |
|---|---|---|
| #kernel | `76.16% <Ã¸> (+0.22%)` | :arrow_up: |
| #user | `65.43% <Ã¸> (-0.46%)` | :arrow_down: |
------
[Continue to review full report at Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=footer). Last update [e921f65...e5619df](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",
"@tcaputi: lol, your choice of input to the open source effectively guarantees you're going to be a bootloader and init system expert for the rest of your days.
@kpande: that sounds reasonable, but we need to be careful with all of this. Root context execution of user supplied inputs seems like it would have pitfalls...",Mocking
Complexity being roughly analogous to attack surface. This should be a KISS exercise to the degree possible.,
I have not tested the current iteration of the patch,
"so... uh, do I need to resubmit this now, since I can't reopen?",Impatience
"@prometheanfire unfortunately, Github won't let me reopening this issue either. But please open a new PR with your updated version. I'd like to think we can continue discussing the relative merrits of the two proposed solutions and then move forward with one of them.",
"@behlendorf I'm actually fine with @kpande's solution, I think ours now differ by newlines only. I just wanted to call out the behaviour.",
I'm glad to hear it was simply a misunderstanding and there was no harm done.,
"is not misunderstanding, he randomly ban ppl from IRC, verbally abuse issue reportes, delete comments... i not know why he can moderate the project, he is very toxic in this community, this is not first time, like prometheanfire say is behaviour.",Bitter Frustration
This is the first time I've had any problem at least.,Bitter Frustration
"no better place becaus you ban me from IRC. respond to criticism is lies, everyone who do not think like yourself you consider stupid",Bitter Frustration
estaba habriendo el servidor por primera vez y la pantalla del juego comenzo a temblar ayuda porfavor! :disappointed:,Bitter Frustration
Abriendo* WTF dude?,Impatience
`s/habriendo/abriendo/g`,
`bugfix/shake-screen-on-hopen`,
i have the same issue (?),Impatience
"#### I'm opening this issue because:
- [ ] npm is crashing.
- [x] npm is producing an incorrect install.
- [ ] npm is doing something I don't understand.
- [ ] Other (_see below for feature requests_):
#### What's going wrong?
#19883 causes issues to production environment. Also https://github.com/npm/npm/issues/19883#issuecomment-367698802 describes a way the behavior introduced by 5.7.0 can be exploited to do things that are not intended.
I understand that CVEs are usually assigned before a responsible disclosure, fix is delivered, but as this was publicly disclosed, the clock is ticking and people should be aware of.",Impatience
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-7408,
"(this lock is preemptive. We'll have an actual response once we talk about it, but I'm heading off the trolls that are still circling like sharks)",Mocking
"I see that someone else filed a CVE, but we wouldn't have filed one ourselves: After internal discussions and consultation with a security team, we decided that a bug that causes unintended behavior, in a prerelease software version especially, does not really warrant a CVE. Regarding the `setuid` thing, we do not consider that a vulnerability because the `setuid` bit is not preserved on a `chown`, therefore not risking creating world-executable setuid executables.
So, with that comment, plus the fact that OP's original request was done _anyway_, I'm gonna close this.",
"<!--- Please follow the instructions below. We receive dozens of issues every week, so to stay productive, we will close issues that don't provide enough information. Please open Android-related issues on the Android Issue Tracker at https://source.android.com/source/report-bugs
Please open Kotlin DSL-related issues at https://github.com/gradle/kotlin-dsl/issues
Please open Gradle Native-related issues at https://github.com/gradle/gradle-native/issues
-->
<!--- Provide a brief summary of the issue in the title above -->
### Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->
### Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
<!--- If suggesting a change/improvement, explain the difference from current behavior -->
### Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->
### Steps to Reproduce (for bugs)
<!--- Provide a self-contained example project (as an attached archive or a Github project). -->
<!--- In the rare cases where this is infeasible, we will also accept a detailed set of instructions. -->
### Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
<!--- A build scan `https://scans.gradle.com/get-started` is ideal -->
* Build scan URL:",
Fuck You,Vulgarity
èŠ±Q,
Shame on you.,Insulting
"<!--- Please follow the instructions below. We receive dozens of issues every week, so to stay productive, we will close issues that don't provide enough information. Please open Android-related issues on the Android Issue Tracker at https://source.android.com/source/report-bugs
Please open Kotlin DSL-related issues at https://github.com/gradle/kotlin-dsl/issues
Please open Gradle Native-related issues at https://github.com/gradle/gradle-native/issues
-->
<!--- Provide a brief summary of the issue in the title above -->
### Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->
### Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
<!--- If suggesting a change/improvement, explain the difference from current behavior -->
### Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->
### Steps to Reproduce (for bugs)
<!--- Provide a self-contained example project (as an attached archive or a Github project). -->
<!--- In the rare cases where this is infeasible, we will also accept a detailed set of instructions. -->
### Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
<!--- A build scan `https://scans.gradle.com/get-started` is ideal -->
* Build scan URL:",
"`Issues` is for issues tracking, not for spamming or showing how stupid you are. Stop doing that.",Bitter Frustration
Please close this meaningless and useless `issue`.,Bitter Frustration
"**Do you want to request a *feature* or report a *bug*?**
Bug
**What is the current behavior?**
If I run .save on a mongoose schema without being connected to the database, the callback is never called. I don't get any errors either. The process seems to be running forever. Also, if I'd run an empty .save(), `message.save()` before the actual save, I get the 200 response back. Not sure if that's the expected behaviour or not.
**If the current behavior is a bug, please provide the steps to reproduce.**
Controller:
```
var message = new Message({
content: ""Much content""
});
});
// message.save(); <--- run this to get the 200 response
message.save(function (err) {
if (err){
console.log('Something went wrong: ' + err);
return res.status(500).send(""Something went wrong: "" + err);
}
return res.status(200).send(""Saved!"");
});
```
Model:
```
var mongoose = require('mongoose');
var Schema = mongoose.Schema;
// Schema.set('validateBeforeSave', false); // to validate data manually
//mongoose.connect('mongodb://localhost:27017/userFeedback'); <--- don't connect to database
mongoose.set('debug', true);
var MessageSchema = new Schema({
content: String
});
module.exports = mongoose.model('Message', MessageSchema);
```
**What is the expected behavior?**
Some sort of error message telling me that I'm not connected to the database and that the data will not be saved.
_I am using version 5.0.9 which is the latest version at this point._",Bitter Frustration
"Hi @beanilsson, This is the expected behavior per the connection docs [here](http://mongoosejs.com/docs/connections.html#buffering). Mongoose will buffer model commands internally until a connection is established by default. If you want to disable this behavior globally you can call ```
mongoose.set('bufferCommands', false);
``` or optionally add the same option to your schema options to set it at the collection level.
```
const testSchema = new Schema ({
name: String
}, { bufferCommands: false })
```",
"Thanks for the clarification. Personally, I think that's a very strange default behaviour. But I do realize the benefits of it.",
"It's a behavior that made a lot more sense when everybody used callbacks, but it's less useful now that we have async/await. As async/await becomes more prevalent we'll consider changing it.",
In what world is this acceptable default behaviour? Please fix.,Bitter Frustration
"Hi,
Jekyll syntax highlighting is broken with Twig. Consider the following code block containing a perfectly valid Twig syntax:
```
{% highlight twig %}
{% raw %}
{% set a = 'b' %}
{% endraw %}
{% endhighlight %}
```
It outputs the following HTML:
```
<code class=""language-twig"" data-lang=""twig"">
<span class=""cp"">{%</span> <span class=""k"">set</span> <span class=""nv"">a</span> <span class=""err"">=</span>
<span class=""s1"">'b'</span> <span class=""cp"">%}</span>
</code>
```
Notice the **err** class attributed to the equal sign.
## Steps to reproduce
* Follow the official quick-start guide: https://jekyllrb.com/docs/quickstart/
* Replace the content of the post created by the installation with this:
```
---
layout: post
title: ""Welcome to Jekyll!""
categories: jekyll update
---
{% highlight twig %}
{% raw %}
{% set a = 'b' %}
{% endraw %}
{% endhighlight %}
```",
"How is the resulting output if you were to use triple-backticks instead?
```twig
{% raw %}
{% set a = 'b' %}
{% endraw %}
```",
"@ashmaroli, same.",Impatience
So you see the issue is not with Jekyll but rather with Rouge that `highlight` and the triple-backticks block uses to highlight code.,
"No, the problem also happens with the `{% highlight %}` syntax.",
"Yes, that's because the `highlight` tag uses `Rouge` for syntax-highlighting by default
https://github.com/jekyll/jekyll/blob/86d86258a8bc912c906776d8f2f9a58b3d376519/lib/jekyll/tags/highlight.rb#L38-L46
If you want to use `pygments` instead of `rouge` as your site's highlighter, add the following to your `_config.yml`:
```yml
highlighter: pygments
```",
This sounds like an issue with https://github.com/jneen/rouge rather than Jekyll. Jekyll has no knowledge of syntax of any language.,
"@pathawks, why did you close this? It's not fixed and I'm not the one explicitely using Rouge. The maintainers of the project are using a dependency that is buggy, they should take care of this. What do you want me to do? I don't even know what Rouge is!",Bitter Frustration
"> What do you want me to do? I don't even know what Rouge is!
Iâ€™ve provided a link to the repository so that you can open an issue there and explain the problem you are having.
I do not know what â€œTwigâ€ is, so it would not make sense for me to be the one to explain what needs changing in Rogue.
There is nothing in Jekyllâ€™s code that can be changed to fix this issue; the fix will have to come from Rogue.
Here is a link to Rogueâ€™s Twig lexar: https://github.com/jneen/rouge/blob/master/lib/rouge/lexers/twig.rb",
"> There is nothing in Jekyllâ€™s code that can be changed to fix this issue; the fix will have to come from Rogue.
That's not my point. You are the one using Rouge to implement a feature that you advertise explitely on your docs! That's your responsibility to take care of things that don't work as expected in the dependencies of your project. As a consumer of your product, I expect it to work as advertised:
https://jekyllrb.com/docs/templates/#code-snippet-highlighting
You are advertising syntax highlighting, you are supposed to deliver! And if you don't, you are supposed to take care of whatever is needed to have your product work as expected.",Bitter Frustration
"@ericmorand We're sorry that you're facing issues while using Jekyll.
I agree that you as an end-user shouldn't concern yourself about bugs in dependencies.
One of the maintainers will get in touch with the developers at Rouge and sort things out for you.",
"@ashmaroli, thanks a lot. I already created an issue, maybe a maintainer could comment on it if the issue is not clear enough or if some things can be added:
https://github.com/jneen/rouge/issues/881",
"@ericmorand Please take a step back and consider that this is an entirely volunteer-run project. We're not contractually obligated to work on every bug and answer every question, seeing as we simply don't have enough resources. So our apologies if some things take too long, or don't end up happening, but it's wrong to blame the maintainers for this.",Impatience
"@oe, I totally understand that and i can relate: maintaining open source projects is very time-consuming. But I'm not having that discussion because I want to see that bug fixed immediately, to be honest this is a low priority bug even by my own standards. My point is that if, when a bug happens, maintainers blame a dependency, close the issue and ask for the reporter to open an issue elsewhere, that could go that way:
Jekyll
Oh, sorry this is a bug with Rouge, go open an issue there.
-> Rouge
Oh, sorry this is a bug with Ruby, go open an issue there.
-> Ruby
Oh, sorry this is a bug with GCC, go open an issue there.
...and so on. At one point, my issue will be invalid because I won't even know what and how to report the bug. Already, Rouge maintainers could totally close my issue as invalid because I'm giving a way to reproduce that imply Jekyll - it would be legitimate for them to say that it's a Jekyll bug or that they want a reproducible example using only Rouge.",Impatience
"@ericmorand The way I see it, this issue could pop up in any software that uses Rouge, and is therefore not specific to Jekyll. I agree however that it could have been better communicated before it was closed, sorry about that.",
"@ericmorand I re-opened the issue to convey that we have not abandoned this report straight away..
Do know that I've kept a tab on the issue-ticket at Rouge and will follow its proceedings as time permits..",
"This issue has been automatically marked as stale because it has not been commented on for at least two months.
The resources of the Jekyll team are limited, and so we are asking for your help.
If this is a **bug** and you can still reproduce this error on the <code>3.3-stable</code> or <code>master</code> branch, please reply with all of the information you have about it in order to keep the issue open.
If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced [hooks](http://jekyllrb.com/docs/plugins/#hooks) which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.
This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions.",
"When i startup my gitkraken, it is stuck in an infinite loop while it is opening the repository. I cannot do anything because the opening repo logo keeps popping up, and when the opening repo logo is gone, it comes back up a second later.
This occurs when i open any repository with gitkraken. (i use the latest version of gitkraken)",Bitter Frustration
"Does it work, e.g. if you view the repo with `git log --all` or perhaps `gitk --all`?
If so, I suggest to report this to gitkraken, because it is probably a bug on their site then.",
Yes. Not sure it's a problem of git-tfs. @GuylianWasHier Are you really sure to know what is git-tfs and that you post your issue in the good project?,Impatience
To the best of my knowledge GitKraken does not use this project at all. It looks like this would be used for command line git or other GUI's that utilize the system's installed git (which GitKraken does not use),
Same for me. It keeps doing it not matter which repo im trying to open,Bitter Frustration
This is also happening for me also. When will you guys have a fix for this.,Impatience
@aboone2000 They will probably have a fix when you stop to post in the first forum you see and provide a reproduction case in their slack channel. And also if you ask them more kindly than what you just did....,Mocking
@pmiossec whats the problem. is there a fix or not. I am using this globally in my company and personal.,Impatience
">whats the problem. is there a fix or not.
Perhaps you are a little long to understand...
I don't care!
**You are in a forum not related in any way to gitkraken.**
Perhaps you should improve your skills to find the support place of a software and not necessarily dumbly post in the first result provided by Google...
But it seems you are not the only one with this lack of skill if it could console yourself.",Mocking
"For me this happen on Fedora 28. As much a I found is that this reboot problem happen only with git repos which use SSH. With HTTP repos everything works fine. As much as I could see it that the system crashes happen because of OpenSSL. IMHO it could be, because of a new version of OpenSSL.",
"Omg, do you people have basic reading skills? This GitHub repo is not related to GitKraken in any way.",Bitter Frustration
"This patch accommodates some breaking changes introduced with MySQL 8.
Closes #1959 In a nutshell, the `caching_sha2_password` plugin (which is used by default since MySQL 8.0.4) hashes the password using SHA-256 and, after a first successful authentication attempt, saves it in a cache. That first attempt needs to be done under one of two conditions. The client either uses SSL and sends the password as clear text or it encrypts the password using the RSA public key generated by the server. On any subsequent attempt, until the password is somehow removed from the cache or the server shuts down, these rules no longer apply.
The handshake process remains unchanged when connecting to any server with version lower or equal to `8.0.3`. Whereas for `8.0.4` or above, the process is now the following:
- the client sends a `ClientAuthenticationPacket` with a scramble computed using a SHA-256 hash
- if the password is not cached, the server sends back a `PerformFullAuthenticationPacket`
- if the client uses SSL, the password is sent to the server (as clear text) via a `ClearTextPasswordPacket` to which the server replies with a `OkPacket`
- otherwise it uses the server authentication public key compute the scramble, sending a `AuthSwitchResponsePacket` to which the server replies with a `OkPacket`
- if the client does not know the server public key (is not provided by the user), it requests it from the server, which sends it back using a `AuthMoreDataPacket`
- after a first successful authentication attempt, and until the password is cached, the server will reply to the initial `ClientAuthenticationPacket` with a `FastAuthSuccessPacket` (which basically just signals that an `OkPacket` will follow)
If the account is created using the `mysql_native_password` authentication plugin, the client will just fall back to the ""traditional"" process during the handshake, keeping compatibility, by default for any previously supported server version.
MySQL 8.0.2 disables the `local_infile` server variable by default, which breaks a couple of integration tests. The tests were updated to enable the feature by themselves (something that does not have any effect on older server versions and allows the tests to pass with newer versions).
Additionally, one of the integration tests was updated to avoid failing after the first run (using any server version) since it tried to create a table that already existed from the previous runs.",
"This is awesome, thank you soo much! The only real issues I'm seeing with the PR to continue to work on is the following:
- [ ] Get support working on Node.js 0.10 and below, or we can decide if this PR should be shelved for the next major release as breaking change to drop those Node.js versions. Being a widely-used base driver we try to support as many Node.js and MySQL versions as possible at once to make it easy to use for everyone.
- [ ] There may not be enough coverage of the new code added, as the coverage with the PR is decreasing. There are probably some areas that can use tests.
- [ ] MySQL 8 image needs to be stood up on the CI server to test against. This is done in the `.travis.yml` file similar to our current MySQL / MariaDB matrix.",
"To address your points.
Regarding the first one, I've missed those tests (sorry), but as far as I can tell there is no sane way to use RSA public key encryption on `node <=v0.12.0`. There are some packages on npm that try to add support through native addons, but I think that might not be a good idea here, since it's pure JavaScript. So the only easy way to work around that is to require users on those older `node` versions to use SSL when connecting to new MySQL versions. If you are OK with that, we can go that route, otherwise, going in the next major version is probably the best possibility (I guess that's mostly your call).
Missed the coverage as well, I'll try to fix that ASAP.
Again, missed the CI config. Should be fine, at least on Travis. However, besides that, I suggest we also create a test account to run the tests (besides `root`) to make sure the handshake works fine for both empty and non-empty passwords, which lead to different behaviors. Any issue with that? Other than that, I guess on windows we are out of luck anyway since appveyor only seems to support MySQL 5.7, correct?",
"> Regarding the first one, I've missed those tests (sorry), but as far as I can tell there is no sane way to use RSA public key encryption on node <=v0.12.0. There are some packages on npm that try to add support through native addons, but I think that might not be a good idea here, since it's pure JavaScript. So the only easy way to work around that is to require users on those older node versions to use SSL when connecting to new MySQL versions. If you are OK with that, we can go that route, otherwise, going in the next major version is probably the best possibility (I guess that's mostly your call).
Ah, very sad :( Did you do a through search on this (it sounds like it)? Because I can always help if you want to focus on the other parts, but just asking before I duplicate the search for no reason. If we really have to drop supported versions, we can, and it wouldn't particularly delay this, just wanted to make sure it was actually required to do so before saying we should :)
> Again, missed the CI config. Should be fine, at least on Travis. However, besides that, I suggest we also create a test account to run the tests (besides root) to make sure the handshake works fine for both empty and non-empty passwords, which lead to different behaviors. Any issue with that? Other than that, I guess on windows we are out of luck anyway since appveyor only seems to support MySQL 5.7, correct?
Yea, so in Travis CI it just uses a Docker image. You can make multiple non-root accounts too; the current setup was just what was needed for testing, so if MySQL 8 requires more elaborate setup, that sounds fine to me. I would ignore AppVeyor.",Impatience
"I'm using `crypto.publicEncrypt()` which was added in `v0.11.14`, and it uses native bindings which I assume were also added around that time. There are some libraries on npm (e.g. [ursa](https://github.com/JoshKaufman/ursa) and [dcrypt](https://github.com/dekz/dcrypt)) that claim support for earlier node versions, however they seem to implement their own native bindings, which I guess would, sort of, defeat the purpose of this module (or maybe not, you tell me).
There's this [node-rsa](https://github.com/rzcoder/node-rsa) thingy which apparently is pure JS, but I couldn't make it work and it's pretty slow. I've also seen other [approaches](https://github.com/ragnar-johannsson/rsautl) like delegating work to openssl via a child process, but that's probably not a good idea either, since it introduces a huge moving part. There's also this [forge](https://github.com/digitalbazaar/forge) kit which brings the entire browser kitchen sink with it and I also haven't managed to figure out how it worked.
In any case, most of these things, when they work, it's usually on node `>=v0.10`, and I guess this module is supposed to support at least `v0.6`. So, no good news from my side. However, if you can do your own research, that would, of course, be great. I'm not a cryptography expert, but from what I can tell, the reason I have not found anything good is because this RSA thingy in pure JavaScript is still not really feasible (`number` precision, primes and what not).
My suggestion here is to fail with a client error (the `HANDSHAKE_SECURE_TRANSPORT_REQUIRED` introduced by this PR) when `process.env.version` tells us that we are below `v0.11.14`, which basically would be the default behavior using `{ ssl: false, secureAuth: false }`. It's a pretty naive solution but at least it's not really a breaking change, because existing implementations will keep working as expected with MySQL 5.7 and, let's be honest, people wo don't upgrade from those ancient node versions will, almost for sure, not upgrade MySQL to series 8 as well.
However, if you still feel you are better off bumping the major version, that's fine.",Bitter Frustration
@dougwilson what's your preference about the issue I mentioned? Do you want me to add the workaround for older node.js versions or do you think it's better to have a major version bump (in that case I guess all we need is to remove the CI setup for older versions)?,
"Sorry I didn't reply. Your comment on a new issue reminded me of this. Yea that works: if the server requires an auth that the client can't do we can error out. We should feature detect that, though, not sniff the version string.",
"Ok. Just to be sure, are you talking about sniffing the Node.js version string, MySQL version string, or both? Because we have two different issues right now. Supporting MySQL 8.0.4 (previous DMR release) which expects the password to be encrypted with the server public key using `RSA_PKCS1_PADDING`, whereas since 8.0.11 (the new GA release), the encryption should use `RSA_PKCS1_OAEP_PADDING`. We can always retry the handshake, but the only way to avoid that is by ""sniffing"" the MySQL server version.
Regarding the Node.js encryption APIs, I guess we can always resort to duck typing to check if they are available (or not) instead of sniffing the version. So, no problem here.
Let me know if there is something else that bothers you about these or other issues. I'll try to push this forward ASAP. Maybe during the weekend.",
"Why do you have to sniff the server version? Shouldn't the server say which one it expects directly? I'm just asking because I think version numbers break down when we start talking about compatible things like MariaDB, right? They may share some version numbers.
I have done Node.js version sniffing before and when it broke the Node.js collaborators just said I should never have been version sniffing their versions and told me feature detect only. So if that is the official direction from there I would go with that.",Impatience
"The case where we might need to sniff the server version is to distinguish MySQL 8.0.10 (pre GA, first one introduced chaching_sha1_passwors) from MySQL 8.0.11 (GA). since 8.0.11 was released yesterday one also could not support 8.0.10, as nobody should be using that RC anyways.
jojannes
On April 20, 2018 1:38:35 PM GMT+02:00, Douglas Wilson <notifications@github.com> wrote:
>Sorry I didn't reply. Your comment on a new issue reminded me of this.
>Yea that works: if the server requires an auth that the client can't do
>we can error out. We should feature detect that, though, not sniff the
>version string.",
"Right. However, in this case we would be sort of ""whitelisting"" MySQL 8.0.4 specifically. I don't think MariaDB has this new authentication plugin (enabled by default or not), but I'm not entirely sure.
In any case, I believe the padding mode was probably a mistake, which was ""fixed"" for GA. So we can just default to `RSA_PKCS1_OAEP_PADDING` (the new 8.0.11 GA version) and optionally do a handshake retry for the specific case where `RSA_PKCS1_PADDING` is needed (MySQL 8.0.4 RC only). Or we can just simply ignore the RC version entirely, like @johannes suggested, by always using `RSA_PKCS1_OAEP_PADDING`.
This is currently my only open question. Other than that, not doing Node.js version sniffing makes perfect sense, so that is settled.",
"By the way, just to be clear, there was a straight jump from 8.0.4 (last RC) to 8.0.11 (GA).
https://mysqlrelease.com/2018/03/mysql-8-0-it-goes-to-11/",
I like the idea to either (a) just ignore the pre GA padding or (b) retry with the old padding because that would not involve the version sniffing :+1:,
"@dougwilson I've kept the option for providing a custom key padding, to allow users to exceptionally connect to a MySQL `8.0.4 RC` server. In any case, the default value is always `RSA_PKCS1_OAEP_PADDING` which means it will work, by omission, with the latest `8.0.11 GA` version.
I couldn't find a sane way to add an additional Travis CI test matrix for users with non-empty passwords though. I was constantly bumping into an authentication error (I believe it might be related to the `mysql` docker setup and environment variables mixup), but maybe you can give it a try.
Let me know if there is something else you would like to see in this PR. Otherwise, feel free to merge it! :wink:",Irony
"This is awesome, thank you. I've been reviewing it, and sorry I didn't reply that I was in progress. I did find at least one regression just in existing behavior so far, and pushed a test to cover this to `master` so we have it covered for the future as well. I noticed the change for the `ChangeUser` sequence no longer passes any of the options down to the `Sequence` constructor, which differs not from every other sequence. Not sure what the purpose of that change was, but it at least prevents the `timeout` option from getting passed down.
I'll look at what the fix is and push it up to your PR branch here ðŸ‘",
"The merge process last so long, and the pr is still not merged. So it there any alternative way to solve the problem of ""ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication authentication ""? I am nodejs startup and just want to connect to mysql 8.0 for demo.",Impatience
"@fan123199 everything should still work if you use the `mysql_native_password` plugin.
```sql
ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'YourRootPassword';
-- or
CREATE USER 'foo'@'%' IDENTIFIED WITH mysql_native_password BY 'bar';
```",
"In addition to what @ruiquelhas said, I also had to:
```
mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)
```
And then restart the server.
If you are using multiple MySQL users, ensure that you update all of them that are connecting from remote (ie, not localhost). In my case I also had to use `'%'` instead of `'localhost'`.",
Any updates on this? Unable to authenticate with MySQL 8,Impatience
"@ruiquelhas Sorry junior dev here but have some questions.
Im getting the:
`Error: ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client
at Handshake.Sequence._packetToError`
i tried the following solution you gave to somene else:
`ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'YourRootPassword';`
-- or
`CREATE USER 'foo'@'%' IDENTIFIED WITH mysql_native_password BY 'bar';`
And that fixes the issue for me but I have some questions.
Questions:
Is this a good fix that can be deploy to production or is this just a workaround for getting things working locally?
I am also assuming that what is happened is that **8.0.11 MySQL Community Server** is just not compatible with the npm mysql library right out the box at the moment. To make it compatible right out the box You have submitted this bug fix but it hasn't been merged to master yet, and that is the reason it still doesn't work for me right out the box even tho i have the latest version of the mysql dependency.
Is my assumption correct?
If I am correct about my assumption, is there a practical way to implement the bug fix to my codebase before it is merged into master? and if there is, is that something that would be advised, or good practice?
thank you so much.",
"@Osuriel
> Is this a good fix that can be deploy to production or is this just a workaround for getting things working locally?
The new default authentication plugin is more secure because it's using SHA256 instead of SHA1 to hash the password, but it's not available for MySQL versions below 8.0, which means most existing MySQL production instances are not using it as well.
> I am also assuming that what is happened is that 8.0.11 MySQL Community Server is just not compatible with the npm mysql library right out the box at the moment. To make it compatible right out the box You have submitted this bug fix but it hasn't been merged to master yet, and that is the reason it still doesn't work for me right out the box even tho i have the latest version of the mysql dependency.
>
> Is my assumption correct?
Correct.
> If I am correct about my assumption, is there a practical way to implement the bug fix to my codebase before it is merged into master? and if there is, is that something that would be advised, or good practice?
I don't know if it is a good practice or not, it mostly depends on your internal policies and processes (and how much you would be ""trusting"" the fork in this case), but you can install and link npm packages from a git repo even using a specific branch or commit.
```bash
$ npm install ruiquelhas/mysql#feature/caching-sha2-password
```",
Any update? Still unable to connecto to MySQL 8,Impatience
@ruiquelhas thank you so much! You just helped me become a better developer hannibwas yesterday! ðŸ’ªðŸ½ðŸ”¥,
@dougwilson Is this still planned to be merged? It obviously keeps accumulating conflicts while it is not :(,Impatience
"This PR regresses behavior and cannot be merged until fixed as I noted above. I volunteered to fix the PR but have not gotten to it. Anyone is welcome to fix the regression in the PR, though, and will merge.",
"@dougwilson Thanks for the update! Not sure I'll get to it in the nearby future personally, though (but it's a possibility).",Impatience
It's np. I did get it mostly rebased and fixed locally but didn't quite finish. I'll bring this close to the top of my current todo list.,
"> I'll look at what the fix is and push it up to your PR branch here
@dougwilson I was under the impression you would take over from there, and just assumed something else was blocking this. In any case, I'll gladly help to push this forward. Please, let me know what I can do.",
You are correct. I volunteered to fix the PR. I did get it mostly rebased and fixed locally but didn't quite finish. I'll bring this close to the top of my current todo list.,
"Alright, perfect. Let me know if there is something I can help with.",
I think you should keep track of this issue :) still on 8.0.,Impatience
"Hi all,
Thanks for your effort!
But please merge this PR. It's been around 8 months that we have this compatibility issue with mysql 8.
Thanks ðŸŽ‰",Impatience
am trying to make a script to search by user by entering in there first name it should display the detail of that user this interacts with the file email.xml please help me !!! i can get it to read the xml doc and display all users i just need someone to give some exmaple code or show me how the hell i am supposed to do this thanks :),Impatience
"Hello!
Thanks for asking this question! Your request for assistance using
Nokogiri will not go unanswered!
However, Nokogiri's Github Issues is reserved for reporting bugs or
submitting patches. If you ask your question on the mailing list, Team
Nokogiri promises someone will provide you with an answer in a timely
manner.
If you'd like to read up on Team Nokogiri's rationale for this policy,
please go to http://bit.ly/nokohelp.
Thank you so much for understanding! And thank you for using Nokogiri.",
"uuhhhh there was a bug with nokogiri its a scuffed gem ???? and annoying
and not very user friendly for something that claims to be user friendly
On Sat, Mar 17, 2018 at 1:52 AM, Mike Dalessio <notifications@github.com>
wrote:
> Closed #1736 <https://github.com/sparklemotion/nokogiri/issues/1736>.
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/sparklemotion/nokogiri/issues/1736#event-1525697311>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AjswfZK3YznF_VG0fzwDk6mYwB3YziqTks5te9GxgaJpZM4StTWR>
> .
>",Bitter Frustration
"# Issue type
- Questions about the server or its usage should be posted to the [users mailing list](http://freeradius.org/list/users.html).
- Remote security exploits **MUST** be sent to security@freeradius.org.
- [ ] Defect - Crash or memory corruption.
- [ ] Defect - Non compliance with a standards document, or incorrect API usage.
- [x] Defect - Unexpected behaviour (obvious or verified by project member).
- [ ] Feature request.
See [here](https://github.com/FreeRADIUS/freeradius-server/blob/v4.0.x/doc/bugs.md) for debugging instructions and how to obtain backtraces.
NOTE: PATCHES GO IN PULL REQUESTS. IF YOU SUBMIT A DIFF HERE, THE DEVELOPMENT TEAM WILL HUNT YOU DOWN AND BEAT YOU OVER THE HEAD WITH YOUR OWN KEYBOARD. # Defect/Feature description
## How to reproduce issue
```
udp
src: 34665 tls
dst: 1812 src: 37494
+----------+ +----------+ dst: 2083 +----------+ udp +-----------+
| +------------> | | | | src: 46774 | |
| radclient| | radsec +-----------> | radsec | dst: 1812 | freeradius|
| | | proxy | | proxy +------------> | |
| | | | | | | |
| | | | <----------+ | <------------+ |
| | <------------+ | | | | |
| | | | | | | |
| | | | | | | |
+----------+ +----------+ +----------+ +-----------+
```
I am writing a radsec proxy in go. When I created the topology shown above, everything works fine below v3.0.7 but post 3.0.7 radtest fails with:
```
radclient: Received reply to request we did not send. (id=119 socket3)
```
The `id` and `socket` number are printed the same as the request. Debugger shows that in the scenario above when it reaches the end of [`fr_packet_cmp`](https://github.com/FreeRADIUS/freeradius-server/blob/release_3_0_16/src/lib/packet.c#L79) the `dst` ports are different (`b->dst_port` is 1812 and the `a->dst_port` is some random number which corresponds to _none_ of the ports shown in the diagram above).
git bisect traced the issue down to commit e4eca2dab. It looks like the code used to set the reply port in case of `server_port > 0`, but after the commit started doing that only for TCP connections. From the commit log it looks like the intent was to reset just the source IP for TCP, but I'm not sure if not setting the _port_ as well was an unintended consequence or not.
(All versions compiled with `--with-udpfromto`)",Bitter Frustration
"That's weird. The code *should* set `packet->src/dst` properly.
In the end, I think the correct thing is to have the src/dst ip/port set in `fr_packet_list_recv()`, which solves the issue of having radclient track it. I'll push some fixes. Please test and verify that they work.",
"The other thing is that I'm *highly* suspicious that radclient can break. It's used by everyone for everything.
Further, the `fr_packet_list_find_byreply()` already sets the src/dst ip/port as appropriate. So nothing should be necessary.
I've pushed some cleanups. They work for me. But then, radclient always worked for me.",Bitter Frustration
"> AFAIU, the proxy itself should not need to rewrite any RADIUS attributes (it should not need to know the shared secret).
That might work... but likely not. The packets are signed with the shared secret. The User-Password attribute (among others) is encrypted with the shared secret.",
"> Usually, when waiting for a UDP reply the src port isn't taken into consideration. That's not how 99% of UDP protocols work. And definitely not how RADIUS works. This is all documented in RFC 2865.
> The radsec RFC does not mention that the proxy should be rewriting/translating attributes and recalculating the message authenticator.
That's how RADIUS works. This is also clear in RFC 2865. Nothing was changed in later RFCs.
Your proxy might work for one RADIUS client. It definitely won't work for multiple RADIUS clients. Or, it will, but they all have to use the same shared secret.
RFC 2865 explains that you find the shared secret by looking up the source IP of the packet. This means that all downstream servers will look up *your* proxy IP to get *one* shared secret. If the originating UDP clients use *multiple* shared secrets, it won't work.
And no, you can't change shared secrets per packet. And no, you can't change shared secrets per destination.
If you want to learn other UDP to TCP proxy issues, read RFC 6613. It explains all of these issues in excruciating detail.
TBH, you can't just write a ""UDP to TCP"" translator for RADIUS and expect it to work. You MUST implement the RADIUS protocol correctly. This involves reading, and following, the RFCs.
Any short-cut you take means that you have a toy implementation. It might work for simple tests, but it will fall over and die in any real-world environment.",Bitter Frustration
"So you admit that your implementation is wrong.
The *UDP* RADIUS clients will not have that shared secret. So simply taking UDP packets *as-is* and dropping them into a TCP / TLS connection will violate RFC 6614.
You MUST implement RADIUS correctly. This means decoding and re-encoding all packets in/out of your proxy. Anything else is wasting everyones time.",Bitter Frustration
"> Unless the very act of being wrong about something related to radius makes me insufferable.
This is projecting.
The *engineering* point (not an emotional one) is that you MUST read the specs and follow them. Taking short-cuts, and then *arguing* when told those short-cuts are wrong is simply the wrong thing to do.",Bitter Frustration
"You didn't ask a follow-up question. You quoted RFC 6614 in an attempt to prove me wrong.
This is changing the subject, and nit-picking. Then you made inappropriate comments about being ""offended"" and ""insufferable"".
If you don't want to make it personal, then don't make it personal.
Now stop arguing. Any further responses will result in you being blocked from the FreeRADIUS repo entirely.",Bitter Frustration
"With my aging eyes that orange color is increasingly hard to read against a black background, and there is no way to change it. Yes I can copy and paste it into another editor, but that shouldn't be necessary just to see a simple message.
This is bad for accessibility and hard on your users. Please change to something more readable",Bitter Frustration
"I agree, e.g. light green (lime) would be much more easy to recognize!",
@jockm any thoughts on the colors used in the last screenshot of #3383 would be welcome. the theme works with 1.9.0-beta (just not the highlights),
"I like the highlights, I think the comment color is fine, if a little subtle. I think the more traditional soft green would be better. However the error color is just as bad as it was before
---
Anything with a heavy component of red or blue is harder to read. The human eye has the fewest cones (the elements of our eyes that see color) for red, followed by blue, and the most for green. So that orange â€” no matter the shade â€” is taxing to read, even for people with good eyesight. Factor in people with vision disabilities, or even just people getting older; and any shade of orange is going to feel downright hostel
I mean the the app is short on accessibility as it is, every bit helps.
There is exactly nothing wrong with making the error text white. It is completely clear that it is the error text from context; but most text is black on white, or white on black for a reason.",Bitter Frustration
@jockm thank you for the input. I [changed the colors](https://raw.githubusercontent.com/lmihalkovic/darkduino/master/sample.png),
"Here is the thing: the error text is still orange. Aside from tradition there is no good reason for that, and clearly tradition doesn't matter considering the changes you made on the code area.
So please just make the error text white on black, or black on white. The color coding on the file:line part is fine.",Bitter Frustration
"Hello, thank you for your input. I have attached a picture with the change in error color from orange to white. If this is what you are asking for, I can make a pull request and see if it gets accepted. I can also show you how to change the color if you are running on command line/ terminal in case it does not get accepted. <img width=""500"" alt=""ffffff"" src=""https://user-images.githubusercontent.com/35776802/39697890-93a799c0-51c0-11e8-900d-a7c349fd1f90.png"">",
"I'm very much against changing error text color to white. It's important for errors and warnings to be visually distinct from the rest of the output. I'd hope there could be a compromise that can improve accessibility for people with visual disabilities without greatly worsening the user experience for everyone else.
As Alogitt said, it's quite easy for anyone to customize the Arduino IDE's theme to their individual preference. I have attempted to document all the theme settings, as well as provide a sketch for testing themes here:
https://github.com/per1234/ThemeTest/blob/master/README.md
Seeing as it doesn't satisfy @jockm, I wonder if there is actually a need for https://github.com/arduino/Arduino/pull/7551? Is there anyone who has trouble seeing the current red color who prefers the orange color used in that PR? I have no strong preference between red and orange.",
"@per1234 Personally, I believe the bright orange color is a bit easier to see against the black background than the original color while still conveying the warning that it is supposed to. I don't have a strong preference; just trying to help out.",
"I think the white is perfect.
@per1234 The error text is 100% visually distinct in that screenshot. It is below the orange bar, and has a different background color than the code. It is distinct.
Additionally I concur that there is no need for #7551, if the goal of it is better readability.",
"I disagree about orange or red - both is invisible to me!
Please make it WHITE, YELLOW or LIGHT-GREEN (LIME) !
I agree with jockm (""The error text is 100% visually distinct in that screenshot. It is below the orange bar, and has a different background color than the code. It is distinct."")
Perhaps you can provide a setting in the preferences window about the text color.
IMO a pull request would not help, it must always be in the main branch for std Arduino downloads for all future releases.",Impatience
I will switch to white in my own theme.. and leave the highlights the way they are now forgot the colors but they are ... high contrast),
"There's an objective way to measure where certain color changes are acceptable such as **[this tool](https://webaim.org/resources/contrastchecker/)**, which checks for W3C's [WCAG 2.0](https://www.w3.org/TR/WCAG20/) compliance in the contrast between a background color and a text color.
In the case of PR #7551, the brighter orange (`#FF8300`) proposed passes in all cases, and the darker orange (`#E34C00`) doesn't pass the WCAG AAA for normal texts case.
Arduino users have grown to instantly spot orange as error messages, and I think it's appropriate to keep it so as to not break UX and, IMO, it's is the best color for representing errors in a black background after red.
Just please don't make errors green or white, one would be REALLY counter-intuitive and the other, while easy to see once found, blends with all the other output printed in the log, which IIRC is also white.",
"@feikname That only checks color contrast, there are other issues with vision and color that just contrast. Error messages are second only to the source in terms of the need for readability. Can we err on the side of an abundance of caution here?
But I am going to argue with you that that white is somehow counter intuitive for error messages, or that the orange is deeply tied to your branding. You are going to have to present some data to justify that argument. I don't think most people would notice the change, except for finding the text easier to read.
Please refer to: http://www.ldau.org/ld-facts/guidelines-for-font-size-color/",Impatience
">@per1234 The error text is 100% visually distinct in that screenshot. It is below the orange bar, and has a different background color than the code. It is distinct.
@jockm I wasn't talking about the code. I was talking about standard output in the console. It seems you are missing the fact that the console may display more than only warnings and errors. Standard output has always been printed in white text in the console. Warnings and error messages have always been printed in red. The difference in colors allows us to quickly notice the critical parts of the output. If you make standard output and error/warning text both white that means I need to read the entire output to find a warning or error message.
![clipboard02](https://user-images.githubusercontent.com/8572152/39745667-c35cfd08-525c-11e8-9c7b-aff661acc4e0.png)
>I am going to argue with you that that white is somehow counter intuitive for error messages
For years white text in the console has meant standard output so making error/warning text white is very much counterintuitive.
>I don't think most people would notice the change
What we'll be less likely to notice is helpful warning messages.",Bitter Frustration
"I strongly disagree with per1234.
Red or even orange on a notebook in a bright environment (e.g., outside) is not readable to my eyes, not even your screenshot above, and even in a darker environment it's hard!
Make the error messages WHITE - or at least YELLOW or LIGHT-GREEN (LIME) !
- or provide a setting in the preferences window about the text color so it can be easily customized!",Bitter Frustration
"@tofrnr you just repeated your previous comment. Please only comment if you have something new to contribute to the discussion.
Remember that this behavior of yours already caused another discussion to be locked:
https://github.com/arduino/Arduino/issues/5033#issuecomment-373317213",Impatience
"@per1234 No I knew exactly what you meant, and I am not against making error messages visually distinct. I am against doing it with colors that can be difficult to see.
I am a 51 years old guy, with dyslexia and just the barest signs of macular degeneration and I cannot read the text in your screenshot without great difficulty. Make it more than a line or two and the only way I can cope is to copy the text and paste it into a text editor. I know I am not the only one from my own conversations.
That is a broken UX
By all means, make errors visually distinct. You could use bold or italics, you could make the words ""Error"", ""Warning"", etc a different color (which I mentioned as a possibility previously), or you could use other forms or text decorations. You have options other than simply changing the color.",Bitter Frustration
Seems like making error texts **bold** and modifying the error color to the brighter orange could be a good compromise.,
"@feikname Bold might be a step in the right direction, but you still have the problems I outlined with red heavy colors. If we must have red/orange, why not put a ""ERROR:"" prefix to the message and make that red/orange, and then let the actual message be one of the two most readable colors we have (i.e. white).
Because in the end, if given the choice between branding or usability (in this case readability), I think we should pick the latter ever time",Bitter Frustration
"> If we must have red/orange, why not put a ""ERROR:"" prefix to the message and make that red/orange, and then let the actual message be one of the two most readable colors we have (i.e. white).
I do not think this is a good idea. The text that is currently printed in a distinct color, is the text that was printed on the ""stderr"" output of subprocesses. Typically, this contains errors, but this is not necessarily so. For the compiler, it is also used for warnings. Avrdude even prints all of its verbose output to stderr. Prefixing each line with ""ERROR"" or something similar would probably only confuse users. More correct would be to prefix each line with ""stdout"" or ""stderr"", but that would still confuse novice users who have no clue what these terms mean.
The current approach of using a different color seems to bypass this problem by not explicitly stating what the colored text means, it just draws more attention to it.
As for using a different color to improve readability: That seems like a viable option to me. Of course, using a red or red-ish color for errors (and some other stuff, as pointed out above) seems common, which is probably why it is done now. Using e.g. green would also draw attention to the text, but I *think* users would not expect green text to contain error messages, so it might be better to consider other colors. @tofrnr also suggested a yellow color, which could work IMHO.
As for making things bold: I'm not sure if that would stand out sufficiently, but that might just need an experiment perhaps?",
"> Prefixing each line with ""ERROR"" or something similar would probably only confuse users. More correct would be to prefix each line with ""stdout"" or ""stderr"", but that would still confuse novice users who have no clue what these terms mean.
What I was implying was that there would be some parsing of the text to inject ""Error:"" before error messages, and maybe ""Warning:"" before warning, etc. I am sorry if I wasn't more clear.
> The current approach of using a different color seems to bypass this problem by not explicitly stating what the colored text means, it just draws more attention to it.
And yet I can point to people who are having trouble reading the error text, and have already pointed to two sources that say color is bad when the goal is readability â€” and error messages are already hard to read if in white.
We have no data on how many people care about colors vis-a-vis branding, have problems with them, or who do have problems with them. Nor is there a good feedback mechanism for the average user to use.
Don't we want to make the Arduino IDE as easy to use for the greatest number of people as possible? This includes making it as accessible as possible. So until and unless there is a theme editor that is directly part of the Arduino IDE along with a set of themes; what is the harm in making errors _more_ readable?",Bitter Frustration
"> What I was implying was that there would be some parsing of the text to inject ""Error:"" before error messages, and maybe ""Warning:"" before warning, etc. I am sorry if I wasn't more clear.
The problem there is that it is hard to reliably decide whether something is an error, warning, etc. Hardcoding regexes might work for (certain versions of) gcc, but third-party cores can also use different compilers and tools. Making these regexes configurable per core/board would be the correct approach to this, but also a *lot* more work.
> > The current approach of using a different color seems to bypass this problem by not explicitly stating what the colored text means, it just draws more attention to it.
> And yet I can point to people who are having trouble reading the error text, and have already pointed to two sources that say color is bad when the goal is readability â€” and error messages are already hard to read if in white.
I was not trying to say that the current approach is free of any problems, I'm just saying that using color to distinguish between stdout and stderr does not have the problem of labeling all stderr output explictly as ""error"".",Bitter Frustration
"> I'm just saying that using color to distinguish between stdout and stderr does not have the problem of labeling all stderr output explictly as ""error"".
Then it could be something other than ""error"". I mean it would be wrong to use ""stderr"", but it could be anything. As an off of the top of my head suggestion ""Important:""?
You put that in some color, and then put the text to be read in white",
"@jockm i totally agree with you that the color for errors of orange/red whatever should be a more distinct color for those with difficulty in reading.
So i have put together some information on changing some parameters in theme.txt that will make the text in the IDE more easily read.
![errormessageinarduinoide](https://user-images.githubusercontent.com/2259579/46522320-cc823d00-c8c5-11e8-8330-0142ec204f54.png)
[changing_error_color.txt](https://github.com/arduino/Arduino/files/2449326/changing_error_color.txt)
other information https://github.com/per1234/ThemeTest",
"# Using Zip files for themes
## Hexcode for colors
http://www.javascripter.net/faq/rgbtohex.htm
https://www.rapidtables.com/web/color/green-color.html
## File locations:
* on mac:
Arduino.app/Contents/Resources/Java/lib/theme/theme.txt * on Win7: C:\Program Files\Arduino\lib\theme\theme.txt
* on Linux: .../arduino-1.6.3/lib/theme/theme.txt
* on Windows 10: when installed as an App, C:\Program Files\WindowsApps\ArduinoLLC.ArduinoIDE_1.8.10.0_x86__mdqgnx93n4wtt\lib\theme\theme.txt, however it does not seem possible to edit it - it can be opened in notepad++ but a save fails.
## Procedure Using zip application create a zip file containing the original theme.txt found in the location of your operating system.
call it theme.txt.original.zip
Now modify the theme.txt file for colors, font sizes, etc
using zip application create a zip file containing the edited theme.txt found in the location of your operating system.
call it theme.txt.custom.zip
In the Arduino IDE goto the menu Arduino-->preferences under the option theme select theme.txt.custom.zip from the drop down list and restart Arduino IDE and the custom theme will take effect.
Copy the theme.txt.custom.zip file to a safe location so it will be available when there is an upgrade or reinstallation.
## Some suggested changes
changed console.font to 18
changed console.error.color to #7CFC00 - lawngreen
# GUI - CONSOLE
console.font = Monospaced,plain,11
console.font.macosx = Monaco,plain,18
console.color = #000000
console.output.color = #eeeeee
# original orange #E34C00
# lime green #BFFF00 # lawngreen	#7CFC00
console.error.color = #7CFC00
changed editor.comment2 to #434F54,bold - dark grey and bold
# TEXT - COMMENTS
editor.comment1.style = #434F54,bold
editor.comment2.style = #434F54,bold
Increased linestatus.font to 14
increased linestatus.height to 30
# LINE STATUS - editor line number status bar at the bottom of the screen
linestatus.font	= SansSerif,plain,14
linestatus.height = 30
you could also copy the attached file to the theme directory
[theme.txt_visually_improved pacav.zip](https://github.com/arduino/Arduino/files/2449671/theme.txt_visually_improved.pacav.zip)",
"@pacav69 in Arduino IDE 1.8.6 and newer, themes can be placed in the `theme` subfolder of the sketchbook folder. You can find the location of the sketchbook folder at **File > Preferences > Sketchbook location**. This way, custom themes will persist through updates to a new IDE version. This should also make it possible to use custom themes with the Windows App Store version of the Arduino IDE:
https://github.com/arduino/Arduino/pull/7115
You can put multiple themes stored in .zip files in that folder and select which one you want from the **File > Preferences > Theme** menu. Note that the theme files must be directly under the .zip file, not in a subfolder of the .zip file:
https://github.com/arduino/Arduino/pull/7124",
"OK it has been two years now and as I slowly develop cataracts I can read everything in the arduino IDE and yet I cannot read the error color. The idea that there is sufficient contrast or that it is accessible is just wrong. Nor should I have to load a theme just to be able to see errors.
If you insist on keeping the error color there needs to be an option in the preferences dialog to *easily* change the color for those who find it unreadable, painful, etc",Bitter Frustration
"I second most of the remarks above. Errors are unreadable, and this is really bad when you are with a laptop in a bright location. I always have to copy to notepad to see them.
It is not possible that in all these years you have not fixed this that is really the ONE SINGLE thing that is NOT OK in the standard interface.
It is really BAD that there is not a simple setting to change the error color (say to white - there is already the ORANGE bar when there are errors, maybe you may put one red square char at line start if you want to permanently mark in some ways the error, no matter the color the end user chooses).
I tried by the way to change theme.txt including
console.error.color = #ffdddd
but errors keep to be that Â£%Â£$ng orange!!",Bitter Frustration
It has been more than 4 years since I first brought this up and nothing has been done. It is disheartening that the app remains unfriendly to people with visual processing issues for what seems like the sake of tradition,Bitter Frustration
"and the change can be optional, I think this the only bad issue I've found till now.
Ok it's not a interface as sophisticated as Delphi, but it can work.
By the way I have finally been able to fix it the hard way: it looks that this damm Win10 blocks theme.txt modification even if you are admin and modify permissions (don't ask me why), so you copy theme.txt to another folder, edit it, delete the original, copy it back.
For me the file was in C:\Program Files (x86)\Arduino\lib\theme
(arduino installed today normally from the download)",Bitter Frustration
"@aldoaldoaldo Agreed, if there was simply a ""visually impaired"", or ""high contrast"", etc option so you could *easily* and *simply* get to a mode for people who have a hard time with the existing color screen it would make this issue go away",Impatience
"Thanks all for your care and input about this important subject. I think the best way forward is to focus on the accessibility of the actively developed Arduino IDE 2.x project.
The discussion here got into implementation details which are specific to the Arduino IDE 1.x code base, and not relevant to Arduino IDE 2.x, which is a complete rewrite in a different programming language. So it will be best for you to create a new issue in the Arduino IDE 2.x's repository:
https://github.com/arduino/arduino-ide/issues",
"@per1234 And how many more years must we wait for accessibility to be a
consideration in 2.0? My problem is that this has been sat on long enough for you to say lets sit on it longer. Does 2.0 have a release date? How long will 1.x be supported after 2.0 is out? Why wait any longer for the simple option of a high contrast/colorless option for people with visual disabilities?
I do not say this lightly: it genuinely feels like you do not care, not in an active hostile way, but in the soft neglectful way that someone who doesn't actually understand the problem and thinks it is fine asking people underserved to wait even longer",Bitter Frustration
"@jockm this sort of negative uncompromising attitude is counterproductive. If you only want to vent, do it somewhere else. If you actually want to work together to make this free open source project better, you need to find a different approach.",Bitter Frustration
@per1234 and that is the kind of ablest rhetoric that is used when you don't understand the issue. Do better,Bitter Frustration
"I migrate a mysql db to sqlAnywhere(sqla) to test my app developed with Doctrine2.
Acces to sqla works,
DQL like
$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng );
$query->setParameters(array(
'lng' => 'DE'
));
$result = $query->getResult();
also runs funny.
$query = $em->createQuery('SELECT e FROM countries e WHERE e.connum = :num );
$query->setParameters(array(
'num' => '4'
));
$result = $query->getResult();
also no problems!
## BUT!
$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');
$query->setParameters(array(
'lng' => 'DE',
'num' => '4',
));
$result = $query->getResult();
doesn't work!!
$result = empty!
with operator OR it's the same problem!
Any idea!
# #",Bitter Frustration
What's the executed SQL query? Can you run that manually and verify the output? Also make sure the parameter types match.,Impatience
"Interactive sql works!
![grafik](https://user-images.githubusercontent.com/38318899/38720441-91189f92-3ef6-11e8-9d20-b188b7e0eb70.png)
But if i change my test-pgm to this ..
![grafik](https://user-images.githubusercontent.com/38318899/38721092-197090d2-3ef9-11e8-9c87-2db46a9e2065.png)
look at the sql statement!
## SELECT e FROM countries e WHERE e.conlng = :lng or e.connum = :num
and the result in the sceenshot.
It woorks with an OR operator??!
## I'm very irritated !",Bitter Frustration
"> `SELECT e FROM countries e WHERE e.conlng = :lng or e.connum = :num`
Is this the same SQL statement that the ORM runs?
Use a [`DebugStack`](https://github.com/doctrine/dbal/blob/0f23ed9ba28db2b392eeaaf5938ce804e52084b9/lib/Doctrine/DBAL/Logging/DebugStack.php) logger to see if the SQL statement is the same.
The one difference I noticed is that sometimes your parameters are `string`, sometimes `int`: try looking for differences there as well as in the executed SQL string.
> I'm very irritated !
This is not a support hotline: if you need to get irritated over volunteers helping you out, you can instead hire somebody to help you out, and be (contractually) allowed to be irritated instead.",Insulting
"![grafik](https://user-images.githubusercontent.com/38318899/38734156-d37fd23c-3f24-11e8-8f14-eae024415161.png)
The result ofDebugStack!?",Impatience
@winnilein can you expand on `params` and `types` there?,
![grafik](https://user-images.githubusercontent.com/38318899/38742174-1fc9735c-3f3c-11e8-9ea9-dc64dbcc730b.png),
"Both are bound as `string` there - can you try passing in an integer and running the SQL statement manually with either `string` or `int`, and see if there is a difference?",Impatience
"native sql..
![grafik](https://user-images.githubusercontent.com/38318899/38754033-914eaabe-3f60-11e8-90ea-475087e3c644.png)
and the result of my script..
![grafik](https://user-images.githubusercontent.com/38318899/38754086-c5245d48-3f60-11e8-87b9-fd36462da3c9.png)
my script..
```
<?php
include ""Bootstrap.php"";
require_once 'Entities/countries.php';
error_reporting(E_ALL);
$stack = new \Doctrine\DBAL\Logging\DebugStack();
$stack->enabled;
$em->getConfiguration()->setSQLLogger($stack);
$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');
$LNG = 'DE';
$NUM = 4;
$query->setParameters(array(
'lng' => $LNG,
'num' => $NUM,
));
$result = $query->getResult();
if (!$result) {
echo 'Noting found!';
} else {
foreach ($result as $row) {
$TXT=$row->getContxt();
echo $TXT; } } echo '<pre>';
\Doctrine\Common\Util\Debug::dump($stack, 5); ```",
"## For your entertainment..
`$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');`
`$NUM = 4;`
result ->
![grafik](https://user-images.githubusercontent.com/38318899/38754410-01e1203a-3f62-11e8-8c54-a62626a5d075.png)
## and now !
`$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng or e.connum = :num');`
` $NUM = '4';`
result..
![grafik](https://user-images.githubusercontent.com/38318899/38754495-627ebbbe-3f62-11e8-9078-388115a1b4fe.png)
f****** web apps
i think i continue programming midrange applications",Bitter Frustration
"Closing: nobody's spare time is worth following up on this attitude.
If you don't have the willpower to contribute to this conversation in a constructive way, then please consider hiring somebody doing it for you instead.",Bitter Frustration
"### Description of Issue/Question
On Arch Linux running `pkg.install vim refresh=true` will upgrade the entire OS instead of just installing the latest version of vim. This does not follow the other package managers and becomes unintuitive when dealing with multiple Linux distros. This behavior is documented. However, It doesn't follow the standard that other pkg modules use for full os upgrade.
### Setup
run `pkg.install vim refresh=true` want vim, get os upgrade.",Bitter Frustration
ZD-2445,
"Yes, that is correct That should be the defaults on arch, if you refresh, you need to do an upgrade, otherwise you end up with broken so names because pacman does not resolve upgrades to sonames and force other packages to upgrade.
If you want to do only a refresh and install, and not upgrade you can end up with an unbootable system, but you can do this by setting `sysupgrade=False` on the commandline with the `refresh=true`",
https://gist.github.com/vodik/5660494 Here is the story that archlinux maintainers and people in #archlinux on freenode give to people that -Sy instead of -Syu.,Bitter Frustration
"By not doing what the command line tools do, you are varying from the expected behavior and breaking convention. THIS IS WRONG.
Your belief that you're doing anyone a favor by varying from the expected command line tool behavior, you should turn in your commit per.issions and get a job in management where you don't have access to break things AGAIN.",Bitter Frustration
"We could make `sysupgrade=False` the default, but then we'd have a lot _more_ people complaining about broken systems from unsupported partial upgrades (though, likely with far more tact than you displayed).
Please read this [article](https://wiki.archlinux.org/index.php/System_maintenance#Partial_upgrades_are_unsupported) from the ArchWiki so that you can better understand the reason for making `sysupgrade=True` the default.
Have a nice day!",Mocking
"Next month (May 2018), a new EU regulation will come into effect, concerning the privacy/ data protection of user data. This new regulation comes with very high potential sanctions: up to 20 Million EUR or 4% of a years revenue, depending on which is _higher_.
This also poses some questions on whether DokuWiki is GDPR-compliant (and what does that even mean?):
- [ ] If a user deletes their account, do we have to delete their username from changelogs/meta?
- Question asked here: https://law.stackexchange.com/q/27795/17677
- [ ] Would that potentially conflict with the license, if that license requires attribution?
- [ ] Do we have to delete IP-addresses from changelogs, maybe after some time?
- [ ] Can we do that in a way that still lets us tell different anonymous users apart from each other?
- [ ] Do we need to show some message / do we need some legally worded user opt-in for some things? (Subscriptions?)
- [ ] Do we need a Privacy Statement for DokuWiki.org?
- [ ] should we provide examples other users can adjust?
- [ ] would it be desirable to extend that with technology (eg. automatically list what data is collected when, why and how long) and provide a way for plugins to hook into that?
Maybe some of you have expertise in this matter or work in a company with legal resources to answer such questions -- Input would be greatly appreciated ðŸ™",
Do we have any donated money left to potentially pay a lawyer to help answer some questions?,Impatience
"Potentially yes. However since all this law *really* achieves is feeding lawyers, I would prefer to not contribute any more to that for ideological reasons ;-)
If anyone out there is already paying a lawyer to answer their GDPR questions, it would be nice if they could sneak in our questions though...",Irony
"As long as the logs only contain a user ID (ie a numerical reference to their user account) then you'd only have to hash their user details in the central account record (where the numerical user ID, user name and email address are associated) to remove the ""personally identifiable information"" (Pii). If the user account is ""deleted"" by marking their account as deleted and replacing their Pii with a one way hash of the information in their account details then there is no longer anything that personally identifies them. The posts could be detected as having a hashed user ID and shown as authored by ""[deleted user]"". If the user rejoined the service with the same details then the a check on the new account's one way hash would match with the deleted accounts hash and the an question asked of the user ""Do you want to associate your previous posts with your new account?"" But I checked the logs and they contain the _actual_ user name, not a numerical reference. So that would be a real chore to update the logs (but potentially still possible).
And then of course you have the issue where the user's Pii is included in the body / text of the posts...
eg in a todo item where the username is listed",Bitter Frustration
"Even though thus is a EU regulation, the actual implementation is in national law so answers will vary across legislations. Starting with a privacy statement or agreement in which you explain what is collected and why and is accessible to whom and when it will be deleted is always a good start.",
"From some fresh readings, this regulation is a law that applies to all EU members with extraterritorial involvements (for outside EU working with Europeans), probably with some variation by land, but not so much (e.g. sensitive data ). To get an quick overview, WordPress has a [roadmap](https://make.wordpress.org/core/2018/03/28/roadmap-tools-for-gdpr-compliance) to address GDPR and its first [implementations](https://core.trac.wordpress.org/query?status=!closed&keywords=~gdpr). It is quite complex stuff but seems logic and normal (see [Max Schrems](https://en.wikipedia.org/wiki/Max_Schrems ) ). Privacy and consent by design, right to erasure, personal data backup, encrypted data, ...etc, just good sense. At this stage, I am not sure a lawyer is necessary but companies would need to use a DokuWiki core and plugins GDPR-compliant.",
I started a privacy policy at https://www.dokuwiki.org/privacy -- keeping it understandable (as requested by the GDPR) and complete is quite hard. Any hint on what's missing is welcome.,
"> I started a privacy policy at https://www.dokuwiki.org/privacy -- keeping
> it understandable (as requested by the GDPR) and complete is quite hard.
> Any hint on what's missing is welcome.
I'm rewriting ours tomorrow. I'll take a look and make some suggestions.
UPDATE : The one at the above link seems to have most of it covered well. The additional sections that we have in ours are mostly to do with marketing which is most likely not relevant.",
"[off-topic]: @splitbrain: I assume this is the page do you mean at the top of https://www.dokuwiki.org/privacy ?!
I already corrected the link there.",
"An excellent article in Bozho's tech blog: [GRPD - a practical guide for developers ](https://techblog.bozho.net/gdpr-practical-guide-developers/). Also, the Drupal GDPR Compliance Team gives a lot of links on their [dedicated page](https://www.drupal.org/project/drupal_gdpr_team)",
"@splitbrain About the privacy page, I think you have to ensure somehow the users data is save and that how it is kept save has to be described somehow internally for accountability.
Serverlogs, do they contain a user ticket number that is related to their account and visible or knowable by google analytics, if so you should mention that possibly.
GDPR is not done something done by lawyers, but rather accountants, they check if their customers are GDPR compliant. Good starting point: https://gdprchecklist.io",
"Everyone, please refrain from posting more links to pages ""that explain everything"". If you want to help out, do one of the following:
* extend the privacy policy at https://www.dokuwiki.org/privacy
** feel free to ask specific questions for things you can not answer (eg. details of the server setup)
* post answers to the questions in the original post, with references to the exact text of the applicable laws",Impatience
"What about the cookie nag box? DW uses cookies, so DW would need such a box. I noticed that the bootstrap3 template offers to activate a cookie nag box, so why not integrate such a thing into the DW core?",
"@daumling Cookies which are required in order to fulfill the requests of the website visitor do not require explicit user consent. But any others â€” including those used for general use statistics eg tracking â€” do require it.
On our local dokuwiki as far as i can see the cookies are only functional to dokuwiki.",
"Not sure about that. Session cookies are considered personal data AFAIK, and consent is required. See e.g. this article: https://www.cookiebot.com/en/gdpr-cookies/ - it mentions session cookies.
There is a cookielaw plugin, but it is rudimentary.",
"AFAIK, https://www.dokuwiki.org/privacy is missing the required bits of [Information to be provided where personal data are collected](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679#d1e2254-1-1) GDPR Â§13, especially the mention of the data subject's rights.
Edit: Link to law replaced w/ official URL.",
"I updated the last part and renamed it to ""Your Rights"" that should make it more clear.
Regarding the ""where personal data are collected"" - is that referring to which country? That would be France (Hetzner's servers are located there). Not sure where to put that though.
@xrat can you make changes to the page where you think clarification is needed?",
"For your inspiration, I've edited Greebo (the installed release) to make the *DOKU_PREFS* cookie a session cookie. A session cookie means no permanent storage, so no user consent required.
The second commit removes recording of IP addresses from the logs. Quite some places need code removal, still the result works just fine. All new changelog entries no longer receive the IP address, so nothing can go wrong. Some retrocompatibility code for dealing with older records is also included.
As making a pull request on Github is a chore and Github refuses to accept patches, I made a Gist: https://gist.github.com/Traumflug/74fd0b4c8968fd0184e503d221b13310 with both patches.
With these patches applied the privacy statement reduces to about this (DokuWiki markup):
----
==== General Data Protection Regulation (GDPR) ====
We're neither interested in personal data, nor do we try to collect or use such data. In detail:
* Pages at reprap-diy.com do not use trackers.
* Visiting pages at reprap-diy.com stores up to three cookies in your browser to follow the session. These cookies get deleted when the session ends (when you close your browser).
* Creating an account at reprap-diy.com stores your email address, content of the //Real name// field and an encrypted hash of your password.
* Logging into an account and checking the //Remember me// checkbox stores another, permanent cookie (valid for one year) to keep you logged in. To remove this cookie, log out.
* Each page edit stores the username of the user who did the edit. This information cannot get removed, but if the related account was removed, it also cannot be mapped to an email address or other personal data.
* During page editing your IP address is used to lock the page against a competing edit. The address gets removed when the edit gets saved.
* Some of the pages on reprap-diy.com may contain external videos. For YouTube we use the ""privacy enhanced"" youtube-nocookie.com domain that will not track your visit. Your IP address will be visible to the server providing the video, though. * To view the data stored about you at reprap-diy.com, look at your [[start?do=profile|user profile]].
* To remove this data, go to your [[start?do=profile|user profile]] and delete your account.
----
VoilÃ¡, no user consent required, problem solved.
The only issue which might remain is fighting spammers. No IP address, no entry into blacklists. But we all have secured account registration against spammers, right?
----
In case somebody doesn't believe that session cookies need no user consent, he may have a look at this pretty official page: http://ec.europa.eu/ipg/basics/legal/cookies/index_en.htm#section_2. It states:
> Cookies clearly exempt from consent according to the EU advisory body on data protection include:
>
> * userâ€‘input cookies (session-id) such as firstâ€‘party cookies to keep track of the user's input when filling online forms, shopping carts, etc., for the duration of a session or persistent cookies limited to a few hours in some cases
> * authentication cookies, to identify the user once he has logged in, for the duration of a session
userâ€‘centric security cookies, used to detect authentication abuses, for a limited persistent duration
> * multimedia content player cookies, used to store technical data to play back video or audio content, for the duration of a session
> [...]",
"If I understand it correctly, I don't think we need to do anything about cookies for GDPR (apart from informing the user about them which is already handled by the privacy statement).
The only cookie which contains personally identifiable information is the `DW<hash>` cookie, and users can choose to delete that one at the end of each session by not ticking ""Remember me"". The `DOKU_PREFS` cookie is not used for personally identifiable information (although plugins could potentially abuse it).",
"Sorry for posting more links, but these are more relevant because this is how two big wiki projects handle GDPR:
* [GDPR in Wikimedia's issue tracker](https://phabricator.wikimedia.org/T194901) gives an overview but doesn't really say that much about what actually needs to be done and includes links to a lot of speculation. Like so many others, Wikipedia has changed their Privacy Policy. I'm not sure how they're (not) dealing with [public contributions](https://wikimediafoundation.org/wiki/Privacy_policy#your-public-contribs) is really compliant?
* [OpenStreetMap's GDPR Position Paper](https://wiki.openstreetmap.org/wiki/File:GDPR_Position_Paper.pdf) is more interesting as it seems quite thorough. Their process around [account removal](https://wiki.openstreetmap.org/wiki/Privacy_Policy#Account_Removal) is not as helpful for us, though, as they don't allow anonymous edits (and use a database which makes that task easier).",
"> The DOKU_PREFS cookie is not used for personally identifiable information
The sheer existence of a cookie means personally identifiable information, because they come with and IP address / DNS entry attached. Content doesn't matter, much less encrypted content.
> Sorry for posting more links, but these are more relevant
D'oh. Those pages pointing to some volunteering efforts are more relevant than an official page. Ouch.
I certainly see this GDPR panic mode everywhere. People try extremely hard to stick to what they're used to, providing endless text blobs in the hope to walk around the problem somehow with lawyer fineprint. Instead of simply fixing the software.",Bitter Frustration
"> The sheer existence of a cookie means personally identifiable information, because they come with and IP address / DNS entry attached.
I don't think this specific cookie comes with IP address and DNS entry attached. That cookie and its contents is not stored on the server but only in the browser.
> D'oh. Those pages pointing to some volunteering efforts are more relevant than an official page. Ouch.
No need to become personal, especially not dissing ""volunteering efforts"" in any Open Source project.
I meant it is much more relevant to us as in no-one else (apart from other version control software or services, like git or GitHub) deals with the one question which none of the official pages deal with: how to deal with user contributions that are intrinsic to the software.
> Instead of simply fixing the software.
If anyone of us would know what is needed to fix the software, we would do it. Can you point out what needs fixing? I don't think that is possible, most certainly not ""simple"".
I have the feeling no-one really understands any specifics about GDPR (and that includes the big guys like Google and Facebook). I think the majority of what's out there is misinformation.
I like how OpenStreetMap (who ""have received professional counsel"") say in their paper:
> Naturally estimating the impact of the GDPR introduction and consequences before it is
actually in force are fraught with the problem that we have to guess how the legislation will
be applied in practice and there is a danger of both over- and underreacting.",Bitter Frustration
"Changing `DOKU_PREFS` into a session does not fix anything but breaks the usability imho. `DOKU_PREFS` is used e.g. for storing the size of the edit window and this should persist across sessions imho. As Anika says, cookies do not store the IP (or even DNS entry) of the user, the IP address is instead sent with every request.
Not storing IP addresses is also not the solution as there is a very valid reason to store them at least temporarily: detect and remove vandalism (by IP address you can identify the connection between several edits, possibly even several user accounts) and to be able to identify the author (at least in court) if the content posted was illegal and the site owner gets sued because of that.
What I think would be a good thing is to have some automatic way to remove IP addresses after some time, at least for changes where the user has been logged in (this could be a plugin of course). For anonymous edits I'm not sure if the IP address can be interpreted as an author identification that needs to be stored because of the license (but this of course depends on the selected license).
Concerning the removal of the user name: my personal (non-lawyer) interpretation is that due to the license of the content (creative commons license at least with attribution), DokuWiki has a legitimate interest to store this attribution as it otherwise cannot use the content and as the Wikimedia issue tracker says ""the right of erasure only exists when the processing is not necessary for some legitimate interest of the data controller"".",
"Thanks @michitux for pointing out the usability aspects. I was just about to do that.
@selfthinker thanks for the links on how other wikis handle it. I'll have a look later.
Regarding removing IP addresses after while, there is now the aptly named gdpr plugin which does exactly that. It also replaces user names in change logs for deleted users.
I will close this ticket now. We will probably not ever get definite answers to all the questions asked in the original issue. And it's an issue people love to discuss for the sake of discussing without getting any further.
For now we should simply focus on having a useful privacy policy for the 0.1 percent of users who care about that. So please, if you think the privacy policy needs adjustments just go ahead and do it.",Bitter Frustration
"> I don't think this specific cookie comes with IP address and DNS entry attached. That cookie and its contents is not stored on the server but only in the browser.
It's the very nature of any cookie to come with IP address or DNS records attached. All of them are stored in the browser, only. Still GDPR considers them to be personal data, which is why they have to become session cookies or ask for user content before being placed. Fairly simple basics.
> If anyone of us would know what is needed to fix the software, we would do it.
Code is provided above. Instead of looking at the code and commenting it, all the extensive comments sum up to ""Go away, we have to find a harder way"".
Very apparently, some people here *want* to stick their head in the sand. Instead of applying these patches and enjoying a GDPR compliant wiki. Enjoy it!
And I just see how you closed the issue to make extra sure nobody sees this solution. Extra compliment to that much stupidity!",Bitter Frustration
"> would it be desirable to extend that with technology (eg. automatically list what data is collected when, why and how long) and provide a way for plugins to hook into that?
I would definitely say Yes to that. That should also include templates, not just plugins. (I know e.g. some templates include Google Analytics.) Maybe have a hook per section (cookies, third party, etc)?",
"> GDPR considers [cookies] to be personal data
That is simply factually not true. The [official original legal text](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679) says only one thing about cookies:
> (30) Natural persons may be associated with online identifiers provided by their devices, applications, tools and protocols, such as internet protocol addresses, cookie identifiers or other identifiers such as radio frequency identification tags. This may leave traces which, in particular when combined with unique identifiers and other information received by the servers, may be used to create profiles of the natural persons and identify them.
This means that cookies only need to be considered **if** they contain personal data or can be used to obtain them in any way. That is not the case with `DOKU_PREFS`.",
"This topic is now locked. @Traumflug that kind of language is not welcome here
To clarify some last points:
* the GDPR is only about Personal Data. Cookies are not per se personal data (they might be if they can be used to identify people - the doku_prefs cookie does not)
* cookies do not ""come with IP address or DNS records attached"" that is technically nonsense. Cookies are sent back by the browser when the domain they are saved for matches
* The IP address of the originating request is always visible to the server. That's how TCP/IP works and nothing to do with cookies or anything
* There is legitimate reason to save the IP addresses for a certain amount of time to react on malicious activity
* making doku_prefs a session cookie makes the UX of the application much worse without improving privacy at all
* if you want code to be discussed for integration into DokuWiki core, open a proper pull request",
"Balancing artillery
I'd like to get the stuff I've already tested out of the way. The current balance team is essentially doing everything I already did in terms of making balance changes and testing them.
A lot of time is being wasted doing what I've already done. I've tested my changes in hundreds of matches with great feedback. The only negative opinions I get are from trolls or people who need an excuse for why they suck at the game.
So, first up, ARTILLERY.
These values allow the artillery to remain effective, but not overpowered. Instead of 3 arty destroying a huge infantry blob, you'll 4-6 of them.
Damage vs vehicles has been buffed slightly to compensate for lack of infantry killing.
A direct hit will kill the infantry, but if it lands off to the side, it can take anywhere from 1-4 shots. With a small group of artillery firing, the aoe will make it a lot more effective.
Cost also went up, and it's speed has been slowed down. I think I buffed it vs buildings too, maybe not.
Now, the artillery infantry tradeoff is fair, and for what it lacks in anti infantry, it makes up for by doing roughly 10-20% more dmg to everything else.
vehicles.yaml:
ARTY:
Mobile:
TurnSpeed: 2 (used to be 4 I think)
Speed: 60 (I think -10 points?)
RevealsShroud:
Range: 8c0 (remains same, i think, mightve increased by 1-2 cells)
RevealGeneratedShroud: False
RevealsShroud@GAPGEN:
Range: 4c0
Passenger:
Weight: 4
----------
ballistics.yaml:
^Artillery:
Range: 12c0 (it shouldnt outshoot a v2 by 4 cells, or at all, really)
Projectile: Bullet
Speed: 200 (little bit slower to help nerf vs infantry.
Blockable: false
LaunchAngle: 110
Inaccuracy: 1c256
Warhead@1Dam: SpreadDamage
Spread: 420
Versus:
None: 40
Light: 60
Heavy: 35
Concrete: 50
155mm:
MinRange: 4c0 (minimum range back to 4 cells instead of 5)",Bitter Frustration
"Sorry, this is not going to work this way. You have to edit the base yaml files directly (and not add new files).",
"> A lot of time is being wasted doing what I've already done. I've tested my changes in hundreds of matches with great feedback. The only negative opinions I get are from trolls or people who need an excuse for why they suck at the game.
This attitude runs counter to our [code of conduct](https://github.com/OpenRA/OpenRA/blob/bleed/CODE_OF_CONDUCT.md) and I have received a complaint about this PR under that code. This PR is one of many examples of this user's behavior across GitHub, the forum, and IRC.
Participating in an open community requires both the ability to come up with good ideas and the interpersonal skills to effectively communicate them. I am sorry, but ""I am better than you, so stop wasting your time and do what I say"" is a completely inappropriate base to start a pull request from.",Insulting
balanced weapon for artillery,
"Sorry, this is not going to work this way. You have to edit the base yaml files directly (and not add new files).",
Closing as per [part 1](https://github.com/OpenRA/OpenRA/pull/15118).,
"#### I'm opening this issue because:
- [ ] npm is crashing.
- [x] npm is producing an incorrect install.
- [ ] npm is doing something I don't understand.
- [ ] npm is producing incorrect or undesirable behavior.
- [ ] Other (_see below for feature requests_):
#### What's going wrong?
using `npm i --no-optional`, but the optionalDependencies still affect the version of other dependencies.
#### How can the CLI team reproduce the problem?
1. `git clone https://github.com/mapleeit/how-to-reproduce-npm-bug.git`
2. `npm i --no-optional`
3. `npm ls | grep request`
```
// it prints â”‚ â”œâ”€â”¬ request@2.81.0
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”¬ request@2.87.0
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”¬ request-promise-native@1.0.5
â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”¬ request-promise-core@1.1.1
â”œâ”€â”€ request@2.81.0 deduped
```
__the version of `request` is 2.81.0__
4. `open package.json`
5. delete the `optionalDependencies`
6. `rm -R node_modules && rm package-lock.json`
7. `npm i --no-optional` or `npm i`
8. `npm ls | grep request`
```
// it prints â”‚ â”œâ”€â”¬ request@2.87.0
â”‚ â”‚ â”‚ â”œâ”€â”€ request@2.87.0 deduped
â”‚ â”‚ â”‚ â”œâ”€â”¬ request-promise-native@1.0.5
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”¬ request-promise-core@1.1.1
```
__the version of `request` is 2.87.0__
### supporting information:
- `npm -v` prints: 6.1.0 - `node -v` prints: v8.1.2
- `npm config get registry` prints: https://registry.npmjs.org/ - Windows, OS X/macOS, or Linux?: OS X
- Network issues:
- Geographic location where npm was run:
- [ ] I use a proxy to connect to the npm registry.
- [ ] I use a proxy to connect to the web.
- [ ] I use a proxy when downloading Git repos.
- [ ] I access the npm registry via a VPN
- [ ] I don't use a proxy, but have limited or unreliable internet access.
- Container:
- [ ] I develop using Vagrant on Windows.
- [ ] I develop using Vagrant on OS X or Linux.
- [ ] I develop / deploy using Docker.
- [ ] I deploy to a PaaS (Triton, Heroku).
<!--
Thank you for contributing to npm! Please review this checklist
before submitting your issue.
- Please check if there's a solution in the troubleshooting wiki:
https://github.com/npm/npm/blob/latest/TROUBLESHOOTING.md
- Also ensure that your new issue conforms to npm's contribution guidelines:
https://github.com/npm/npm/blob/latest/CONTRIBUTING.md
- Participation in this open source project is subject to the npm Code of Conduct:
https://www.npmjs.com/policies/conduct
For feature requests, delete the above and uncomment the section following this one. But first, review the existing feature requests
and make sure there isn't one that already describes the feature
you'd like to see added:
https://github.com/npm/npm/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+label%3Aalready-looked-at
-->
<!--
#### What's the feature?
#### What problem is the feature intended to solve?
#### Is the absence of this feature blocking you or your team? If so, how?
#### Is this feature similar to an existing feature in another tool?
#### Is this a feature you're prepared to implement, with support from the npm CLI team?
-->",
"Seems to be related to https://github.com/npm/npm/issues/17633, which is getting no love :*(",Irony
dupe of https://github.com/npm/npm/issues/17633,
"#### I'm opening this issue because:
- [x] npm is crashing.
- [x] npm is producing an incorrect install.
- [x] npm is doing something I don't understand.
- [x] npm is producing incorrect or undesirable behavior.
all of the above.
#### What's going wrong?
This afternoon all servers in our AWS EU-Frankfurt environment started throwing the error regardless of what package we are trying to install
`
npm ERR! code E418
npm ERR! 418 I'm a teapot: commander@~2.3.0
`
It then locks up and does not exit the process.
#### How can the CLI team reproduce the problem?
`npm install`
There is nothing being written to the npm-debug.log as the process does not do anything
### supporting information:
- `npm -v` prints:
5.6.0
- `node -v` prints:
8.9.0
- `npm config get registry` prints:
http://registry.npmjs.org/
- Windows, OS X/macOS, or Linux?:
Amazon Linux AMI release 2017.09
- Network issues:
- Geographic location where npm was run:
AWS - EU
- [x ] I use a proxy to connect to the npm registry.
- [x ] I use a proxy to connect to the web.
- [x ] I use a proxy when downloading Git repos.
- [ ] I access the npm registry via a VPN
- [ ] I don't use a proxy, but have limited or unreliable internet access.
- Container:
No container.",
"I'm getting the same error, started today. Using Azure:
> 2018-05-28T23:54:24.4436370Z npm ERR! code E418
> 2018-05-28T23:54:24.4481896Z npm ERR! 418 I'm a teapot: @angular/cli@^1.6.5
command (using Azure PowerShell 2 extension):
`npm install @angular/cli`",
"I'm having the same issue.
```
$ npm install --save-dev express
npm ERR! code E418
npm ERR! 418 I'm a teapot: express@^4.16.3
$ npm -v
5.6.0
$ node -v
v8.11.2
$ npm config get registry
http://registry.npmjs.org/
$ $ cat /etc/os-release
NAME=""Ubuntu""
VERSION=""16.04.4 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.4 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```
The server is running in Japan and using a proxy to connect to the npm registry and the web.",
"Same here, started today. Error on my local machine (windows) and a VM (linux).",
"I'm having same issue.
```
citron: [Application] % npm install
npm ERR! code E418
npm ERR! 418 I'm a teapot: ember-test-selectors@^1.0.0
```",
"Having the same issue on our build server:
`
29-May-2018 10:07:56 | npm ERR! code E418
29-May-2018 10:07:56 | npm ERR! 418 I'm a teapot: source-map@0.5.7
`",Bitter Frustration
"Running into the same issue here, running into problems on both local and build server
npm ERR! 418 I'm a teapot: express@^4.15.3",Bitter Frustration
"Same issue here.
I'm behind a corporate proxy too. SSHed into my home computer over my phone, and installs went through just fine.",
"Ditto, npm config get registry
https://registry.npmjs.org/
npm -v
5.6.0
node -v
v8.9.4
npm install
npm ERR! code E418",Impatience
"Yep, same problem here. `npm install <package>` returns 418 I'm a Teapot, and install stalls.
Related: https://github.com/npm/registry/issues/335",Bitter Frustration
"I know the Internet loves being cute, but this error is unhelpful.",Bitter Frustration
"We're having this issue on AWS, behind a corporate proxy. Unfortunately we don't have the chance to go around this proxy.
```
$ npm install
npm ERR! code E418
npm ERR! 418 I'm a teapot
$ npm -v
5.3.0
$ node -v
v8.4.0
```",Bitter Frustration
"Corporate proxy - no option to bypass
```
$ npm - v
5.6.0
$node -v
v9.6.1
```",Bitter Frustration
"My colleague tried to run `npm install` and he was succeeded. His environment:
```
$ npm -v
3.5.2
$ node -v
v4.2.6
$ npm config get registry
https://registry.npmjs.org/
```
log:
```
$ npm install express
/home/username
tqq express@4.16.3
tqq thomash-node-audio-metadata@1.0.1 (git://github.com/osyo-manga/thomash-node-audio-metadata.git#c8af2b43ef97b2753f2ba712e828e0ff131976e8)
mqq xx@0.0.3
```
I also tried that in an older version of npm and node.
```
$ nvm install 4.2.6
Downloading and installing node v4.2.6...
Downloading https://nodejs.org/dist/v4.2.6/node-v4.2.6-linux-x64.tar.xz...
######################################################################## 100.0%
Computing checksum with sha256sum
Checksums matched!
Now using node v4.2.6 (npm v2.14.12)
$ npm -v
2.14.12
$ npm install express
npm WARN package.json rest-test@1.0.0 No description
npm WARN package.json rest-test@1.0.0 No repository field.
npm WARN package.json rest-test@1.0.0 No README data
express@4.16.3 node_modules/express
$ ls node_modules/ | grep express
express
```
Does NOT the issue reproduce in older versions of npm?",
"Yeah, rolling back to Node 4.26 works. 8 still doesn't. This is dumb.",Bitter Frustration
"ðŸ‘ Me too.
For those looking for a workaround, yarn still seems to be healthy.",
"I have started experiencing this issue from just today. I am behind an organisational firewall. $ npm install
npm ERR! code E418
npm ERR! 418 I'm a teapot: @react-ag-components/core@latest
Other people in my team have no issues running the same npm code from their workstation. Only me is having the problem.",Bitter Frustration
"The body of the 418 is `{""error"":""got unknown host (registry.npmjs.org:443)""}`. Looks like some npm clients are appending the port to the Host header, but only when going through a proxy, and that's confusing the registry:
```
$ wget --header='Host: registry.npmjs.org:443' https://registry.npmjs.org/
--2018-05-29 01:22:08-- https://registry.npmjs.org/
Resolving registry.npmjs.org (registry.npmjs.org)... 104.18.95.96, 104.18.97.96, 104.18.94.96, ...
Connecting to registry.npmjs.org (registry.npmjs.org)|104.18.95.96|:443... connected.
HTTP request sent, awaiting response... 418 I'm a teapot
2018-05-29 01:22:08 ERROR 418: I'm a teapot.
```",
"yes, we are meeting the same issue now.",
"Me too,
npm ERR! code E418
npm ERR! 418 I'm a teapot: moment@latest
node -v
v8.2.1
npm -v
5.3.0
I'm behind a proxy too.",Bitter Frustration
Same issue here. We have a successful install without a proxy and a 418 error code when installing from inside a container hooked up to the proxy.,
"I'm gonna lock this for now, because I'm sure it's gonna get plenty of traffic. You really don't need to respond to repeat what every other poster is saying. The registry team has been informed.",Impatience
"We've fixed this -- we now accept the port appended to the host. @wgrant's comment was most helpful, thank you!",
"### What does this PR do?
Improves the error message when Python version mismatch detected by providing solutions.
### What issues does this PR fix or reference?
See below.
### Previous Behavior (Python3 origin - Python2.7 target)
```bash
bash-4.4# salt-ssh -l quiet -i --out json --key-deploy --passwd admin123 container__wjQFc test.ping
{
""container__wjQFc"": {
""stdout"": ""ERROR: salt requires python 2.6 or newer on target hosts, must have same major version as origin host"", ""stderr"": """",
""retcode"": 10
}
}
```
### New Behavior (Python3 origin - Python2.7 target)
```bash
bash-4.4# salt-ssh -l quiet -i --out json --key-deploy --passwd admin123 container__wjQFc test.ping
{
""container__wjQFc"": {
""stdout"": ""ERROR: Depending on the Python version on the target, you need to install python2-salt on origin to add support for Python2.7 targets or install py26-compat-salt on origin to add support for Python2.6 targets or upgrade to Python==3.x on target"",
""stderr"": """",
""retcode"": 10
}
}
```
### Tests written?
No
### Commits signed with GPG?
No
Please review [Salt's Contributing Guide](https://docs.saltstack.com/en/latest/topics/development/contributing.html) for best practices.
See GitHub's [page on GPG signing](https://help.github.com/articles/signing-commits-using-gpg/) for more information about signing commits with GPG.",
@dincamihai There are a few lint errors here: https://jenkins.saltstack.com/job/PR/job/salt-pr-lint-n/22487/violations/file/salt/client/ssh/__init__.py/,
"@terminalmage I've forgot to change the last error message, which is ""invalid Python"". That just misleads people and they don't understand what is going on, hence the fix. Salt SSH still needs to have both library sets (dependencies) on the Master for the specific Python versions, so the `.tar.gz` is carrying over those in `py<major version>` subdirectories.
@dincamihai NOTE: actually if you are running Salt on a Master from the specific version, you _do not_ need to install Salt again for the alternative version, as it is anyway works for both versions. This is just packaging convenience. But in fact you need only couple of version-specific libraries to be installed so they will be picked up by the `thin` creation procedure. That is, probably we should not say ""install Salt for alternative Python X"" (which implies you will get all the needed dependencies automatically), but ""install Salt _dependencies_ only for the alternative Python X"".
@gtmanfred nope, this is only needed in Fluorine.",
"@cachedout i've fixed the lint errors but now, after rebase, it seems to have some unrelated lint errors in the tests.
@terminalmage Bo and Daniel already provided an explanation. sorry, for not being clear enough about this in the description.",
"@dincamihai I would still minimise the text according to the example I showed above. As well as the ""3.x"" thing.",
"@isbm having 3.x there means that it matches all the 3 subversions. if I remove the .x someone might think it would only apply to 3 and this would make it more confusing.
Adding new line to the text is ignored and I'm not sure I should spend more time on this.
Your example seems to remove some of the useful information and has this ""on a Master"" that I don't understand. Is that intended or should it be ""on the Master""?
I was using origin and target in order to avoid Master and Minion because none of these machines have salt-master or salt-minion running.",Bitter Frustration
"@dincamihai the ""3.x"" is the same as ""3"", as I see it. I also suggested to use integers as in the `version_info` instead of just strings. And so if we have some issue with the 3.8 (e.g.) one would just add a minor version key. Otherwise default should go. But ""3.x"" is more to me looks like a hack.
The ""origin"" is something that might not be understood. Personally to me it is very odd terminology here. And you are running ""SaltSSH"" from the Master (or want-to-be-master).",Bitter Frustration
"@isbm i'm checking first for major.minor, so if we add 3.8 it will go to that first. only if there is not major.minor it will match major.x. i think we are fine here.",
"@dincamihai correct. So that means there **is something** that needs to be shown _instead of_ the default message. And if you have none of '8', then you will get the default one:
```python
msg = {
2: {
6: 'Too old',
7: 'Soon to be dead',
'default': 'Problem between the keyboard and the chair',
},
3: {
4: 'Many things are missing in this release',
6: 'Many things are incompatible with the previous release, ha-ha!',
8: 'Larry Wall was hired to design new Python language syntax',
'default': 'I like you begging, do it again',
}
}
import sys
mj, mn = sys.version_info[:2]
print(msg[mj].get(mn, msg[mj]['default']))
```
P.S. keep the code, replace the messages. :wink:",Mocking
"@isbm if there is no 3.8 specific message, 3.x would be shown. is this not enough?",Impatience
The code I exampled above does this. I am just not in favour of your structure that keeps strings that needs to be parsed instead of just direct map to versions. And your messages are too big and using foreign terminology for Salt-specific domain. And so therefore I would change that.,Bitter Frustration
"@isbm ok. thanks for the suggestion, but if the upstream is fine with my version (changes approved) please merge the PR. thanks!",Impatience
@terminalmage you like the proposed error messages and the structure?,
"OK, as per speaking with @dincamihai it would be also an option to:
- Add all the options into the documentation.
- Add short minimally intrusive to the target machine option (i.e. update the SaltSSH side w/o touching the target)
That said, CLI would return something like:
```
The ""my-to-be-minion.greatdomain.com"" machine is running Python 2.6 version.
Please install Salt for 2.6 Python on the Salt SSH machine and set it up.
```
or:
```
The ""my-to-be-minion.greatdomain.com"" machine is running Python 2.7 version.
Please install Salt for 2.7 Python on the Salt SSH machine.
```
So such info on the CLI would suggest what to do now, quickly and easiest way. And the following would go to the documentation (rephrased into more extended version):
""Depending on the Python version on the target, you need to install Python2.7 compatible salt on origin to add support for Python2.7 targets or install Python2.6 compatible salt on origin to add support for Python2.6 targets or upgrade to Python==3.x on target""",
"One note:
sometimes, when targeting 1000 machines, maybe 999 are ok but one has an old python. there are two options and we don't know what it is better for the user:
- update the origin machine (that works on 999 of the clients) or
- update the one client that doesn't work
Other than that, this is open source, so I don't have any problem with adapting my PR to whatever is a better fit for saltstack/salt.",
"I would only say that ""target"" is that part between `salt` and the `command`... More here: https://docs.saltstack.com/en/latest/topics/targeting/ So calling it ""target"" and ""origin"" brings lots of confusion from this POV. I would definitely keep it Master, especially if SaltSSH is ""to execute salt commands and states over ssh without installing a salt-minion"". Which implies ""Master"" at first place!",Bitter Frustration
"@terminalmage i am not an expert in salt terminology. I just proposed what I thought it would be appropriate and so far saltstack did not disagree. sure, please propose better messages.
the changes were already approved so why should i make more changes to the PR or invest more time into it?",Bitter Frustration
@terminalmage by the way https://cse.google.com/cse?cx=004624818632696854117:yfmprrbw3pk&q=target,
"@dincamihai actually ""approved"" here means _everyone_ agrees, JFYI. And your google result is suggesting exactly what I mean.",Mocking
"@isbm and @terminalmage I have tested my changes on different combinations.
It would not make any sense to make additional changes and test the whole thing again with python2.6, 2.7 and 3 and combinations of them just because someone likes it more is some way. (i'm not talking here about the formulation of the messages, that is a valid change that is worth doing). but changing some structure to another structure just because... is not enough reason to redo the whole testing again.",Bitter Frustration
@isbm what about target in the context of rosters https://docs.saltstack.com/en/latest/topics/ssh/roster.html,Impatience
"I would have loved to work with you, but since you were not willing to accept any recommendations, I am closing this and we will do any remaining work in https://github.com/saltstack/salt/pull/48058.",
"[gatling-stuck.zip](https://github.com/gatling/gatling/files/2107919/gatling-stuck.zip)
Basically when project includes <dependency>
<groupId>io.kubernetes</groupId>
<artifactId>client-java</artifactId>
<version>1.0.0</version>
</dependency>
gatling does not start execution of scenario. Repro case is attached
mvn clean gatling:execute
and gatling is stuck doing nothing, last message
14:34:15.070 [main] INFO io.gatling.http.ahc.HttpEngine - Start warm up
now comment out kubernetes client in pom.xml and all will be well",Bitter Frustration
"You've messed up the ""writers"" option in gatling.conf.
Works just fine for me.",Mocking
"Hmm @slandelle , do not you think that simply stating that I 'messed up ""writers"" ' without explaining why is rude?
As far as I know I did not mess them, if I do not put writers in square braces and leave as in default config then Gattling would not even start with this exception:
/src/test/resources/gatling.conf: 118: gatling.data.writers has type STRING rather than LIST
java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at io.gatling.mojo.MainWithArgsInFile.runMain(MainWithArgsInFile.java:50)
at io.gatling.mojo.MainWithArgsInFile.main(MainWithArgsInFile.java:33)
Caused by: com.typesafe.config.ConfigException$WrongType: gatling.conf @ file:/Users/kgignatyev/dev/reprocases/gatling-stuck/src/test/resources/gatling.conf: 118: gatling.data.writers has type STRING rather than LIST
at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:159)
at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170)
at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:176)
at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:176)
at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184)
at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189)
at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:258)
at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:329)
at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:387)
at io.gatling.core.config.GatlingConfiguration$.mapToGatlingConfig(GatlingConfiguration.scala:215)
at io.gatling.core.config.GatlingConfiguration$.load(GatlingConfiguration.scala:98)
at io.gatling.app.Gatling$.start(Gatling.scala:54)
at io.gatling.app.Gatling$.fromArgs(Gatling.scala:45)
at io.gatling.app.Gatling$.main(Gatling.scala:37)
at io.gatling.app.Gatling.main(Gatling.scala)
... 6 more",Bitter Frustration
"> Hmm @slandelle , do not you think that simply stating that I 'messed up ""writers"" ' without explaining why is rude?
No, I don't think spending some personal time investigating a problem you have with a software you got from me for free is ""rude"".
I've pointed out your mistake and you are perfectly capable of spotting the difference between the original configuration that was working and you've commented out and the one you've changed and that don't.
From the `gatling.conf` in the sample you've provided (L217):
```
#writers = [""graphite"", ""console"", ""file""]
writers = [""console, file""]
```",Bitter Frustration
"Too bad that you do not consider this rude, @slandelle . If you are not willing to spend your time it is your right. I let Gatling community know that Gatling exhibits strange behavior in the presence of other libraries, and you @slandelle do not care.
It is OK too, it just not helpful to make false statements like: 'works for me' and 'you messed up writers' because you did not bother to run the repro-case following very simple instructions.",Bitter Frustration
"> It is OK too, it just not helpful to make false statements like: 'works for me' and 'you messed up writers' because you did not bother to run the repro-case following very simple instructions.
If I state that your sample ""works for me"", it of course means that I did bother running it on my side and didn't get to reproduce the issue you're mentioning.
> I let Gatling community know that Gatling exhibits strange behavior in the presence of other libraries
That's mostly likely an invalid statement, as:
1) your sample works, at least on my side
2) the library you're mentioning, `io.kubernetes:client-java`, is built on top of Square's okhttp and doesn't share any dependency with Gatling which is built on top of Netty.
> and you @slandelle do not care.
Let's agree that we disagree and that nothing good will come from this discussion.
```
MacBook-Pro-de-slandelle:gatling-stuck slandelle$ mvn clean gatling:execute
[INFO] Scanning for projects...
[WARNING] The project com.inspur.k8s.testing:gatling--testing:jar:1.0-SNAPSHOT uses prerequisites which is only intended for maven-plugin projects but not for non maven-plugin projects. For such purposes you should use the maven-enforcer-plugin. See https://maven.apache.org/enforcer/enforcer-rules/requireMavenVersion.html
[INFO]
[INFO] --------------< com.inspur.k8s.testing:gatling--testing >---------------
[INFO] Building gatling--testing 1.0-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ gatling--testing ---
[INFO] Deleting /Users/slandelle/Downloads/gatling-stuck/target
[INFO]
[INFO] --- gatling-maven-plugin:2.2.4:execute (default-cli) @ gatling--testing ---
06:20:02.536 [main][WARN ][ZincCompiler.scala:141] i.g.c.ZincCompiler$ - Pruning sources from previous analysis, due to incompatible CompileSetup.
06:20:07,571 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]
06:20:07,571 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.groovy]
06:20:07,572 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Found resource [logback.xml] at [jar:file:/Users/slandelle/.m2/repository/io/kubernetes/client-java/1.0.0/client-java-1.0.0.jar!/logback.xml]
06:20:07,572 |-WARN in ch.qos.logback.classic.LoggerContext[default] - Resource [logback.xml] occurs multiple times on the classpath.
06:20:07,572 |-WARN in ch.qos.logback.classic.LoggerContext[default] - Resource [logback.xml] occurs at [jar:file:/Users/slandelle/.m2/repository/io/kubernetes/client-java/1.0.0/client-java-1.0.0.jar!/logback.xml]
06:20:07,572 |-WARN in ch.qos.logback.classic.LoggerContext[default] - Resource [logback.xml] occurs at [jar:file:/Users/slandelle/.m2/repository/io/gatling/gatling-maven-plugin/2.2.4/gatling-maven-plugin-2.2.4.jar!/logback.xml]
06:20:07,586 |-INFO in ch.qos.logback.core.joran.spi.ConfigurationWatchList@61dc03ce - URL [jar:file:/Users/slandelle/.m2/repository/io/kubernetes/client-java/1.0.0/client-java-1.0.0.jar!/logback.xml] is not of type file
06:20:07,636 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - debug attribute not set
06:20:07,637 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]
06:20:07,644 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [console]
06:20:07,649 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property
06:20:07,700 |-INFO in ch.qos.logback.classic.joran.action.RootLoggerAction - Setting level of ROOT logger to INFO
06:20:07,700 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [console] to Logger[ROOT]
06:20:07,700 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - End of configuration.
06:20:07,701 |-INFO in ch.qos.logback.classic.joran.JoranConfigurator@50f8360d - Registering current configuration as safe fallback point
06:20:08.648 [GatlingSystem-akka.actor.default-dispatcher-2] INFO akka.event.slf4j.Slf4jLogger - Slf4jLogger started
06:20:08.934 [GatlingSystem-akka.actor.default-dispatcher-3] INFO i.g.c.stats.writer.LogFileDataWriter - Initializing
06:20:08.934 [GatlingSystem-akka.actor.default-dispatcher-2] INFO i.g.c.stats.writer.ConsoleDataWriter - Initializing
06:20:08.941 [GatlingSystem-akka.actor.default-dispatcher-2] INFO i.g.c.stats.writer.ConsoleDataWriter - Initialized
06:20:08.953 [GatlingSystem-akka.actor.default-dispatcher-3] INFO i.g.c.stats.writer.LogFileDataWriter - Initialized
06:20:09.316 [main] INFO io.gatling.http.ahc.HttpEngine - Start warm up
06:20:09.697 [main] INFO io.gatling.http.ahc.HttpEngine - Warm up done
Simulation com.inspur.k8s.perf_tests.BasicSimulation started...
06:20:09.906 [GatlingSystem-akka.actor.default-dispatcher-4] INFO i.gatling.core.controller.Controller - InjectionStopped expectedCount=1
06:20:10.054 [gatling-http-thread-1-2] INFO io.gatling.core.action.Pause - Pausing for 7000ms (real=6988ms)
================================================================================
2018-06-19 06:20:13 5s elapsed
---- Requests ------------------------------------------------------------------
> Global (OK=2 KO=0 )
> request_1 (OK=1 KO=0 )
> request_1 Redirect 1 (OK=1 KO=0 )
---- Scenario Name -------------------------------------------------------------
[--------------------------------------------------------------------------] 0%
waiting: 0 / active: 1 / done:0
================================================================================
06:20:17.063 [GatlingSystem-akka.actor.default-dispatcher-5] INFO i.gatling.core.controller.Controller - All users are stopped
================================================================================
2018-06-19 06:20:17 8s elapsed
---- Requests ------------------------------------------------------------------
> Global (OK=2 KO=0 )
> request_1 (OK=1 KO=0 )
> request_1 Redirect 1 (OK=1 KO=0 )
---- Scenario Name -------------------------------------------------------------
[##########################################################################]100%
waiting: 0 / active: 0 / done:1
================================================================================
06:20:17.073 [GatlingSystem-akka.actor.default-dispatcher-6] INFO i.gatling.core.controller.Controller - StatsEngineStopped
Simulation com.inspur.k8s.perf_tests.BasicSimulation completed in 7 seconds
Parsing log file(s)...
06:20:17.134 [main] INFO i.gatling.charts.stats.LogFileReader - Collected ArrayBuffer(/Users/slandelle/Downloads/gatling-stuck/target/gatling/basicsimulation-1529382008914/simulation.log) from basicsimulation-1529382008914
06:20:17.142 [main] INFO i.gatling.charts.stats.LogFileReader - First pass
06:20:17.151 [main] INFO i.gatling.charts.stats.LogFileReader - First pass done: read 5 lines
06:20:17.161 [main] INFO i.gatling.charts.stats.LogFileReader - Second pass
06:20:17.203 [main] INFO i.gatling.charts.stats.LogFileReader - Second pass: read 5 lines
Parsing log file(s) done
Generating reports...
================================================================================
---- Global Information --------------------------------------------------------
> request count 2 (OK=2 KO=0 )
> min response time 50 (OK=50 KO=- )
> max response time 86 (OK=86 KO=- )
> mean response time 68 (OK=68 KO=- )
> std deviation 18 (OK=18 KO=- )
> response time 50th percentile 68 (OK=68 KO=- )
> response time 75th percentile 77 (OK=77 KO=- )
> response time 95th percentile 84 (OK=84 KO=- )
> response time 99th percentile 86 (OK=86 KO=- )
> mean requests/sec 0.25 (OK=0.25 KO=- )
---- Response Time Distribution ------------------------------------------------
> t < 800 ms 2 (100%)
> 800 ms < t < 1200 ms 0 ( 0%)
> t > 1200 ms 0 ( 0%)
> failed 0 ( 0%)
================================================================================
Reports generated in 0s.
Please open the following file: /Users/slandelle/Downloads/gatling-stuck/target/gatling/basicsimulation-1529382008914/index.html
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 16.852 s
[INFO] Finished at: 2018-06-19T06:20:17+02:00
[INFO] ------------------------------------------------------------------------
```",
"326 / 600 tests executed, then hangs.
```
19 06 2018 15:54:19.133:WARN [Firefox 52.0.0 (Windows 8.1 0.0.0)]: Disconnected (1 times)
Firefox 52.0.0 (Windows 8.1 0.0.0) ERROR
Disconnected
```
Tried with Chrome - same thing. ```
19 06 2018 16:11:34.594:WARN [Chrome 67.0.3396 (Windows 8.1 0.0.0)]: Disconnected (1 times)
Chrome 67.0.3396 (Windows 8.1 0.0.0) ERROR
Disconnected
```
Current config:
```js
browsers: ['Firefox'],
singleRun: true,
autoWatch: false,
/**
* How long will Karma wait for a message from a browser before disconnecting from it (in ms).
*/
browserNoActivityTimeout: 100000,
/**
* maximum number of tries a browser will attempt to reconnect in the case of a disconnection
*/
browserDisconnectTolerance: 2,
browserDisconnectTimeout: 210000,
captureTimeout: 60000,
retryLimit: 5,
concurrency: Infinity,
frameworks: ['browserify', 'jasmine'],
client: {
mocha: {
/*
@see https://github.com/karma-runner/karma-phantomjs-launcher/issues/126
*/
timeout: 20000
}
},
reporters: ['mocha', 'junit', 'html', 'coverage'],
plugins: [
'karma-junit-reporter',
'karma-jasmine',
'karma-chrome-launcher',
'karma-firefox-launcher',
'karma-html-reporter',
'karma-browserify',
'karma-coverage',
'karma-babel-preprocessor',
'karma-mocha-reporter',
],
```
hardware:
* CPU: Intel Xeon E5-1620 v3 @ 3.5GHz
* RAM: 128 GB
* GPU: Nvidia Quadro K2200 4GB RAM
environment:
* OS: win 8.1 x64
* node: v10.0.0
* npm: 5.6.0
modules:
```js
""jasmine-core"": ""2.99.1"",
""karma"": ""2.0.3"",
""socket.io"": ""1.4.5""
""jasmine-ajax"": ""3.4.0"",
""karma-babel-preprocessor"": ""6.0.1"",
""karma-browserify"": ""5.3.0"",
""karma-chrome-launcher"": ""2.0.0"",
""karma-coverage"": ""1.1.1"",
""karma-firefox-launcher"": ""1.1.0"",
""karma-html-reporter"": ""0.2.7"",
""karma-jasmine"": ""1.1.2"",
""karma-junit-reporter"": ""1.2.0"",
""karma-mocha-reporter"": ""2.2.0"",
""babel-polyfill"": ""6.23.0"",
""babel-preset-es2015"": ""6.16.0"",
""babel-preset-es2016"": ""6.22.0"",
""babelify"": ""7.3.0"",
""browserify"": ""14.4.0"",
""browserify-istanbul"": ""2.0.0"",
""eslint"": ""3.19.0"",
""eslint-config-airbnb-base"": ""11.1.3"",
""eslint-plugin-import"": ""2.2.0"",
""gulp"": ""3.9.1"",
""gulp-babel"": ""6.1.2"",
""gulp-clean"": ""0.3.2"",
""gulp-connect"": ""5.0.0"",
""gulp-notify"": ""2.2.0"",
""gulp-param"": ""1.0.3"",
""gulp-replace"": ""0.5.4"",
""gulp-sonar"": ""3.0.1"",
""gulp-sourcemaps"": ""1.6.0"",
""gulp-util"": ""3.0.7"",
""isparta"": ""4.0.0"",
""istanbul"": ""0.4.5"",
""node-notifier"": ""4.6.1"",
""proxyquireify"": ""3.2.1"",
""shelljs"": ""0.7.8"",
""vinyl-buffer"": ""1.0.0"",
""vinyl-source-stream"": ""1.1.0"",
""watchify"": ""3.9.0"",
```
Notable observations:
* Browsers tab saturates 1 CPU core usage (~100%)
* Memory usage keeps growing, * from about 100mb to 750 in 1 minute on FF
* from about 60 to 180 on Chrome
* tab is unresponsive entirely until gets killed by karma or crashes with crash report (only on FF).
* karma guys say it's not their fault
* jasmine guys say it's not their fault
* karma-jasmine guys say it's not their fault
related issues:
https://github.com/jasmine/jasmine/issues/1327
https://github.com/karma-runner/karma/issues/1788
https://github.com/karma-runner/karma/issues/762
https://github.com/karma-runner/karma-phantomjs-launcher/issues/126
https://github.com/karma-runner/karma-phantomjs-launcher/issues/55
https://github.com/karma-runner/karma-phantomjs-launcher/issues/126
May this help those who come after us fare better :wink:",Bitter Frustration
"If your suite always failing in the same spot, it might have something to do with your tests themselves, and how they interact with the various test frameworks you're using. Take a look at [Steve's comment](https://github.com/jasmine/jasmine/issues/1327#issuecomment-379574146) in #1327 for some options on how to troubleshoot this error. I would also suggest updating to Jasmine 3.1 (and corresponding karma-jasmine updates) ensure this hasn't already been fixed in the 3.0 or later releases.
We (Jasmine maintainers) haven't been able to reproduce this case, as it seems to require a fairly complex async suite.
Hope this helps. Thanks for using Jasmine!",
"@slackersoft > Take a look at Steve's comment in #1327 for some options on how to troubleshoot this error. Thank you for your response. I have linked that issue too, and I have read and tried everything that would not require node version change or break our build pipeline that was proposed in all of the posts on all of those issues.
My post is not really a call for help, I have spent sufficient time here to be convinced that it's not worth more investment from our side. Instead I hope for this to be a good overview for others, and perhaps it will help others some of the time that I have invested.
> We (Jasmine maintainers) haven't been able to reproduce this case, as it seems to require a fairly complex async suite.
Your choice of words is not very accurate. You didn't try to reproduce this issue. Your reasoning is on point though - outlined configuration is much too complex and has only started failing after certain tests were added (added tests run 100% fine, but others don't and the whole thing freezes).",Bitter Frustration
"> My post is not really a call for help,
Then it doesn't really sound like it belongs as an issue here. We want to keep this space clear for bugs and issues in Jasmine itself.
I would also urge you to be careful and considerate when telling others what they have and haven't done. We've invested a significant amount of time in trying to eliminate various timing issues with Jasmine, but this one seems to come primarily from the suite being executed.
Thanks for using Jasmine!",
"This is the current (july 2018) GLTF status.
Test case: the same skinned mesh exported from 3DS Max 2018.
Standard material: Diffuse + Specular + Normal
1) SEA3D exporter + SEA3D importer:
http://necromanthus.com/Test/html5/Lara.html SEA file size: 658 KB
Result: close to perfect
2) Babylon3D GLTF exporter + GLTF importer:
http://necromanthus.com/Test/html5/Lara_gltf.html GLB file size: 1,850 KB
Result: messed up materials
I've also tested the FBX2GLTF utility (by Facebook): the same wrong results
Important note: there is nothing wrong with THREE.js and PBR materials:
http://necromanthus.com/Test/html5/Lara_PBR.html In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.",Bitter Frustration
"I don't think this repo is the right place for this post. It's neither a feature request, nor a bug. So my question is: What are you trying to accomplish with this issue? Bashing `glTF`?
If you encounter problems with an exporter or converter, I suggest you open an issue at the respective github repo.
> In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.
I generally reject such Trump-like statements. They have a provocative nature and are not objective at all.",Bitter Frustration
"> I don't think this repo is the right place for this post.
It's the best place for sure and it shows the current GLTF status.
> I generally reject such Trump-like statements.
Really?
Here is a statement from Trump: Google already failed with UTF8.
> If you encounter problems with an exporter or converter, I suggest you open an issue at the respective github repo.
Many of them blame THREE.js for bad GLTF results.
The posted samples prove they are wrong.",Mocking
@RemusMar are you suggesting any particular actions we should take? If not I vote to close this issue.,Impatience
"> RemusMar are you suggesting any particular actions we should take?
Three possible causes for the wrong GLTF results:
1. the Babylon3D exporter is buggy (they say it's not)
2. the FBX2GLTF converter is buggy (they say it's not)
3. the GLTF importer is buggy.
But you and Mugen87 want to close the topic because there is no issue and everytbody is happy ...",Bitter Frustration
"> the GLTF importer is buggy
You mean the GLTFLoader? Can you identify what the bug is? It would be especially helpful if you can find a very simple model that demonstrates the bug.
> (they say it's not)
If you've made bug reports on the Babylon3D exporter and FBX2GLTF can you link to them here?",Impatience
"> You mean the GLTFLoader? Yes.
Importer = Loader + Parser
> It would be especially helpful if you can find a very simple model that demonstrates the bug.
You have everything you need to study the issue.
Any PHONG material (Diffuse + Specular + Normal) exported or converted to GLTF gives wrong results.
p.s.
PHONG represents 50-60% of the current samples, compared to Physical less than 1%.
That's why I said that PBR was a bad choice for GLTF.",Bitter Frustration
"> You have everything you need to study the issue
In other words you want someone else to do the work for you ðŸ™„
Regarding Phong materials and glTF, I agree that this makes glTF a bad choice for converting older models - especially models originally exported as FBX which only supports Phong or Lambert shading.",Mocking
"> n other words you want someone else to do the work for you
I don't have enough spare time for ""GitHub activities"".
You got the report and the working samples.
That's all for now.
cheers",Bitter Frustration
"In that case, I still don't consider this to be a complete or actionable bug report and I continue to vote to close this issue.",
"Hey @RemusMar,
Thanks for reporting this. Some notes...
**File size**
I'm not sure where you're getting these numbers, this is what I see:
```
Lara.sea: 1,032,525 bytes Lara.glb: 1,850,164 bytes
Lara.sea.gz: 1,031,840 bytes
Lara.glb.gz: 919,712 bytes ```
**Materials**
Your model uses `Diffuse + Specular`. Unfortunately, seems like the specular texture is not being exported. GLTF supports 2 PBR modes: `Metalness + Roughness` and `Specular + Glossiness`. You want to export your model using the second mode. `GLTFLoader` supports both but maybe the Babylon.js doesn't have an option to export in that mode? In that case you may want to do a feature request on their project.
Let us know what you find out.",
"Hi Ricardo,
1. File size.
If you download the files (with Firefox) you'll get:
Lara.sea: 658,901 bytes
Lara.glb: 1,850,964 bytes
Even more: the GLB file does not even contain the equivalent (Metalness) of the Specular texture !
2) Materials
As I said before, the original MAX and FBX files contain a standard PHONG material:
Diffuse + Specular + Normal None of the current GLTF exporters and converters is able to generate a correct Physical material.
PHONG is way more popular than Physical.
p.s.
I'm not interested in the GLTF format (SEA3D is better from any point of view).
I just want to help other poeple.",Bitter Frustration
"Another userful sample:
Here is a NATIVE Physical material in 3DS Max 2018 exported to GLTF with Babylon3D exporter:
http://necromanthus.com/Test/html5/Lara_gltf_physical.html Now the metalness is present, but the result is still wrong:
- it looks emissive (but it's not)
- it has a red color bump
However, this GLB file looks better (compared to THREE) in the Babylon sandbox ( https://sandbox.babylonjs.com ).
Again, this is how the Physical material should look in THREE:
http://necromanthus.com/Test/html5/Lara_PBR.html So let's forget now about PHONG and buggy exporters and converters.
We should investigate why the GLB file looks better in the Babylon sandbox.
Buggy GLTF Loader in THREE ?",Bitter Frustration
If one could quickly prototype some hacks over the existing phong / standard implementations i bet it would be pretty useful ðŸ˜‰,Irony
"@RemusMar Seems like the glb includes a `AmbientLight`.
<img width=""766"" alt=""screen shot 2018-07-08 at 7 44 30 pm"" src=""https://user-images.githubusercontent.com/97088/42419015-7a49ede0-82e7-11e8-8067-ae27f0b0937a.png"">
If you set `visible` to `false` to the imported `AmbientLight` the character starts to look less red.
The last thing to do is setting `renderer.gammaOutput = true`. (Needed when using GLTF #12766)
<img width=""634"" alt=""screen shot 2018-07-08 at 7 48 00 pm"" src=""https://user-images.githubusercontent.com/97088/42419034-dca797b2-82e7-11e8-8548-e9f204aa7b54.png"">",
"> Seems like the glb includes a AmbientLight.
Good catch Ricardo! :thumbsup: > If you set visible to false to the imported AmbientLight the character starts to look less red.
Something better (no wasted resources):
```javascript gltf.scene.remove( gltf.scene.children[2] );
```
> The last thing to do is setting renderer.gammaOutput = true. (Needed when using GLTF #12766)
That indicates buggy GLTF Loader (and it has to be fixed).
If I use that for other loaders, I get wrong colors for the loaded models.
Just think about this scenario: use various loaders for the same scene and one of them is GLTF.
It will mess up the entire scene!
Anyway, after removing that ambient light and using this workaround, this is the result:
http://necromanthus.com/Test/html5/Lara_gltf_physical.html Much better compared to the initial GLTF one, but the material quality is far away from this one:
http://necromanthus.com/Test/html5/Lara_PBR.html Impressive lighting response and great metalness for bra and bikini.
At this stage I won't use GLTF in any serious project.",Bitter Frustration
Can you try adding a `envMap` cubemap to these examples too? PBR looks the best when a `envMap` is supplied.,
"> Can you try adding a envMap cubemap to these examples too?
> PBR looks the best when a envMap is supplied.
I've added offline.
Of course it looks better, but PHONG with Environment map still looks WAY better (in THREE).
Also, the envMap does not fix the GLTF Loader issue(s).",Bitter Frustration
"> I've added offline.
Could you update the online samples?",Impatience
"The glTF format supports PBR and unlit shaders at this time. Whether the BabylonJS and FBX2GLTF tools do the Phong-to-PBR conversion in a way that preserves Phong specular maps, I don't know â€” that would be a question for the repos of those tools. If you are trying to preserve the exact appearance of models using classic Phong shaders, you may have an easier time with other formats.
> > The last thing to do is setting renderer.gammaOutput = true.
> > That indicates buggy GLTF Loader (and it has to be fixed).
This is a deliberate decision and not a bug. Base color and emissive textures in glTF (and, typically, diffuse textures in any format...) are in sRGB colorspace. GLTFLoader marks them as such (`material.map.encoding = THREE.sRGBEncoding`), so that they're converted to linear colorspace for correct PBR lighting calculations. Finally colors should be converted back to sRGB (e.g. `renderer.gammaOutput=true`).
If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the `.encoding` property of any texture, and leave it to the end user to change texture colorspace and renderer colorspace. I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace, and getting results that are ""good enough"" but inconsistent with other engines and authoring environments. For correct results you should be using `renderer.gammaOutput=true`, and marking sRGB textures as sRGB.
None of these issue are specific to glTF (see https://github.com/mrdoob/three.js/issues/11337), but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments, and have chosen to treat all sRGB textures as sRGB for a first step. If you're mixing models from other formats in the scene, then yes it's awkward, and you'd need to either mark the diffuse textures of those formats as sRGB or mark the colorspace on the glTF models to linear (the latter is incorrect for all model formats involved, but may look good enough if you don't need precise colors).
***
It does not seem like there is anything actionable here, unless there are specific issues we can report to the tools mentioned. @RemusMar if you are happy with your SEA3D workflow, that's great â€” I'm not interested in debating formats or persuading you to change from something that is already working well for you.",
"> Could you update the online samples?
Ricardo,
PHONG looks great with Diffuse + Specular only.
PBR does not look great with BaseColor + MetallicRoughness only.
That's the main problem here.
The Normal/Bump and Environment textures are irrelevant at this point.
On top of that: more texture layers = bigger file size and performances drop
> If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the .encoding property of any texture
That was a wise decision.
> I'm not interested in debating formats or persuading you to change from something that is already working well for you.
Don,
I'm not debating the ""PBR only"" bad choice for GLTF.
This topic shows GLTFLoader design flaws.
We don't reinvent the wheel here, so ""renderer.gammaOutput = true"" is not an option now, when GLTF represents less than 1% of the market.
cheers",Bitter Frustration
"> Can you try adding a envMap cubemap to these examples too? PBR looks the best when a envMap is supplied.
> Could you update the online samples?
Because you asked me to:
SEA3D + Phong: http://necromanthus.com/Test/html5/Lara_envMap.html vs
GLTF + PBR: http://necromanthus.com/Test/html5/Lara_gltf_envMap.html The quality drop is obvious.
Also, in 3DS Max 2018 the Physical material looks WAY better than the GLTF result.",Bitter Frustration
"> None of these issue are specific to glTF (see #11337), but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments,
That's completely wrong Don!
Here we're talking about GLTFLoader and THREE.js
The users are interested in the best results with THREE.
Other engines and 3D authoring environments are irrelevant here.
And you still don't understand the main problem here.
For the last time:
1) JSONLoader (or SEA3D loader) + PBR = GOOD results (close to Phong):
http://necromanthus.com/Test/html5/Lara_PBR.html 2) GLTFLoader + PBR = BAD results
http://necromanthus.com/Test/html5/Lara_gltf_physical.html p.s.
In the first sample you don't even need ""renderer.gammaOutput = true"" to get good results !!!",Bitter Frustration
"> The users are interested in the best results with THREE. Other engines and 3D authoring environments are irrelevant here.
Being able to author a PBR model in Substance Painter or download one from Sketchfab, and have the model appear as the artist designed it, is good for three.js users. I don't think there's any definition of ""best result"" where that sort of consistency can be dismissed.
> This topic shows GLTFLoader design flaws. We don't reinvent the wheel here, so ""renderer.gammaOutput = true"" is not an option now...
This isn't reinventing any wheels, and it isn't a design flaw. PBR calculations are done in linear space, with every engine I'm aware of. If you pass sRGB data into the renderer and pretend it's linear, the lighting and blending math will come out wrong. The difference is not huge, and so this not a major concern for many three.js users, but nevertheless it is not as good as it could be. For that reason, your ""good"" result example is not actually correct. But as you've said before, backward-compatibility is important, so I'm not here to advocate for changing any three.js defaults. But because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.
See [this article about Unreal](http://artbyplunkett.com/Unreal/unrealgamma.html) â€” > ...textures that are used for color information should have the sRGB flag checked, and textures that are used for masks and numerical calculations in shaders and effects (like Normal maps) should have it unchecked. And if you follow this simple guideline you mostly get the best effect.
This is precisely what we are doing.",
"> ...I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace
You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?
> ...because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.
@donmccurdy should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible.",Impatience
"> You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?
Yes, the problem is the same for Phong materials or PBR materials loaded in any other format.
> ...should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible.
If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB. But making the change now would cause confusion and break backward-compatibility, and the `gammaOutput=true` setting needed to fix output is not intuitive â€” I don't think changing other loaders can be justified given those issues.
Let's keep an eye on https://github.com/mrdoob/three.js/issues/11337. I hope the resolution there will make color workflows more intuitive. With that and NodeMaterial, there may be opportunities to fix some existing issues without breaking anyone's existing applications.",
"> Being able to author a PBR model in Substance Painter or download one from Sketchfab,
They are irrelevant.
I get much better results in 3DS Max and that tells me that the GLTFLoader and/or your PBR model are not properly implemented.
> The difference is not huge, and so this not a major concern for many three.js users, but nevertheless it is not as good as it could be.
Your girlfriend looks bad but you're happy because your boss told you that's normal.
OMG ...
> For that reason, your ""good"" result example is not actually correct.
In fact you should fix your ""correct"" example to look good.",Mocking
"> If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB. Yeah, it's unfortunate but I agree that it's not worth breaking backwards compatibility over this. However, as I've been working with larger FBX scenes consisting of multiple models, animated cameras and so on I've found myself wishing that the output of the loader was something more like GLTFLoader's output - that is, it should return an `fbx` object with properties:
```
fbx.animations; // Array<THREE.AnimationClip>
fbx.models; // Array <THREE.Group, THREE.Mesh, THREE.SkinnedMesh>
fbx.cameras; // Array<THREE.Camera>
fbx.asset; // Object
```
There may be other loaders that would benefit from a similar change. We should add this to the backburner (and certainly wait on #11337), but if any loaders do have breaking changes made for whatever reason, then we can use that as opportunity to apply this change as well. Perhaps we should open a new issue to keep track of this?",
"> Perhaps we should open a new issue to keep track of this?
Please do. That's better than resume the conversation in this closed thread.",Impatience
"Just removed the FORCED sRGB encoding in the GLTFLoader.
```javascript
//	if ( material.map ) material.map.encoding = THREE.sRGBEncoding;
//	if ( material.emissiveMap ) material.emissiveMap.encoding = THREE.sRGBEncoding;
//	if ( material.specularMap ) material.specularMap.encoding = THREE.sRGBEncoding;
```
The result is better from any point of view:
- better lighting and material quality
- no need of ""renderer.gammaOutput = true"" anymore (a bad idea anyway)
- now you can use the GLTFLoader with other loaders for the same scene (renderer)
SEA3D + Phong: http://necromanthus.com/Test/html5/Lara_envMap.html
vs
GLTF + PBR: http://necromanthus.com/Test/html5/Lara_gltf_envMap.html",
"I've addressed each of those points in https://github.com/mrdoob/three.js/issues/14419#issuecomment-403513554 â€” we will not be removing the sRGB encoding assignment to sRGB textures in GLTFLoader. If you would like to override that, it is easy to change the texture encoding after loading the model.
If there are no other actions to take here, this issue should be closed.",
"> we will not be removing the sRGB encoding assignment to sRGB textures in GLTFLoader.
That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( @mrdoob ) if they will be removed or not.
In any case, you (and @looeee ) continue to be on the wrong path.
Again: your workflow is not good and is not backwards compatible.
You should start learning from the more experienced people:
https://forum.allegorithmic.com/index.php?topic=8745.0 **In Unity you don't have to do anything**. Maps that are placed in the metallic/smoothness, ambient occlusion are treated as linear by the shader and maps in the albedo are treated as sRGB. What the shader does for the albedo and specular is ""linearize"" the maps. It removes the gamma-encoded values from the map in the shader code by applying an inverse gamma to it of 0.4545. In the Unity workflow, this is done automatically and **you don't need to flag the images as sRGB**.",Bitter Frustration
"> In the Unity workflow, this is done automatically and you don't need to flag the images as sRGB.
Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start.",Bitter Frustration
">That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( @mrdoob ) if they will be removed or not.
Errr.... this is extremely fuzzy. `GLTFLoader` appears to be a community provided example, that lives in `/examples`. If you load three.js alone (`three.min.js`) there wont be any mention of GLTF.
If you use three.js off the shelf you get a scenegraph and various webgl abstractions. In this context, GLTF is just another one of many many examples of how you can translate some generic 3d data / scene file into three.js's structs. So, at a glance, three.js seems like this generic library, that draws stuff to screen. It doesn't care if you fetch that data from some remote server, and it doesn't care how you parse it (collada, fbx, sea3d, gltf... and 40 others). At the end of the day, you are rendering a `THREE.Mesh` with `THREE.Geometry` and `THREE.Material`. **Absolutely all of the loaders share this feature. All of them result in this data structure.**
However, in practice, this is not the case, and you seem to be in the right. GLTF is a ""first class citizen"" of three.js. @mrdoob wrote that several times. SEA3d is some **random format** written by **some unknown people** while GLTF has the backing of THE khronos group. On top of that, it's probably meant to be the backbone of the whole VR/AR revolution, hence so much backing by other big players. This is just an unfortunate circumstance that three.js found itself in as the most widely used webgl library. People want to do 3d, which three.js solves really well with it's scene graph and other abstractions (things like `Mesh`, `Geometry`, `Material`, `Texture` etc.). Unfortunately, people also want to make experiences and expect three.js to be able to do that. This is of course complicated, hence, favoring one particular format and giving it preferential treatment makes sense. It may somewhat hurt the very essence of the library (draw stuff on screen) but it's a trade off. If you care to read all the guidelines, there's a document called [owners](https://github.com/mrdoob/three.js/wiki/Owners). @donmccurdy owns that particular loader, so his word carries as much weight as @mrdoob's, so good luck there :)
The directive right now is:
>three.js must support gltf
So anything that the khronos group comes up with has to be reflected in three.js. If you look at the discussion historically, @mrdoob doesn't really follow what's going on and @donmccurdy is the authority on all things khronos/gltf related.
So for example, khronos has defined a specification for gltf ""extensions"". Out of infinitely many extensions, one involves texture transformations. It has been ratified by the khronos group and because of that, **three.js absolutely must support it**. This warrants a PR with **11 thousands line of code** and increasing the size of the library by 1/3. I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed.",Bitter Frustration
"> I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed.
good point
As I said before, Google already failed with UTF8.
In the current development status, GLTF (and PBR) follow the same path.",Bitter Frustration
">Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start.
Unity has an editor which is a foreign concept for three.js. When you ""load"" a texture into unity, you bring it into the editor which can do all kinds of transformations before actually storing it for game's use. I think that three.js has a different paradigm, since it doesn't care how you create the assets nor how you store them. An analogy would be if three.js came with folder `/utils` and then `/textureTools` or something like that, where you could prepare the assets for three.js. This difference should possibly be considered. The whole gamma correct workflow is extremely complicated, and if i recall correctly you may lose some precision if you store the textures in one way versus another. With three.js this is still kinda low-levelish so i'd prefer to leave multiple options to the user.",
"> If you care to read all the guidelines, there's a document called owners.
Wow, I never say that before. And apparently I ""own"" the `FBXLoader` ðŸ˜ ðŸ˜†",Mocking
"I think it's a software engineering term. You ""own"" a code base?",Mocking
"> And apparently I ""own"" the FBXLoader
So in your opinion you're free to mess it up, isn't it?",Bitter Frustration
"In my opinion everyone who owns something is free to mess it up. I'm not sure if there should be place for disagreement with this, in the modern world :). If you have a Ferrari and decide to put a brick on the gas pedal and have it drive off a cliff, i may think it's a waste but it's YOUR Ferrari, can't get simpler than that. Unless it's 1917 and you're in tsarist Russia.",Irony
"That right there is disruptive content... :(
You're free to fork the project, the examples and do whatever work you deem fit. If it makes you feel any better, what the fbx loader owner did is not at all different - the loader did exist for a couple of years and was written by other people before he took ownership. Fork's no different than this.",Bitter Frustration
"I think the documents wouldn't hurt by having some guideline on how to author textures. The texture can still be in the wrong space regardless of the gltf format, like under calibrated or over calibrated. Something saying ""when making a texture in photoshop do that, when making it in GIMP do this"".",
"> Cut the ""disruptive content"" crap and grow up !!!
@RemusMar I have no idea what you are talking about. I've never marked yours or anyone else's comments as disruptive. However, I would like to say that, much as I have a thick skin and don't care about your opinions re usernames, ad hominem attacks of the kind that you frequently make are not acceptable and should be grounds for being excluded from this community.",Bitter Frustration
"@pailhead agreed, that would be a very useful addition.",
"> I've never marked yours or anyone else's comments as disruptive.
I'm the one who was doing this. As collaborators we have to moderate issues and ensure correct language and behavior.",
"> I'm the one who was doing this. As collaborators we have to moderate issues and ensure correct language and behavior.
Grow up first!",Bitter Frustration
"@Mugen87 I agree with this, and will start to be more vigilant in marking disruptive or otherwise useless comments. In this case, since I was the one being targeted, I didn't think it was appropriate to do so. However, @RemusMar has consistently exhibited what can only be described as disruptive behaviour over a long period of time and I don't think that marking comments is going to bring about any kind of change.
We're consistently having to deal with ad hominem attacks from this user in the form of name calling or other vaguely aggressive statements, and it's quite frankly tedious and detrimental to the development of the three.js community.",Bitter Frustration
"when we have storage OOM gradle doesn't sygnalize that, instead proper info we have a bullshit :)
`$ gradle clean`
>FAILURE: Build failed with an exception.
>
>* What went wrong:
>Unable to start the daemon process.
>This problem might be caused by incorrect configuration of the daemon.
>For example, an unrecognized jvm option is used.
>Please refer to the user guide chapter on the daemon at >https://docs.gradle.org/4.7/userguide/gradle_daemon.html
>Please read the following process output to find out more:`
>-----------------------
>* Try:
>Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log >output. Run with --scan to get full insights.
Unable to start the daemon process... **when explicity run with false flag in props file**
the next try maybe gradle didn't read the prop file proper .. **so explicity set no deamon arg**
`$ gradle --info --no-daemon clean`
>Initialized native services in: /opt/gradle/ceph3us/native
To honour the JVM settings for this build a new JVM will be forked. Please consider using the daemon: https://docs.gradle.org/4.7/userguide/gradle_daemon.html.
Starting process 'Gradle build daemon'. Working directory: /opt/gradle/ceph3us/daemon/4.7 Command: /opt/jdk1.8/bin/java -XX:+AggressiveOpts -XX:+UseG1GC -Xmn512m -XX:MaxMetaspaceSize=1g -XX:SurvivorRatio=40 -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:-OmitStackTraceInFastThrow -XX:SoftRefLRUPolicyMSPerMB=100 -XX:-HeapDumpOnOutOfMemoryError -Xms512m -Xmx3g -Dfile.encoding=UTF-8 -Duser.country=PL -Duser.language=pl -Duser.variant -cp /opt/gradle/lib/gradle-launcher-4.7.jar org.gradle.launcher.daemon.bootstrap.GradleDaemon 4.7
Successfully started process 'Gradle build daemon'
An attempt to start the daemon took 1.005 secs.
FAILURE: Build failed with an exception.
>* What went wrong:
Unable to start the daemon process.
This problem might be caused by incorrect configuration of the daemon.
For example, an unrecognized jvm option is used.
Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
Please read the following process output to find out more:
-----------------------
>* Try:
Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output. Run with --scan to get full insights.
>* Get more help at https://help.gradle.org
still some daemon shit WTF ??? one more try brings same results of bulshit doeasnt reveal the TRUE CAUSE for BUILD FAILED '$ gradle --debug --no-daemon clean'
>{ unrelated sensitive data cut}
>16:50:14.639 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTING
16:50:14.640 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Waiting until process started: Gradle build daemon.
16:50:14.655 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTED
16:50:14.656 [INFO] [org.gradle.process.internal.DefaultExecHandle] Successfully started process 'Gradle build daemon'
16:50:14.656 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is starting. Waiting for the daemon to detach...
16:50:14.657 [DEBUG] [org.gradle.process.internal.ExecHandleRunner] waiting until streams are handled...
16:50:14.659 [DEBUG] [org.gradle.launcher.daemon.bootstrap.DaemonOutputConsumer] Starting consuming the daemon process output.
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: DETACHED
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Process 'Gradle build daemon' finished with exit value 0 (state: DETACHED)
16:50:15.611 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is now detached.
16:50:15.613 [INFO] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] An attempt to start the daemon took 0.982 secs.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Unable to start the daemon process.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] This problem might be caused by incorrect configuration of the daemon.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] For example, an unrecognized jvm option is used.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please read the following process output to find out more:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] -----------------------
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Try:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Run with --stacktrace option to get the stack trace. Run with --scan to get full insights.
16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Get more help at https://help.gradle.org
**pleaes add OOM storage watcher during task execution that will throw some sort of StorageOOM exception**
**distracts from the fact that there was no memory left at the start of the gradle .. which should be signalized apriori any taskl start / evaluate**
tneet to consider the SPACE -- WHERE PROJECT IS EXECUTED (build output) -- WHEN GRADLE WRITES (cache, etc) **those places should be checked during start gradle (LOW MEM WARNING) and monitored as mentioned above**
i have wasted 5 min befor i did to know the real cause..",Bitter Frustration
Closing due to abusive language. Please read our code of conduct and open an issue using appropriate language.,
"@oehme you are funny :) you can learn a bit about semantics, grammar and the meanings of words - essentialy about language and usage :) can you tell me who was here offended ?
**maybe better was to use here a euphemisms so that the dumb people didn't think they were being offended when in fact nobody was...** if you fell so then it is your right to feel as you want to :) understanding is key to everything.. so **try to understand it does not hurt!**
cipa, kutas, jebnij, pierdolenie best ceph3us",Mocking
"If this is your way of communicating, please stay away from our community.",Insulting
"* Mojolicious version: 7.87
* Perl version: v5.26.1
* Operating system: ubuntu
### Steps to reproduce the behavior
perl -MMojo::File -le 'Mojo::File->new(q(a.txt))->spurt(qq(\x{100}))'
### Expected behavior
a.txt written with 2 bytes c4 80
### Actual behavior
get error: Wide character in syswrite at /home/dk/perl5/perlbrew/perls/perl-5.26.1/lib/5.26.1/x86_64-linux/IO/Handle.pm line 483.
a.txt is zero bytes",Bitter Frustration
"Mojo::File::spurt is documented to take bytes, but ```qq(\x{100})``` is a character. Encode it to your favourite encoding first.",Impatience
"> Write all data at once to the file.
Doesn't seem like bytes to me",Irony
"https://metacpan.org/pod/Mojo::File#spurt
```$path = $path->spurt($bytes);```",
"Yes, write all bytes from the scalar.",
"Wow, just ""closed the issue"" as a non-issue? That's mature :D",Mocking
The answer @pink-mist gave was 100% correct. So this is not a bug. If you require additional help please use our official support channels. https://mojolicious.org/perldoc#SUPPORT,
"Well it might be correct, but the documentation ""Write all data at once to the file"" doesn't say anything about the byte/character distinction, and $bytes can be understood as whatever bytes are there in the scalar, which I would expect. So that's at least a case to either fix the documentation or the attitude.",Bitter Frustration
"I don't see this discussion end constructively, therefore i'm locking this issue.",
"* Mojolicious version: 7.85
* Perl version: 5.28.0
* Operating system: macos
### Steps to reproduce the behavior
Just just the delay helper. ### Expected behavior
A deprecation warning with *explaination* what to do, alternative options. How can i replace the helper code with what ? ### Actual behavior
Warnings that delay helper is DEPRECATED. Problem: warning without alternative options. User just panics that the code will stop to work one day.",Bitter Frustration
Please use our official support channels. https://mojolicious.org/perldoc#SUPPORT,
"This discussion is meant to either change/introduce documentation or code. You can mark it as a suggestion, but closing it is not very helpful. For instance, Net::OAuth2::AuthorizationServer tests upon installation produce many of these warnings, just as an example. It affects other CPAN authors along the way, and web searches should end up here, i suppose.",Bitter Frustration
"You did not propose any specific changes that could be discussed here. Until you reach that stage, please use our official support channels for discussions. https://mojolicious.org/perldoc/Mojolicious/Guides/Contributing#CONTRIBUTING-DOCUMENTATION",Impatience
"Please see the following issue:
https://github.com/eslint/eslint-scope/issues/39
`eslint-scope` 3.7.2 has been published an hour ago which is a hacked version that steals the NPM accounts or something.
Please pull the version 3.7.2 from the npm and freeze the account so this does not get propagated.
As a matter of fact, there is no release tag for 3.7.2 on Github, so I think it would be great to consider double checking with Github repository before publishing any code. This would at least limit the possibility of uploading the malicious code to NPM without having Github credentials to tag the release/version. Additionally I believe it's possible to check if the release was signed and somehow enforce all tagged commits to be signed. I think Github returns such information via their API, at least you can see the verified commits via Github's web interface so there must be a way. Developer may be able to opt-in for this extra security via some `.rc` file stored in Git repo.",Bitter Frustration
3.7.2 seems to have been unpublished now. Would love to hear what actually happened.,Impatience
"@akx shortly, the npm credentials have been stolen and the malicious release has been made directly to npm (not GitHub). The malicious code ran upon `npm install` and attempted to transfer the npm authentication token from `.npmrc` to remote server. Surprisingly, the tampered code only contained a bootstrap script that downloaded and executed the main script (with `eval`) from pastebin and the pastebin script had a syntax error in it so that's how it revealed itself. All of what happened is something to think about, why didn't npm spot the calls to `eval` and reject the update. I know many packers use `eval` as well but come on, it's not 1999 again and there are simple measures and protocols that can be implemented in order to strengthen the security of npm ecosystem.",Bitter Frustration
"The first question that came to mind after seeing this issue is: why is there a difference between the published code on npm, and the code on github? I know why, because when you run `npm publish`, you publish from a local directory.
I think npm can increase security by **not letting people publish code from their local directories but only download code from a public repo** like githubâ€™s (ala server-to-server). It would of course only be possible for public modules/repositories. That way all published code always has a history.
It increases security because to publish malicious code you need to:
1. steal both npm credentials and github credentials
2. commit and push malicious code to github
3. pass security a project's build pipeline (and possible security scans) I'm probably not the first with this idea, but has it been considered by the npm team yet?
I can also imagine that the world would be a safer place if `npm publish` would require an npm account with 2FA enabled.",
"@branneman that's true, but some code is transpiled upon release so this has an impact of forcing developers to store all their stuff in git or put the strain on npm servers to pull the code and run all build procedures before publishing. Anyway, there must be a way. ðŸ‘",Bitter Frustration
"> Curious thing, all tampered files had the modification date set to Oct 26, 1985. So I guess now we know the birthday of whoever who did this...
fyi @pronebird all files showing Oct 26, 1985 (a Back to the Future reference joke) is actually intented behavior for NPM (see https://github.com/npm/npm/issues/20439#issuecomment-385121133) and not related to this hack.",Irony
"The actual solution would be https://reproducible-builds.org/
Fixating on one build environment is not a good idea in the grand scheme of things.",
"@jpdriver Thanks, I didn't know this. I mostly use `yarn`, so 1985 came as a surprise. ðŸ‘ (I removed the mention of this everywhere)",
"Has already the attacker got tokens by this attack? If so, will other packages be hacked again?
Do you have a plan to revoke all npm users' token or to stop publishing any packages?",Impatience
"It seems like the reasonable thing to do would be:
1. Temporarily halt all package publications
2. Search the entire registry for references to this virus and unpublish any infected packages
3. Globally revoke all authentication tokens 4. Re-enable package publications",
"The key is revoking all npmrc tokens globally before the attacker takes action. If this isn't done soon enough, the only way to completely prevent the damage will be the manual audit of the almost the npm registry, and that is borderline impossible.
(so I'm really hoping npm revokes everything soon enough, otherwise most people will just have to stop using it)",Bitter Frustration
"@EdwardDrapkin Agree with all of your points, but I'd go a step further with number 2: in addition to searching the registry, unpublish all packages published since the affected version was published (and even then, it's not clear yet if the eslint-scope infection was a result of a further upstream infection). The virus could have modified itself (or have been manually modified, or use a different version entirely) so as to be unrecognizable from the original. There's nothing to say that the pastebin code from this incident is the same as what would be infected in other packages of authors with their credentials compromised. The credentials are being logged to web counters, which to use the credentials they would have to be monitored from an outside source, so there's nothing stopping that outside source from publishing a different script entirely.
Edit: upon further thought, I realize that my suggestion should only be done if it is determined that a significant amount of packages are affected after halting all package publications and revoking all tokens. It's possible that could effectively resolve this specific incident. However this should at least be a ""shot across the bow"" that a single token being compromised can have disastrous ripple effects across the ecosystem.",Bitter Frustration
"Everyone, I am not sure where else to put this and the eslint-scope issue has been locked, but is there anyway to search the npm cache to make sure this version is not cached? I am on windows if it helps or doesn't.",Impatience
"> Surprisingly, the tampered code only contained a bootstrap script that downloaded and executed the main script (with eval) from pastebin and the pastebin script had a syntax error in it so that's how it revealed itself.
The script seems to have run successfully on one user's Windows system. The ""problem"" in the script seems to have been that it would only run correctly if the full source code was fetched in the first chunk - the malware author forgot to ensure all chunks had been fetched. I gleaned this from the original eslint-scope issue.
So it is reasonable to expect many users' auth tokens to have been stolen.",
I +1 the global unpublishing of all packages and revoking all tokens until this is resolved. This does NOT look good.,Bitter Frustration
"@branneman and others suggesting npm integration into git,
Not every npm package uses git, and certainly not everyone uses github. Marrying the two concepts doesn't make sense to me.
I think the best solution is faster communication and awareness of exploited packages. Some brainstorming ideas:
* Maybe integrate the npm ecosystem into something like node security project. Warnings should be shown inside of NPM cli when a vulnerable package is found in the dep tree.
* Easy flagging of infected code to notify maintainers.
* Have stricter publish regimens for foundational (heavily depended on) dependencies. Maybe enforce manual audits with two factor authentication. Realistically dependencies that have thousands and thousands of dependents should not be modified so frequently, or they must go through a manual audit step. * Perhaps have a badge awarded to releases that have been manually audited and approved with a 2 factor verification.
... I could go on",
"Just forbid all packages containing a string ""eval"", or make them require manual reviewal. But my advise would be just ban them for good",Bitter Frustration
"@gedgaudasnikita how would that help? then they write the code to a file, and `require()` it.. or download a compiled binary instead of using JS for their malware.
btw, why do you have two accounts? @nikita-gedgaudas-ht / @gedgaudasnikita - AFAIK the GitHub ToS only allow one account per human",Mocking
"@gedgaudasnikita yea but what about using Function constructor syntax::
```
new Function('console.log(1 + 1)')()
```
I feel like you are addressing the symptom by banning eval not the root problem.",Bitter Frustration
"@gedgaudasnikita That won't work. Eval is needed for a lot of things, and banning it won't do. We need package signing - which NPM actively rejected, but I feel like this has a use right now judging from the current events right now",Bitter Frustration
"@gedgaudasnikita > Just forbid all packages containing a string ""eval"", or make them require manual reviewal. But my advise would be just ban them for good
This does not work well with some JS perf optimization patterns. See the monomorphization that improved webpack's speed by ~80% between version 3 and 4.",Bitter Frustration
"Probably the only acceptable long-term solution is to allow and encourage package signing, with the ability to use a second factor (such as a hardware key) for the signing. Whoever stole the eslint-scope creds could also have stolen signing creds, if they were not using 2 factor auth.
It will take a long time for the majority of package maintainers to reach this level of security awareness, but it's a project that must be started.",Bitter Frustration
@OliverUv +1 on that but I don't know why NPM rejected this - it was a majorly a good security practice,Bitter Frustration
"We've [updated our status page](https://status.npmjs.org/incidents/dn7c1fgrr7ng) about this incident. I'm going to lock this thread because it's likely to draw a lot of traffic and I think actual information will be buried. Please keep an eye on the status page, our social media, [npm.comminuty announcements](https://npm.community/c/announcements), or this thread for any particularly big things.
While you're here, a reminder that [this issue tracker is closing](https://blog.npmjs.org/post/175587538995/announcing-npmcommunity) in favor of [npm.community](https://npm.community), so please direct issues and such to that space. This issue tracker will be closing very soon.",
"(update: apologies, my original post included a link to this issue instead of the actual status page. The link has been updated)",
"#### What's the feature?
IMO `npm install` should only install exact packages unless explicitly told not to via a flag e.g. `--allow-package-upgrades`.
#### What problem is the feature intended to solve?
- Would make CI builds repeatable by default
- Would make it impossible to accidentally, silently upgrade to a compromised package e.g. #21202",
Please open this at <https://npm.community/c/ideas>.,
"Thanks, @KenanY. I'm also going to lock this since it's likely to draw attention from #21202.
As a note: this feature request is covered by `package-lock.json` as of `npm@5`. Please upgrade to the latest npm. The feature, as described, would also not make CI builds repeatable by default, since that would involve all transitive dependencies having been published with this default, and that would likely take an immense amount of time and never be quite done, considering the scale of the registry, so I don't consider it a very good solution.
There are other things in the work to address issues with unintended dependency updates, but now's not the time to talk about them. Have a good one!",
"[I have read the previous stance on the idea of allowing composer to manage multiple versions of a given package.](https://github.com/composer/composer/issues/2167) However, I would like to re-open the discussion as there are valid use cases for wanting multiple versions installed in parallel. I would counter that this is completely possible based on the following arguments:
1. It's not composer's responsibility to ensure that a developer doesn't try to do something they aren't supposed to do; mind you, it would be fair of composer to issue a stern warning, but ultimately allow a developer to shoot themselves in the foot if that's what they so choose to do.
1. PHP's namespace aliasing and/or composer's autoloader might be a way to work around loading two classes from different versions of a package -- in other words, manipulate the namespace in the autoloader so that if I install versions `2.0` and `3.0` of `package/xyz` (with a namespace of `Package\Xyz`) I could reference them in code as either `Package\Xyz\2.0\MyClass or `Package\Xyz\3.0\MyClass`. And if I wanted to import both of them, I could alias the second one to a non-conflicting class name (see point 1 above).
1. There are use-cases where packages are installed to be run as commands (e.g., phpunit, pdepend, phpcs, phpdoc, phpstan, et al) and would not be included/required directly into a PHP project's code base
One use case I've run into is that I want to install `ramsey/jenkins-php`, which inheritantly depends on `nikic/php-parser: ^1.0`, as well as `phpstan/phpstan-symfony` which inheritantly depends on `nikic/php-parser: ^4.0`. Both are commands to be run from a shell, so there would be no actual conflict with each other. The only thing stopping them from both being installed side by side is that composer does not support multiple versions of the same library.
I understand that this is probably going to be a decisive feature request. I also understand I've probably not thought out every single possible hurdle to making this happen. But I feel like this is an important request to discuss with you and the composer community.
Thank you!",
"For some of your use-cases there are already existing work-arounds. In other cases, we simply do not support it. This discussion will lead nowhere.",Bitter Frustration
"I have been using Handbrake for a long time and after the h265 codec appeared, I started using it including HE-AAC for sound. Would it be possible to add its support directly to the Windows and Linux installation file just like it is for macOS? Thank you.",
"Unfortunately, no. There are currently no viable encoders for this that can be included within HandBrake releases for Windows and Linux.
If you compile from source, you can enable the fdk-aac encoder which supports it, but sadly this encoder is not compatible with HandBrakes GPL license so we can not include this in official releases. If the situation changes, we'll look into it.",
For what it's worth Opus> HE-AAC in most ABX listening tests. 96kbps approaches transparency with 128kbps practically transparent with a few edge cases when it comes to music. Hydrogenaud.io has good material on opus if curious.,
"> For what it's worth Opus> HE-AAC in most ABX listening tests. 96kbps approaches transparency with 128kbps practically transparent with a few edge cases when it comes to music. Hydrogenaud.io has good material on opus if curious.
With non-consumer equipment, Opus has a distinct sonic signature, even where the sampling rate change on encode has been avoided. Plain AAC-LC sounds better to me, and I can differentiate AAC at any bit rate compared to lossless even with most consumer equipment, including mobile devices. Do the highest possible quality 44.1->48 kHz conversion to music I'm familiar with, in hardware or software, and I will still be able to identify it. That Opus does this right off the bat makes it a no-go in my book, and I disagree with their reasoning for it.
ABX listening tests are pointless unless they take into account the intimate familiarity many people have with their music; they rarely do. That's a much longer discussion.
Anyway, use what works for you. Just remember that perceptual coding is generally subjective by nature and not everyone will agree with your likes and dislikes. Especially people that can hear an artifact, identify it, and verify what they're hearing with proper analysis tools, in that order. ðŸ˜¸",Bitter Frustration
so that's how it went in FDK. i just recently read it. i was wondering if Nero AAC Encoder will work on handbrake. but is Nero AAC Encoder license can be use in this project? i just wonder..,
"you can dlopen/loadlibrary fdk-aac so/dll, if you do not distribuite it along your software, it does not violate the the gpl license",
"No, actually you can't. See: https://www.gnu.org/licenses/gpl-faq.en.html#GPLPlugins
I'm sure you've seen applications that dynamically link to non-GPL libs in an effort to circumvent GPL restrictions. They are not following the guidelines provided by GNU.",Bitter Frustration
"@jstebbins handbrake dynloads libdvdcss on my system (debian), it's almost the same thing",Impatience
libdvdcss is GPL,
"but not shipped in many distributions due to possible patent problems, still compatible?",Impatience
"There are a couple reasons distributions don't ship dvdcss. In the US, the big one would be aiding piracy. They can't afford to be sued by the MPAA. Second would be patents, but I'm not sure those patents haven't expired by now. They are quite old at this point. Regardless, patents have nothing to do with copyright. The software license is compatible so there is no copyright issue that prevents using it.",
"fdk-aac license is incompatible with gpl because of patents, otherwise it would be a simple BSD, even all mp3 encoding libraries have same problem, not having explicitly stated free patent licensing does not mean you have the rights.
Anyway it's not the point, if you dynload fdk-aac and you do not distribuite the dll, you are violating nothing, the code you compiled in is the wrapper that is still compatible with gpl.
I suspect even coreaudio on osx may have a licensing problem a this point.",Bitter Frustration
"@sherpya No offense intended, but it sounds like you're new to some of this stuff. I suggest you do more reading and leave our project out of your hypotheses. You're not helping.",Insulting
"@sebastinas fdk aac license is a bsd with explicit no patent grant, every aac encoder around has no patent grant (but is not explicit), sorry I don't understand the difference, gnu does not like fdk aac license because in their opinion they promote buying a license for the patent",Bitter Frustration
"To your previous point, we believe Core Audio AAC is not in violation of any license, as it is a system library. We also have periodic contact with Apple, such as recently when they suggested some threading changes to improve compatibility with future macOS versions, and they have never expressed any of the concerns you have.",
"> sorry I don't understand the difference
This is not an appropriate forum for this kind of research, sorry.",Impatience
"I've already made my researches:
```
Fraunhofer FDK AAC license (#fdk)
This is a free software license as far as it goes. It is incompatible with any version of the GNU GPL.
It has a special danger in the form of a term expressly stating it does not grant you any patent licenses, with an invitation to buy some. Because of this, and because the license author is a known patent aggressor, we encourage you to be careful about using or redistributing software under this license: you should first consider whether the licensor might aim to lure you into patent infringement. If you conclude that the program is bait for a patent trap, it would be wise to avoid the program.
It is possible that the pertinent patents have expired. Depending on whether Fraunhofer still has active patents covering the work, the software might be a trap now, or not. (Of course, any program is potentially threatened by patents, and the only way to end that is to change patent law to make software safe from patents.)
```
The fdk aac license: https://directory.fsf.org/wiki/License:Fdk
look at point 3, even ffmpeg aac encoder that is lgpl does not explicit says that has no patent grant, but it does not means that it has, the only difference is that GNU FSF does not like Fraunhofer:
```
It has a special danger in the form of a term expressly stating it does not grant you any patent
licenses, with an invitation to buy some. Because of this, and because the license author is a known patent aggressor, we encourage you to be careful about using or redistributing software under this
license: you should first consider whether the licensor might aim to lure you into patent infringement
If you conclude that the program is bait for a patent trap, it would be wise to avoid the program.
```
about coreaudio, https://github.com/nu774/qaac
uses coreaudio dll on windows, it needs application support library included in itunes, I suspect they are not system libraries, at least on windows, but the api is the same on osx
My idea is the incompatibily is bogus, at least with gplv2, v3 has explicit denial",Bitter Frustration
This very issue has been debated at length on many occasions. We will not debate it again here. I suggest you find some of those other debates and read them.,Impatience
"HandBrake does not use Core Audio AAC on Windows.
Again, please avoid dragging HandBrake into your hypotheses. It only creates doubt for the uninformed.",Bitter Frustration
"Topic locked from further conversation. This isn't open for debate. Nero, FDK, Apple Core Audio are not license compatible for Windows. Thus, per my original feedback, if a license compatible encoder becomes available, we will look into adding it.",
"Sadly it seems that a viable HE-AAC encoder is not forthcoming. As such, I'm closing this for now.
If this changes and a GPL compatible encoder becomes available in the future we can re-visit this as it would be good for feature parity. For now this is unfortunately a macOS exclusive feature in HandBrake.",Bitter Frustration
"As a user i want to be able to configure a sender name per email alias so i don't expose my real name for any of my anonymous email addresses.
### Acceptance criteria
- Allow to set a different name for every email alias and the main email address.
- If there is no sender name configured for the used address, the sender name in the email should remain empty (no fallback).
- Sender name is used when writing an email (new email, reply, forward)
- Sender name is used in calendar invites/replies
- Sender name is used in external notification emails
- Sender name is stored encrypted with group key of the mailbox
- Sender name can be modified by the owner (even if non-admin) of the mailbox that the email address is assigned to.
- When creating a user the user name is also used as the sender name of the user's default email address
- When an admin creates an alias for a user (or for themselves), the sender name for the alias can also be specified. The user name is shown in the text field by default.
- When changing the user name or an email address name other names are not changed automatically (user name and email address names are independent)
- Existing sender names are retained (main sender name and user group). All email addresses inherit the configured sender name.
- Users can still log in with aliases
### Implementation hints
- Introduce configuration object per email address (users mail address and email aliases) which holds the name in the first place and later can also have the signature and out of office notification.
- Run a migration on the client that creates the config object and writes existing email addresses with name to it.
- Currently, a non-admin user can't see their aliases or change their sender name. We need to show alias list to all users and not only admins so that they can change their sender name.
GUI: Replace email alias table with new email address table/list were also the users main email address is listed. Show it also for non admin users. Allow specifying name but not activation/deactivation/deletion. One entry in the list should have two rows. First row shows email address, second row status and sender name were status is either ""main address"", ""activated"" or ""deactivated""
## Tasks
- [x] Design and change model
- [x] Add client migration
- [x] Add configuration UI to settings to configure the name per alias
- [x] Allow non-admins to see mail aliases and configure the names for it
- [x] Change the client to use the new model when sending mails/saving drafts
- [x] Make sure that calendar invites include the right name
- [x] Clarify with support/marketing that it behaves like expected
- [x] Change UI for adding alias to allow configuring the name
## Test notes
- [x] Test new model: release new tutadb without schema upgrade and without a new client, test that spam report preference can still be changed
- [x] Test desktop client migration from the previous DB version
Prepare an existing account with admin, local admin and at least two normal users
- [x] Previously unmigrated account, log in, open mail editor. Sender name dropdown should pick up existing sender name
- [x] Go to settings and change sender name for some mail addresses. It should now be used in mail editor
- [x] Check that it is used in calendar event invites
- [x] Check that it is used in external notification emails
- [x] Check that you can add an alias with sender name, it should default to user name
- [x] Check that you can still edit the user name and it doesn't change sender name
- [x] Log in as a global admin, check that your own sender names can be changed via mail or user management settings
- [x] Check that you can view and change previous user's sender names
- [x] Check another user (user B) (who hasn't logged in yet). Should be able to do the same things
- [x] Check that you can add an alias with sender name, it should default to user name
- [x] Log in as a local admin. Check that you can also view and edit things for both users.
- [x] Log in as user B, check that the settings by admins have been applied, you can view and edit sender names
- [x] Check that mail address table shows primary mail address and email aliases and their statuses correctly
- [x] Check that as non-admin you don't see how many aliases you can add nor you can add/remove aliases
- [x] Check that number of available aliases is correctly updated when an alias is added
- [x] Check that as an admin aliases can be enabled/disabled and the status is displayed correctly
- [x] Check that whitelabel aliases can be deleted but not disabled",
The user must be able to configure the names (not only the admin). The user name is not used as a name for the mail address anymore.,
"I think so. I only have one user, so it's the `admin`.
This one was towards the ability to set a different ""sender name"" for all the registered e-mail addresses (including aliases). If you go to `Settings > User Management`, there is only one `Sender name`; the value there will be used for all outgoing e-mails, regardless of the alias in use.
I think that has to be fixed because not all aliases will/should use the same `Sender name` value â€“ the same applies for the main account address.",
"This suggestion is similar to what is normally present in the UX of provider or an e-mail client (which can do this independently from the provider):
![image](https://user-images.githubusercontent.com/54138438/63581112-8e9e0e00-c596-11e9-9c95-841b87832cca.png)
A specific example (and compliant with the privacy mindset) would be this - one wants to create an identity which is not real (but sounds legit) for services where one does't feel comfortable sharing and/or is not required to provide a real name, but considers these services legitimate enough that one may want to still preserve some shape of form of an identity, especially when there is an e-mail correspondence (contacting support, sales,...).
In a nutshell, one can still do it, but only via aforementioned account-wide Sender name option in the settings, which I would be genuinely surprised if anyone had it filled in as it would ""sign"" every sent e-mail with that name, which we at that point we all could use just aliases with a standard firstname+surname syntax.",
"This is the only issue stopping me from using Tutanota. I've got five aliases, but they have different names. Not being able to have a unique name per alias makes that functionality much less... functional. Creating and paying for five different accounts and then going through the login/logout logistics is not a great workaround. Any idea on when improving this feature is going to be available?",Bitter Frustration
I just started using Tutanota and i'm not sure if i will continue. Protonmail allowed me to have a different sender name for each email as also filtering emails per email address. I think it is critical for tutanota to implement this feature. I really hope they do it soon. It's the only thing i miss in protonmail.,Impatience
"[**@mpfau**](https://github.com/mpfau) commented on [Nov 14, 2018, 8:01 PM GMT+11](https://github.com/tutao/tutanota/issues/516#issuecomment-438587608 ""2018-11-14T09:01:27Z - Replied by Github Reply Comments""):
> The user must be able to configure the names (not only the admin). The user name is not used as a name for the mail address anymore.
Yes, a user can only configure 1 _**sender name**_ per account (they can change the sender name to what ever they like).
The problem here is that all aliases use the same sender name, we want to assign different _**sender name**_ simultaneously to every _**email aliases**_ created by a single tutanota user.
It has been few years and this feature has not yet been implemented. This was proposed in 2015 or 2016 on _**tutanota.uservoice\.com**_ (website is <ins>not in service</ins>, there's an archived page with an idea title of _[Set a different Â«NameÂ» for alias](https://web.archive.org/web/20160731191224/https://tutanota.uservoice.com/forums/237921-general/filters/top)_) with hundreds of likes and possibly thousands or more by now.
We have been informed that it was _planned_, but haven't heard anything since then...",Bitter Frustration
"@0Ky what also to say about it other than ""it's planned, look at the [roadmap](https://tutanota.com/roadmap/)""?
Please take into account that some things from top of it are already implemented and are either in beta or about to be released",Impatience
1,
+1! Feature in demand!,
+1 Essential!,
"+1, coming from a reddit topic where this was linked as the fix for the isolation of aliases being used as login alternatives from the main mail.",
"+1 actually this does matter, the alias is only practical at the moment if the account-level display name is removed entirely or set to something bland.",
What is the status? This is is really need for almost everybody to use their Tutanota account. It has been open for a year and a half.,Impatience
I'm honestly baffled that this isn't in yet. Tutanota must not understand how important this feature is for business use.,Impatience
1,
"I am also really in need of this feature, since I need my aliases to be confidential (not containing my name and surname). Changing back and forth is ridiculous.",Bitter Frustration
What does it take from a technical/developer perspective to implement this feature and thus solve the issue?,
Basically nothing for the backend. The more difficult part is making the user interface for it.,
"We need to make a database change, that's the main part. If there would be a UI it would be nice but that's not the biggest problem.",
"Why is it difficult to do something like this (pseudocode):
```
void SendEmail(EmailAddress senderAddress)
{
var senderName = database.senderNames[senderAddress]
Send(senderAddress, senderName)
}
```
I have almost no experience working with Tutanota-like programs, but I don't see why it would be hard at all to add alias names to the database.",Impatience
"Short answer is that we need to move name from `GroupInfo` (if I remember correctly). There's currently no place to put them so we need to create this mapping. This may affect some other things like user management and creation so it ripples.
In theory it's not hard but it's something that needs time investment.",
"Fair enough.
I hope you get to this soon. For months I have been switching my ""Sender Name"" 2-3 times per day, which is quite annoying, not to mention error-prone.",Bitter Frustration
"I get it (and can relate to this), thanks for the patience!",
Me too is desperate for this feature. I can't get it together. It's a part of privacy in general. But also in just simple daily use. For example: In some official mail I want to use my first and last name. In private mail just my first name.,
Another 1+ for this feature. Looking forward to when it is introduced. Until then I wont be able to use my real name on this service as I always go by my internet pseudonym wherever I sign up.,Impatience
"2 years and no progress, and they are making useless features like calendar. This and crypto payment are far more important than that.",Bitter Frustration
1,
"I've spent 1 hour looking for a way to change sender name according to alias. I've been thinking ""it MUST be somewhere"". Then I gave up, googled it and... here I am with my + 1.",Bitter Frustration
This is a big problem. Please prioritize this.,Impatience
Seconded. It boggles my mind that this hasn't been added yet.,Impatience
"1+
This is an important feature, in order to make the email aliases feature work for real life usages.",
"Actually there is a way to personalize the displayed name : and that is to not set up any account sender name (delete the current sender name or leave it blanck)
So, the system is forced to use whatever username you set up ( either username@tutanota.com or username@my_custom_domain.com
It's not perfect, as it's in all lowercase letters, but still it's better than a privacy breach",
"> It's not perfect, as it's in all lowercase letters, but still it's better than a privacy breach
Definitely! Good tip, thank you. That's a useful workaround waiting for a proper aliases' development.
I still wonder why this ""feature"" has still to be begged after 6 years instead of being considered as basic for any standard email service.",Bitter Frustration
"I know that this feature is still planned as many years before, my request is to **re-prioritize it**... I just learned I've exposed sender name to those who should not know it. Pain.",Bitter Frustration
"It would probably be nice to also allow the configuration of different signatures depending on the chosen sender alias. I think, this can be an issue of its own, but it would be good to keep this in mind when making the required model changes.",
"Och nÃ¶, jetzt wollte ich endlich mal zu Tutanota umziehen und finde das hier. Ich hatte angenommen so etwas sei eine Standardfunktion. Ich bin enttÃ¤uscht :(.
Ich habe mich trotzdem fÃ¼r Tutanota entschieden. Die Vorteile Ã¼berwiegen. Dass Kommentare ausgeblendet werden...nun ja...",Bitter Frustration
Ich habe mich trotzdem fÃ¼r Tutanota entschieden. Die Vorteile Ã¼berwiegen. Dass Kommentare ausgeblendet werden...nun ja...,Bitter Frustration
"### Example 1
This came up with a student who upgraded from 0.6 to 1.0 directly, so never even got a chance to see a deprecation warning, let alone find an explanation for new behavior:
```julia
julia> beforefor = true
true
julia> for i in 1:2
beforefor = false
end
julia> beforefor # this is surprising bit
true
julia> beforeif = true
true
julia> if 1 == 1
beforeif = false
end
false
julia> beforeif # Another surprise!
false
julia> function foo()
infunc = true
for i in 1:10
infunc = false
end
@show infunc
end
foo (generic function with 1 method)
julia> foo() # ""I don't get this""
infunc = false ```
### Example 2
```julia
julia> total_lines = 0
0
julia> list_of_files = [""a"", ""b"", ""c""]
3-element Array{String,1}:
""a""
""b""
""c""
julia> for file in list_of_files
# fake read file
lines_in_file = 5
total_lines += lines_in_file
end
ERROR: UndefVarError: total_lines not defined
Stacktrace:
[1] top-level scope at ./REPL[3]:4 [inlined]
[2] top-level scope at ./none:0
julia> total_lines # This crushs the students willingness to learn
0
```
I ""get"" why this happens in the sense that I think I can explain, with sufficient reference to the arcana in the manual about what introduces scopes and what doesn't, but I think that this is problematic for interactive use.
In example one, you get a silent failure. In example two, you get an error message that is very there-is-no-spoon. Thats roughly comparable to some Python code I wrote in a notebook at work today.
I'm not sure what the rules are in Python, but I do know that generally you can't assign to things at the global scope without invoking global. But at the REPL it does work, presumably because at the REPL the rules are different or the same logic as if they were all are in the scope of function is applied.
I can't language-lawyer the rules enough to propose the concrete change I would like, and based on Slack this isn't even necessarily perceived as an issue by some people, so I don't know where to go with this except to flag it.
Cross-refs:
#19324
https://discourse.julialang.org/t/repl-and-for-loops-scope-behavior-change/13514
https://stackoverflow.com/questions/51930537/scope-of-variables-in-julia",Bitter Frustration
"(Per @mlubin, this is the relevant change https://github.com/JuliaLang/julia/pull/19324)",
Stefan suggested [here](https://discourse.julialang.org/t/repl-and-for-loops-scope-behavior-change/13514/9) that one possibility to solve this issue is automatic wrapping of REPL entries in `let` blocks,
"But wouldn't that be confusing in that you couldn't do
```
a = 1
```
and use `a` after that? Unless `global` is inserted for all the toplevel assignments, I guess?",
The behavior wouldn't be just to wrap everything in a `let` blockâ€”it's more complicated than that. You need to let-bind any global that's assigned inside the expression and then extract the let-bound value to a global at the end of the expression.,
"So you would turn `a = 1` into something like `a = let a; a = 1; end`. And something like
```jl
for i in 1:2
before = false
end
```
would be turned into this:
```jl
before = let before = before
for i in 1:2
before = false
end
end
```
Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months.",Bitter Frustration
"I'm guilty of not having followed master very closed until recently, so this feedback is indeed a bit late. More than a concern for programmers (most `for` loops will be inside a function in library code) I'm afraid this is a concern for teaching. Often `for` loops are taught before functions or scopes (of course you need to understand scopes to really understand what's going on but in teaching things are often simplified).
Here it becomes a bit difficult to teach a beginner how to sum numbers from 1 to 10 without explaining functions or global variables.",
"> Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months.
To be fair, Julia 0.7 was released 13 days ago. This is a new change for most Julia users.",Bitter Frustration
"> Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months
Unfortunately for those of us who can not handle living on the edge, its brand-new from our perspective.",Bitter Frustration
"> Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months.
And for those of us who have been encouraged to stay off the development branches, ""it's brand-new from our perspective.""",Bitter Frustration
"Can we please go back to focus on the issue at hand now, instead of having a meta discussion about how long people have had to test this. It is what it is right now, so let's look forward.",Impatience
">I'm guilty of not having followed master very closed until recently, so this feedback is indeed a bit late. More than a concern for programmers (most for loops will be inside a function in library code) I'm afraid this is a concern for teaching. Often for loops are taught before functions or scopes (of course you need to understand scopes to really understand what's going on but in teaching things are often simplified).
>Here it becomes a bit difficult to teach a beginner how to sum numbers from 1 to 10 without explaining functions or global variables.
This is a big point. After finding out what the issue really is, it's surprising how little it actually shows up. It is less of an issue with a lot of Julia code in the wild and in tests, and it did reveal a lot of variables which were accidentally global (in both Julia Base's tests according to the original PR, and I noticed this on most of DiffEq's tests). In most cases it seems that the subtly wrong behavior isn't what you get (expecting a change in a loop), but rather expecting to be able to use a variable in a loop is what I've found to be the vast majority of where this shows up in updating test scripts to v1.0. So the good thing is that in most cases the user is presented with an error, and it's not difficult to fix.
The bad thing is that it's a little verbose to have to put `global x` inside of the loops, and now your REPL code is also different from the function code. Whether or not it's more intuitive behavior than before is a tough opinion because [there were definitely some edge cases in hard/soft local scoping](http://ucidatascienceinitiative.github.io/IntroToJulia/Html/ScopingExperiment) and so this is clearly easier to explain. But at the same time, while having a much more succinct explanation than the behavior of before, it's now easier to hit the edge cases where understanding scoping rules matters. ðŸ¤·â€â™‚ï¸. I for one would like to see the experiments with `let` blocking. This would keep the ""you didn't really want so many globals"" aspect of it, along with the simplified scoping explanation, while at the same time make REPL code behave like function interiors (which is seemingly what we've always wanted). Or inversely, making people specify variables they want to act as globals
```julia
global x = 5
for i = 1:5
println(x+i)
end
```
could be a nice way to keep the explicitness, and would make the ""REPL code is slow because of globals"" be much more obvious. The downside is that once again throwing things into a function would not require the `global` markers. But given how this tends to show up, it's not really gamebreaking or a showstopper. I'd classify it as a wart that should get a mention in any workshop but it's not like v1.0 is unusable because of it. I hope that changing this behavior isn't classified as breaking and require v2.0 though.",
"I'm not so sure I like the idea that the REPL should behave like a function interior. It clearly isn't, so I expect it to behave like global scope. To me the REPL not behaving like global scope would be potentially even more confusing than the discrepency that causes this issue.
Regardless, at the very least I think that the documentation should be somewhat more explicit about this issue. Casually reading the docs I would have assumed that you would need to use the `local` keyword to get the behavior occurs in global scope by default.",Bitter Frustration
"> I for one would like to see the experiments with `let` blocking. This would keep the ""you didn't really want so many globals"" aspect of it, along with the simplified scoping explanation, while at the same time make REPL code behave like function interiors (which is seemingly what we've always wanted)
If we're going for ""REPL is the same as the inside of a function"" we should also think about `outer`:
```julia
julia> i = 1
1
julia> for outer i = 1:10
end
ERROR: syntax: no outer variable declaration exists for ""for outer""
```
versus:
```julia
julia> function f()
i = 0
for outer i = 1:10
end
return i
end
f (generic function with 1 method)
julia> f()
10
```",
"> Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months.
People haven't been using master for interactive use or for teaching, they've been using it to upgrade packages, which are only minimally affected by this and are mostly written by experienced programmers.
(I was one of the few people who did give feedback in #19324, though, where I argued [for the old behavior](https://github.com/JuliaLang/julia/pull/19324#issuecomment-356484761).)
A non-breaking way out of this would be to change back to the old behavior (ideally not by inserting implicit `let` blocks or anything â€” just restore the old code in `julia-syntax.scm` as an option) in the REPL. Or rather, to make it available in environments like IJulia that might want it, add a `soft_global_scope=false` flag to `include`, `include_string`, and `Core.eval` to restore the old behavior.",Bitter Frustration
"> (I was one of the few people who did give feedback in #19324, though, where I argued for the old behavior.)
Yes, and I greatly appreciate it. It doesn't much matter now since we made the choice, let it bake for ten months and have now released it with a long-term commitment to stability. So the only thing to do now is to focus on what to do going forward.
Having an option to choose between the old behavior and the new one is interesting but it feels very hacky. That means we not only sometimes have a scoping behavior that everyone apparently found incredibly confusing, but we don't always have it and whether we have it or not depends on a global flag. That feels pretty unsatisfactory, I'm afraid.",Bitter Frustration
"> Having an option to choose between the old behavior and the new one is interesting but it feels very hacky.
If someone implements an ""unbreak me"" soft-scope AST transformation, it will be very tempting to use it in IJulia, OhMyREPL, etcetera, at which point you get the even more problematic situation in which the default REPL is seen as broken.",Bitter Frustration
"That's not what I'm saying. Clearly we should use the same solution in all those contexts. But implementing it as two different variations on scoping rules seems less clean than implementing it as a code transformation with one set of scoping rules. But perhaps those are functionally equivalent. However, it seems easier to explain in terms of the new simpler scoping rules + a transformation that takes REPL-style input and transforms it before evaluating it.",
"That could be done as `Meta.globalize(m::Module, expr::Expr)` that transforms an expression by automatically annotating any globals which exist in the module as global if they are assigned inside of any top-level non-function scope. Of course, I think that's equivalent to what the old parser did, but a bit more transparent since you can call `Meta.globalize` yourself and see what the REPL will evaluate.",
"> That could be done as `Meta.globalize(m::Module, expr::Expr)` that transforms an expression by automatically annotating any globals which exist in the module as global if they are assigned inside of any top-level non-function scope.
I actually started looking into implementing something like this a few minutes ago. However, it looks like it would be *much* easier to implement as an option in `julia-syntax.jl`:
* Writing an external AST transformation is possible, but it seems like there are lots of tricky corner cases â€” you basically have to re-implement the scoping rulesÂ â€” whereas we already had the code to get it right in `julia-syntax.scm`.
* It's even more tricky for something like IJulia that currently uses `include_string` to evaluate a whole block of code and get the value of the last expression. Not only would we have to switch to parsing expression by expression, but some hackery may be needed in order to preserve the original line numbers (for error messages etcetera). (Though I found a [hack for ChangePrecision.jl for this sort of thing](https://github.com/stevengj/ChangePrecision.jl/blob/master/src/ChangePrecision.jl#L105-L111) that may work here also.)
* Not to mention of the case of people that `include` external files, which would not be caught by your AST transformation.
> However, it seems easier to explain in terms of the new simpler scoping rules + a transformation that takes REPL-style input and transforms it before evaluating it.
I seriously doubt this would be easier to explain to new users than just saying that the rules are less picky for interactive use or for `include` with a certain flag.",Bitter Frustration
"Here is a rough draft of a `globalize(::Module, ast)` implementation: https://gist.github.com/stevengj/255cb778efcc72a84dbf97ecbbf221fe",
"Okay, I've figured out how to implement a `globalize_include_string` function that preserves line-number information, and have added it to [my gist](https://gist.github.com/stevengj/255cb778efcc72a84dbf97ecbbf221fe).
A possible (non-breaking) way forward, if people like this approach:
1. Release a SoftGlobalScope.jl package with the `globalize` etc. functions.
2. Use SoftGlobalScope in IJulia (and possibly Juno, vscode, and OhMyREPL).
3. Fold the SoftGlobalScope functions into a future release of the REPL stdlib package and use it in the REPL.
Or is it practical to roll it into REPL.jl immediately? I'm not completely clear on how stdlib updates work in 1.0.
Please take a look at my implementation, in case I'm missing something that will cause it to be fragile.",
Can't we have it as a non-default feature of the REPL in 1.1?,Impatience
"Duplicate of #28523 and #28750. To those saying they don't want to teach people about global variables, I suggest teaching functions first, before `for` loops. Functions are more fundamental anyway, and this will help set the expectation that code should be written in functions. While I understand the inconvenience, this scoping behavior can be turned into a pedagogical advantage: ""In fact, global variables are such a bad idea, particularly using them in loops, that the language makes you bend over backwards to use them.""
Adding a non-default feature to the REPL for this seems ok to me though.",
"@JeffBezanson, remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics. These are *not* programming courses and the students often have no programming background. We never do structured programming â€” it's almost *all* interactive with short snippets and global variables.
Furthermore, the reason I'm using a dynamic language in the first place is to switch fluidly between interactive exploration and more disciplined programming. The inability to use the same code in a global and a function context is a hindrance to that end, even for someone who is used to scoping concepts, and it is much worse for students from non-CS backgrounds.",Bitter Frustration
"> remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics. These are not programming courses and the students often have no programming background. We never do structured programming â€” it's almost all interactive with short snippets and global variables.
Many of us Julia users have absolutely 0 CS background (including myself), but it seems to me that the proper attitude (*especially* for students) is a willingness to learn rather than demanding things be changed for the worse to accommodate our naivete.
Now, I'm not necessarily implying that this particular change would be for the worse as I only have a limited understanding of what's going on here, but if it *is* the case that this is a significant complication or makes it excessively easy to write needlessly badly performing code it does not seem worth it to make a change in order to have a better lecture example. You can't change the laws of physics so that the electrostatics examples you show to freshman are more applicable to real life.
So my question as a non-CS user who also cares about performance is how would I be likely to screw up if this were made the default behavior. Is it literally just the sorts of examples we are seeing here that are a problem (which I was already aware of), or are we likely to often screw this up badly in more subtle ways?
For what it's worth, I do agree that having code behave differently depending on its enclosing scope is a generally undesirable feature.",
"Making code harder to write interactively, forcing beginners writing their first loops to understand obscure scoping rules, and making code pasted from functions not work in global scopes does not help programmers write fast code in functions. It just makes it harder to use Julia interactively and harder for beginners.",Bitter Frustration
"> Can't we have it as a non-default feature of the REPL in 1.1?
Making an ""unbreak me"" option the default seems wiser, especially an option that is aimed squarely at beginning users. If it is a non-default option, then precisely those people who need it most will be those who don't have it enabled (and don't know it exists).",
"What would the proposed REPL-mode do to `include`ed scripts? Would the evaluation of global statements depend on whether the REPL mode is activated? If so, IMO this would be at odds with the 1.0 stability promise.",
"If we did something like this it seems like it might make sense for the module to determine how it works. So `Main` would be a ""soft scope"" module while by default other modules would be ""hard scope"" modules.",
"I was interested to see if it was possible to monkey patch the REPL to use @stevengj's `globalize` function and it appears it is without too much effort (though quite hacky). See the [gist](https://gist.github.com/dawbarton/0388715fb56fb5cd05e0e4b12c322815). This doesn't work with Juno (or anything else that calls `Core.eval` directly).
I'm **not** going to be recommending this to people, but it's quite useful to me when doing quick-and-dirty data analysis. I would very much like to see a (better thought out) solution since it really is quite confusing for inexperienced and often reluctant coders (i.e., my students) when you can't copy and paste in code from a function into the REPL to see what it does and vice-versa.
```julia
julia> a = 0 0 julia> for i = 1:10 a += i end ERROR: UndefVarError: a not defined Stacktrace: [1] top-level scope at .\REPL[2]:2 [inlined] [2] top-level scope at .\none:0 julia> using SoftGlobalScope [ Info: Precompiling SoftGlobalScope [363c7d7e-a618-11e8-01c4-4f22c151e122] julia> for i = 1:10 a += i end julia> a 55 ```
(BTW: the above is about as much testing as it has had!)",Bitter Frustration
"> What would the proposed REPL-mode do to included scripts?
Nothing. Basically, the proposal is that this would only be for code entered at an interactive prompt. As soon as you start putting things in files, you need to learn the ""hard scope"" rules. Hopefully, when you start putting code into files you should start using functions.
It's not ideal for there to be pickier scoping rules for global code in files than at the prompt. But I think that #19324 combined with the Julia 1.0 stability promise leaves us with no ideal options.",Bitter Frustration
"@stevengj: > @JeffBezanson, remember that many of us would like to use Julia as a substitute for Matlab etcetera in technical courses like linear algebra and statistics. These are not programming courses and the students often have no programming background. We never do structured programming â€” it's almost all interactive with short snippets and global variables.
Having taught courses using Julia to students with prior exposure to Matlab/R/..., I sympathize with this concern. But at the same time, I don't think that using Julia just as a Matlab etc substitute is a viable approach: as demonstrated countless times by questions on Discourse and StackOverflow, this can lead to performance pitfalls that are difficult to fix and understand, possibly entailing an even larger cost than investing in understanding how Julia is different from these other languages (cf posts with topics ""I translated this code from Matlab and it is 10x slower"").
I think that the key issue is the *silent* failure; the issue *per se* is easy to understand and fix. I would suggest keeping the new behavior, but giving a warning in `Main` (by default; it should be possible to disable it).",
"For me, the larger issue is the perceived inconsistency. That is, I'm ok with Julia doing things differently, but:
- Why should code pasted from a function not work in a REPL?
- No other language I've ever used has this behavior, and it's another barrier to adoption
- Why do `for` blocks behave differently from `begin` and `if` blocks? (`if` I sort of understand, but a block is [should be] a block.).
Regarding bullet 2, I think this is a bigger deal than we who have been using Julia for a while (and are committed to the language) might understand. I can tell you I'm currently 0 for 7 in convincing my group to write code in Julia; two of those were due to this `for` loop problem which I couldn't explain because I hadn't been exposed to it before. The remainder I guess we can chalk up to my lack of charisma.
My preference would be to ensure that code pasted from a function into a REPL behaves identically to the function, and that `for` loops do the expected thing when using them to analyze data interactively; that is, specifically, that they mutate external / global variables when directed without any special keywords.",Bitter Frustration
"> I don't think that using Julia just as a Matlab etc substitute is a viable approach: as demonstrated countless times by questions on Discourse and StackOverflow, this can lead to performance pitfalls that are difficult to fix and understand, possibly entailing an even larger cost than investing in understanding how Julia is different from these other languages.
Sorry, but this argument is ridiculous to me. I'm not talking about classes where I'm teaching programming. There's a place for simple interactive computations, and in non-CS classes it's common to be introduced to programming languages as a ""glorified calculator"" to start with. Teaching [performance computing in Julia](https://github.com/stevengj/18S096) is an entirely different process â€”Â but it doesn't hurt if they've already been using Julia as their ""calculator.""
If you start by introducing students to Matlab as their ""calculator,"" it's much harder to make the transition to ""real"" programming, because their first instinct is to do as much as possible with Matlab before jumping ship, at which point their bad habits are ingrained and they are reluctant to learn a new language. In contrast, if you start with Julia as your glorified calculator, when it comes time to do more serious programming you have a much wider array of options available. You don't have to train them to cram everything into ""vector"" operations or force them to do things badly before they do it right.
Are you saying I shouldn't use Julia in [my linear-algebra course](https://github.com/stevengj/1806/blob/fall17/summaries.md)? Or that I should only use it if I'm prepared to teach computer science as well as linear algebra?",Impatience
"I agree with @stevengj both on the problem (teaching to non programmers becomes much harder) and on the solution (make things work in the REPL and the various IDEs). Including a script would still have the Julia 1.0 scoping rules but that's less of a concern, one just has to be careful to have the ""we can put our for loop in a function and then call the function"" class before the ""we can put our for loop in a file and include the file"" class.
This sounds like a good compromise as interactive debugging at the REPL doesn't become more painful than it needs to be (or more confusing to new users), while normal code in scripts has to follow strict scoping rules and is safe from bugs overwriting some variables accidentally.",
"> Are you saying I shouldn't use Julia in my linear-algebra course? Or that I should only use it if I'm prepared to teach computer science as well as linear algebra?
You may have misunderstood what I was saying (or I did not express it clearly). I was talking about courses that *use* Julia to teach something domain-specific (eg I taught numerical methods to econ grad students), not CS courses (which I have no experience with).
The point that I was trying to make is that it is reasonable to expect a certain level of difference between Julia and language X (which may be Matlab); conversely, ignoring this can (and does) lead to problems. Personally, when learning a new language, I prefer to face these issues early on; also, I think simplicity and consistency of the language semantics is more important than similarity to other languages in the long run. But I recognize these preferences as subjective, and reasonable people can have different ones.",
"I've created the (unregistered) package https://github.com/stevengj/SoftGlobalScope.jl
If this seems reasonable, I can go ahead and register the package and then use it by default in IJulia (and perhaps submit PRs to Juno etcetera).",
"> The point that I was trying to make is that it is reasonable to expect a certain level of difference between Julia and language X (which may be Matlab)
Obviously. When I say ""use Julia instead of Matlab"", I don't mean I'm trying to teach them Matlab syntax in Julia, nor am I specifically targeting former Matlab users.
> I prefer to face these issues early on
It's not about differences from Matlab per se. I would really rather not talk about global vs local scope and the utility of a `global` keyword for static analysis the first time I write a loop in front of non-CS students, or the first time they paste code from a function into the REPL to try it interactively. I would rather focus on the math I'm trying to use the loop to express.
**No one here is arguing for soft interactive scope just because that's what Matlab users expect. We are arguing for it because that is what *all* first-time users will expect,** and because long **digressions into the unfamiliar concept of ""scope"" are certain to derail** any non-CS lecture where you are showing a loop for the first time. (And even for experienced users, it's rather inconvenient to be forced to add `global` keywords when we are working interactively.)",Bitter Frustration
One other fix not mentioned here is to simply stop making â€˜forâ€™ define a scope-block (just function and let would create new scope),
"@vtjnash, I'd rather focus this discussion on things that we can do *before* Julia 2.0. I agree that having interactive mode behave differently is only a stopgap, though, and we should seriously contemplate changing the scoping rules in a few years.",
"Good point, this also needs `import Future.scope` ðŸ˜€
(I think this module/namespace/behavioral effect already is reserved/exists)
As a reminder here, the change was to ensure that code behaves the same in all global scope environments, regardless of what else had previously been evaluated in that module. Before this change, you could get completely different answers (resulting from different scope assignment) simply by running the same code twice or by moving it around in a file.",
"> Before this change, you could get completely different answers (resulting from different scope assignment) simply by running the same code twice or by moving it around in a file.
The number of complaints I saw about that in practice (zero) are certain to be dwarfed by the number of complaints and confusion you will see (and are already seeing) about the current behavior.",Bitter Frustration
"> Before this change, you could get completely different answers (resulting from different scope assignment) simply by running the same code twice
Do you mean that in the below code, `a` changes between the first and second `for` loops? In my mind, that's expected behavior, not a bug.
```
a = 0
for i = 1:5
a += 1
end
for i = 1:5
a += 1
end
```",
"> What would the proposed REPL-mode do to included scripts?
@mauro3 @stevengj I suppose adding a function (say, `exec(""path/to/script.jl"")`) can be done with just a minor version bump? We can also warn `exec`'ing another file from `exec`'ed script and then put some pedagogical messages there to nudge them to use `include`.",
"Some thoughts I wrote down last night while trying to wrap my head around this issue (yet again) to try to figure out what the best course of action might be. No conclusion, but I think this lays out the problem quite clearly. After having thought about this issue for some years, I don't think there is any ""ideal solution""â€”this may be one of those problems where there are only suboptimal choices.
---
People naively view global scope as a funny kind enclosing local scope. This is why global scopes worked the way they did in Julia 0.6 and prior:
- If an outer local scope creates a local variable and an inner local scope assigns to it, then that assignment updates the outer local variable.
- If an outer global scope creates a global variable and an inner local scope assigns to it, then that assignment previously updated the outer global variable.
The main difference, however, is:
- Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope.
- Whether a global variable exists, however, cannot be independent of order, since one evaluates expressions in global scope, one at a time.
Moreover, since global scopes are often quite lengthyâ€”not infrequently spread across multiple filesâ€”having the meaning of an expression depend upon other expressions an arbitrary distance from it, is a â€œspooky action at a distanceâ€ effect, and as such, quite undesirable.
---
This last observation shows why having the two different versions of a for loop at global scope behave differently is problematic:
```jl
# file1.jl
for i = 1:5
a += 1
end
```
```jl
# file2.jl
a = 1
```
```jl
# file3.jl
for i = 1:5
a += 1
end
```
```jl
# main.jl
include(""file1.jl"")
include(""file2.jl"")
include(""file3.jl"")
```
Also note that the contents of `file1.jl` and `file3.jl` are identical and we could simplify the example by including the same file twice with a different meaning and behavior each time.",
Another problematic case is a long-running REPL session. Try an example from somewhere online? It fails because you happened to have a global variable by the same name that the example uses for a local variable in a for loop or similar construct. So the notion that the new behavior is the only one that can cause confusion is definitely not accurate. I agree that the new behavior is a usability issue in the REPL but I just want to temper the conversation and present the other side clearly here.,
"My small suggestion, that does not deal with the repl problem, but would be useful for didactic purposes when teaching the language not-interactively, at least: define a main block named ""program"", like can be done in fortran (it is the same as the ""let...end"" above, just with a more natural notation):
program test
...
end one could teach the language without going into the scope details and only eventually discuss that point.",
"> Another problematic case is a long-running REPL session. Try an example from somewhere online? It fails because you happened to have a global variable by the same name that the example uses for a local variable in a for loop or similar construct. How many mailing-list complaints and github issues have been filed about this by upset users? Zero, by my count. Why? Probably because this behavior is **fundamentally unsurprising** to people â€” if you work in global scope, you depend on global state.
> So the notion that the new behavior is the only one that can cause confusion is definitely not accurate.
I think this is a false equivalence â€”Â there is a **vast disparity in the level of potential confusion** here. In Julia 0.6, I could explain your example to a student in seconds: ""Oh, see this loop depends on `a`, which you changed here."" In Julia 1.0, I'm honestly worried about what I will do if I'm in the middle of a linear-algebra lecture and have to mysteriously type a `global` keyword in front of students who have never heard the word ""scope"" in the CS sense.",Bitter Frustration
"> we should seriously contemplate changing the scoping rules in a few years.
Absolutely not. Do you seriously want to go back to the pre-v0.2 world (see #1571 and #330) of loop scope?
We have actually never fully supported copying and pasting code from a function line-by-line into the REPL. So we can view this as an opportunity to make that work. Specifically, while it ""worked"" for `for` loops, it did not work for inner functions:
```
x = 0
f(y) = (x=y)
```
Inside a function, `f` will mutate the `x` from the first line. In the REPL it won't. But with a transformation like that in SoftGlobalScope.jl it could work. Of course, we probably wouldn't want that on by default since then pasting stand-alone function definitions wouldn't work. The first thing that comes to mind is a REPL mode for line-by-line function debugging.",
"> Do you seriously want to go back to the pre-v0.2 world
No, I want to go back to the 0.6 world. ðŸ˜‰",Irony
"I guess I was responding more to:
> One other fix not mentioned here is to simply stop making â€˜forâ€™ define a scope-block",
"> We have actually never fully supported copying and pasting code from a function line-by-line into the REPL. So we can view this as an opportunity to make that work.
I very much appreciate this sentiment and for my use cases it would really help. From my perspective it is really about making the REPL as useful as possible rather than changing the scoping rules of the language directly. That said, the more I think about this problem the more I see the conflicting views I (personally) hold as to what the REPL should do.
To be concrete, I'd very much like it if the REPL matched the scoping rules of a function body; i.e., variables are local rather than global and you can just copy-and-paste code directly from a function and know that it will work. I imagine a naive implementation would be something like let-block wrapping (as has been mention previously) of the form
```
julia> b = a + 1
```
being transformed into
```julia
let a = _vars[:a]::Float64 # extract the variables used from the backing store
# Code from the REPL
b = a + 1
# Save assigned variables back to the backing store
_vars[:b] = b
end
```
Done properly (i.e., by someone who knows what they are doing), I imagine that this would have a number of benefits over the existing REPL. 1. previous workflows with interactive data analysis/computation just work. 2. far fewer posts on Discourse where the basic response is ""stop benchmarking with global variables"" - everything would be local and so hopefully fast! :) 3. copy-and-paste to/from a function body works as expected. 4. a `workspace()` like function is trivial if the backing store is some sort of Dict; just clear it out. 5. globals become explicit - things are local unless you specifically ask for them to be global; this is a big advantage from my perspective, I don't like implicitly creating globals. A very minor final point (and I hestiate to add this!), this would match the behaviour of Matlab making it easier for people transitioning - at the Matlab REPL all variables seem to be local unless explicitly annotated as global. Until a few hours ago this story sounded great to me. But after Jeff's comment about functions I thought about pasting in stand-alone function definitions and how this approach would basically prevent that since function definitions should go in the global scope (at least, that is probably what is intended); but then what if they *were* intended to go into the local scope (an inner function)? There is no information to disambiguate the two possibilities. It would seem that two REPL modes are needed, one with local scope and one global scope. On one hand that could be very confusing (imagine the Discourse posts...) but on the other it could be extremely useful. (Having both REPL modes would also be non-breaking since you are just introducing new functionality :) )
Going for the halfway house of `SoftGlobalScope.jl` might end up being the least confusing compromise but my worry is that it's just another set of rules to remember (which things work in the REPL but not in my function body/global scope and vice-versa).
Apologies for the long post but I think this is important for usability (and it helped me think it through!).",
"> How many mailing-list complaints and github issues have been filed about this by upset users? Zero, by my count. Why? Probably because this behavior is fundamentally unsurprising to people â€” if you work in global scope, you depend on global state.
Hmm, did you really make a systematic study of this? I must have missed that. Nevertheless, this does not mean that this behavior is not a source of bugs or unexpected results; just that after the user has figured it out, it was recognized as correct behavior and thus did not prompt an issue/complaint.
> In Julia 1.0, I'm honestly worried about what I will do if I'm in the middle of a linear-algebra lecture and have to mysteriously type a global keyword
I sympathize with this problem. When I taught into some simple programming to econ students necessary for a course, I usually suggested that they go back and forth between wrapping code in functions, and simply commenting out `function` and `end` and running things in the global scope, so they could inspect what is happening. This pretty much made up for the lack of debugging infrastructure at that time in Julia.
It appears this approach is no longer feasible. But I wonder if it was really the right way to do it anyway, and in the meantime various things have improved a lot (#265 was fixed, [Revise.jl](https://github.com/timholy/Revise.jl) and recently [Rebugger.j](https://github.com/timholy/Rebugger.jl) have improved workflow/debugging considerably).
It seems that this issue does not bother experienced users very much, the main concern is confusion in a pedagogical setting. I have not experimented with this myself yet, but I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope. These are elements of good style anyway and would benefit students.",
"Just a wee note: whilst special casing the global scope of the REPL, will allow copy-pasting code into and from functions, it will not allow copy-pasting into/from the global scope of another module.",
"> I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope.
This is totally impractical in a class that is not focused on teaching programming. I might as well not use Julia in my classes if I can't use it interactively and/or have to write functions for everything first.
(And it's not just pedagogical. Loops in global scope are *useful* for interactive work. And one of the main reasons people like dynamic languages for technical computing is their facility for interactive exploration. Not all coding is performance-oriented.)",Bitter Frustration
"There have been dozens of threads and issues over the years in which people are confused or complaining about the old ""soft/hard scope"" distinction, so claiming that no one has ever been confused by or complained about the old behavior is just... not true. I could dig some of them up, but you were around, @stevengj, so you can dig them up just as easily and I have a hard time believing that you didn't notice or don't remember these complaints and conversations.",Bitter Frustration
"@StefanKarpinski, I'm specifically referring to people complaining that a global loop depends on global state. I don't recall anyone complaining that this was bad behavior, nor can I find any examples of this.
I agree that people have been confused about when and where assignment defines new variables, but it has usually been in the other direction â€” they wanted local scopes to act more global (rather than vice versa), or to not have a distinction between `begin` and `let`. IIRC, the complaint was never that assigning to a global variable in a global loop had the surprising side effect of modifying a global.
The whole issue of scoping is confusing to new users, and it will continue to be so. But the confusing part was not cases where assigning to a global variable name affected the global state. The current behavior makes this worse, not better.",Bitter Frustration
"@StefanKarpinski: I have the feeling that previously, the confusion with soft/hard scope was more of a theoretical nature (of people reading the manual) rather than of a practical on (of people getting unexpected results). That was definitely how it was for me and what, e.g., the search results [here](https://groups.google.com/forum/#!searchin/julia-users/soft$20hard$20scope) support this; I found one counter-example [here](https://discourse.julialang.org/t/mysterious-error-undefined-variable/7819).
On the other hand, this new behavior will not confuse people when reading the manual, but when using the REPL. Arguably the latter is worse.",Bitter Frustration
"[SoftGlobalScope.jl](https://github.com/stevengj/SoftGlobalScope.jl) is now a registered package. My intention is to enable it by default (opt-out) for IJulia, at least this semester.",
"@mauro3, even your ""counter-example"" is about someone confused by *hard* scope, not by soft scope. Making more scopes ""hard"" in 0.7 is certain to create more of this sort of confusion.",Bitter Frustration
"I would point out that IJulia has the interesting possibility of making variables local do blocks by default. I.e. if you do this in a single block then it works:
```
t = 0
for i = 1:n
t += i
end
t
```
... and `t` is only visible within this evaluation block. If you wanted it to be visible outside, you would have to do this:
```
global t = 0
for i = 1:n
global t += i
end
t
```
I have also considered a similar approach for Julia where the blocks are files rather than module. In other words just doing `t = 0` at top scope creates a variable that is file-local rather than global. To declare a truly global variable, you'd need to write `global t = 0` which would then be visible throughout the module. Perhaps too weird, but it has occurred to me many times over the years.",
"> IJulia has the interesting possibility of making variables local do blocks by default
@StefanKarpinski, I think this would be even more confusing, and would run counter to how notebooks are normally used. It is common for the same variable to be used/modified in multiple cells, so requiring a `global` keyword for *all* inter-cell variables is a nonstarter to me â€”Â it would require even *more* discussion of scope concepts than the issue with `for` loops we've been discussing here.",
"I think as long as we all agree --- as we seem to --- that this is mostly or entirely an issue for interaction, then we have a way forward. If we special-case this in the REPL (as is being done for IJulia), the only bad case is developing something in the REPL and then moving it to top-level script code. Arguably that's the point where you should introduce functions, so I don't think it's so bad. Copy-pasting code between the REPL and function bodies will work (mostly), which is probably good enough.
Then we also have the option of further justifying/clarifying the distinction by making REPL variables somehow local to the REPL --- i.e. not normal global variables, not available as `Main.x`. This is very similar to what @StefanKarpinski just proposed above, but shared among all input blocks/cells.",
"From a practical point of view, getting this ""fixed"" in the REPL is not
only important for teaching/non-programmer users. This behaviour also
makes interactive debugging via the REPL (by copy-pasting parts) very
unpractical. This mode of debugging can sometimes be preferable (even to a
good debugger and) even for experienced programmers (and is often one of
the reasons to prefer a dynamic language). Of course for the experienced
programmers, being optional shouldn't be a problem; For novice users it
would be preferably the default.
@StefanKarpinski As a naive programmer, I don't really see what is so wrong in viewing the
global scope as a funny kind enclosing local scope, especially in dynamic
languages. I do understand that from a compiler point of view it is not
necessarily correct (in Julia), but it is a nice, easy and useful model
for a (naive) programmer. (I also suspect it might be actually implemented that way in
some languages).
Julia also seems to presents it that way to the programmer:
The following function function will give the error ""a not defined"", which
it will not do if a=1 is put before the for loop.
function test()
for i = 1:10
a=a+i
end
a=1
@show a
end
which, unless I complete misunderstood, seems at odds with ""Whether an
outer local variable exists, by design, does not depend on the order of
appearance or execution of the expressions in the outer local scope"".
I very much agree with avoiding ""spooky action at a distance"", and much
prefer explicit definition for using globals at the function/call stack
level and would personally also like having something like loading from a file in
its own scope, and requiring explicit definition for using global variables.
At the level of loops is going a bit to far for me though, as the
definitions/context is usually quite near.
The 3 files example is a bit contrived (and fails with the expected ""a not
defined"" error): You would normally put the initial definition in the same
file. There is actual spooky danger in this (and I have been bitten by it
in other languages) in that includes are run in the global scope, so you
are inadvertently defining a global variable that may interfere with other
code. However, having to use global in the loop is not a solution to
this problem.
wrt to the long-running REPL session:
The current behaviour replaces a very rare and easy to spot failure mode
for running an online example in the REPL (you miss copy/pasting the
initial definition of the variable before the loop, and already have the
same variable defined globally from something previous) with not being
able to run an online example correctly if it is part of a function
(without adding global everywhere), and not solving the problem if it is
not (if global is already there in the online code, you will still use the
wrong value in the already existing global variable)",Bitter Frustration
"I should have tuned into this earlier, but after a brief moment of concern all seems to be well.
> We have actually never fully supported copying and pasting code from a function line-by-line into the REPL...The first thing that comes to mind is a REPL mode for line-by-line function debugging.
Indeed Rebugger (which is exactly that) works properly on 1.0 only because it lacks the scope deprecation of 0.7, and could never be made to work on 0.6. However, I'm pleased to be able to verify that SoftGlobalScope.jl seems not to break that. For example, if you step deeply enough into `show([1,2,4])` you get here:
```julia
show_delim_array(io::IO, itr::Union{SimpleVector, AbstractArray}, op, delim, cl, delim_one) in Base at show.jl:649
io = IOContext(Base.TTY(RawFD(0x0000000d) open, 0 bytes waiting))
itr = [1, 2, 4]
op = [
delim = ,
cl = ]
delim_one = false
i1 = 1
l = 3
rebug> eval(softscope(Main, :(@eval Base let (io, itr, op, delim, cl, delim_one, i1, l) = Main.Rebugger.getstored(""bbf69398-aac5-11e8-1427-0158b103a88c"")
begin
print(io, op)
if !(show_circular(io, itr))
recur_io = IOContext(io, :SHOWN_SET => itr)
if !(haskey(io, :compact))
recur_io = IOContext(recur_io, :compact => true)
end
first = true
i = i1
if l >= i1
while true
if !(isassigned(itr, i))
print(io, undef_ref_str)
else
x = itr[i]
show(recur_io, x)
end
i += 1
if i > l
delim_one && (first && print(io, delim))
break
end
first = false
print(io, delim)
print(io, ' ')
end
end
end
print(io, cl)
end
end)))
[1, 2, 4]
```
So it works fine on 1.0 (with or without `softscope`). On 0.7, evaluating this (with or without `softscope`) will yield
```julia
â”Œ Warning: Deprecated syntax `implicit assignment to global variable `first``.
â”‚ Use `global first` instead.
â”” @ none:0
â”Œ Warning: Deprecated syntax `implicit assignment to global variable `first``.
â”‚ Use `global first` instead.
â”” @ none:0
[ERROR: invalid redefinition of constant first
Stacktrace:
[1] top-level scope at ./REBUG:9 [inlined]
[2] top-level scope at ./none:0
[3] eval(::Module, ::Any) at ./boot.jl:319
[4] top-level scope at none:0
[5] eval at ./boot.jl:319 [inlined]
[6] eval(::Expr) at ./client.jl:399
[7] top-level scope at none:0
```
So 0.7/1.0 are definitely a step forward, and if `softscope` makes certain things easier without breaking important functionality that's great. The biggest concern, therefore, is simply how to intercept this appropriately without tanking other packages (https://github.com/stevengj/SoftGlobalScope.jl/issues/2).",
"@timholy, SoftScope does not touch the arguments of macro calls (since there is no way to know how the macro would rewrite it), so `:(@eval ...)` is protected.",
"> seems at odds with ""Whether an
outer local variable exists, by design, does not depend on the order of
appearance or execution of the expressions in the outer local scope"".
The (outer) local variable `a` exists, but has not been assigned yet. If the loop tried to assign to `a` before reading it, the assignment would be visible outside as well.
In general, creating a variable binding and assigning a value to it are separate steps.",
"What is the time-line on this? It seems it would be a great improvement to user usability. And at this ""critical"" time of Julia with 1.0 out, it would seem advantageous to get this fixed asap (in the way suggested by Jeff above) and tag a new Julia version or REPL version. (Sorry for this arm-chair comment, as I certainly will not fix this!)",Impatience
"@JeffBezanson I was going to argue that while this is true (for the implementation/compiler), the naive julia programmer cannot not see any different behaviour from his simpler conceptual model (a variable starts existing at the moment it is defined). Unfortunately you are right, the following code will not give an error, while it will give an error if you leave out the a=2 at the end
function test()
for i = 1:10
a=1
end
println(a)
a=2
end
I'll explain the unfortunately: I can understand the behaviour (because i've worked with compiled languages before) but still find it confusing and unexpected. How bad must it be to someone with only scripting experience or new to programming. Also, I found some code that shows the behaviour, I do not see a useful application (maybe you can help me there)
On the REPL:
I just got more convinced that changing the scoping back to ""normal"" at least in the REPL (no need to add global in loops) is high priority: I was testing some things in the REPL today and got (again) bitten by it, taking some time to realize it. Given that I follow Julia already some time, really like a lot of it, am even following this thread about the problem, I would even call it a showstopper: A newbee (to the Julia) testing out the language is very likely not to find out the problem and just give up.",Bitter Frustration
"@jeffbezanson and I are both on long-awaited vacations (I should not be reading this). We can figure out what to do in a week or so.
@derijkp, while the feedback is appreciated, scoping rules are not up for a broader debate or revision. The only thing on the table is special-casing interactive eval. The SoftGlobalScope package is already an excellent experimental implementation and it may just be a matter of making that part of Base and using it in the REPL.",Impatience
"@derijkp A short answer is that I think it's easier if the scope of a variable corresponds to some block construct (e.g. the body of a function or loop). With your suggestion, the scope of a variable would be some subset of a block, which I think is ultimately more complex and confusing --- you can't point to a syntactic form that corresponds to the scope of the variable.
Yes, I can believe this is a mismatch for some people's intuition. But you can only optimize for the first ten minutes of using a language up to a point. The real question is, how hard is it to teach/learn how it works, and which design will save time in the long run (by making the language simpler, making it easier to develop tooling, etc.)?",
"(in agreement with much of the above about modifying the behavior of the REPL)
I'd like to see the REPL be in a way that does not lead to [this stackoverflow question](https://stackoverflow.com/questions/52187073/does-scoping-in-julia-1-0-0-with-for-loops-make-sense-to-beginners) and sooner would be best as many new eyes are looking at Julia",Impatience
"I agree... And also think that the scoping rules shouldn't necessarily change, just all of the interactive interfaces (i.e. the REPL, Jupyter, and Juno control enter)
This is not just about beginners learning a new rule. If you can't copy and paste fragments of code into the REPL, jupyter etc and also into functions, it is a major annoyance for intermediate programmers as well. Of course, I also agree with the other posters... with beginners they going to take code fragments they see within functions, copy I into scripts, and be completely confused when it doesn't have the same behaviour when copied inside of a function, in juno, the repl, and jupyter. There will be 100 stack exchange questions which come down to the same issue. Intermediate programmers are going to have all sorts of homegrown solutions with wrapping in `let` blocks, etc which will confuse things further",Bitter Frustration
"> There will be 100 stack exchange questions which come down to the same issue. Intermediate programmers are going to have all sorts of homegrown solutions with wrapping in `let` blocks, etc which will confuse things further
Possibly, but at this stage this is hypothetical (also the OP of the question linked is asking about the rationale for the scoping rule, as opposed to being confused about it).
Also, while I respect the teaching experience of everyone who has concerns about this, whether this turns out to be a big deal in the classroom is something that time will tell.",
"the questioner appears to have been confused by it: ""I wonder if this is intuitive to beginning julia users. It was not intuitive to me ...""",
"> the questioner appears to have been confused by it:
Not to mention that this is someone who clearly knows enough about programming languages to understand the nuances of scope. What about all of the matlab type users that are completely ignorant of these topics..., and probably will never invest enough time to understand the nuances.",Mocking
"> Possibly, but at this stage this is hypothetical
I've already answered multiple questions related to this on stackoverflow, mostly by new users, and even more in real life (last one just yesterday, from a Matlab user, who saw this as a no go).",Bitter Frustration
"> There will be 100 stack exchange questions which come down to the same issue.
In my ""spare time"", I've been adding `scope`, `scoping`, and `global-variables` tags to the SE questions. I only stop because of lack of time, not because there aren't more.",Bitter Frustration
"Conclusion after much discussion including triage: we're going to include something along the lines of `SoftGlobalScope` in Base and use it in the REPL and all other interactive evaluation contexts. @JeffBezanson has pointed out that the way this is implemented is actually essentially the same as how soft scope was previously implemented, so to some extent we're coming around full circle. The difference is that now there is no scope behavior in modules or scripts, only in REPL-like contexts. I also think that _explaining_ soft scope as a source rewrite is clearer than trying to distinguish between hard and soft scopes (which we never how Jeff explained it, I might point out).",
"These two statements confuse me a bit as they seem a bit contradictory:
> and use it in the REPL and all other interactive evaluation contexts
> there is no scope behavior in [...] scripts, only in REPL-like contexts.
Does this mean that the module `Main` has sometimes a soft scope (say at the REPL prompt) and sometimes a hard scope (say when `julia -L script.jl`)? Would it not make sense to say that `Main` always has soft scope? And a module can opt-in to soft scope by `using SoftGlobalScope`?",
"(I guess) scoping rules cannot be changed in scripts because it would be backwards incompatible, i.e. would break the promise that any code written for 1.0 will run on any 1.* version. You are correct though that the same problem with scoping for the REPL also applies to scripts (naive user at a complete loss why his/her code does not work properly when run as a script). A way to solve/alleviate this problem without major incompatibilty would be to add an option to the julia cmdline to use softscope (or alternative) , e.g. julia -f programfile, and show this option in any description/tutorial that a beginner is likely to come across.
I also see a potential alternative for the softscope that may have some advantages (though i am probably overlooking disadvantages): What if a file (a called script) would always introduce its own local scope: scoping rules would be in complete consistency with those in functions, and with the expectations of a lot of users. It would also remove a lot of the performance liabilities with new users: No more unneeded globals (globals would have to be explicitly defined), and code might be compiled
(How many times have you had to say to put everything in a function, and to avoid using globals?)",
"I've just hit this and was completely boggled to be honest, having never seen it before in any other language. I'm planning on introducing an optional Julia course for advanced R users in my uni later this year once things have settled down, and my students will hit this on day 0 when they start randomly typing things in the REPL. And the fact that `for` loops behave differently from `if` statements just rubs salt in the wound, however logical this may be in terms of scoping. Scope inside functions is sufficiently hard to get biology students to grasp, the idea of having to explain _albeit perceived_ glaring inconsistencies in it in the REPL / in a script / in a for loop / in an if statement (because that's what we're talking about here) in a way that is different from every other language on earth makes me very sad.
I understand the backward compatibility promise that was made, but having this work _as expected by every non-cs person on the planet (and most cs people I suspect)_ seems like a bugfix rather than a backward compatibility issue - we're not saying that every bug will be reproduced for ever are we? The REPL fix is obviously essential, so it's great that you're proposing this, but then having to explain you can't copy a script into the REPL and expect the same behaviour seems as bad as or worse than the original problem.
Please, please, please think about treating this as a bugfix and pushing it out with scripts as well as the REPL - even if there's an switch to go to the ""old"" behaviour - and doing it as soon as possible in 1.0.1.",Bitter Frustration
A colleague that I was trying to get to learn julia also just ran into this. Having to explain the whole global vs. local variable thing at the first steps is not ideal...,Bitter Frustration
"I don't think treating this as a ""bugfix"" is in the cards, because it would break the 1.0 stability contract. However, it seems reasonable to me to use softscope for scripts run with `julia -i` (i.e. ""interactive"" mode).
(That is, there would be a flag `--softscope={yes|no}` and it would default to the value of `isinteractive`.)",
We'll have to consider the script mode choice.,
"For that matter, it's not crazy to me to default to `--softscope=yes` for any ""script"", i.e. for `julia foo.jl`, and only turn on the ""hard"" scoping rules for modules and `include` (at which point you should really be putting most code into functions).",
"> For that matter, it's not crazy to me to default to --softscope=yes for any ""script"",
That. The other one to seriously consider is Juno. Remember that people will `<shift-enter>` through their code to do interactive development(especially when working with the regression tests) and then later expect to be able to run the same file. Should it matter if the code is in a `@testset` or not (which I think might introduce a scope)? It would be very confusing to the user if the same text changes when in a `@testset` vs. not when using Atom's integration, and is inconsistent with doing `] test` as well.
It sure sounds to me like the best solution is that the hard-scope is simply an opt-in thing, where if every other usage (including `include` within scripts) uses `softscope` unless you say otherwise.",
"> different from every other language on earth
Do you want to write `var x = 0` to introduce every variable? That would also ""fix"" this, and be more like other languages.
> we're not saying that every bug will be reproduced for ever are we
That is not how this works. You can't get any change to the language you want just by calling the current behavior a bug.
I reeeally don't think there should be a command line option for this. Then every piece of julia code will have to come with a comment or something telling you which option to use. Some kind of parser directive in a source file would be a bit better, but even better still would be to have a fixed rule. For example, hard scope inside modules only might make sense.
Let me try again to provide an explanation of this that might be useful for avoiding the mania, hysteria, and carnage people are seeing in the classroom:
""
Julia has two kinds of variables: local and global. Variables you introduce in the REPL or at the top level, outside of anything else, are global. Variables introduced inside functions and loops are local. Updating global variables in a program is generally bad, so if you're inside a loop or function and want to update a global, you have to be explicit about it by writing the `global` declaration again.
""
Perhaps that can be improved; suggestions welcome. I know, you'd rather not need any sort of explanation at all. I get that. But it doesn't seem so bad to me.",Bitter Frustration
"> I reeeally don't think there should be a command line option for this. Then every piece of julia code will have to come with a comment or something telling you which option to use. Some kind of parser directive in a source file would be a bit better, but even better still would be to have a fixed rule
I agree. Sounds like a teaching and communication headache to me.
> For example, hard scope inside modules only might make sense.
Just so I understand: if I had a short script (not in a module!) in a `.jl` file which I had copied from an IJulia notebook, then if I ran that code in either the REPL directly or shift-enter in Juno, then it would behave consistently as soft-scope... but if I copied it instead of a `module` block then it would yell at me about globals? But if I copied that code inside of functions inside of a module, then it should work.
If so, that makes complete sense,is very teachable and coherent. Top-level scripts are an interactive interface for exploration, etc. but you would never put that kind of code in a module. Modules are something that you should fill with functions are very carefully considered globals. It would be easy to tell people about those rules.",
"> Do you want to write var x = 0 to introduce every variable? That would also ""fix"" this, and be more like other languages.
No, I'd rather not! But scripting languages that have a REPL rarely do that (e.g. ruby, python, R, ...), they behave like Julia v0.6 did.
> Julia has two kinds of variables: local and global. Variables you introduce in the REPL or at the top level, outside of anything else, are global. Variables introduced inside functions and loops are local. Updating global variables in a program is generally bad, so if you're inside a loop or function and want to update a global, you have to be explicit about it by writing the global declaration again.
I completely understand what you're saying here, and I won't (touch wood!) make this mistake again. But the whole problem I'm worried about is not me. I've found it relatively easy to introduce scope (without mentioning it directly) when I explain that variables inside functions can't see ones outside and vice-versa (even though that's more an aspiration than a fact in R!), because functions themselves are already a _relatively_ advanced concept. But this hits much earlier in the learning curve here where we don't want anything remotely as complicated as scope to be impinging on people...
Note also it's not just ""_variables you introduce in the REPL or at the top level, outside of anything else, are global_"" and ""_variables introduced inside functions and loops are local_"", it's also that variables in if statements in the REPL or at the top level are global but variables in a `@testset` are local. We end up down a rabbit-hole of ""just try it and work out for yourself whether it's local or global, good luck"".
However, I agree with @jlperla - the proposal that ""hard scope inside modules only might make sense"" seems completely fine to me! Modules are a sufficiently advanced concept again... if soft scope works for the REPL and scripts, that's absolutely fine.",Bitter Frustration
"> we don't want anything remotely as complicated as scope to be impinging on people...
> at the top level are global but variables in a `@testset` are local
What I'm trying to get at is that I feel a simple description of global vs. local is sufficient for early-stage teaching --- you don't even need to say the word ""scope"" (it does not occur at all in my explanation above). When you're just showing some simple expressions and loops in the REPL, you're not teaching people about testsets and you don't need an exhaustive list of the scoping behavior of everything in the language.
My only point is, this change does not suddenly make it necessary to teach lots of details about the language up front. You can still ignore the vast majority of stuff about scopes, testsets, etc., and a simple line on global vs. local should suffice.",
"> and a simple line on global vs. local should suffice.
In a world where everyone started writing all of their code from scratch, I would agree completely.
The issue is that you need to teach students not just about scope, but also about understanding the scope of where they copy-pasted code they got from. You need to teach them that if they copy-paste code that is on stackexchange within a function or a let block that they need to scan through it and find where to add ""global"" if they are pasting it into the REPL or a `.jl` file. But if they are copying that code inside a function or into the Jupyter notebook. they shouldn't. And if they find code inside of a stackexchange or tutorial page that has global variables in it, but they want to copy and modify that code inside of their own function, then they need to strip out the global.
And then students start asking why does `for` create this scope they need to worry about but not other things....",Bitter Frustration
"> We end up down a rabbit-hole of ""just try it and work out for yourself whether it's local or global, good luck"".
Pop quiz: in julia 0.6, is `x` global or local:
```
for i = 1:10
x = i
end
```
The answer is that there's no way to know, because it depends on whether a global `x` has been defined before. Now, you can say for sure that it is local.",Mocking
"Folks, this discussion is verging on no longer being productive. Jeff knows very well that the old behavior was nice in the REPL. Who do you think designed and implemented it in the first place? We have already committed to changing the interactive behavior. A decision still needs to be made about whether a ""script"" is interactive or not. It sounds interactive when you call it ""a script"" but it sounds far less interactive when you call it ""a program""â€”yet they are exactly the same thing. Please keep the replies short and constructive and focused on the things which still must be decided. If there's comments that deviate from this, they may be hidden and the thread may be locked.",Bitter Frustration
"One thought that I had but we dismissed as being ""too annoying"" and ""likely to cause the villagers to get out their pitchforks"" was that in non-interactive contexts, we could require a `local` or `global` annotation in ""soft scope"". That would guarantee that code from a module would work the same if pasted into the REPL. If we applied that to ""scripts""/""programs"" then the same would be true of them.",
"When I was first introduced to Julia (not a long time ago, and I come from a Fortran background mostly), I was taught that ""Julia is compiled and fast at the function level, thus everything that must be efficient must be done inside functions. In the main 'program' it behaves like a scripting language"". I found that fair enough, as I cannot imagine anyone doing anything too computationally demanding without understanding that statement. Therefore, if there is any sacrifice in performance at the main program for using the same notation and constructions than in the functions, I find that totally acceptable, much more acceptable than trying to understand and teach these scoping rules and not being able to copy and paste codes from one place to another.
By the way, I am a newbie in Julia yet, having chosen it to teach some high-school and undergraduate students some basics of simulations of physical systems. And I am already hopping this issue returns to the 'normal' behavior of previous versions, because it gives us quite a headache.",Bitter Frustration
This conversation is locked now and only Julia committers can comment.,
"@JeffBezanson, what would be the plan to implement the semantics you suggested in [this discourse thread](https://discourse.julialang.org/t/another-possible-solution-to-the-global-scope-debacle/15894), initially only in the REPL and opt-in elsewhere?
It sounds like you are planning to put that directly into the lowering code (`julia-syntax.scm`), rather than as a syntax rewriting ala SoftScope.jl? Or would you rather have it as syntax rewriting first (modifying SoftScope to the proposed rule and converting it to a stdlib), and defer putting it into the lowering code for a later Julia release?",
"Per https://twitter.com/dhh/status/1032050325513940992, I'd like for Rails to set a good example and tone by using better terminology when we can. An easy fix would be to replace our use of whitelist with allowlist and blacklist with denylist.
We can even just use them as verbs directly, as we do with the former terms. So something is allowlisted or denylisted.
I took a quick look and it seems like this change is mostly about docs. We only have one piece of the code that I could find on a search that uses the term whitelist with `enforce_raw_sql_whitelist`. Need to consider whether we need an alias and a deprecation for that.",
"Good intentions, but I doubt there's any relation of the origin of the terms blacklist/whitelist to race. There are many idioms and phrases in the English language that make use of colours without any racial backstories.
I haven't met any black person (myself included) who was ever offended by the use of ""blacklist"". Frankly, a good number find it patronising to make this kind of change.",
At least [one source from a search](https://www.etymonline.com/word/blacklist) suggests the word had its origins around union members.,
"Regardless of origin, allow/deny are simply clearer terms that does not require tracing the history of black/white as representations of that meaning. We can simply use the meaning directly.",
"1. [etymology is quite important](
https://www.quora.com/Is-the-term-blacklist-racist?share=1). in the end, we might consider plain words â€žblackâ€œ and â€žwhiteâ€œ racist and enter the realms of newspeak which i figure you especially, @dhh, are familiar with.
2. â€œallow/deny are simply clearer termsâ€ â€” now thatâ€™s an actual, technically useful argument.
3. can we please stop jumping onto political bandwagons? i am here for the sanity.",Bitter Frustration
The terms **Blocklist** and **Clearlist** are sometimes used in place of Blacklist and Whitelist.,
I'm gonna go ahead and get started on this... ðŸ˜„,Irony
"One could also argue that in color theory, black is the absence of color (photons which make up the spectrum) and white is the accumulation of all colors. Thus a blacklist is a list which contains elements that are to be absent and a whitelist to be allowed...",
"I think this is a great idea, I have proposed internally at multiple companies I've worked at changing master/slave and blacklist/whitelist to leader/replica and allowlist/denylist, if only because in an industry with poor representation it feels incredibly overt to be standing in a room full of mostly white people using these terms outloud.
It doesn't matter what the origin or intent of this was, or whether people can find a narrow lens through which to see it as not a problem. Consider using these terms in a coding or systems interview with someone you have just met.
I understand that some black people may not consider these terms offensive, but I would rather someone not want to work with me because they think I am too politically correct than because they think I am too insensitive and blind.
@minaslater thanks for beating me to the first PR on this. :) ðŸ’¯",
"@bitmonk I respectfully disagree with your opinion that your (or my) future colleagues would impose that kind of cultural bias on words that exist in the [English](https://www.merriam-webster.com/dictionary/white%20list) [language](https://www.merriam-webster.com/dictionary/blacklist) as well as are well-defined [on](https://en.wikipedia.org/wiki/Whitelisting) [Wikipedia](https://en.wikipedia.org/wiki/Blacklist_(disambiguation)), of which `blacklist` has far more far reaching disciplines than just comp-sci. That said and since I think this issue will garner overwhelming support in this community, I do support a compromise suggested by @rafaelfranca's review to replace the actual words with their definition or some other phrase that is more fluid than a single term.",
"I think the question here should be: is replacing `whitelist` and `blacklist` with `allowlist` and `denylist` a better option? If presented with the word `blacklist` and `denylist`, which is most likely self-explanatory as to the action to be performed?",
A quick search and replace in the codebase shows that all entries for those words were already changed. I'll close this issue.,
"After #6917 it should now be possible to clear up all deprecation warnings by:
* Replacing `update()` with `updateOne()`, `updateMany()`, or `replaceOne()`
* Replacing `remove()` with `deleteOne()` or `deleteMany()`
* Setting `mongoose.set('useFindAndModify', false);`
* Setting `mongoose.set('useCreateIndex', true);`
* Setting `mongoose.set('useNewUrlParser', true);`
Need a docs page to summarize this.",
"If these are set, would it be incompatible with or breaking the current program?",
"@isLishude the only differences we are aware of are:
1) Support for MongoDB 3.0.x
2) `updateOne()` and `updateMany()` do not support the [`overwrite` option](https://mongoosejs.com/docs/api.html#query_Query-update). If you use `update(filter, update, { overwrite: true })`, you should use `replaceOne(filter, update)`, **not** `updateOne(filter, update, { overwrite: true })`
If you run into any other problems, please don't hesitate to open up a GitHub issue.",
"I set `mongoose.set('useCreateIndexes', true)` and still get `DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead`.
I've tried it before and after `mongoose.connect()`.",Bitter Frustration
@ulrichb Are you sure that you're using mongoose@5.2.10 that released an hour ago?,Impatience
"I can replicate what @ulrichb is seeing on 5.2.10 with:
### 6922.js
```js
#!/usr/bin/env node
'use strict';
const mongoose = require('mongoose');
mongoose.connect('mongodb://localhost:27017/test', { useNewUrlParser: true });
mongoose.set('useCreateIndexes', true);
const conn = mongoose.connection;
const Schema = mongoose.Schema;
const schema = new Schema({
name: {
type: String,
unique: true
}
});
const Test = mongoose.model('test', schema);
const test = new Test({ name: 'one' });
async function run() {
console.log(`mongoose version: ${mongoose.version}`);
await conn.dropDatabase();
await test.save();
return conn.close();
}
run();
```
### Output:
```
issues: ./6922.js
mongoose version: 5.2.10
(node:7186) DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.
issues:
```",
"I believe it is `mongoose.set('useCreateIndex', true)`",
updated the original comment to reflect @saagar2000's observation. Thanks @saagar2000 !,
"@vkarpov15 is it possible to rename the document method `remove` to `delete` to be consistent as well? (Or provide an alias)?
Currently I'm refactoring all Model based usage of `remove` to `deleteOne` and `deleteMany`, but the document methods are still called `remove()` (which is also inconsistent with the `isDeleted()` method).
E.g.
```
const Foo = mongoose.model('Foo');
const foo = new Foo();
foo.remove(); //<-- create alias/rename to foo.delete()
```",Impatience
"I'm seeing the `collection.{remove|update} is deprecated` warning on `Document.{remove|update}` operations. ```
const doc = await model.findById(someId);
await doc.update({ $set: { someProp: true }}); // Deprecation warning
await doc.remove() // Deprecation warning
```",
"@adamreisnz yeah we should add an alias `doc.delete()`, ditto for `doc.update()` @mycompassspins",
"mongoose.set('useCreateIndex', true) has no affect for me. ðŸ˜ž still getting the 'DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.'",Bitter Frustration
@ParikshitChavan Did you set this option before calling `mongoose.model()`?,Impatience
Using mongoose 5.2.12. I also get this: `the options [useCreateIndex] is not supported` in addition to the deprecation warning. More info: I only get this extra message when pass `useCreateIndex` as a connection option. When i use `mongoose.set()` I don't get that message.,
"looks like it should work properly when pass as connection option, but it doesn't. only after explicit `mongoose.set()`",Bitter Frustration
"@Fonger @govindrai I tried doing this through connection options and though mongoose.set()
before and after connection to the DB.
the 'DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.' always seems to persist.",Bitter Frustration
"Same here, cant get rid of `collection.ensureIndex is deprecated`warning ... No matter if use set or as an option...",Bitter Frustration
"@ParikshitChavan @govindrai @Avcajaraville You have to put `mongoose.set('useCreateIndex', true)` before **EVERY** `mongoose.model()` call
(Note: not `mongoose.connect()` ).
See the discussion in #6890. This is fixed in 5.2.13 (but it's not released yet).",Impatience
"@Fonger @vkarpov15 after upgrade to 5.2.13 the issue still exists
```
(node:24769) DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.
```
package.json:
```
""dependencies"": {
""bcrypt"": ""^3.0.0"",
""connect-mongo"": ""^2.0.1"",
""ejs"": ""^2.6.1"",
""express"": ""^4.16.3"",
""express-session"": ""^1.15.6"",
""mongodb"": ""^3.1.4"",
""mongoose"": ""^5.2.13"",
""morgan"": ""^1.9.0"",
""passport-local"": ""^1.0.0""
},
```
node and npm info:
```
$ node -v
v10.6.0
$ npm -v
6.4.1
```",
"@arastu can you create a reproducible example with one or more files where you demonstrate that you are using 5.2.13 and are seeing one or more of these warnings?
*Note that I am calling `mongoose.set()` **before** requiring my models*
For Example:
### index.js
```js
#!/usr/bin/env node
'use strict';
const assert = require('assert');
const mongoose = require('mongoose');
mongoose.set('useCreateIndex', true);
mongoose.set('useFindAndModify', false);
mongoose.connect('mongodb://localhost:27017/test', { useNewUrlParser: true });
const conn = mongoose.connection;
const One = require('./one');
const Two = require('./two');
const one = new One({ name: '1' });
const two = new Two({ name: '2' });
async function run() {
assert.strictEqual(mongoose.version, '5.2.13');
await conn.dropDatabase();
await Promise.all([One.init(), Two.init()]);
await one.save();
await two.save();
await One.findOneAndUpdate({}, { name: 'one' });
await Two.findOneAndUpdate({}, { name: 'two' });
console.log(`${mongoose.version} should show no deprecations with the new settings.`);
return conn.close();
}
run();
```
### one.js
```js
'use strict';
const mongoose = require('mongoose');
const schema = new mongoose.Schema({ name: { type: String, unique: true } });
module.exports = mongoose.model('one', schema);
```
### two.js
```js
'use strict';
const mongoose = require('mongoose');
const schema = new mongoose.Schema({ name: { type: String, unique: true } });
module.exports = mongoose.model('two', schema);
```
### Output:
```
6922: ./index.js
5.2.13 should show no deprecations with the new settings.
6922:
```",
"working for me in 5.2.13. @lineus will useCreateIndex be supported as a connection option? Currently getting `the options [useCreateIndex] is not supported`
i.e.
`mongoose.connect(
process.env.MONGODB_URI,
{
useNewUrlParser: true,
autoIndex: false,
useCreateIndex: true,
}
)
`",Impatience
"@govindrai right now you should do `mongoose.set('useCreateIndex', true)` as specified in the [deprecation warning docs](https://mongoosejs.com/docs/deprecations.html#summary). But we'll add that as well.",
"I've upgraded to `5.2.14` and am still seeing deprecation warnings that don't make sense. For instance, `collection.update is deprecated . . .` when I'm actually using `document.update`. Same for the `remove` method of both the model and the document. Switching to `document.updateOne` produces` TypeError: document.updateOne is not a function`. Am I missing something here?",Bitter Frustration
"@mycompassspins Did you track the stack producing the warning ?
For instance, in my case turns out the deprecation warning was happening due to [connect mongo](https://github.com/jdesboeufs/connect-mongo#connect-mongo).
If you follow the above instructions, at least for me, all deprecation warning where gone :)",
"Here's the stack:
```
DeprecationWarning: collection.update is deprecated. Use updateOne, updateMany, or bulkWrite instead.
at NativeCollection.(anonymous function) [as update] (/MyProject/node_modules/mongoose/lib/drivers/node-mongodb-native/collection.js:143:28)
at NodeCollection.update (/MyProject/node_modules/mquery/lib/collection/node.js:66:19)
at model.Query._updateThunk (/MyProject/node_modules/mongoose/lib/query.js:3233:23)
at model.Query.Query._execUpdate (/MyProject/node_modules/mongoose/lib/query.js:3245:23)
at process.nextTick (/MyProject/node_modules/kareem/index.js:333:33)
at process._tickCallback (internal/process/next_tick.js:150:11)
```
Obviously, `Model.update` is actually executed, even though I'm calling `MongooseDocument.update`. This is what I'm doing, more or less:
```
async UpdateMyDocument(id:string, update:MyDocument):Promise<MyDocument>
{
const doc = await MyModel.findById(id)
.populate({ path: 'somePath', populate: [{ path: 'somePath.nested' }] });
return await doc.update(update); // <= MongooseDocument.update is producing a Model.update deprecation warning
}
```",Bitter Frustration
@mycompassspins are you sure you're on 5.2.14? We [added `doc.updateOne(update)` in 5.2.13](https://github.com/Automattic/mongoose/blob/master/History.md#5213--2018-09-04),
There's an internal configuration typo in `@5.2.16` that causes the deprecation notice for createIndexes to reemerge when you call createIndexes explicitly. PR openedâ˜ï¸,
"(node:6455) DeprecationWarning: GridStore is deprecated, and will be removed in a future version. Please use GridFSBucket instead. What about this ??.
""gridfs-stream"": ""^1.1.1"",
""mongoose"": ""^5.3.1"",
""multer"": ""^1.4.0"",
""multer-gridfs-storage"": ""^3.0.0""",Bitter Frustration
"@venkyyPoojari that's an old one, predates mongoose 5, but worth addressing. We will fix.",
"Thanks, I am going to use this for production, so please help as soon as possible",Impatience
"> Thanks, I am going to use this for production, so please help as soon as possible
I have the same issue, I'm piping the readstream to the res object, is this what is wrong? I mean does it prevent the files to be read out?",Impatience
@venkyyPoojari @romain10009 that's because gridfs-stream is out of date. The MongoDB driver has a new streaming API for GridFS that you should use instead: https://thecodebarbarian.com/mongodb-gridfs-stream,
"Hello, as referenced by others like this issue [here](https://github.com/zfsonlinux/zfs/issues/6001), I have a pool that was mistakenly upgraded under Linux that needs to go back to FreeBSD. Unfortunately userobj_accounting is enabled, which means that FreeBSD won't take the pool due to an unsupported feature.
I saw mention of [this commit](https://github.com/zfsonlinux/zfs/commit/83025286175d1ee1c29b842531070f3250a172ba) enabling feature disable functionality. I grabbed a Ubuntu 14.04 boot disk and installed ZoL (and an SPL from about that time) from that pull request, but am still getting reference to userobj_accounting being an ""unsupported feature"" when I try to import the pool.
I'm not sure if the pool should be mounted or not?
With the pool exported, trying zhack returns:
```
root@ubuntu:~/zfs# zhack feature disable media org.zfsonlinux:userobj_accounting
zhack: feature 'org.zfsonlinux:userobj_accounting' is active, can not disable
```",Bitter Frustration
"Why are you ignoring both rules and requests (https://github.com/zfsonlinux/zfs/issues/6466#issuecomment-320420368)? Please use the issue template if you found a bug, otherwise ask the mailing lists (https://github.com/zfsonlinux/zfs/wiki/Mailing-Lists).
Thank you for your cooperation.",Impatience
"The question I asked was in regard to the lead developer of this project making a patch for himself to use on a catch22 in feature flags, and therefore not addressed to you. Nor is it addressed to anyone else that is using a current version of this software, since the relevant patch is over three years old (and now deprecated).
> Thank you for your cooperation.
What makes you think I'm agreeing with anything you typed? Don't thank me, I'm not.
Thank *you*, for nothing. I'd file a bug report about people with pedophile'ish usernames with such high exposure in a project that Canonical seems to care so much about, but your form is (predictably) excessive. No thanks.
For those who might find this on Google, don't bother with that patch/commit. If your pool got userobj_accounting enabled it is now a Linux-only pool. You'll have to destroy it, since per the (broken) explanation in the manpage, once active it cannot be decactivated. Sort of like that Doomsday Machine that Slim Pickens was after....",Bitter Frustration
"> The question I asked was in regard to the lead developer of this project making a patch for himself to use on a catch22 in feature flags, and therefore not addressed to you. Nor is it addressed to anyone else that is using a current version of this software, since the relevant patch is over three years old (and now deprecated).
Still, you post on a bug report platform watched by > 400 people that get emailed for each of your posts.
> What makes you think I'm agreeing with anything you typed? Don't thank me, I'm not. [childish personal attacks]
Nobody asked for your opionion. There are rules established by the community, people who largely work on this project for free, an you can either stick to their rules, not contribute (anything) at all, or fork the project and work on your fork under your own rules. :wink:
---
I'll unsubscribe from this issue and won't comment here further.",Bitter Frustration
"We should release the next version asap and rebuild our packages.
~~~
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1) but 1.14.0-0ubuntu1.1 is to be installed
E: Unable to correct problems, you have held broken packages.
~~~",Impatience
Having the same issue on Ubuntu 18.04.01 LTS,
"As a workaround while they rebuild passenger, you can install the old version with:
```sh
sudo apt install \
libnginx-mod-http-auth-pam=1.14.0-0ubuntu1 \
libnginx-mod-http-cache-purge=1.14.0-0ubuntu1 \
libnginx-mod-http-dav-ext=1.14.0-0ubuntu1 \
libnginx-mod-http-echo=1.14.0-0ubuntu1 \
libnginx-mod-http-fancyindex=1.14.0-0ubuntu1 \
libnginx-mod-http-geoip=1.14.0-0ubuntu1 \
libnginx-mod-http-headers-more-filter=1.14.0-0ubuntu1 \
libnginx-mod-http-image-filter=1.14.0-0ubuntu1 \
libnginx-mod-http-lua=1.14.0-0ubuntu1 \
libnginx-mod-http-ndk=1.14.0-0ubuntu1 \
libnginx-mod-http-perl=1.14.0-0ubuntu1 \
libnginx-mod-http-subs-filter=1.14.0-0ubuntu1 \
libnginx-mod-http-uploadprogress=1.14.0-0ubuntu1 \
libnginx-mod-http-upstream-fair=1.14.0-0ubuntu1 \
libnginx-mod-http-xslt-filter=1.14.0-0ubuntu1 \
libnginx-mod-mail=1.14.0-0ubuntu1 \
libnginx-mod-nchan=1.14.0-0ubuntu1 \
libnginx-mod-stream=1.14.0-0ubuntu1 \
nginx-common=1.14.0-0ubuntu1 \
nginx-core=1.14.0-0ubuntu1 \
nginx=1.14.0-0ubuntu1
```
You will have to uninstall the new version before trying the command above.",
"Same problem ```
xxx:~# apt install libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1.1) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
xxx:~# aptitude install libnginx-mod-http-passenger
The following NEW packages will be installed:
libnginx-mod-http-passenger{b} passenger{a} passenger-dev{a} passenger-doc{a} ruby-rack{a} 0 packages upgraded, 5 newly installed, 0 to remove and 0 not upgraded.
Need to get 6,823 kB of archives. After unpacking 50.9 MB will be used.
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1.1) but it is not going to be installed
The following actions will resolve these dependencies:
Keep the following packages at their current version:
1) libnginx-mod-http-passenger [Not Installed] Accept this solution? [Y/n/q/?] ^C
xxx:~# cat /etc/lsb-release DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION=""Ubuntu 18.04.1 LTS""
```",
After `apt install nginx-common` message looks like `libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1.1) but 1.14.0-0ubuntu1.2 is installed`,Bitter Frustration
"Like @nleo , I'm also running into the issue, today with a fresh install of **Ubuntu 18.04.1 LTS** after following the install instructions on the PhusionPassenger website here:
https://www.phusionpassenger.com/library/install/nginx/install/oss/bionic/
Here's my output:
```
deploy@localhost:~$ sudo apt-get install -y libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1.1) but 1.14.0-0ubuntu1.2 is to be installed
E: Unable to correct problems, you have held broken packages.
deploy@localhost:~$ The following packages have unmet dependencies:
```",
"Ditto, same issue here.",Impatience
"Oh boy. It's not good when I see @excid3 show up with the same isse as me :/
I tried following @hamiltonc 's instructions above ðŸ‘† and while it appears to have installed no problem, I am unable to start NGINX.
```
deploy@localhost:~$ sudo apt install \
> libnginx-mod-http-auth-pam=1.14.0-0ubuntu1 \
> libnginx-mod-http-cache-purge=1.14.0-0ubuntu1 \
> libnginx-mod-http-dav-ext=1.14.0-0ubuntu1 \
> libnginx-mod-http-echo=1.14.0-0ubuntu1 \
> libnginx-mod-http-fancyindex=1.14.0-0ubuntu1 \
> libnginx-mod-http-geoip=1.14.0-0ubuntu1 \
> libnginx-mod-http-headers-more-filter=1.14.0-0ubuntu1 \
> libnginx-mod-http-image-filter=1.14.0-0ubuntu1 \
> libnginx-mod-http-lua=1.14.0-0ubuntu1 \
> libnginx-mod-http-ndk=1.14.0-0ubuntu1 \
> libnginx-mod-http-perl=1.14.0-0ubuntu1 \
> libnginx-mod-http-subs-filter=1.14.0-0ubuntu1 \
> libnginx-mod-http-uploadprogress=1.14.0-0ubuntu1 \
> libnginx-mod-http-upstream-fair=1.14.0-0ubuntu1 \
> libnginx-mod-http-xslt-filter=1.14.0-0ubuntu1 \
> libnginx-mod-mail=1.14.0-0ubuntu1 \
> libnginx-mod-nchan=1.14.0-0ubuntu1 \
> libnginx-mod-stream=1.14.0-0ubuntu1 \
> nginx-common=1.14.0-0ubuntu1 \
> nginx-core=1.14.0-0ubuntu1 \
> nginx=1.14.0-0ubuntu1
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
Setting up libnginx-mod-mail (1.14.0-0ubuntu1) ...
Setting up libxpm4:amd64 (1:3.5.12-1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
Setting up libnginx-mod-http-xslt-filter (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-upstream-fair (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-geoip (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-ndk (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-perl (1.14.0-0ubuntu1) ...
Setting up libwebp6:amd64 (0.6.1-2) ...
Setting up libnginx-mod-http-lua (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-uploadprogress (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-fancyindex (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-stream (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-echo (1.14.0-0ubuntu1) ...
Setting up libgd3:amd64 (2.2.5-4ubuntu0.2) ...
Setting up libnginx-mod-http-image-filter (1.14.0-0ubuntu1) ...
Setting up nginx-core (1.14.0-0ubuntu1) ...
Job for nginx.service failed because the control process exited with error code.
See ""systemctl status nginx.service"" and ""journalctl -xe"" for details.
invoke-rc.d: initscript nginx, action ""start"" failed.
â— nginx.service - A high performance web server and a reverse proxy server
Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)
Active: failed (Result: exit-code) since Thu 2018-11-08 15:53:24 UTC; 13ms ago
Docs: man:nginx(8)
Process: 10420 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=1/FAILURE)
Main PID: 968 (code=exited, status=0/SUCCESS)
Nov 08 15:53:24 localhost systemd[1]: Starting A high performance web server and a reverse proxy server...
Nov 08 15:53:24 localhost nginx[10420]: nginx: [emerg] unknown directive ""passenger_root"" in /etc/nginx/conf.d/mod-http-passenger.conf:1
Nov 08 15:53:24 localhost nginx[10420]: nginx: configuration file /etc/nginx/nginx.conf test failed
Nov 08 15:53:24 localhost systemd[1]: nginx.service: Control process exited, code=exited status=1
Nov 08 15:53:24 localhost systemd[1]: nginx.service: Failed with result 'exit-code'.
Nov 08 15:53:24 localhost systemd[1]: Failed to start A high performance web server and a reverse proxy server.
dpkg: error processing package nginx-core (--configure):
installed nginx-core package post-installation script subprocess returned error exit status 1
E: Sub-process /usr/bin/dpkg returned an error code (1)
```
@FooBarWidget Any chance you can reopen this issue? It doesn't appear to have been fixed and from my basic testing (it works on 18.04 but not 18.04.1) it seems to be a bad build.",Bitter Frustration
"Part of the problem is that the previous version of NGINX that Passenger works with is no longer available.
```bash
E: Version '1.14.0-0ubuntu1.1' for 'nginx' was not found
```",Bitter Frustration
"I was able to get things going by going all the way back to Passenger 5.3.4. ```
sudo apt install libnginx-mod-http-auth-pam=1.14.0-0ubuntu1 libnginx-mod-http-cache-purge=1.14.0-0ubuntu1 libnginx-mod-http-dav-ext=1.14.0-0ubuntu1 libnginx-mod-http-echo=1.14.0-0ubuntu1 libnginx-mod-http-fancyindex=1.14.0-0ubuntu1 libnginx-mod-http-geoip=1.14.0-0ubuntu1 libnginx-mod-http-headers-more-filter=1.14.0-0ubuntu1 libnginx-mod-http-image-filter=1.14.0-0ubuntu1 libnginx-mod-http-lua=1.14.0-0ubuntu1 libnginx-mod-http-ndk=1.14.0-0ubuntu1 libnginx-mod-http-perl=1.14.0-0ubuntu1 libnginx-mod-http-subs-filter=1.14.0-0ubuntu1 libnginx-mod-http-uploadprogress=1.14.0-0ubuntu1 libnginx-mod-http-upstream-fair=1.14.0-0ubuntu1 libnginx-mod-http-xslt-filter=1.14.0-0ubuntu1 libnginx-mod-mail=1.14.0-0ubuntu1 libnginx-mod-nchan=1.14.0-0ubuntu1 libnginx-mod-stream=1.14.0-0ubuntu1 nginx-common=1.14.0-0ubuntu1 nginx=1.14.0-0ubuntu1 nginx-extras=1.14.0-0ubuntu1 libnginx-mod-http-passenger=1:5.3.4-1~bionic1 passenger=1:5.3.4-1~bionic1
```",
Going back to Passenger 5.3.4 also worked for me.,
"FYI, I had to uninstall nginx & passenger, before installing the last nginx & passenger.
```bash
apt autoremove --purge libnginx-mod* nginx python-certbot-nginx python3-certbot-nginx
sudo apt-get install -y dirmngr gnupg
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7
sudo apt-get install -y apt-transport-https ca-certificates
sudo sh -c 'echo deb https://oss-binaries.phusionpassenger.com/apt/passenger bionic main > /etc/apt/sources.list.d/passenger.list'
sudo apt-get update
sudo apt-get install -y libnginx-mod-http-passenger python3-certbot-nginx
```",
"Eeek, what a mess. Hopefully we get a fresh release asap. I'm on Debian Stretch and can't start Nginx after its last apt upgrade.",Impatience
We're aware of the issue and are working on a fix.,
Any chance you could reopen the issue for those poor people struggling to figure out why they're unable to install Passenger?,Bitter Frustration
"I had this problem on the docker images and fixed by rolling back to phusion/passenger-ruby25:0.9.35 rather than latest, just in case anyone else is having a similar issue.",
A fix has been found and committed. Will release asap.,
Any update so far?,Impatience
The new packages have already been released on Sunday.,
"Awesome, thanks for getting on this so quickly! ðŸ˜Š",
And working correctly,
"Having a similar issue when running with nginx version 1.14.1.
```
ubuntu@18.04.01:~$ sudo apt-get install -y libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (< 1.14.1) but 1.14.1-0+bionic0 is to be installed
```",
"@tbonz I cannot reproduce by following these steps on a clean bionic install:
```
sudo apt-get install -y dirmngr gnupg
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7
sudo apt-get install -y apt-transport-https ca-certificates
sudo sh -c 'echo deb https://oss-binaries.phusionpassenger.com/apt/passenger bionic main > /etc/apt/sources.list.d/passenger.list'
sudo apt-get update
sudo apt-get install -y libnginx-mod-http-passenger nginx
```
In what way does your approach differ?",Impatience
"@CamJN thank you I was able to get this to work with the default nginx version 1.14.0 on bionic. However, when I try updating to the the latest version of nginx mainline 1.15.8 I run into issues and am unable to upgrade. Are there additional steps needed to get nginx to upgrade to the latest along with passenger module?",Impatience
"@tbonz how are you installing nginx 1.15.8? Ubuntu only ships 1.14.0. Our nginx module package targets the ubuntu provided nginx package, if you want to target another nginx, you can use the instructions here: https://www.phusionpassenger.com/library/install/nginx/install_as_nginx_module.html.",
Might be using the Nginx repos? https://www.nginx.com/resources/wiki/start/topics/tutorials/install/#official-debian-ubuntu-packages,
"@CamJN I'll review the notes you referenced. I only wanted to try using the latest version 1.15.8 of nginx since that is what the release notes for passenger 6.0.1 now lists as the `preferred` version.
@excid3 Yes, I'm following what they've outlined at https://www.nginx.com/resources/wiki/start/topics/tutorials/install/#official-debian-ubuntu-packages
Adding the following to `/etc/apt/sources.list.d/nginx.list`
```
deb http://nginx.org/packages/mainline/ubuntu/ bionic nginx
# deb-src http://nginx.org/packages/mainline/ubuntu/ bionic nginx
```
Then, running
```
sudo apt update
sudo apt upgrade
```
Gives the following:
```
Reading package lists... Done
Building dependency tree
Reading state information... Done
Calculating upgrade... Done
The following packages have been kept back:
nginx
0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.
```",
"@tbonz if you're using a non system nginx, then you'll want to use these instructions to compile the passenger dynamic module against your nginx sources: https://www.phusionpassenger.com/library/install/nginx/install_as_nginx_module.html",
"I'm having the same problem, fresh ubuntu 18.04.2 LTS install. Followed the steps described in the docs:
```
sudo apt-get install -y dirmngr gnupg
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7
sudo apt-get install -y apt-transport-https ca-certificates
sudo sh -c 'echo deb https://oss-binaries.phusionpassenger.com/apt/passenger bionic main > /etc/apt/sources.list.d/passenger.list'
sudo apt-get update
sudo apt-get install -y libnginx-mod-http-passenger nginx
```
```
# apt-get install -y libnginx-mod-http-passenger nginx
Reading package lists... Done
Building dependency tree Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: passenger (= 1:6.0.2-1~bionic1) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
```
I'm not using any custom repositories for nginx:
```
# apt-cache policy nginx
nginx:
Installed: (none)
Candidate: 1.14.0-0ubuntu1.2
Version table:
1.14.0-0ubuntu1.2 500
500 http://archive.ubuntu.com/ubuntu bionic-security/main amd64 Packages
500 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages
1.14.0-0ubuntu1 500
500 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages
```
Any clues?",Bitter Frustration
"I dont know the reason, but my /etc/apt/sources.list was missing these lines:
```
deb http://archive.ubuntu.com/ubuntu/ bionic universe
deb http://archive.ubuntu.com/ubuntu/ bionic-updates universe
```
After added, the instalation succeded.",
"I lowered the system version to 10.4.
everything is normal
ðŸ˜´",Irony
"Still not working...
Solution?
Modification to the docs?
Time wasted on this...",Bitter Frustration
Still broken on a fresh ubuntu install. Very disappointing. Looks like I'm going back to puma. It's a pain with config but at least it works!,Bitter Frustration
"@activestylus You haven't described what you did or what output you saw. But I can assure you that installing passenger and nginx on a fresh bionic (title of this issue) install does work if you follow the instructions: https://www.phusionpassenger.com/docs/tutorials/deploy_to_production/installations/oss/ownserver/ruby/nginx/
I've just gone through the process myself to verify it.",
"@activestylus please take a look at my comment:
https://github.com/phusion/passenger/issues/2122#issuecomment-468668676
Not sure why, but in a fresh install, I didn't have universe repositories enabled. Not passenger's fault IMO.",
"This is a nightmare to install, I'm getting the same error as the opener.
None of the mentioned solutions worked for me.
@FooBarWidget could you re-open the issue?",Bitter Frustration
@phpduck the versions mentioned in your error message must be different because we released the version of passenger compatible with the nginx in the first comment. Please include your actual output.,Impatience
"Facing same issue on Ubuntu 19.04 on running command `sudo apt-get install -y libnginx-mod-http-passenger`
Error:
> The following packages have unmet dependencies:
> libnginx-mod-http-passenger : Depends: nginx-common (< 1.14.1) but 1.15.9-0ubuntu1 is to be installed
> E: Unable to correct problems, you have held broken packages.",Bitter Frustration
Ubuntu 19.04 is blocked on our migration to a new linux package host.,Bitter Frustration
"@CamJN Is there a possibility to get support soon so that I can install Passenger/Nginx on Ubuntu 19.04?
It'd be great if you can please share the alternative solution for the installation.",Impatience
"You can build from the tarball method: https://www.phusionpassenger.com/docs/tutorials/deploy_to_production/installations/oss/ownserver/ruby/nginx/
I'm close to finishing the repo migration, so it shouldn't be much longer before I can get a proper 19.04 release out.",
"Hello
I have one. I am using a Linode Ubuntu 18.04 LTS instance. I have `deb https://oss-binaries.phusionpassenger.com/apt/passenger stretch main
`
in passenger.list and I have
`deb http://nginx.org/packages/ubuntu/ bionic nginx
deb-src http://nginx.org/packages/ubuntu/ bionic nginx
`
in nginx.list.
My error:
>
sudo apt-get install -y libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (>= 1.10.3-1+deb9u2) but it is not going to be installed
Depends: nginx-common (< 1.10.4) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
I have done this successfully on AWS before a few months ago by following the documentation provided. I can't seem to get it to work now though. I searched and found this here so I thought since it was active, I would post.
Any suggestions? I'm considering going back to an older version - that could work.
Thanks",Bitter Frustration
"@blueMesaEngineering please take a look at my comment:
https://github.com/phusion/passenger/issues/2122#issuecomment-468668676
Not sure why, but in a fresh install, I didn't have universe repositories enabled. Looks like you have the same problem.",
"> @blueMesaEngineering please take a look at my comment:
> > [#2122 (comment)](https://github.com/phusion/passenger/issues/2122#issuecomment-468668676)
> > Not sure why, but in a fresh install, I didn't have universe repositories enabled. Looks like you have the same problem.
Hi Fernando - thanks for the response.
Here is the pertinent info in my sources.list:
deb http://mirrors.linode.com/ubuntu/ bionic universe
deb http://mirrors.linode.com/ubuntu/ bionic-updates universe
So I assumed these would be the same as the ones you suggested in your comment, since ""bionic universe"" and ""bionic-updates universe"". However, it's a mirror on Linode, so maybe it's different.
I'll try with the archive repos you suggested and see what happens.
Thanks",
"EDIT: Booyah! Fixed it. In my case, I had ""stretch"" in my passenger.list file instead of ""bionic"". Changed to ""bionic"" and viola! Installed successfully.
Resolved.
Heyo,
After a couple installation cycle and purges of nginx, I'm still getting a similar response:
sudo apt-get install libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: passenger (= 1:6.0.2-1~stretch1) but it is not going to be installed
Depends: nginx-common (>= 1.10.3-1+deb9u2) but it is not going to be installed
Depends: nginx-common (< 1.10.4) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
I guess perhaps I'll have to tinker with installing older versions :/
Thanks",Bitter Frustration
"In the end, I solved this problem. It is my fault, my system is 16.04, I thought it was 18.04, the installation method is different...",
I get the same error on ngnix 1.17.4 and Ubuntu 18.04 (aws ec2). How can I fix it?,
"@sanjayojha and anyone else that still has issues, please file a new bug this one is too crowded to keep track of what each person is experiencing.",Impatience
"> > > @activestylus You haven't described what you did or what output you saw. But I can assure you that installing passenger and nginx on a fresh bionic (title of this issue) install does work if you follow the instructions: https://www.phusionpassenger.com/docs/tutorials/deploy_to_production/installations/oss/ownserver/ruby/nginx/
> > I've just gone through the process myself to verify it.
05/05/2020 and this is still an issue, followed every piece of instructions specially the link below, still same error. Ubuntu 18.04 (aws ec2). Error occurs when entering in sudo apt-get install -y libnginx-mod-http-passenger
[https://www.phusionpassenger.com/docs/tutorials/deploy_to_production/installations/oss/ownserver/ruby/nginx/](url)
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: passenger (= 1:6.0.4-1~bionic1) but it is not going to be installed
Depends: nginx-common (>= 1.14.0-0ubuntu1.6) but 1.10.3-0ubuntu0.16.04.5 is to be installed
E: Unable to correct problems, you have held broken packages.",Bitter Frustration
"Under Ubuntu 18.04 LTS
```
apt install aptitude
aptitude install -y libnginx-mod-http-passenger
```
aptitude instead of stopping will downgrade nginx to appropriate version. nginx will be older but working with passenger.",
"Hello. I am using Gentoo and ZFS-master.
All was fine, until I decided to upgrade ZFS.
In gentoo for upgrade I need to merge 2 packages: zfs and zfs-kmod. Zfs was merged OK, but stopped working.
local ~ # zfs list
bad property list: invalid property 'name'
I thought it will work when latest kernel module will be merged. But it fails to compile.
CC [M] /var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.o
CC [M] /var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_vfsops.o
/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.c: In function â€˜zfs_kobj_add_attrâ€™:
/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.c:151:38: error: assignment of member â€˜nameâ€™ in read-only object
zkobj->zko_attr_list[attr_num].name = attr_name;
^
/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.c:152:38: error: assignment of member â€˜modeâ€™ in read-only object
zkobj->zko_attr_list[attr_num].mode = 0444;
Please help me fixing these issues.
Gentoo-Hardened, zfs-9999, zfs-kmod-9999, Linux-grsecurity-4.9.74, GCC 7.3.0",Bitter Frustration
"I can test under gentoo in a little while. Testing now with gentoo-sources-4.14.72. I am not using 4.9.X on any systems. Compiled for me:
>>> Verifying ebuild manifests
>>> Jobs: 0 of 2 complete, 1 running Load avg: 6.81, 8.56, 6.40
>>> Emerging (1 of 2) sys-fs/zfs-kmod-9999::gentoo
>>> Jobs: 0 of 2 complete, 1 running Load avg: 6.81, 8.56, 6.40
>>> Installing (1 of 2) sys-fs/zfs-kmod-9999::gentoo
>>> Jobs: 0 of 2 complete, 1 running Load avg: 3.98, 2.84, 3.82
>>> Jobs: 0 of 2 complete Load avg: 3.98, 2.84, 3.82
>>> Jobs: 1 of 2 complete Load avg: 3.74, 2.81, 3.80
>>> Jobs: 1 of 2 complete, 1 running Load avg: 3.74, 2.81, 3.80
>>> Emerging (2 of 2) sys-fs/zfs-9999::gentoo
>>> Jobs: 1 of 2 complete, 1 running Load avg: 3.74, 2.81, 3.80
>>> Installing (2 of 2) sys-fs/zfs-9999::gentoo
>>> Jobs: 1 of 2 complete, 1 running Load avg: 3.38, 3.57, 3.88
>>> Jobs: 1 of 2 complete Load avg: 3.38, 3.57, 3.88
>>> Jobs: 2 of 2 complete Load avg: 3.01, 3.49, 3.85
Although I am using gcc version 6.4.0 (Gentoo 6.4.0-r1 p1.3)
```
vm_gentoo ~ # uname -a
Linux vm_gentoo 4.14.72-gentoo-20180926-1600-vm_gentoo #1 SMP Wed Sep 26 16:09:18 EDT 2018 x86_64 Intel(R) Core(TM) i7 CPU 860 @ 2.80GHz GenuineIntel GNU/Linux
vm_gentoo ~ # eselect kernel list
Available kernel symlink targets:
[1] linux-4.14.67-gentoo
[2] linux-4.14.70-gentoo
[3] linux-4.14.72-gentoo *
vm_gentoo ~ # cat /sys/module/z
zavl/ zcommon/ zfs/ zlua/ znvpair/ zswap/ zunicode/
vm_gentoo ~ # cat /sys/module/zfs/version
0.8.0-rc1_21_gd1261452
vm_gentoo ~ #
```",
"> Hello. I am using Gentoo and ZFS-master.
> All was fine, until I decided to upgrade ZFS.
> In gentoo for upgrade I need to merge 2 packages: zfs and zfs-kmod.
> Zfs was merged OK, but stopped working.
> > local ~ # zfs list
> bad property list: invalid property 'name'
Could be a mismatch between kernel and userland ABI:
In case you upgraded from a way older version (=didn't `emerge zfs` for quite some time) you still might have the old userland binaries somewhere in your path, their location changed a while ago.
Similar problem (when you can only locate the new version in your path) could happen when the old kernel module is still loaded but the new version of the userland is used.
The compilation error could be caused in concert with grsecurity, see https://forums.grsecurity.net/viewtopic.php?f=3&t=3890 for a similar case (and possibly a solution).",
"BTW, I also tried gcc 7.3 on gentoo-sources-4.18.10 (different machine) and the compile worked fine as well.",
"Thanks for replies. About zfs binary stopped working I know, it is because I compiled a new version of zfs and still use old version of zfs-kmod.
I am using this kernel for a long time and compilation of zfs worked well always. My last successful compilation was about 4-6 commits ago to the master. I read this link about grsecurity, but didn't found a solution. Also, I don't see any grsecurity errors at dmesg while compiling. So now I have new zfs and old zfs-kmod and I am afraid to reboot, I don't know how to compile the same version in gentoo or back-forward for some commits.
I think the problem is related to some new code in last commits, the kernel version or gcc version. I am using zfs (master) 1.5+ years at 4.9.24-grsec and about a year on 4.9.74-grsec (minipli), and compilation always was successful. Please help me resolve these issues. Glibc-2.26-r7, gcc-7.3.0-r5 hardened profile, binutils-2.31.1",Bitter Frustration
"@UralZima just fork the repo and create a custom branch if newer commits don't work for you
you can modify zfs-kmod-9999.ebuild e.g. like so:
```
if [ ${PV} == ""9999"" ]; then
AUTOTOOLS_AUTORECONF=""1""
#	EGIT_REPO_URI=""https://github.com/zfsonlinux/zfs.git""
EGIT_REPO_URI=""https://github.com/kernelOfTruth/zfs.git""
inherit git-r3
else
#	SRC_URI=""https://github.com/zfsonlinux/zfs/releases/download/zfs-${PV}/zfs-${PV}.tar.gz""
SRC_URI=""https://github.com/kernelOfTruth/zfs/releases/download/zfs-${PV}/zfs-${PV}.tar.gz""
S=""${WORKDIR}/zfs-${PV}""
KEYWORDS=""~amd64 ~arm ~ppc ~ppc64""
fi
EGIT_BRANCH=""zfs_master_26.05.2018""
```
that way I'm e.g. selecting my repo and via EGIT_BRANCH the specific branch to be used.
This needs to be done **both for sys-fs/zfs and sys-fs/zfs-kmod !**
Hope that helps.",
"Hello. I tried to compile it on linux-4.9.131-dappersec, the latest grsec-updated kernel, and it also failed.
I have compiled it successfully on latest linux-4.18.14-gentoo. As I know, 4.9.x is a longterm, so it is kernel-version-related or grsecurity-related. But as I told you, 4-5 commits back it compiled and worked fine. Is it possible to fix master to work with 4.9.x grsecurity kernels? It was working for years before. Thanks.",Bitter Frustration
"Hello. Anything on this? Because of some commit (I think it is related to https://github.com/zfsonlinux/zfs/commit/7a23c81342df05ace730bd303b4a73854dba43dd), I can't use dappersec kernel anymore. I also tried with 4.9.132-dappersec, and the error is the same.
It was always compatible with grsecurity patches, please fix it. Thanks so much",Bitter Frustration
"Hello. Some update. I decided to go back to the commit it was working. I was looking at history of zfs_sysfs.c file https://github.com/zfsonlinux/zfs/commits/master/module/zfs/zfs_sysfs.c and found that it started to fail since commits from Sep 2. So it is not related to https://github.com/zfsonlinux/zfs/commit/7a23c81342df05ace730bd303b4a73854dba43dd
I compiled it successfully with a commit before that EGIT_COMMIT=""bb91178e60df553071ce2e18b0067ef703f7b583""
I hope this information will help fix building on dappersec kernels. Thanks!",
"The proposed fix is here, http://paste.ubuntu.com/p/prrDP6Kv4z/ @vogelfreiheit can you please open a PR so the CI and test it and we can get it reviewed and integrated.",Impatience
"I am ready to test it after PR is open and I am able to modify gentoo's ebuild to include it.
As I said, reverting to commit https://github.com/zfsonlinux/zfs/commit/7a23c81342df05ace730bd303b4a73854dba43dd not resolved the problem, I am able to use it on commit EGIT_COMMIT=""bb91178e60df553071ce2e18b0067ef703f7b583"", but the related file I think is zfs_sysfs.c",
"Hi, I just saw this. I will let someone know so a PR can be opened, meanwhile I had some other fixes written for a few other spots that need testing.",
"This is long overdue, this project needs a code of conduct.",Impatience
"Eh, after reviewing that other issue where someone is calling for a Code of Conduct (which is a fairly dumb idea), I'm surprised to see that you assumed that person was me (perhaps the intention of the troll all along). Since this begs finally some proper attention, I will describe this issue (feel free to remove the comment later as you obviously are seemingly easily to have your feelings hurt regardless of whether the other person intended so or not...).
I'll just clarify a few things: I've never asked for help in the channel and treated someone ""poorly"". I responded ironically to ptx0's messages, after he spent the past few hours being dismissive or condescending to other people, obviously in some cases users with a very limited skill around ZFS, but that does not merit acting like an imbecile. None of them were insults or hostile, they simply pointed out that ptx0 was assuming things constantly and then replying with an air of arrogance and usually in a condescending tone, despite himself being far from a beacon of excellence as far as technical skill or coding go. Immediately after he proceeded to ban me from the channel, to which I responded in -social pointing out that his attitude and actions making him look puerile and petty regardless of the nature of the ""offense"" by someone else. This knee-jerk reaction came motivated more by petty personal sentiments (and sharing those with some others who, well, fit that profile) than any actually reasonable reasons.
Now, on the project: you are routinely accepting patches quite obviously without really doing your due diligence, ensuring proper QA, etc. Case in point, the one we have here. It's painful to see that you waste more time on petty nothings over actually making your code or merged patches look like something with a modicum of technical quality. The control over whether contributors are just copy-pasting from other spots (*hint* licensing violations get people sued), if the patches actually conflict with major kernel features, or symbol exports, or..... is lacking all over the place.
How about instead of making up half-truths about people offending your feelings, or your buddies' feelings, you go and properly benefit from the resources you so adamantly defend (and ask others to work with): How difficult was it to grab the patch I sent you from the link, test it, test it again, sign it off and merge it? You need to go around deleting comments and engaging in conversation with the other troll, but you are too busy to actually merge an out of band patch while you waste time over petty fights? Well, good luck with that. And you are surprised ZoL is not taken seriously anywhere but by the homelab/non-enterprise crowd. If you want to be a primadonna, at least have the cards in your sleeve to back it up: you can't justify it right now with the standard you are providing your users. And it runs in ring0, so go figure a better way to fend off all those users popping up on IRC asking about destroyed data because you merged a patch from a pull-request that would not stand a moment in the LKML.",Bitter Frustration
"I am temporarily locking this thread. If I could please ask everyone to calm down, I'm not about to pick sides in whatever conflict that is currently taking place here. Brian has asked for someone to submit a PR to fix this issue. Until that takes place, there is no reason for this issue to remain wide open given its current state.",
"The discussion in this thread (now hidden) has not been productive. In the future, please speak respectfully to fellow contributors. @behlendorf and I are working on putting a process in place for handling inappropriate behavior. In the mean time, please feel free to contact either Brian or I privately if you have concerns about the behavior of ZFSonLinux community members.",
"Hello,
what can I do if my templates are broken or something else in the asset pipe is missing?
I override some CSS things, imported it in the scss files and then i was figuring out how to get those assets in the system, i did following a few times:
RAILS_ENV=production bundle exec rails assets:clobber
RAILS_ENV=production bundle exec rails assets:clean
RAILS_ENV=production bundle exec rails assets:precompile
Then I also changed the logo on my pod (diaspora/app/assets/branding/logos)
But they all worked yesterday.
First thing which come up was that the branding/logos/asterisk_white_mobile.png was not found, but it was there, i changed something on templates and could not remember, so i put the original back in.
`[2018-10-13T15:28:15] INFO PID-32105 TID-19394480 ActionController::Base: Processing by HomeController#show as HTML
[2018-10-13T15:28:15] INFO PID-32105 TID-19394480 ActionController::Base: Completed 500 Internal Server Error in 89ms (ActiveRecord: 4.5ms)
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails: [2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails: ActionView::Template::Error (The asset ""branding/logos/asterisk_white_mobile.png"" is not present in the asset pipeline.):
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails: 14: .hidden-xs
15: = link_to AppConfig.settings.pod_name, root_path, class: ""navbar-brand""
16: .visible-xs-block.header-title
17: = link_to(image_tag(""**branding/logos/asterisk_white_mobile.png**"", class: ""img-responsive""),
18: root_path, class: ""navbar-brand"")
19: = render ""layouts/header_not_connected""
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails: [2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails: app/views/layouts/_header.html.haml:17:in `_app_views_layouts__header_html_haml__3542251370305246389_89831620'
app/views/layouts/with_header.html.haml:2:in `block in _app_views_layouts_with_header_html_haml__4381373114498847661_97260960'
app/views/layouts/with_header.html.haml:1:in `_app_views_layouts_with_header_html_haml__4381373114498847661_97260960'
app/views/layouts/with_header_with_footer.html.haml:4:in `_app_views_layouts_with_header_with_footer_html_haml__1720277477106993151_92204540'
app/controllers/home_controller.rb:25:in `show'`
**Then it worked somehow I think and the shit was moving forward:**
`[2018-10-13T15:30:23] FATAL PID-32105 TID-19394480 Rails: [2018-10-13T15:30:23] FATAL PID-32105 TID-19394480 Rails: app/views/layouts/_header.html.haml:17:in `_app_views_layouts__header_html_haml__3542251370305246389_85561160'
app/views/layouts/with_header.html.haml:2:in `block in _app_views_layouts_with_header_html_haml__4381373114498847661_87208320'
app/views/layouts/with_header.html.haml:1:in `_app_views_layouts_with_header_html_haml__4381373114498847661_87208320'
app/views/layouts/with_header_with_footer.html.haml:4:in `_app_views_layouts_with_header_with_footer_html_haml__1720277477106993151_92204540'
app/controllers/home_controller.rb:25:in `show'
[2018-10-13T15:30:27] INFO PID-32105 TID-19394480 Rails: Started GET ""/notifications?per_page=10&page=1&_=1539433227331"" for 2400:cb00:75:1024::a29e:ca94 at 2018-10-13 15:30:27 +0200
[2018-10-13T15:30:27] INFO PID-32105 TID-19394480 ActionController::Base: Processing by NotificationsController#index as JSON
[2018-10-13T15:30:27] INFO PID-32105 TID-19394480 ActionController::Base: Parameters: {""per_page""=>""10"", ""page""=>""1"", ""_""=>""1539433227331""}
[2018-10-13T15:30:27] INFO PID-32105 TID-19394480 ActionController::Base: Completed 500 Internal Server Error in 59ms (ActiveRecord: 8.7ms)
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails: [2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails: ActionView::Template::Error (The asset ""user/default.png"" is not present in the asset pipeline.):
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails: 9: .aspect-membership-dropdown.placeholder{data: {person_id: note.target.id}}
10: 11: .media-object.pull-left
12: = person_image_link note.actors.first, :size => :thumb_small, :class => 'hovercardable'
13: .media-body
14: = notification_message_for(note)
15: %div
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails: [2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails: app/models/profile.rb:70:in `image_url'`
**Then the mobile.js was missing in the asset pipeline**
`[2018-10-13T15:31:43] INFO PID-32105 TID-19394480 ActionController::Base: Completed 500 Internal Server Error in 476ms (ActiveRecord: 53.9ms)
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails: [2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails: ActionView::Template::Error (The asset ""mobile/mobile.js"" is not present in the asset pipeline.):
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails: 6: %html{lang: I18n.locale.to_s, dir: (rtl? ? ""rtl"" : ""ltr"")}
7: %head{prefix: og_prefix}
8: - content_for :javascript do
9: = javascript_include_tag ""mobile/mobile""
10: = load_javascript_locales
11: 12: = render ""head""
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails: [2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails: app/views/layouts/application.mobile.haml:9:in `block in _app_views_layouts_application_mobile_haml___4598889698582784532_59880980'
app/views/layouts/application.mobile.haml:8:in `_app_views_layouts_application_mobile_haml___4598889698582784532_59880980'
config/initializers/devise.rb:9:in `to_mobile'
app/controllers/tags_controller.rb:43:in `show'`
I tried to change the _header.mobile.haml and _header.haml to something like that and also tried to insert skip_pipeline, but syntax was wrong or it not worked, but it not worked.
` = link_to(image_tag(""branding/logos/asterisk.png"", class: ""img-responsive"", skip_pipeline: true),
`
What's the point?
Regards 001101",Bitter Frustration
"Hello,
GitHub is not a support forum; it's an issue tracker. For support issues, please post in the [podmin support section](https://discourse.diasporafoundation.org/c/support/podmin-support) of our Discourse forum.",
@goobertron You are wrong,Insulting
"No, he is not. And since you already posted there, I will close this issue.",
"He is and I posted after he told me, postfactum century here we are!!
The main reason why Github is the first choice, cause no noob in the forum has a clue about the code these days, got one answer, which repeats what I already wrote, great support!
https://discourse.diasporafoundation.org/t/templates-failed-did-no-changes/1951",Bitter Frustration
"Before you contribute, comment, or somehow interact with this project any further, I'd like you to read [our community guidelines](https://diasporafoundation.org/community_guidelines), as well as our [Code of Conduct](https://github.com/diaspora/diaspora/blob/develop/CODE_OF_CONDUCT.md). If you disagree with those rules and do not want to follow them, please stop interacting with this project.
When members of the project team, clearly indicated by a ""member"" badge in the header of all GitHub comments, ask you in a friendly tone do do something, then they do this because our project has rules, and everyone needs to follow them. In this case, GitHub is there to track software issues and track actionable feature requests. Support requests of any kind are handled on Discourse, as this allows more people to join. If you don't like this, well, too bad. This decision has been made by the majority of our community, and we for sure will not change our work just because you feel like it.",Bitter Frustration
"At the moment the test summary is logged using println https://github.com/gatling/gatling/blob/11308fcf5b2048f0a28a585529d13b5c0337d85f/gatling-core/src/main/scala/io/gatling/core/stats/writer/ConsoleDataWriter.scala#L75
This causes issues when shipping logs to Kibana in a logstash encoded format using the logstash-logback-encoder as println does not use that encoder. Is this done intentionally?
I did originally raise this issue here https://groups.google.com/forum/#!topic/gatling/Kq-hDp3oFkk but got no response. If it's ok to change this to use a logger I'm happy to contribute.",Impatience
"Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of [FrontLine](https://gatling.io/frontline), our Enterprise version.
For example, FrontLine provides a public API you can use to extract the stats, and a Grafana datasource.",
I think this is more about logging rather then stats. At the moment we can't even turn these logs off if we don't want them. This is a problem with the open source tool.,Bitter Frustration
"> At the moment we can't even turn these logs off if we don't want them.
Yes you can, remove `console` from the `writers` in `gatling.conf`.",
"Ok, that's my mistake. Though I'm still not sure why this would use a `println` rather then a logger that you can have more control over.",Impatience
"> Though I'm still not sure why this would use a println rather then a logger that you can have more control over.
So people can't mess up with the formatting and break the line length.
Again, this in not intended for extension, hence the format is not an public/stable API to be consumed.",
Would you be open to have a new implementation of `DataWriter` that used a logger instead?,
"No, for the very same reason: we need to make a living.
> Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of FrontLine, our Enterprise version.
I'll sound pushy, but I think it would be fair that [billion dollars companies](https://www.statista.com/statistics/273693/bskybs-annual-revenue/) that have been using an open source software for years would consider helping the company behind the technology.",Entitlement
"Ok, I'll concede on this. Though I think it's unrealistic to expect someone/a company to pay money for a tool that works perfectly fine for them, just to have finer grain control over their logs. We're not asking for real time live graphs or monitoring of injectors. This is something we are more then happy to do the work for to improve the tool, so it's shame we can't contribute in that way.",Bitter Frustration
"Sorry if I sounded harsh, that wasn't my intent.
We've drawn a clear line between what goes in Gatling OSS and what goes in Gatling FrontLine:
all new things related to stats/exports/integrations go into Gatling FrontLine.
We're more than happy with contributions related to core Gatling features such as HTTP and users orchestration. But we can't make any exception to the above rule.",
"@slandelle I am sorry but I don't really understand your reasoning, although I understand your concern around protecting the Enterprise value.
Just to confirm, your strategy is to prefer for us to fork this project so that we can use a logger for a single line of code where you are currently using println, pushing us further away from wanting the Enterprise solution because you are classifying using a logger to output the summary as part of `stats/exports/integrations`, although that information is already being outputted in a different form? To be honest, this entire thread so far makes me far less inclined to introduce a dependency on the enterprise project. I believe gatling is great as a project, I believe you should get paid to support and continue working on it full time, I also believe large companies should pay and help support opensource initiatives. The problem is that I also believe your approach will alienate many people, or at least me and all the people I talk to about this. It's a very simple code change, that actually makes it more consistent, as the majority of of your code base uses loggers (as any developer would expect). You are not even arguing about not outputting that data because it's `stats/enterprise` data. The data is there and available in the logs already! The only difference is the primitive output mechanism. The fact that many modern log shipping frameworks expect json/structured logs means that using println automatically makes you a less attractive product for both enterprise and non enterprise adopters. These println statements just create noise in most modern log shipping frameworks I have worked with as they cannot be interpreted as a single event. This is genuine feedback.
I would urge you to reconsider the quick decision, because I believe this suggestion exposes no more information than you currently do, moves you towards a more standardized output like the rest of your code base, and would actually show you positively engaging in improving your entire product offering, OSS and Enterprise. The functionality you are planning as part of Enterprise are still attractive to companies like ours, e.g. aggregation of metrics etc, grafana datasources etc. I think you've convinced yourself that any improvements to OSS is a risk to Enterprise. Just doesn't sit well with me.",Bitter Frustration
"@balooo Thank you for sharing your concerns. We donâ€™t see improvements of the OSS as a risk, quite the contrary: you can have a look at all the work thatâ€™s been put into upcoming Gatling 3 OSS.
Our Enterprise version, Gatling FrontLine, aims to provide our users with advanced features in terms of metrics, integration and automation. This Enterprise version was designed by the feedback of our users who built their own integrations with Gatling but then struggled to stabilize them and maintain them over time.
We believe this Enterprise version is a win-win for everyone: we make you save time and money, we guarantee you stability and, at the same time, you help the open-source project continue to develop.
We would be more than happy to continue this discussion live and convince you that we are still committed to improve load testing, both for our OSS users and for our enterprise customers.
Anyway, thank you for using Gatling",
"@slandelle Thanks for the reply, but I still don't think you actually addressed the points I was trying to raise regarding this particular issue. I still don't understand how the suggested change is against your commercial strategy. The points I was trying to articulate are that this request:
- doesn't expose any data that would previously be inaccessible / enterprise.
- doesn't suggest structuring the results in a way that is easier to parse/interpret programatically
- only suggests what we believe is better practice by outputting using a logger which is the standard practice
- would allow for common, modern log shippers to ship your output with our creating message noise, where each println is interpreted as a new log event.
I literally don't see a single reason you would be against it this request. I've read through your comments multiple times and I still don't see a single argument you have given explaining why . > Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of FrontLine, our Enterprise version.
Fair enough to that statement, but we're not talking about additional stats, pushing stats or anything about stats. We're talking about not using a logger for your output which causes problems when using industry standard shipping practices that expect structured logs.",Bitter Frustration
"I am sorry to read that my message didn't answer your questions.
We have a consistent strategy regarding our open-source solution and our Enterprise version since the R&D started in 2015. We don't plan to change it for the moment, I hope you understand. Anyway, as I said before, we would be more than pleased to discuss this with you live.
Closing this thread.",
"This so good called ""Software"" named Arduino IDE is literally the worst Software i will ever encounter in my entire LIFE !
Its so [expletive deleted] buggy that it wont even ever [expletive deleted] WORK ON ANY Computer like my HomePC, in schools, so everywhere !
If you try to connect one [expletive deleted] Thing via the Right Com port, rates and so on it never EVER will start to upload the code to it even if its just the start code with setup and loop function.
So i ask myself why is ist possible that i try every Combination of any Setting i can in my lifespan and it will NEVER Recognize even A [expletive deleted] VOLTAGE thats applied to the Usb Port.
So if its possible to throm my eq outta the window and buy hole new pc and This nasty IDE works then i might change this comment to a word less rude if its not banned until then by some guys saying,
uhhhh youre just to bad at IT in a whole or dont understand anything and this other basic replies to a form of criticism i dont mind cause i know that this IDE wont Run for what so ever unknown broken Transistor in my CPU or so ever. so bye hope you dont have problems with that [expletive deleted] at all.",Bitter Frustration
"This issue tracker is only to be used to report bugs or feature requests. You haven't provided any information that indicates a bug so I'm going to close this. This topic is more appropriate for the Arduino Forum. I'm sure we'll be able to help you with your problem over there.
Please do this:
1. Take some deep breaths and calm down. You won't get a good response on the forum with this sort of attitude. If you just wanted to vent, then mission accomplished. But if you're interested in getting a solution to your problem then you'll need to provide precise details.
1. Read http://forum.arduino.cc/index.php?topic=148850
2. If you haven't already done so, create a new thread in the appropriate section of the forum (http://forum.arduino.cc/), following all the rules. Be sure to post your code, using code tags (`</>` button on the toolbar).",Impatience
"**This issue tracker is a tool to address bugs in Flask itself.
Please use the #pocoo IRC channel on freenode or Stack Overflow for general
questions about using Flask or issues not related to Flask.**
If you'd like to report a bug in Flask, fill out the template below. Provide
any extra information that may be useful / related to your problem.
Ideally, create an [MCVE](http://stackoverflow.com/help/mcve), which helps us
understand the problem and helps check that it is not caused by something in
your code.
---
### Expected Behavior
```flask-tutorial
$ coverage run -m pytest
Name Stmts Miss Branch BrPart Cover
------------------------------------------------------
flaskr/__init__.py 22 0 2 0 100%
flaskr/auth.py 54 0 22 0 100%
flaskr/blog.py 54 0 16 0 100%
flaskr/db.py 24 0 4 0 100%
------------------------------------------------------
TOTAL 154 0 44 0 100%
```
### Actual Behavior
```flask-tutorial
$ coverage run -m pytest
============================================================== test session starts ==============================================================
platform darwin -- Python 2.7.10, pytest-3.9.1, py-1.7.0, pluggy-0.8.0
rootdir: /Users/adamg9999/Development/flask-tutorial, inifile: setup.cfg
collected 24 items
tests/test_auth.py ........ [ 33%]
tests/test_blog.py ............ [ 83%]
tests/test_db.py .. [ 91%]
tests/test_factory.py .. [100%]
=========================================================== 24 passed in 1.15 seconds ===========================================================
```
for coverage > 4.0 need to first run `coverage run -m pytest` this will print the pytest output to STDOUT, then a followup `coverage report` will output the expected report:
```$ coverage report
Name Stmts Miss Branch BrPart Cover
------------------------------------------------------
flaskr/__init__.py 22 0 2 0 100%
flaskr/auth.py 54 0 22 0 100%
flaskr/blog.py 54 0 16 0 100%
flaskr/db.py 24 0 4 0 100%
------------------------------------------------------
TOTAL 154 0 44 0 100%```
### Environment
$ python --version
Python 2.7.10 :: Continuum Analytics, Inc.
(py2.7.10) adamg9999 flask-tutorial
$ flask --version
Flask 1.0.2
$ coverage --version
Coverage.py, version 4.5.1 with C extension",
"IMO, `coverage run` $ `coverage report` is the normal usage.
See [ Coverage documentation](https://coverage.readthedocs.io/en/v4.5.x/) for more detail.",
"I didn't receive your reply and it's one day long, so I closed this. BTW, you can reopen it anytime at your will. Sorry for the inconvenience, so what do you mean actually?",Impatience
"The roles of â€œexpectedâ€ & â€œactualâ€ are reversed in context of the tutorial.
The tutorial needs update according to your comment.
On Tue, Oct 23, 2018 at 6:24 PM Grey Li <notifications@github.com> wrote:
> I didn't receive your reply and it's one day long, so I closed this. BTW,
> you can reopen it anytime at your will. Sorry for the inconvenience, so
> what do you mean actually?
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/pallets/flask/issues/2959#issuecomment-432478397>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AAibkk-0ta5DbrlreZLnT-ttz_Jh4sbWks5un8FSgaJpZM4X0RJS>
> .
>",
"The behavior described by the [document ](http://flask.pocoo.org/docs/1.0/tutorial/tests/#running-the-tests) seems to be consistent with what you describe. Are you reading the latest documentationï¼Ÿ
![image](https://user-images.githubusercontent.com/13200012/47400969-f8c90500-d771-11e8-8b9b-42948239141f.png)",Impatience
"I need to be able to push generated results to a DB, by exposing the report generator as public interface, I can provide a custom report generator which can push results to DB.",
Additional stats export fall into the scope of FrontLine.,
"Thanks, will have to move to a real open source tool like JMeter instead of gatling. Sad.",Bitter Frustration
"We are really sorry to hear this. We sincerely hope you will continue to use Gatling. As we explained to you earlier, our team spends time on both projects: our open-source solution and our Enterprise solution, Gatling FrontLine.
The way we decided to split our R&D is as follows: Everything related to Gatling itself, like new support protocols, will be open-sourced (eg. HTTP/2, closed workload model and new feeders features in our latest release); On the other hand, everything related to reporting and automation features is in the scope of Gatling FrontLine. Please note that our Enterprise version helps develop both projects and have a team of 7 full-time employees. We hope you understand.
Anyway, we would gladly continue this talk with you, feel free to contact us directly. We will now close this thread.
Best regards",
"Yesterday composer install works fine. Without any changes to composer.json & composer.lock today it fails with:
```
Roberts-iMac:zagis robert$ php composer.phar install Loading composer repositories with package information
Installing dependencies (including require-dev) from lock file
Your requirements could not be resolved to an installable set of packages.
Problem 1
- Conclusion: remove symfony/symfony v2.8.45
- don't install symfony/symfony v2.8.45|remove symfony/var-dumper v3.4.15
- don't install symfony/var-dumper v3.4.15|don't install symfony/symfony v2.8.45
- Installation request for symfony/symfony v2.8.45 -> satisfiable by symfony/symfony[v2.8.45].
- Installation request for symfony/var-dumper v3.4.15 -> satisfiable by symfony/var-dumper[v3.4.15].
```
Rolling back to 1.7.2 allows installing the old .lock file, however our CI/CD pipeline uses the latest composer which fails.
What changed? Why would you break working .lock files?",Bitter Frustration
"Same with:
```
donâ€™t install symfony/symfony v3.4.17|donâ€™t install symfony/cache v4.1.6
```",
"We also have the problem since this morning.
`composer install` still worked with 1.7.2, but fails with 1.7.3
```
Problem 1
- Installation request for symfony/asset v4.1.6 -> satisfiable by symfony/asset[v4.1.6].
- don't install symfony/symfony v3.4.17|don't install symfony/asset v4.1.6
- Installation request for symfony/symfony v3.4.17 -> satisfiable by symfony/symfony[v3.4.17].
```",Impatience
"These are all due to https://github.com/composer/composer/pull/7454 most likely.. Edge cases that should have broken before but were allowed accidentally.
In most of these cases you have both symfony/symfony AND some of the individual components installed, in different versions. This doesn't make any sense and might end up with unexpected results as one or the other package gets autoloaded.
To resolve this make sure you remove symfony/symfony or the other components depending on your app's requirements.",Bitter Frustration
I understand the reasoning to not allow these combinations anymore when you run composer update but this breaks existing .lock files. This breaks existing installations when updating a minor version increment of composer.,Bitter Frustration
"Yes that's unfortunate but it's not really something we can selectively fix as the solver is needed as well when running `install`. And technically it was a bug, albeit you surely can argue there, but fixing it in 1.8.0 would also lead to the same problem, and maybe more people affected by then, so IMO the sooner the better.",
"I am having the same issue also with 1.7.3
```
Do not run Composer as root/super user! See https://getcomposer.org/root for details
Loading composer repositories with package information
Installing dependencies (including require-dev) from lock file
Dependency resolution completed in 0.003 seconds
Your requirements could not be resolved to an installable set of packages.
Problem 1
- Conclusion: remove symfony/polyfill v1.7.0
- Installation request for symfony/polyfill v1.7.0 -> satisfiable by symfony/polyfill[v1.7.0].
- Conclusion: remove symfony/stopwatch v4.1.1
- symfony/polyfill v1.7.0 requires symfony/intl ~2.3|~3.0|~4.0 -> satisfiable by symfony/symfony[v3.4.12].
- don't install symfony/stopwatch v4.1.1|remove symfony/symfony v3.4.12
- don't install symfony/symfony v3.4.12|don't install symfony/stopwatch v4.1.1
- Installation request for symfony/stopwatch v4.1.1 -> satisfiable by symfony/stopwatch[v4.1.1].
```",Bitter Frustration
"@Seldaek : A compatibility break between minor versions is ""better""? Surely you jest!",Irony
"Please do not post ""me too"" comments if you aren't going to provide any additional information. It spams everyone ands makes it harder to find help in this thread.
**To resolve this make sure you remove symfony/symfony or the other symfony components depending on your app's requirements.** e.g. `don't install symfony/symfony v3.4.12|don't install symfony/stopwatch v4.1.1` states that those two are incompatible, you have to find a way to remove either of them, which might involve upgrading/downgrading the other.",Impatience
"I ran into this issue with the following command:
```sh
$ composer require acquia/lift-sdk-php typhonius/acquia-php-sdk-v2
```
This appears to be because the `typhonius/acquia-php-sdk-v2` requires `acquia/http-hmac-php`, and `acquia/lift-sdk-php` library requires a FORK of _that_ library (`nickveenhof/http-hmac-php`). Both **http-hmac-php** libraries use different namespaces, so I'm not sure why I can't have both present in my project. My standing guess is that both libraries indicate they are `replace: acquia/hmac-request`, and that is the source of the problem? But I am not requesting that library at all, so that doesn't completely make sense either... I'm not sure I understand what is causing this issue...?
Just to piggyback on some of the comments above, this does seem like a poor choice of a change to include in a patch release. It's clear this is affecting a lot of people based on the activity here 6 hours after the release. I think some kind of composer hints that this will break in the future minor release would have been a better DX, and give users time to update their libraries before it fubars people's builds.",Bitter Frustration
"# Issue type
- **Questions about the server or its usage MUST be posted to the [users mailing list](http://freeradius.org/list/users.html). If you post those issues here, they will be closed and locked.**
- **Remote security exploits MUST be sent to security@freeradius.org.**
***
**REMOVE THOSE WHICH DO NOT APPLY**
- Defect - Crash or memory corruption.
- Defect - Non compliance with a standards document, or incorrect API usage.
- Defect - Unexpected behaviour (obvious or verified by project member).
- Feature request.
See [here](https://github.com/FreeRADIUS/freeradius-server/blob/master/doc/bugs.md) for debugging instructions and how to obtain backtraces.
NOTE: PATCHES GO IN PULL REQUESTS. IF YOU SUBMIT A DIFF HERE, THE DEVELOPMENT TEAM WILL HUNT YOU DOWN AND BEAT YOU OVER THE HEAD WITH YOUR OWN KEYBOARD. # Defect/Feature description
## How to reproduce issue
*REMOVE SECTION IF FEATURE REQUEST*
## Output of ``[radiusd|freeradius] -X`` showing issue occurring
(you may need to run ``[radiusd|freeradius] -fxx -l stdout`` if using eg RADIUS with TLS)
*REMOVE SECTION IF FEATURE REQUEST*
```text
COPY/PASTE OUTPUT HERE (WITHIN BACKTICKS). NO PASTEBIN (ET AL) LINKS!
```
## Full backtrace from LLDB or GDB
*REMOVE SECTION IF FEATURE REQUEST OR NOT CRASH/MEMORY CORRUPTION*
```text
COPY/PASTE OUTPUT HERE (WITHIN BACKTICKS). NO PASTEBIN (ET AL) LINKS!
```",Threat
"There is an ""unsubscribe"" link at the bottom of every mail sent to the list.
Instead of unsubscribing, you spam the list with hundreds of stupid messages.
Stop this idiotic behavior.",Bitter Frustration
"Just as a public record of this insanity, he's replying to every single message that anyone sent to the freeradius-users mailing list in the last 4 years. And emailing every person privately, asking to be unsubscribed.
This displays an amazing level of anti-social stupidity.",Bitter Frustration
Why this change?,Impatience
What's the reasoning here?,Impatience
@drupol @Jan0707 Fabien explained the reasons in this Twitter thread: https://twitter.com/fabpot/status/1064946698089365505,
"I don't understand you here @fabpot: in which case(s) removing Symfony project from PHP Fig will help the interoperability and PHP standards in the future? Do we want to come back 6 years ago with custom autoloading, custom coding styles and custom ways to do everything ?",Bitter Frustration
"> Do we want to come back 6 years ago with custom autoloading, custom coding styles and custom ways to do everything ?
Is this really about autoloading and coding style? Because Fabien's thread is clear about this: they both are great PSRs, it's mostly the ""new wave of incoming PSRs"" that's not interop and kinda breaks the initial project into a more opinionated set of rules instead of an interop one",Bitter Frustration
"@fabpot is completely correct, on all points. And to his point about PSR-7 specifically, there's actually an inherent security flaw in its design pertaining to file uploads. I tried to alert the FIG to it some years ago, but given that they [deliberately hide the GitHub issues feature from the repository](https://github.com/php-fig/http-message), I took it as a strong indicator that they don't want to know.",Bitter Frustration
This isn't a place for this discussion. Please use the mailing list.,
Merged in 2e54756b87e895bf4e5238beafa0e20a36bad9d3 but github seems to be having issues,Impatience
"Plase, add a step on the Step by Step Tutorial to demonstrate how to theme the just finished demo app.
## Motivation
1. Theming is not an obvious task for a newbie like me. Also, doc speaks about gem based and 'regular' file themes. It's confusin a bit.
2. At the end of demo app it's more beautiful to leave in the han of the user a well looking demo
## Idea
Please include istructions on - how to enable default gem base theme
- how to customized if possible a gem theme
- how to switch to a regular file based theme
- idem, how to customize it
... or ...
Create a guide on how to start theming AND then include previous steps into the new tutorial.
Thanks for this excellent piece of software !!!!",
"On how to start theming for Jekyll, I'd recommend @daviddarnes articles:
1. https://www.siteleaf.com/blog/making-your-first-jekyll-theme-part-1/
2. https://www.siteleaf.com/blog/making-your-first-jekyll-theme-part-2/",
"Thanks, but it's better have an official doc, and not links to external resources. IMHO",Impatience
If it's validation you're looking for then I can 100% endorse these tutorials âœ¨. However if you want this set in stone then maybe we could link to these tutorials in the official docs? Maybe in https://jekyllrb.com/tutorials/home/ or https://jekyllrb.com/resources/. What do you day @DirtyF?,
"Oooh, I like @desiredpersonaâ€™s idea.",
"IMHO, Documentation is one thing and a (sponsored) community blog is an entirely different league altogether. They should not be intermingled.
Documentation is translating ""the code base"" into layman lingo. (*Point-of-View: Maintainers -> Users*)
Community blog is about sharing experience with the code base. (*Point-of-View: One User -> Other Users*)
Bringing the sponsors into the mix, is just complicating things, unnecessarily.........",Bitter Frustration
Sorry but ... To rest in topic... A simple step added to main step by step intro is so wrong?,Bitter Frustration
"I donâ€™t see what is wrong with referring to a tutorial on another site? Yes the tutorial was paid for, but tutorials of that size take time to create. Maybe we could add some more detail on the points people are getting stuck on in the docs?",
"If I see a tutorial on an external website it soon or later will become obsolete.
If I see a tutorial on main site I think that must be ok, tested, and updated. Simply. Close this issue
A company that does not understand the need of a full doc do not will offer a serious support in the long time
Bye bye.",Bitter Frustration
"> A company that does not understand the need of a full doc do not will offer a serious support in the long time
Jekyll is an open-source project **freely maintained by volunteers**, it has been around for 10 years now and powers hundred of thousands of websites around the world. Jekyll has a great [community](https://talk.jekyllrb.com/) and docs are continuously improved with the help of the contributors.
Jekyll themes _are_ [documented](https://jekyllrb.com/docs/themes/#understanding-gem-based-themes). I'll see if I can add some links to point to theming at the end of the step-by-step guide.",
"<!-- âš ï¸ If you do not respect this template your issue will be closed. -->
<!-- âš ï¸ Make sure to browse the opened and closed issues before submitting your issue. -->
<!-- Please also submit your idea on the Strapi Product Board:
https://portal.productboard.com/strapi/tabs/2-under-consideration/submit-idea
If your request on the product board is accepted this feature request issue will be closed,
but will still accept public discussion.
-->
- [ ] **I have created my request on the Product Board before I submitted this issue**
- [ ] **I have looked at all the other requests on the Product Board before I submitted this issue**
**Please describe your feature request:**
### Background
**Context**
I have been working over the last few days with the strapi code-base add support for geographical data types (GeoJSON, to be more precise). I was pleased to see that strapi already had support for JSON, so I repurposed the code for `InputJSON` and `InputJSONWithErrors` components to create the GeoJSON content-type. To do so, I've followed the instructions posted on [this issue](https://github.com/strapi/strapi/issues/483#issuecomment-358971985) **Why a separate GeoJSON content-type? (A real world use-case)**
Our organization primarily works with local governments in Nepal to build official portals that help them connect with their citizens. In doing so, some offices have realized a need to include a feature that allows them to visualize all of capital-projects for the fiscal year. See image below:
![image](https://user-images.githubusercontent.com/24402285/50380691-535ddf00-0697-11e9-9e77-2cad4e1674d7.png)
We want government officials to be able to upload geographic information on their own. Due to their limited technical knowledge, we found that the best way to get this done is by providing them with a user-interface to draw out the project.
I've gone ahead and created a component to facilitate the same. ### Steps taken:
**Steps to display new input in the content type builder**
- To display my new field I have added it it in the `/plugins/content-type-builder/admin/src/containers/Form/forms.json`file.
- I've also populated the corresponding translations in the `/plugins/content-type-builder/admin/src/translations` folder. However, since I don't know language other than English, I've used english everywhere.
- I added another `switch-case` statement inside `strapi/plugins/strapi-hook-mongoose/lib/utils/index.js` file for my new content-type (geojson), simply copy pasting the one for json. The plugin didn't run until I did that.
- I have also added images for my new content type in the `AttributeRow` and `AttributeCard` components.
- I was thus able to display my new attribute in the plugin (this might have an impact on the plugin's component)
**Using my new type in the content manager**
- I had to add a few new packages npm packages `(mapbox gl-js, @mapbox/mapbox-gl-js-draw, @mapbox/geojson-extent etc.)` and css files, I did not know how to add CSS files so i directly used the index.html file in the `admin/admin/src/index.html` I know this is not optimal, what are my options?
- When adding `@mapbox/mapbox-gl-js-draw` I ran into issues as I rebuilt the `content-manager` plugin. So for my present purpose, I have served the CDN through my public directory `/public`, Iâ€™m aware this is not the best way to do things. Looking forward to your inputs here too.
- I also had to update the `containers/saga.js` and update the following line :
`const cleanedData = (attrType === 'json' || attrType==='geojson') ? record[current] : cleanData(record[current], 'value', 'id');`
- In the content manager plugin I modified the `Edit` container and its children in order to display my new field type.
- I created the `InpuGJSON` and `InputGJSONWithErrors` component and served them through the `Edit` component as suggested. ### How it looks
![image](https://user-images.githubusercontent.com/24402285/50380851-b05c9380-069d-11e9-8525-9452bbb743fa.png)
### Features:
The GeoJSON input component allows a user to:
- Upload a `.geojson` file.
- Preview and modify uploaded `.geojson` in the browser.
- Draw custom points, lines and polygons (multiple points, lines and polygons also allowed) directly through the tool.
- Search for a place using a search box (using `mapbox-generic-geocoder` with OSM Nominatim), and zoom in to the location.
### Questions/Comments:
- What is the right way to use CDNs for JS and CSS in strapi? Im not sure if i followed standard practices for the same.
- Currently I've used [Maptiler Cloud](https://cloud.maptiler.com/) for hosting my vector tiles. This service allows upto 100000 free views in a month. If needed I can set up a map tile server for the same.
- All of my changes can be found at [my fork of strapi here](https://github.com/arkoblog/strapi).
- I'm looking forward to someone from strapi going through my code and pointing out blunders, mistakes so that I can work on the same. I've also submitted a pull request to facilitate the same.",
"Hi @arogyakoirala thanks for your inputs here are some answers:
> What is the right way to use CDNs for JS and CSS in strapi? Im not sure if i followed standard practices for the same.
Maybe you can try something like this in the `content-manager/admin/src/components/InputGJSONWithErrors`:
```
import '<my-package/dist/<stylesheet>.css';
// ... rest of the code
```
If it doesn't work, well you can try to create the tag manually and insert it into the dom or just hardcode it in the index.html like you did. I prefer the first option which is to import the stylesheet.
> Currently I've used Maptiler Cloud for hosting my vector tiles. This service allows upto 100000 free views in a month. If needed I can set up a map tile server for the same.
I think that @lauriejim or @Aurelsicoko are the one who could answer this question
> All of my changes can be found at my fork of strapi here.
That's awesome thanks for providing your fork! I'll take a look into it!
> I'm looking forward to someone from strapi going through my code and pointing out blunders, mistakes so that I can work on the same. I've also submitted a pull request to facilitate the same.
You can DM on Slack (@soupette) I answer most of the time!
Thanks again for providing a such detailed issue!",
"finding a way to search geo locations within my frontend (nuxt), from the strapi backend.
https://mongoosejs.com/docs/geojson.html
I believe I need a point schema, is it possible to add something like this to the model? Since point is currently not available as a strapi type (from the mongoose docs):
```
location: {
type: {
type: String,
enum: ['Point'], required: true
},
coordinates: {
type: [Number],
required: true
}
}
```
It would be awesome if a resolver for finding GeoJSON values would be baked in within strapi,
for both API and graphql.. or some clear documentation where to implement:
```
db.locations.find({
location: {
$nearSphere: {
$geometry: {
type: 'Point',
coordinates: [-122.5, 37.1]
},
$maxDistance: 900 * 1609.34
}
}
})
```
thanks",
"The GeoJSON field wasn't one of priority. The GeoPoint (aka Map) is a priority for example. However, the work done here seems pretty good. I would love to see it merged to the project.
I'm not sure to understand the questions:
- Where do you expect to store the vectors tiles?
- Can you submit the PR so we'll be able to review the code and continue the work together,",Impatience
"> The GeoJSON field wasn't one of priority. The GeoPoint (aka Map) is a priority for example. > However, the work done here seems pretty good. I would love to see it merged to the project.
Thanks! I would be more than happy to help in any way.
> I'm not sure to understand the questions:
> > Where do you expect to store the vectors tiles?
The vector tiles that I'm currently using comes from MapTiler Cloud ([see pricing here](https://www.maptiler.com/cloud/plans/)). I'm currently using the free plan which allows upto 100,000 requests a month. I guess what I'm asking is if that would be enough. If not, we will have to look for other free options (which I'm sure are available, I just haven't spent enough time researching on the same).
> Can you submit the PR so we'll be able to review the code and continue the work together,
I did this the same time I opened this issue. You can find it [here.](https://github.com/strapi/strapi/pull/2526)",Impatience
"> The GeoJSON field wasn't one of priority. The GeoPoint (aka Map) is a priority for example.
@Aurelsicoko I'm not sure if I understand what a GeoPoint is. Can you elaborate?",
"Should I open a different feature request?
I 'just' :-) need a field within contenttypes to add points (geojson)
So I can add markers to my strapi model
```
""marker"":{
""type"": ""Point"",
""default"":[],
""coordinates"": [0,0]
},
```
These markers can be found with something like
```
Message.find({
marker: {
$near: {
$maxDistance: 1000,
$geometry: {
type: ""Point"",
coordinates: [long, latt]
}
}
}
}).find((error, results) => {
if (error) console.log(error);
console.log(JSON.stringify(results, 0, 2));
});
```
and display on any map library client sided",
"@arogyakoirala So if we have to depend on a third-party, we should add a provider system to this new type of field.
About the GeoPoint is nothing more than a pin that you put on Map. It just works like a GPS (lat, lon coordinates).",
"> @arogyakoirala So if we have to depend on a third-party, we should add a provider system to this new type of field.
I'm assuming this refers to the map tiles, let me do my research on this and get back soon.
> About the GeoPoint is nothing more than a pin that you put on Map. It just works like a GPS (lat, lon coordinates).
Oh that can be easily done through this new content-type. In fact, it is multi-geometry by default, meaning you can add one (or many points), lines and polygons, as part of a single entry. It gets captured as a FeatureCollection ([https://macwright.org/2015/03/23/geojson-second-bite.html#featurecollection](https://macwright.org/2015/03/23/geojson-second-bite.html#featurecollection))",
"@arogyakoirala Is it possible to change the map background in your field? In the beginning, I was thinking about using something like OpenStreetMap ;)",Irony
@Aurelsicoko this is OpenStreetMap. However I've used vector tiles (mapboxgljs) as opposed to raster (leafletjs).,
"@arogyakoirala Your fork is only for MongoDB ? Not for PostgreSQL/PostGIS ?
I am starting the same, but with PG and the ""geometry"" data type...",Impatience
"@sign0 I've merely customized the JSON input type to easily take geometries in GeoJSON format. I do see value in extending this functionality to 'geometry' data type, as it would enable backend geospatial querying.",
"@arogyakoirala Yes, especially since the latest versions of PostgreSQL efficiently manage the JSON with the JSONB type.
Postgres is also suitable for storing vector or raster tiles (I use that: https://openmaptiles.org/ with SQLite/MBTile or Postgres/gis, and it's compatible with the mapbox-gl workflow like gl-style). Unfortunately, I'm not familiar with Knex ORM, especially when it comes to interacting with PostGIS (I more familiar with Sequelize ORM).
And the management of geometry type would mean that it would be an ""exclusive feature"" for Postgres : which would go may be against the philosophy of this project which claims to be a NoSQL / SQL compatible CMS... But for example, Wordpress is ""optimized"" for MySQL).
I have not yet tried your contribution, but the description is impressive ! :)",
"@arogyakoirala - Awesome work. I was pleasantly surprised to see that you are working on this. I was planning on having to do this on my own. I look forward to checking out your fork. I'd love to help if I can. Assuming the strapi folks can see the obvious benefits of this, a few areas that I'd love to work on:
1. Topojson - an extension to GeoJSON that encodes topology rather than discrete geometries, allowing for smaller files and on-the-fly simplification.
2. GeoJSON model - I haven't checked out the fork yet and I don't know if it is your intent to fully model the GeoJSON spec. It would be great to have the ability for instance to create relations between GeoJSON `Features` and other strapi models.",
"> Postgres is also suitable for storing vector or raster tiles (I use that: https://openmaptiles.org/ with SQLite/MBTile or Postgres/gis, and it's compatible with the mapbox-gl workflow like gl-style).
There's postGIS as well https://postgis.net/",
"> > Postgres is also suitable for storing vector or raster tiles (I use that: https://openmaptiles.org/ with SQLite/MBTile or Postgres/gis, and it's compatible with the mapbox-gl workflow like gl-style).
> > There's postGIS as well https://postgis.net/
Yes it is!
Postgis also supports geometry -> TopoJSON with AsTopoJSON() function.",
"> @arogyakoirala - Awesome work. I was pleasantly surprised to see that you are working on this. I was planning on having to do this on my own. I look forward to checking out your fork. I'd love to help if I can. Assuming the strapi folks can see the obvious benefits of this, a few areas that I'd love to work on:
> > 1. Topojson - an extension to GeoJSON that encodes topology rather than discrete geometries, allowing for smaller files and on-the-fly simplification.
> > 2. GeoJSON model - I haven't checked out the fork yet and I don't know if it is your intent to fully model the GeoJSON spec. It would be great to have the ability for instance to create relations between GeoJSON `Features` and other strapi models.
@cupofnestor thanks for your appreciation, I'm really glad (first open-source contribution woot woot). I feel both of the features you suggest will ad significant value. TopoJSON will reduce the load, and a GeoJSON model should help facilitate spatial queries which would be awesome. Feel free to fiddle with my fork and make changes. Do let me know how I can help.",
"Just wanted to chime in to say (and maybe up the priority a bit :)), that currently this is the only thing holding me back to fully switch to strapi for a project, since it needs an easy way for editors to input location information. datocms has a similar field which works really well for addresses, I think that would be a great start which could further be extended to more complex geodata.",Impatience
"It would be nice to support this GeoJSON type, so more applications could be developed with Strapi.",
"This will come with the custom field feature. With that, every field type will be able to be supported.
I'm locking this issue. We well understood the need.
Thank you all for your contribution.",
"Hey all, we are currently moving many feature requests to our new feedback and feature request website to help clean up the GitHub issues and make it easier for us to review bug reports and fix them.
I have moved this feature to the following URL: https://feedback.strapi.io/plugin-requests/p/geo-mapping-plugin
If you are interested in this feature please feel free to go there and add more comments and/or upvote it there.
For now I will close this and lock it so that all new information goes into our new feedback website.
Thanks!",
"<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->
<!--- Provide a general summary of your changes in the Title above -->
<!---
Documentation on ZFS Buildbot options can be found at
https://github.com/zfsonlinux/zfs/wiki/Buildbot-Options
-->
### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
Frequently-requested feature to help performance on SSDs and on various other SAN-like storage back-ends which support UNMAP/TRIM.
### Description
<!--- Describe your changes in detail -->
This is a port of Nextenta's TRIM support to the current master branch - post-device removal and post-device initialization.
### How Has This Been Tested?
New ZTS tests. Also additions to ztest.
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [x] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [x] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [ ] All new and existing tests passed.
- [ ] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).",
"# [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=h1) Report
> Merging [#8255](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=desc) into [master](https://codecov.io/gh/zfsonlinux/zfs/commit/0a10863194b0e7c1c64f702f868c10d5dac45ea5?src=pr&el=desc) will **decrease** coverage by `0.04%`.
> The diff coverage is `88.01%`.
[![Impacted file tree graph](https://codecov.io/gh/zfsonlinux/zfs/pull/8255/graphs/tree.svg?width=650&token=NGfxvvG2io&height=150&src=pr)](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=tree)
```diff
@@ Coverage Diff @@
## master #8255 +/- ##
==========================================
- Coverage 78.33% 78.29% -0.05% ==========================================
Files 380 382 +2 Lines 115719 116678 +959 ==========================================
+ Hits 90653 91350 +697 - Misses 25066 25328 +262
```
| Flag | Coverage Î” | |
|---|---|---|
| #kernel | `78.93% <86.25%> (+0.04%)` | :arrow_up: |
| #user | `65.8% <87.99%> (-1.21%)` | :arrow_down: |
------
[Continue to review full report at Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=footer). Last update [0a10863...3a184b8](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",
This has been refreshed with fixes to issues discovered during testing. There are still some deadlock issues which occur mainly when the `autotrim` property is changed and another pool management is performed (mainly adding and possibly removing vdevs). The deadlocks are being actively investigated.,
"Status update: This has just been refreshed with some fixes to the various deadlocks observed while running ztest (which have been left as separate commits). Reviewers should look at 1eda73773 and the related follow-on commits in particular. With these changes in place, zloop can now generally survive an overnight run. I've not yet done any actual testing to make sure this plays well (i.e. actually works as opposed to simply not panicking) with allocation classes or non-trivial cases of device removal (other than those which are performed via ztest, which are plenty). We (@behlendorf and myself) will begin addressing the review feedback shortly. Also, some of the new ZTS tests need fixing.
[EDIT]: Note that there were actually 3 (force) pushes in rapid succession because there was an experimental fix attempt left in by accident and also to rebase on current master with recent changes to metaslab.c. Also, I'll note that as new fixes are added, they're commited to the [dweeezil:ntrim3-next](https://github.com/dweeezil/zfs/tree/ntrim3-next) branch.",
"Include `atrim` and `mtrim` latency histograms in the man page
```
--- a/man/man8/zpool.8
+++ b/man/man8/zpool.8
@@ -1820,6 +1820,10 @@ Amount of time IO spent in asynchronous priority queues. Does not include
disk time.
.Ar scrub :
Amount of time IO spent in scrub queue. Does not include disk time.
+.Ar atrim :
+Amount of time IO spent doing a trim operation during an autotrim.
+.Ar mtrim :
+Amount of time IO spent doing a trim operation during a manual trim.
.It Fl l
Include average latency statistics:
.Pp
```",
You'll want to add the new `trim_start` and `trim_end` events to `man/man5/zfs-events.5`.,
Is it policy to license all new files under CDDL? May authors supply code under a more permissive license? (Sorry for the OT),
"> the CDDL header is copyrighted text
Not to make a fuss about this, but
- I think Tony's initial question was not about the CDDL header itself but on what's written underneath
- the CDDL header has a clear end marker so any copyright of that header should end there (?!)
- is it even legal to antedate a copyright on new code and attribute it to an organisation where you cannot even be an employe because it no longer exists?",Bitter Frustration
"I'm going to be closing this PR. @behlendorf has been doing some major re-working of the original TRIM work to, among other things:
- Harmonize its operation with that of device initialization so it's possible, for example, to TRIM individual vdevs.
- Add per-vdev trim stats
- Re-work the underlying discard mechanism to use a new ZIO type rather than the DKIOCFREE ioctl.
- Utilize the new ""xlat"" vdev method from device initialization where appropriate.
- Switch from the legacy ioctl interface to the new nvlist-style ioctl interface.
- Plenty of other restructuring, in particular w.r.t. the management of the trim threads.
Expect a new PR to be posted soon.",
"### System information
<!-- add version after ""|"" character -->
Type | Version/Name
--- | --- Distribution Name	| Fedora
Distribution Version	| 30 (rawhide)
Linux Kernel	| 5.0.0-0.rc1.git0.1.fc30.x86_64
Architecture	| x86-64
ZFS Version	| master
SPL Version	| master
<!-- Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version -->
### Describe the problem you're observing
`current_kernel_time64()` was removed from recent kernels. Looks like `ktime_get_coarse_real_ts64()` should be used instead.
https://lkml.org/lkml/2018/7/11/202
https://lkml.org/lkml/2018/12/17/110
### Describe how to reproduce the problem
Build master in rawhide
### Include any warning/errors/backtraces from the system logs
``` CC [M] /home/hutter/zfs/module/icp/illumos-crypto.o
In file included from /home/hutter/zfs/include/spl/sys/condvar.h:33,
from /home/hutter/zfs/include/sys/zfs_context.h:38,
from /home/hutter/zfs/include/sys/crypto/common.h:39,
from /home/hutter/zfs/module/icp/illumos-crypto.c:35:
/home/hutter/zfs/include/spl/sys/time.h: In function â€˜gethrestimeâ€™:
/home/hutter/zfs/include/spl/sys/time.h:76:8: error: implicit declaration of function â€˜current_kernel_time64â€™; did you mean â€˜core_kernel_textâ€™? [-Werror=implicit-function-declaration]
*ts = current_kernel_time64();
^~~~~~~~~~~~~~~~~~~~~
core_kernel_text
/home/hutter/zfs/include/spl/sys/time.h:76:6: error: incompatible types when assigning to type â€˜inode_timespec_tâ€™ {aka â€˜struct timespec64â€™} from type â€˜intâ€™
*ts = current_kernel_time64();
^
/home/hutter/zfs/include/spl/sys/time.h: In function â€˜gethrestime_secâ€™:
/home/hutter/zfs/include/spl/sys/time.h:86:24: error: invalid initializer
inode_timespec_t ts = current_kernel_time64();
^~~~~~~~~~~~~~~~~~~~~
cc1: all warnings being treated as errors
```",
Looks related to https://lore.kernel.org/patchwork/patch/562186/,
"### System information
<!-- add version after ""|"" character -->
Type | Version/Name
--- | --- Distribution Name	| Fedora
Distribution Version	| 30 (rawhide)
Linux Kernel	| 5.0.0-0.rc1.git0.1.fc30.x86_64
Architecture	| x86-64
ZFS Version	| master
SPL Version	| master
<!-- Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version -->
### Describe the problem you're observing
The 4.2 kernel refractored the FPU code: https://lwn.net/Articles/643235/ It seems the latest kernels removed the old compatibility headers.
We need to rename our #includes `<asm/i387.h> -> <asm/fpu/api.h>` and `<asm/xcr.h> -> <asm/fpu/internal.h>` on newer kernels.
### Describe how to reproduce the problem
`make` on rawhide with ZFS master
### Include any warning/errors/backtraces from the system logs
```
/home/hutter/current_kernel_time64/include/linux/simd_x86.h:98:10: fatal error: asm/i387.h: No such file or directory
#include <asm/i387.h>
^~~~~~~~~~~~
compilation terminated.
```",
"The underlying issue comes from kernel commit 12209993e98c5fa1855c467f22a24e3d5b8be205 (""x86/fpu: Don't export __kernel_fpu_{begin,end}()""), which unexports __kernel_fpu_begin(), and causes the configure tests not to define HAVE_FPU_API_H. Changing the header locations will just lead to another failure later because kernel_fpu_begin/end() are exported GPL, and can't be used directly from the zfs module.
See the thread on LKML starting here: https://marc.info/?l=linux-kernel&m=154689892914091
Marc",
@mdionne thanks for bringing this to our attention. Hopefully they change their minds and `EXPORT_SYMBOL(kernel_fpu_begin)` so we can use that instead.,Impatience
"https://marc.info/?l=linux-kernel&m=154714516832389&w=2
Gotta love the replies. Greg K-H:
> My tolerance for ZFS is pretty non-existant. Sun explicitly did not
> want their code to work on Linux, so why would we do extra work to get
> their code to work properly?",Mocking
It would be nice if Oracle would just fix the license once and for all.,Bitter Frustration
"I think we need to operate under the assumption that they're not going to export the functions, and just disable the vectorized versions of the checksums if `_kernel_fpu{begin,end}` are not detected.",
"Looks like the kernel people may have backported their patch to older kernels:
> For the record, it appears the FPU export was removed in 4.19.38 (released 2019-05-02) via d4ff57d0320bf441ad5a3084b3adbba4da1d79f8 and 4.14.120 (released 2019-05-16) via a725c5201f0807a9f843db525f5f98f6c7a4c25b.
https://github.com/zfsonlinux/zfs/issues/8793#issuecomment-496709454",
"When someone tries to refresh the refs buffer, they expect it to reflect the situation in the origin (beside other things). For no reason, Git itself doesn't update this information if you do a fetch or similar... so, this needs extra work and fixing. You already have the list of all origins, so it would be nice if refresh in this buffer would also synchronize your local view with the actual state of the origins.
Right now, if your colleague working on the same repository deleted a branch, you will never know it is gone, until you try to pull it / delete it etc, which confuses people and, sometimes will result in you restoring the branch deleted by your colleague from your (typically incomplete) history, so once the other person sees it they will be really puzzled about what had happened.",Bitter Frustration
"You can configure Magit to always prune when fetching: `f` (enter popup), `- p` (set `--prune`), and finally `C-x C-s` (save the arguments).
> For no reason, Git itself doesn't update this information if you do a fetch or similar...
It makes it less likely that accidental deletions propagate unnoticed.
> When someone tries to refresh the refs buffer, they expect it to reflect the situation in the origin (beside other things).
What refreshing has always done in this and other buffers is to update to reflect the situation in the local repository. *That* is what most users would expect and to instead perform a destructive operation would be very surprising and dangerous.
So I won't add the requested feature, but you can easily implement it using `magit-post-refresh-hook`.",
"You probably don't understand what `git remote prune origin` does...
It doesn't do anything destructive in the remote repository. It
synchronizes your *local* refs with the remote refs, taking *remote*
as the source of truth, and updating the local to reflect it. Thus,
it doesn't do any destructive changes at all, neither locally nor
remotely. The functionality of Git is just wrong when it does a
fetch, it should've also fetched the refs, but... it doesn't. And I'm
just too tired of Git's boolshit to try to fight it.
Think about this operation as cache invalidation, all it does is it
instructs to invalidate the local cache of remote refs, which it
should've been doing by default anyways.
On Fri, Jan 11, 2019 at 3:34 PM Jonas Bernoulli
<notifications@github.com> wrote:
>
> You can configure Magit to always prune when fetching: f (enter popup), - p (set --prune), and finally C-x C-s (save the arguments).
>
> For no reason, Git itself doesn't update this information if you do a fetch or similar...
>
> It makes it less likely that accidental deletions propagate unnoticed.
>
> When someone tries to refresh the refs buffer, they expect it to reflect the situation in the origin (beside other things).
>
> What refreshing as always done in this and other buffers is to update to reflect the situation in the local repository. That is what most users would expect and to instead perform a destructive operation would be very surprising and dangerous.
>
> So I won't add the requested feature, but you can easily implement it using magit-post-refresh-hook.
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.",Bitter Frustration
"> You probably don't understand what `git remote prune origin` does...
Every time I disagree with you, things escalate quickly. I do not want to do this again.",Bitter Frustration
"Closing, erasing, locking and blocking the author. This is their first and last interaction on this communication level.",Threat
"e.g. something like this:
```lua
Spring.GiveOrderToUnit(unitID,
CMD.INSERT,
{-1,CMD.ATTACK,''CMD.OPT_SHIFT'',''unitID2''},
{""timeout"" = 500 }
);
```
Related to https://springrts.com/mantis/view.php?id=6128",
"That would work, but the (badly named) command ""options"" have special meaning. Timeout is a separate parameter.",
"@rtri I appreciate the comment but you closed it before I could fix the PR!
No need for the trigger happy close, unless you're trying to put off
contributions?
Perhaps next time you should use the built in review functionality and
request changes? E.g. switching to an additional parameter because options
have special meaning?
On Wed, 6 Feb 2019 at 00:07, rtri <notifications@github.com> wrote:
> Closed #427 <https://github.com/spring/spring/pull/427>.
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/spring/spring/pull/427#event-2120001408>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AADl598QGC_BSGkh38MqMyeyXMN4RDV2ks5vKhy8gaJpZM4ajxVn>
> .
>",Impatience
"You seem to have missed be999101, which I started before this PR arrived.",Impatience
"@rtri **that's no excuse for rudeness**, I opened the PR to try and be helpful.
Next time:
- If you already have a PR, link to it
- Leave a review requesting changes
For all you know I may have come back and refactored it shortly after. If it had been a few days then sure, close it, as is common practice here, but instead you were hostile. Stop that.",Bitter Frustration
"@tomjn:
next time: please make a pull request to the develop branch. we don't accept pull requests to the master branch!",Impatience
"@abma I wasn't aware, perhaps a [pull request template](https://github.blog/2016-02-17-issue-and-pull-request-templates/) would help document this? That way it would be clear to anybody creating a pull request that they need to apply it to the `develop` branch not the `master` branch, even a prompt to link to any related Mantis tickets
---
As a general note, as a part of my job and general activities I regularly make and comment on pull requests on a wide range of projects on a daily basis, but this experience was poor, and below average, and entirely avoidable.
I don't know if the rude response from @rtri was because english isn't the primary language, or habit, but it's very easy to be polite and at the same time close responses. GitHub provides various processes to avoid these.
For example, the original comment doesn't explain why the PR was closed. If it had instead just been:
> Thanks, it's been fixed in [be99910](https://github.com/spring/spring/commit/be999101be20d10fe46ce6921d7ec7d88ccdece5)
That would have been perfectly acceptable on its own. The follow up comment was also catty and rude. Or a review left that was literally just the comment he left, requesting changes.",Bitter Frustration
"@tomjn https://springrts.com/wiki/Development:Getting_Started#How_to_get_your_code_included
i presume you knew that. please stop complaining.",Impatience
"@tomjn > the original comment doesn't explain why the PR was closed
When closing it I presumed you had taken at least a cursory glance at the log (or the mantis ticket) and would understand your PR had crossed incoming commits, which was clearly a mistake on my part. Now please get off your lecturing high horse.",Bitter Frustration
"<!-- âš ï¸ If you do not respect this template your issue will be closed. -->
<!-- âš ï¸ Make sure to browse the opened and closed issues before submitting your issue. -->
<!-- Please also submit your idea on the Strapi Product Board:
https://portal.productboard.com/strapi/tabs/2-under-consideration/submit-idea
If your request on the product board is accepted this feature request issue will be closed,
but will still accept public discussion.
-->
- [ ] **I have created my request on the Product Board before I submitted this issue**
- [x] **I have looked at all the other requests on the Product Board before I submitted this issue**
**Please describe your feature request:**
Add auto increment option to the model attributes",
Hi @abdonrd can you give us more details about your need please.,
"@lauriejim I mean an integer attribute that auto increment.
Like we have here at GitHub with the issues and the PRs.",
"Hum something like that ?
```
{
...
""options"": {
""timestamps"": true,
""increment"": ""number""
}
...
}
```",Mocking
"Maybe! If we want to replace the current ID.
Or something like this if we want it as another field:
```
""identifier"": {
""type"": ""integer"",
""autoIncrement"": true
},
```",
@abdonrd can you make a PR for that?,Impatience
@Aurelsicoko for now I will not be able to dedicate time. ðŸ˜•,Impatience
"I tried adding these mongoose plugins `mongoose-auto-increment` and `mongoose-sequence` to strapi in `config/functions/mongoose.js`. The counter collections are getting create, but the sequence counts are not getting updated. Is there a way to get these plugins working or is there a way to implement this myself?",Impatience
"I just found out that this was indeed an option in version 1.x.x of strapi.
Link: https://strapi.io/documentation/1.x.x/models.html#autoincrement
I suppose this is no more a part of the current version.
Any way to add it back?",Impatience
1,
"Hey all, we are currently moving many feature requests to our new feedback and feature request website to help clean up the GitHub issues and make it easier for us to review bug reports and fix them.
I have moved this feature to the following URL: https://feedback.strapi.io/developer-experience/p/add-auto-increment-option-to-the-model-attributes
If you are interested in this feature please feel free to go there and add more comments and/or upvote it there.
For now I will close this and lock it so that all new information goes into our new feedback website.
Thanks!",
"And you removed my issue now, wow, good!
Do you know what ? I'll try to set my framework without SFML because I see SFML is not a serious projet.",Bitter Frustration
"1. The issue was closed with a corresponding reason. This did not prevent you from commenting on why you think it should have been re-opened.
2. You did not provide any information that would help anybody else in resolving the problem, if it was even caused by SFML.
3. You disregarded the very text in the issue template that suggested asking on the forum first about any issues with the usage of SFML before opening an issue, which we prefer only contain confirmed bugs with appropriate information.
4. The issue was not removed. Please familiarize yourself with the way GitHub works before making such claims.
5. Threatening people in any way is not constructive and will not benefit anyone in any way.
6. You are entitled to believe what you want to believe, but making blanket statements such as ""SFML is not a serious project"" on an open platform has a high chance to mislead people into believing it as a fact. As such, in the future please refrain from making such remarks.
7. I have temporarily blocked you from the SFML organization for 7 days. Please take this time to calm down and reflect on what has just happened.
8. If you have any comments regarding the handling of this issue, please either take it to the forum or contact the SFML team via other means.
9. If, after you are unblocked, you intend to continue to post clearly non-constructive content, you will be permanently blocked from the SFML organization. This is your last warning.
Have a pleasant day.",Threat
"Adding minutes to a date around daylight saving creates NAs
```R
tsp_initial = ymd_hms('2019-03-09 07:22:03') %>% force_tz('US/Pacific')
tsp_initial + minutes(1160)
## NA
```
Bug or Feature? It definitely broke my until-now-working code.
----------------
## FIX:
```R
tsp_initial = ymd_hms('2019-03-09 07:22:03') %>% force_tz('US/Pacific')
tsp_initial + dminutes(1160)
## ""2019-03-10 03:42:03 PDT""
```
---------
## Explanation
`period` and `duration` act differently when adding a timespan to a date. Consider a simpler example: on the 30th of January what's the date on the calendar in a month from now? `period` will tell you it's 30th of February and as this is not possible will produce an NA
```r
ymd_hms(""2021-01-30 11:00:00"") + months(1) # NA
```
`duration` will add some vague 30 days and return a different date depending on the year. ```r
ymd_hms(""2021-01-30 11:00:00"") + dmonths(1) # ""2021-03-01 21:30:00 UTC""
ymd_hms(""2020-01-30 11:00:00"") + dmonths(1) # ""2020-02-29 21:30:00 UTC""
```
RTFM, I guess
https://lubridate.tidyverse.org/reference/duration.html
https://lubridate.tidyverse.org/reference/period.html",Bitter Frustration
"Compared to Postgres behaviour. Just for an example of how this _could_ be handeled. ```SQL
my-db=> select (
select (
select '2019-03-09 07:22:03'::timestamp without time zone at time zone 'US/Pacific'
) + interval '1160 mins'
) at time zone 'US/Pacific'
;
timezone ---------------------
2019-03-10 03:42:03
(1 row)
my-db=> select (
select (
select '2019-03-09 07:22:03'::timestamp without time zone at time zone 'US/Pacific'
) + interval '1160 mins' - interval '1 hour'
) at time zone 'US/Pacific'
;
timezone ---------------------
2019-03-10 01:42:03
my-db=> select (
select (
select '2019-03-09 07:22:03'::timestamp without time zone at time zone 'US/Pacific'
) + interval '1160 mins' + interval '1 hour'
) at time zone 'US/Pacific'
;
timezone ---------------------
2019-03-10 04:42:03
```",
"The same thing happens with hours(). It's a new bug because I had code that worked last year that was broken this year. As a workaround I changed my code to something like `with_tz(with_tz(mydate, 'GMT')+hours(1),'America/Los_Angeles')`",Bitter Frustration
"I'm finding a similar issue when you try and parse a time that is incompatible with daylight savings time (i.e. 2:00 AM until 3:00 AM doesn't exist when we ""spring ahead"" in the states). You get an NA and a failed to parse warning. Technically this makes sense but maybe this should be an Error rather than a warning, and maybe a more informative message. Surprisingly this doesn't break the ""strptime"" function. ```r library(lubridate)
test_date <- '03/10/19 02:00 AM'
mdy_hm( test_date, tz = 'America/New_York')
#[1] NA
#Warning message:
# 1 failed to parse. ```
Using strptime ```r d2 <- strptime(test_date, format = '%m/%d/%y %I:%M %p', tz = 'America/New_York')
print( d2 )
# [1] ""2019-03-10 02:00:00""
attributes(d2)
# $names
# [1] ""sec"" ""min"" ""hour"" ""mday"" ""mon"" ""year"" ""wday"" ""yday"" ""isdst"" # [10] ""zone"" ""gmtoff""
# $class
# [1] ""POSIXlt"" ""POSIXt"" # $tzone
# [1] ""America/New_York"" ""EST"" ""EDT"" ``` ```r sessionInfo()
R version 3.5.3 (2019-03-11)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.4
Matrix products: default
BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
attached base packages:
[1] stats graphics grDevices utils datasets methods base other attached packages:
[1] shiny_1.3.0 reprex_0.2.1 lubridate_1.7.4
loaded via a namespace (and not attached):
[1] Rcpp_1.0.1 compiler_3.5.3 later_0.8.0 remotes_2.0.3 [5] prettyunits_1.0.2 tools_3.5.3 pkgload_1.0.2 digest_0.6.18 [9] pkgbuild_1.0.3 jsonlite_1.6 evaluate_0.13 memoise_1.1.0 [13] rlang_0.3.4 cli_1.1.0 rstudioapi_0.10 yaml_2.2.0 [17] xfun_0.6 withr_2.1.2 stringr_1.4.0 knitr_1.22 [21] desc_1.2.0 fs_1.2.7 devtools_2.0.2 rprojroot_1.3-2 [25] glue_1.3.1 R6_2.4.0 processx_3.3.0 rmarkdown_1.12 [29] sessioninfo_1.1.1 callr_3.2.0 clipr_0.5.0 magrittr_1.5 [33] whisker_0.3-2 usethis_1.5.0 backports_1.1.3 ps_1.3.0 [37] promises_1.0.1 htmltools_0.3.6 rsconnect_0.8.13 assertthat_0.2.1 [41] mime_0.6 xtable_1.8-3 httpuv_1.5.1 stringi_1.4.3 [45] miniUI_0.1.1.1 crayon_1.3.4 ```",
2:AM is borderline. It probably makes sense to parse it actually. I would say this is a bug.,
"I just hit this too with `<datetime> - hours(1)`. It was pretty unfortunate, it wrecked a pretty long series spanning multiple years. Crossing over the DST boundary is definitely what did it.
``` r
library(lubridate, warn.conflicts = FALSE)
bad_time <- with_tz(as_datetime(1300000380) , ""America/New_York"")
bad_time
#> [1] ""2011-03-13 03:13:00 EDT""
bad_time - hours(1)
#> [1] NA
in_est <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
in_est
#> [1] ""2011-03-13 01:59:59 EST""
dst(bad_time)
#> [1] TRUE
dst(in_est)
#> [1] FALSE
```
<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>",Bitter Frustration
"Okay, so I think that there is a bug that has been there since essentially the beginning of the package.
The summary is that I think that right here, there should be another if branch that says if we are ONLY updating using hours/min/sec or lower, then it should use `cl_new.pre` if the new civil seconds amount is above the old civil seconds amount (i.e. we went forward), otherwise use `cl_new.post` (we went backwards).
https://github.com/tidyverse/lubridate/blob/cd8267976e51bfab9bd43f9eb30f2749d45e4ff9/src/update.cpp#L168
I've read the docs in `?Period-class`, and I know you want this reversible property:
```
date + period - period = date
```
I would argue that if it the arithmetic only involves hours/minutes/seconds then this is reversible. For my example above, I think that this is perfectly reasonable behavior (lubridate doesn't do this right now):
``` r
library(lubridate, warn.conflicts = FALSE)
border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
border
#> [1] ""2011-03-13 01:59:59 EST""
border + seconds(1)
#> [1] ""2011-03-13 03:00:00 EDT""
border + seconds(1) - seconds(1)
#> [1] ""2011-03-13 01:59:59 EST""
```
<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>
With days, you can't enforce the reversible behavior, so this behavior would result in an `NA`, not the problematic result shown here (again, this is my custom version)
``` r
library(lubridate, warn.conflicts = FALSE)
border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
border
#> [1] ""2011-03-13 01:59:59 EST""
border_minus_a_day <- border - days(1)
border_minus_a_day
#> [1] ""2011-03-12 01:59:59 EST""
one_day_one_second <- days(1) + seconds(1)
one_day_one_second
#> [1] ""1d 0H 0M 1S""
# This SHOULD result in an NA
border_minus_a_day + one_day_one_second
#> [1] ""2011-03-13 03:00:00 EDT""
# Because this is not reversible
border_minus_a_day + one_day_one_second - one_day_one_second
#> [1] ""2011-03-12 02:59:59 EST""
```
<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>
Update: Hmm, this wouldn't be reversible either. I'm not sure I have the rule quite right yet.
``` r
library(lubridate, warn.conflicts = FALSE)
border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
border
#> [1] ""2011-03-13 01:59:59 EST""
border_minus_a_day <- border - days(1)
border_minus_a_day
#> [1] ""2011-03-12 01:59:59 EST""
one_day_in_hours_one_second <- hours(24) + seconds(1)
border_minus_a_day + one_day_in_hours_one_second
#> [1] ""2011-03-13 03:00:00 EDT""
border_minus_a_day + one_day_in_hours_one_second - one_day_in_hours_one_second
#> [1] ""2011-03-12 02:59:59 EST""
```
<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>
Preexisting rationale for this behavior in Java:
- https://www.javacodex.com/Date-and-Time/Testing-Daylight-Saving-By-Adding-One-Second",
"Hmm okay I think the rule could be if we land in the spring forward daylight savings time gap and `(period size) <= (dst gap size)` then it is a reversible operation and we should get a real date and not an `NA`. 99% of the time this means if `(period size) <= 1 hour`, but there are the _rare_ cases where a dst gap is not 1 hour.
(Note that this would imply that the original issue would still be an `NA` because `minutes(1160)` is larger than 1 hour, and would not be reversible)
I would try to implement this, but I can't figure out how to get the size of the dst gap using cctz. The newest version of google's cctz has a method for `<time_zone>.next_transition()` and `.previous_transition()` which might could be used to get the borders of the gap. Then the difference would be the size. https://github.com/google/cctz/blob/b4935eef53820cf1643355bb15e013b4167a2867/include/cctz/time_zone.h#L188
``` r
library(lubridate, warn.conflicts = FALSE)
one_hour_before_border <- force_tz(as_datetime(""2011-03-13 01:00:00"") , ""America/New_York"")
one_hour_before_border
#> [1] ""2011-03-13 01:00:00 EST""
# this is what i would expect...
one_hour_before_border + hours(1)
#> [1] ""2011-03-13 03:00:00 EDT""
# because this is reversible! good!
one_hour_before_border + hours(1) - hours(1)
#> [1] ""2011-03-13 01:00:00 EST""
one_hour_one_second_before_border <- one_hour_before_border - seconds(1)
one_hour_one_second_before_border
#> [1] ""2011-03-13 00:59:59 EST""
one_hour_and_one_second <- hours(1) + seconds(1)
# my tweaked version currently gives this, but
# this should actually give NA because...
one_hour_one_second_before_border + one_hour_and_one_second
#> [1] ""2011-03-13 03:00:00 EDT""
# ...this is not reversible!
one_hour_one_second_before_border + one_hour_and_one_second - one_hour_and_one_second
#> [1] ""2011-03-13 01:59:59 EST""
```
<sup>Created on 2019-09-07 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>
Here is a strange case where there was a 20 minute DST gap. Found from the [date library examples](https://github.com/HowardHinnant/date/wiki/Examples-and-Recipes#tz_search)
``` r
library(lubridate, warn.conflicts = FALSE)
one_second_before_africa_border <- with_tz(as_datetime(""1920-08-31 23:59:59""), ""Africa/Accra"")
one_second_before_africa_border
#> [1] ""1920-08-31 23:59:59 GMT""
# this is what i would expect...
one_second_before_africa_border + seconds(1)
#> [1] ""1920-09-01 00:20:00 +0020""
# because this is reversible! good!
one_second_before_africa_border + seconds(1) - seconds(1)
#> [1] ""1920-08-31 23:59:59 GMT""
twenty_minutes_one_second_before_africa_border <- one_second_before_africa_border - minutes(20)
twenty_minutes_one_second_before_africa_border
#> [1] ""1920-08-31 23:39:59 GMT""
twenty_minutes_one_second <- minutes(20) + seconds(1)
# my tweaked version currently gives this, but
# this should actually give NA because...
twenty_minutes_one_second_before_africa_border + twenty_minutes_one_second
#> [1] ""1920-09-01 00:20:00 +0020""
# ...this is not reversible!
twenty_minutes_one_second_before_africa_border + twenty_minutes_one_second - twenty_minutes_one_second
#> [1] ""1920-08-31 23:59:59 GMT""
```
<sup>Created on 2019-09-07 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>",
"> Okay, so I think that there is a bug that has been there since essentially the beginning of the package.
> > The summary is that I think that right here, there should be another if branch that says if we are ONLY updating using hours/min/sec or lower, then it should use `cl_new.pre` if the new civil seconds amount is above the old civil seconds amount (i.e. we went forward), otherwise use `cl_new.post` (we went backwards).
> > https://github.com/tidyverse/lubridate/blob/cd8267976e51bfab9bd43f9eb30f2749d45e4ff9/src/update.cpp#L168
> > I've read the docs in `?Period-class`, and I know you want this reversible property:
> > ```
> date + period - period = date
> ```
> > I would argue that if it the arithmetic only involves hours/minutes/seconds then this is reversible. For my example above, I think that this is perfectly reasonable behavior (lubridate doesn't do this right now):
> > ```r
> library(lubridate, warn.conflicts = FALSE)
> > border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
> border
> #> [1] ""2011-03-13 01:59:59 EST""
> > border + seconds(1)
> #> [1] ""2011-03-13 03:00:00 EDT""
> > border + seconds(1) - seconds(1)
> #> [1] ""2011-03-13 01:59:59 EST""
> ```
> > Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)
> > With days, you can't enforce the reversible behavior, so this behavior would result in an `NA`, not the problematic result shown here (again, this is my custom version)
> > ```r
> library(lubridate, warn.conflicts = FALSE)
> > border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
> border
> #> [1] ""2011-03-13 01:59:59 EST""
> > border_minus_a_day <- border - days(1)
> border_minus_a_day
> #> [1] ""2011-03-12 01:59:59 EST""
> > one_day_one_second <- days(1) + seconds(1)
> one_day_one_second
> #> [1] ""1d 0H 0M 1S""
> > # This SHOULD result in an NA
> border_minus_a_day + one_day_one_second
> #> [1] ""2011-03-13 03:00:00 EDT""
> > # Because this is not reversible
> border_minus_a_day + one_day_one_second - one_day_one_second
> #> [1] ""2011-03-12 02:59:59 EST""
> ```
> > Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)
> > Update: Hmm, this wouldn't be reversible either. I'm not sure I have the rule quite right yet.
> > ```r
> library(lubridate, warn.conflicts = FALSE)
> > border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
> border
> #> [1] ""2011-03-13 01:59:59 EST""
> > border_minus_a_day <- border - days(1)
> border_minus_a_day
> #> [1] ""2011-03-12 01:59:59 EST""
> > one_day_in_hours_one_second <- hours(24) + seconds(1)
> > border_minus_a_day + one_day_in_hours_one_second
> #> [1] ""2011-03-13 03:00:00 EDT""
> > border_minus_a_day + one_day_in_hours_one_second - one_day_in_hours_one_second
> #> [1] ""2011-03-12 02:59:59 EST""
> ```
> > Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)
> > Preexisting rationale for this behavior in Java:
> > * https://www.javacodex.com/Date-and-Time/Testing-Daylight-Saving-By-Adding-One-Second
seems like a time for a PR!",
"This is a bug and it has to do with the fact that arithmetics with periods are not well defined. > I've read the docs in ?Period-class, and I know you want this reversible property:
That rule is unfortunate. There is no solid reason to follow it and it caused a lot of trouble with the package. There is a new lower level package `timechange` which has a clear defined semantics and allows a full control over how things are rolled both on month change and DST.
```R
> timechange::time_add(time, minutes = 1160)
[1] ""2019-03-10 03:42:03 PDT""
```
I wanted to roll lubridate on top of timechange for at least a year now, but just don't have time. Hopefully by the end of this year.",Bitter Frustration
"Minimal reprex:
``` r
library(lubridate, warn.conflicts = FALSE)
x <- ymd_hms('2019-03-09 07:22:03', tz = 'US/Pacific')
x + minutes(1160)
#> [1] NA
timechange::time_add(x, minutes = 1160)
#> [1] ""2019-03-10 03:42:03 PDT""
```
<sup>Created on 2019-11-19 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>",
"Adding to this - my pipeline broke last night, this time adding `days(1)` to 2:00 AM on March 7th. I'm not sure if this should output 2:00 AM on March 8th (which ""springs ahead"" to 3:00 AM on March 8th) or 3:00 AM on March 8th.
Funny enough, it only seems to affect the `2:00 AM` hour. Again reprex:
``` r
library(lubridate)
# 1:59:59 AM
start_date <- ymd_hms('2020-03-07 01:59:59', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] ""2020-03-08 01:59:59 PST""
# 2:00:00 AM
start_date <- ymd_hms('2020-03-07 02:00:00', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] NA
# 2:59:59 AM
start_date <- ymd_hms('2020-03-07 02:59:59', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] NA
# 3:00:01 AM
start_date <- ymd_hms('2020-03-07 03:00:01', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] ""2020-03-08 03:00:01 PDT""
```
<sup>Created on 2020-03-08 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>
Upon reflection here, I'll just not base things on the `2:00 AM` hour",Bitter Frustration
"The logic here is exactly as with `ymd(""2009-01-29"") + months(1)` which results in `NA`. I am seriously considering changing this unfortunate default to a more desirable behavior, and leaving the current as an opt-in.",Bitter Frustration
"Just wanted to reup this. @pitakakariki pointed out, perhaps, a more troublesome case. NA seems like a fine result because it flags troubles. The other direction problem generates an answer that is plainly incorrect.
![image](https://user-images.githubusercontent.com/22829624/113522079-b31fe180-9552-11eb-9aa9-8e974986624c.png)",
"For anyone doing arithmetic with _hours, minutes, or seconds_ with lubridate, you likely want to use Durations objects (like `dhours()`) rather than Period objects (like `hours()`). For arithmetic with more granular units of time like days, months or years, you probably want to stick with Period objects.
So, if using lubridate, my advice for @Demetrio92 and @robertwwalker is to switch to `dminutes()` and `dhours()` as that seems to be what you are expecting. That will match your intuition if you were expecting a result equal to ""sitting in a chair for 1160 minutes, then looking at the clock"" (assuming that your clock automatically adjusts itself for DST).
You could also try using [clock](https://clock.r-lib.org/). The POSIXct API in clock defaults to the behavior I described in the first paragraph, so it might feel more intuitive. It also errors when landing on a nonexistent time in a DST gap, or on an ambiguous time in a DST fallback, rather than returning `NA`.
@Demetrio92's original example:
```r
library(clock)
library(lubridate)
library(magrittr)
# This example has to do with DST Gaps (nonexistent times)
x <- as.POSIXct(""2019-03-09 07:22:03"", tz = ""US/Pacific"")
x
#> [1] ""2019-03-09 07:22:03 PST""
# This is lubridate Period addition:
# - converts to naive (2019-03-09 07:22:03, with no assumed time zone)
# - adds 1160 minutes (2019-03-09 07:22:03 -> 2019-03-10 02:42:03)
# - converts back to US/Pacific, but this is now in a DST gap from
# 01:59:59 -> 03:00:00, so lubridate returns NA
x + minutes(1160)
#> [1] NA
# clock - With POSIXct, hours, minutes, and seconds add like lubridate Durations
add_minutes(x, 1160)
#> [1] ""2019-03-10 03:42:03 PDT""
# so clock matches this result of lubridate Duration arithmetic:
x + dminutes(1160)
#> [1] ""2019-03-10 03:42:03 PDT""
# To get the lubridate Period behavior in clock if you want it
nt <- x %>%
as_naive_time() %>%
add_minutes(1160)
# naive time with no implied time zone - note that this would be in a # DST gap if we tried to apply a US/Pacific zone to it
nt
#> <time_point<naive><second>[1]>
#> [1] ""2019-03-10 02:42:03""
# unlike lubridate, clock treats this as an error rather than returning NA
as.POSIXct(nt, tz = date_zone(x))
#> Error: Nonexistent time due to daylight saving time at location 1. Resolve nonexistent time issues by specifying the `nonexistent` argument.
# control how to handle this nonexistent time with `nonexistent`,
# use `""NA""` to match lubridate behavior
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""roll-forward"")
#> [1] ""2019-03-10 03:00:00 PDT""
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""roll-backward"")
#> [1] ""2019-03-10 01:59:59 PST""
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""NA"")
#> [1] NA
```
@robertwwalker's example:
```r
library(clock)
library(lubridate)
library(magrittr)
# This example has to do with DST Fallbacks (ambiguous times)
x <- as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
x
#> [1] ""2021-04-04 02:15:00 NZDT""
# This is lubridate Period addition:
# - converts to naive (2021-04-04 02:15:00, with no assumed time zone)
# - adds one hour (02 -> 03)
# - converts back to Pacific/Auckland with no issues, but the result is
# potentially not intuitive
x + hours(1)
#> [1] ""2021-04-04 03:15:00 NZST""
# clock - With POSIXct, hours, minutes, and seconds add like lubridate Durations
add_hours(x, 1)
#> [1] ""2021-04-04 02:15:00 NZST""
# so clock matches this result of lubridate Duration arithmetic:
x + dhours(1)
#> [1] ""2021-04-04 02:15:00 NZST""
# To get lubridate Period behavior in clock if you want it
x %>%
as_naive_time() %>% # this is the key
add_hours(1) %>%
as.POSIXct(tz = date_zone(x))
#> [1] ""2021-04-04 03:15:00 NZST""
```",
"@DavisVaughan this is awesome! I'm glad there is a reliable fix. However: `dminutes` should not act different from `minutes`. This is confusing at best. Better: both act the same with a parameter `nonexistent` changing the behavior. Long run: deprecate usage of `minutes` as it involves a lot of autocasting under the hood and will inevitably lead to various issues.
-------
Imagine two programmers wrote together a lot of R code on one project. One of them used `minutes` the other one `dminutes`. Some parts of the code produce NAs while other parts don't. Happy debugging.",Bitter Frustration
"> However: dminutes should not act different from minutes. This is confusing at best.
I disagree. The whole reason there are two functions for adding minutes is that they work differently. I would encourage you to read the docs for [Durations](https://lubridate.tidyverse.org/reference/duration.html) and [Periods](https://lubridate.tidyverse.org/reference/period.html) again, as they are quite different.",
"> > However: dminutes should not act different from minutes. This is confusing at best.
> > I disagree. The whole reason there are two functions for adding minutes is that they work differently. I would encourage you to read the docs for [Durations](https://lubridate.tidyverse.org/reference/duration.html) and [Periods](https://lubridate.tidyverse.org/reference/period.html) again, as they are quite different.
Reading `period` docs
> Within a Period object, time units do not have a fixed length (except for seconds) until they are added to a date-time. ... When math is performed with a period object, each unit is applied separately. How the length of a period is distributed among its units is non-trivial. For example, when leap seconds occur 1 minute is longer than 60 seconds.
> Periods track the change in the ""clock time"" between two date-times. They are measured in common time related units: years, months, days, hours, minutes, and seconds. Each unit except for seconds must be expressed in integer values.
I do not see why this description contradicts my expected output. Specifically it says stuff like >or example, when leap seconds occur 1 minute is longer than 60 seconds. My impression here would be that periods are more robust than durations. -----
I generally agree, the issue is more deep than just two functions that supposed to do the same. Still, worst case, rename `minutes` into `pminutes`. ----
I am glad `period` now contains a warning. At least people who carefully study the docs will be aware. > Note: Arithmetic with periods can result in undefined behavior when non-existent dates are involved (such as February 29th in non-leap years). Please see Period for more details and %m+% and add_with_rollback() for alternative operations.
Coming from postgres I just did `+minutes()` and it worked as expected without any issues until it didn't. I wasn't even aware there were two ways of doing this in `lubridate`",Bitter Frustration
"TL;DR `ymd(""2009-01-29"") + months(1)` this should probably go to the top of the demos in both docs. Highlights the difference very well.",
@DavisVaughan Thank you very much.,
"Maybe worth noting that this behaviour breaks the reversibility rule:
```
x <- as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
x
# [1] ""2021-04-04 02:15:00 NZDT""
x + hours(1)
# [1] ""2021-04-04 03:15:00 NZST""
x + hours(1) - hours(1)
# [1] ""2021-04-04 02:15:00 NZST""
```",
"I've been thinking (too much probably) about the period-duration distinction.
Imagine the (completely implausible I hope) scenario where the time-lords decide that they're going to do a leap second at the same time as a daylight savings change.
Here's how I'd expect `hours` and `dhours` to act:
""2021-04-04 02:15:00 NZDT"" + hours(1) == ""2021-04-04 02:15:00 NZST""
""2021-04-04 02:15:00 NZDT"" + dhours(1) == ""2021-04-04 02:14:59 NZST""
On the other hand when it comes to `days` I have no idea what I should even expect. It doesn't appear to be reversible currently though:
```
> as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
[1] ""2021-04-04 02:15:00 NZDT""
> > as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"") + days(1)
[1] ""2021-04-05 02:15:00 NZST""
> > as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"") + days(1) - days(1)
[1] ""2021-04-04 02:15:00 NZST""
```
I'd also like to add that all of this is really annoyingly complex and I'm really glad there are people who are not me making it easier to deal with. So a big thank you to everyone involved in lubridate development.",Bitter Frustration
"> Here's how I'd expect hours and dhours to act:
Hmm, @pitakakariki from this example it looks to me like you might misunderstand how Period and Duration are supposed to be working. Let me try explaining. I'll use types from clock, since they allow me to explicitly show the intermediate steps that are happening under the hood.
``` r
library(clock)
x <- date_time_parse(""2021-04-04 02:15:00"", ""Pacific/Auckland"", ambiguous = ""earliest"")
x
#> [1] ""2021-04-04 02:15:00 NZDT""
# Adding a lubridate ""Period"" is like:
# - Dropping the original time zone completely
# - Adding the unit of time
# - Adding the original time zone back (if possible)
# Adding a lubridate ""Duration"" is like:
# - Converting the original time zone to UTC (where DST never affects you)
# - Adding the unit of time
# - Adding the original time zone back (which is always possible)
# We can show these explicitly with clock's naive-time and sys-time types, which
# mimic dropping the original time zone and converting to UTC respectively.
```
``` r
## Period example:
# Notice this has no implied zone attached
nt <- as_naive_time(x)
nt
#> <time_point<naive><second>[1]>
#> [1] ""2021-04-04 02:15:00""
# Now we add the time. Since there is no implied time zone, this doesn't
# have to do with the DST fallback whatsoever.
nt_plus_hour <- nt + duration_hours(1)
nt_plus_hour
#> <time_point<naive><second>[1]>
#> [1] ""2021-04-04 03:15:00""
# Now we convert back to Pacific/Auckland. Since the 3 o'clock hour is past
# the DST fallback, there is no ambiguity in the conversion, but it may or
# may not be what you want.
as.POSIXct(nt_plus_hour, ""Pacific/Auckland"")
#> [1] ""2021-04-04 03:15:00 NZST""
x + lubridate::hours(1)
#> [1] ""2021-04-04 03:15:00 NZST""
```
``` r
## Duration example:
# This is in UTC time, notice how the hour has shifted
st <- as_sys_time(x)
st
#> <time_point<sys><second>[1]>
#> [1] ""2021-04-03 13:15:00""
# Now we add the time in UTC. DST can never affect this.
st_plus_hour <- st + duration_hours(1)
st_plus_hour
#> <time_point<sys><second>[1]>
#> [1] ""2021-04-03 14:15:00""
# Now convert back to Pacific/Auckland. This just shifts by the appropriate
# UTC offset to get the Pacific/Auckland time.
as.POSIXct(st_plus_hour, ""Pacific/Auckland"")
#> [1] ""2021-04-04 02:15:00 NZST""
x + lubridate::dhours(1)
#> [1] ""2021-04-04 02:15:00 NZST""
```
<sup>Created on 2021-04-07 by the [reprex package](https://reprex.tidyverse.org) (v1.0.0)</sup>",
"Thanks @DavisVaughan but I think we might be talking at cross purposes. Your comment helps explain the intuition behind the implementation, but my expectations come from the reversibility principal in the documentation.
If adding `hours(1)` to 2:15am DT and 2:15am ST both give us 3:15am ST, then there's no way to subtract `hours(1)` from 3:15am ST that's consistent with reversibility.",
"As Vitalie mentioned here https://github.com/tidyverse/lubridate/issues/759#issuecomment-536286765, the reversibility rule is probably not the best idea for date time arithmetic, even if it sounds like a good idea (I agree with him now). So I wouldn't base too many assumptions on it.",
"@DavisVaughan this is an issue tracker, not a help forum. An inconsistency between behaviour and documentation is a bug, which is why I'm reporting these examples. Your insistence that this is somehow a ""misunderstanding"" on my part is starting to feel quite rude.",Bitter Frustration
@pitakakariki I don't think this conversation is going anywhere useful so I'm going to lock this thread for now.,
"I think a real valid issue here is that addition and subtraction do not behave symmetrically with respect to DST transition. I opened #1088 for this. Reversibility can never by achieved no matter what we do. I think we should re-export or provide an alternative to `timechange::time_add` in order for people to have full control over operations with periods. In most cases though working with durations (`dhours`, `dminutes` etc) is probably what people actually want.",
"I have 20+ screenshots of moderators running Slack channels (1000's of members in Rails) who are using Slack as their personal bully pulpit. Last night was the last straw for me. I saw a moderator call a girl a troll after another guy made sexist comments to her. The moderators are acting like bullies. I love Rails. But I am going to drop it like a hot potato if the creators of Ruby and Rails don't come up with some way to establish specific CoC's. Right now, moderators are going around calling people names like trolls, cursing left and right and causing arguments. Then they accuse everyone else of what they started. There needs to be some kind of controls and certifications that is granted as an authorized representative of Rails before anyone can open a group under the copyright. A lot of the Rails groups have chat rooms for politics and unrelated matters. Yet, half the CoC forbids talking about those things. I've seen entire Rails groups where a few friends of the moderators use it as their political pulpit. Then they lure people into discussions and ban them when they don't agree.
This is just 1 example of how a moderator talks to people.
![image](https://user-images.githubusercontent.com/4573756/55576276-ea96f180-56de-11e9-96bf-ee91f6edd312.png)
I have not seen this level of behavior on any other community. Not Python, NYC, Clojure, Java etc... It's only been on Rails/Elixir groups over the past 2 years.",Bitter Frustration
"I'm sorry this situation is happening and I feel very sad about those people behavior.
Unfortunately, we don't have control over user created slacks and forums. While Rails, the project, follow a code of conduct in its official channels (GitHub organization, website, blog and the Rails core mailing list), we can't force other groups to do so. @dhh controls the trademark of the name Rails, but I think we can't use it to enforce users groups to not use the name Rails.",
"## Description
With vertically oriented games, Retroarch switches the emulator-provided width and height. For example, if the width/height is supposed to be 320x240, RA will switch this to 240x320. This only occurs with vertically oriented games. I tested this in both MAME and FBA using the Windows 64 bit version of RA. ### Expected behavior
The game to be displayed using the correct resolution (width/height). ### Actual behavior
The height and width is switched when a vertically oriented game is loaded. ### Steps to reproduce the bug
1. load any vertically oriented game in either FBA or MAME using Windows 64 bit version of RA (example: DonPachi)
2. under video settings, enable integer scale. 3. set ""custom aspect ratio width/height"" to 1x.
4. look at ""custom aspect ratio width/height"" to see that it says 240 for width and 320 for height. This is the reverse of what it should be; it should be 240 for height and 320 for width. ### Bisect Results
I first noticed this a few days ago. ### Version/Commit
You can find this information under Information/System Information
- RetroArch: 1.7.6
Git version: 9750719074
### Environment information
- OS: Windows 10 - Compiler: NA
![image](https://user-images.githubusercontent.com/12267058/55672818-c2cd9800-585c-11e9-8ba2-0fd5024aee7c.png)
![image](https://user-images.githubusercontent.com/12267058/55672822-d7aa2b80-585c-11e9-8747-b31df5c07da0.png)",
some arcades have the monitor rotated 90 degrees. The can the emulator can either rotate the image to the displays orientation or display it as intended and you rotated your display 90 degrees,
"I understand that, but the emulator-provided resolution should not be changed from the correct resolution, regardless of orientation. If the game is 320x240 it should be output as 320x240, not 240x320. The user can flip it or rotate it or resize it or whatever, but the output resolution from the emulator should remain the same (eg, 320x240 should be output as 320x240 before the user does stuff to it). Otherwise you get scaling artifacts unless you just happen to adjust the y axis to a multiple of both 240 and 320 (eg, 960). And if youâ€™re playing a game that uses a weird resolution like 19XX (384x240), then it causes big problems with overscan or underscan when using integer scaling. As of right now, video aspect ratio for vertical games must be configured manually (custom ratio width/height, non-integer) along with custom ratio x/y position, because the resolution/ratio for vertical games isnâ€™t being detected correctly.",Bitter Frustration
"the thing is the topic say the width and height are switched they simply arent it just a rotated monitor. It draws the image on the monitor as 4:3 then you flip the monitor physically this changes it to 3:4 on real hardware as far as the hardware is concerned its a normal 4:3 device but your rendering the image 90 degrees rotated.
In mame2003 or plus just use tate mode this stop the rotation and you need to flip your monitor like a real one. look at sf2 this is on a 4:3 screen look at the resolution of the game it use crt timing when rendering to look the way it does in a crt if you used the literal resolution for sf2 the gfx would be overly stretched.",
"it looks like your talking about capcom games watch this to understand whats going on and it is indeed 4:3
https://www.youtube.com/watch?v=LHfPA4n0TRo&feature=youtu.be",
"Iâ€™m not referring exclusively to capcom games. You can see this behavior in the example screenshots I posted above, which are from Donpachi. I understand very well that certain games are meant to be displayed vertically. This doesnâ€™t change anything that Iâ€™ve said. With a vertically-oriented 320x240 game, width should say 320 and height should say 240, and the displayed image should be sideways. On a 4:3 CRT monitor the image would be 320x240 and sideways. You then rotate the monitor 90 degrees. This doesnâ€™t change the image to 240x320. It is still 320x240 but now it is rotated 90 degrees so it is no longer sideways.
Altering the internal resolution of the game is incorrect, and will always result in scaling artifacts unless you happen to use a resolution that is a multiple of both 320 and 240.",Bitter Frustration
If you display the arcade pcb to a CRT monitor in normal orientation it will be a 320x240 image that is sideways. You rotate the CRT 90 degrees. The resolution of the game remains unchanged.,
yes resolution of you display remains unchanged. I dont have a crt to test whats going on there I would assume you should be using core provided information and tate mode on for mame2003 and plus dont know if the others support tate mode,Bitter Frustration
You just arenâ€™t getting what Iâ€™m saying. Iâ€™m going to have to post some screenshots later to help illustrate whatâ€™s going on currently and what should be going on instead.,Bitter Frustration
can you make them screenshot in mame2003 and 2003+ with tate mode on so i can see what going on make sure your using core provided for aspect ratio as well in them screenshots will help a lot in seeing whats going on,Impatience
"First of all, just look at the first two screenshots I posted above. See where it says ""custom aspect ratio width"" and ""custom aspect ratio height"" Both of these screenshots were taken with video rotation disabled. Width/height should be the reverse of what is shown in the above screenshots.",
"Here is what it SHOULD look like with video rotation disabled. This is what is NOT happening, currently. I created this in GIMP by manually switching the height/width in GIMP. ![image](https://user-images.githubusercontent.com/12267058/55687288-89faf500-5928-11e9-9508-00ba5fc81b1e.png)",Bitter Frustration
"Here is what currently happens. Both of these are incorrect. video rotation disabled: ![image](https://user-images.githubusercontent.com/12267058/55687236-135df780-5928-11e9-9070-10774a5cf9fe.png)
video rotation enabled: ![image](https://user-images.githubusercontent.com/12267058/55687245-240e6d80-5928-11e9-8063-120409cd94f9.png)",
"Next, here are some tests conducted by HunterK. See how video height/width are switched from what it is in my screenshots in the very first post? That's what it should be. ![image](https://user-images.githubusercontent.com/12267058/55687268-51f3b200-5928-11e9-91f6-379ac47df734.png)
![image](https://user-images.githubusercontent.com/12267058/55687272-5d46dd80-5928-11e9-8edd-af60b66eafa8.png)",
what emulator is this from here is screenshots from with ?,Impatience
"Next, I made this shot by manually resizing the resolution to 1600x1200, and used a scanlines shader. Notice how the scanlines and mask are perfectly scaled with no scaling artifacts present. This could only be the case if the game's resolution is indeed 320x240. With the currently reported resolution of 240x320, you get scaling artifacts. ![image](https://user-images.githubusercontent.com/12267058/55687343-04c41000-5929-11e9-8e50-27049b2a3079.png)",
"In my initial post, I said that the behavior is present in both MAME and FBA in the Windows 64 bit version of RA. As you can see in the tests conducted by HunterK, this behavior is not present in the Linux version of RA.",
"this is mame2003 with tate mode on and off using core provided aspect ratio I cant speak for the other emulators
![fshark-190407-183154](https://user-images.githubusercontent.com/6128601/55687360-0f4fcb00-5964-11e9-872f-05b77581644a.png)
![fshark-190407-183227](https://user-images.githubusercontent.com/6128601/55687361-0f4fcb00-5964-11e9-8bbc-2be81a66a576.png)",
have you even tried running mame2003 with these settings it what it shoud do tate mode skips the rotating and renders like its should.,Bitter Frustration
"How are we supposed to know what the core reported width and height are from the screenshots you just posted? Again, the issue is that the emulator reported height and width are not correct.",Impatience
"Tate mode ON solves the issue in MAME 2003. Tate mode off:
![image](https://user-images.githubusercontent.com/12267058/55687436-31c4f280-592a-11e9-8287-09ddfcff8861.png)
Tate mode on:
![image](https://user-images.githubusercontent.com/12267058/55687485-ae57d100-592a-11e9-8068-62d3bcc17dec.png)",
i think mame current has rotation options in the tab menu dont know if they will work as expected though,
"The problem is, tate mode ON in MAME 2003 should be the default for all emulators including FBA. This is what the image looks like when you connect the actual hardware to a CRT that isn't rotated. There shouldn't be any automatic switching for vertical-oriented games. Switching height/width (Tate mode off) should be something that the user has to select because it's altering the original output of the emulated hardware. Anything done to alter the original output of the emulated hardware in this way should be something that the user has to manually select. Still no idea how HunterK was able to get the image to to display correctly using FBA, and I'm still unable to get the other version of MAME to display the image correctly.",Bitter Frustration
sure that can be true im sure most people dont have there displays rotated unless they have a barcade of some sort setup for vertical games only. Its a personal opinion but feel free to post an issue on it is easy enough to change the defaults through issues in the emulators in question. I dont really feel strongly about this its one for the people in charge of the repos it switchable in mame2003 and plus anyway in the options so the default wont really matter it can be changed to suit your display,
"I'm not just being pedantic, here. The width/height being switched isn't helpful and causes numerous potential issues. With the current behavior of height/width being automatically switched with vertical games, you still have to rotate the image if you want correct aspect ratio and orientation or to avoid a huge amount of letterboxing, and with height/width being switched like this, it results in scaling artifacts unless you happen to choose a resolution that is a multiple of both 320 and 240. Furthermore, there is still no way that I know of to get any emulator besides MAME 2003 to work correctly. The current rationale of ""people don't want to rotate their displays"" just isn't sufficient to justify this.",Bitter Frustration
"first of all i completely agree with you all options should be clear to change and i feel fba does cover this option well its just you have to restart for it to take effect/
ill need to look into mame current at a later time. I will tell you how to to fix fba got quick menu options turn vertical mode on and restart the emulator it should work as expected.",
"In FBA, enabling vertical mode in the quick menu options and restarting the emulator does not fix the problem. I'm still getting this: ![image](https://user-images.githubusercontent.com/12267058/55688174-e95e0280-5932-11e9-867a-751931535c68.png)",Bitter Frustration
"Another problem related to this is the sheer number of knobs and dials related to video rotation. There are options related to video rotation in all of these places: main menu ->quick menu -> options
settings -> video
settings -> core If I'm an average user, how am I supposed to know what to do with all of this? Even as an experienced user, this is quite confusing.",Bitter Frustration
"SUCCESS. Actually, in FBA I have to turn vertical mode OFF under quick menu -> options. ![image](https://user-images.githubusercontent.com/12267058/55688303-40b0a280-5934-11e9-9b0b-b53909c16ef2.png)
So, yes, there is a lot going on here. Each of the cores seems to be handling this in a different way and there are far too many knobs and dials related to this. Disabling vertical mode makes it work in FBA, while enabling tate mode in MAME 2003 makes it work there... how does that make any sense?! As things currently stand, the options for this are an incoherent mess. Even you thought that enabling vertical mode would be the same as enabling tate mode, which makes sense, intuitively. There just seems to be no rhyme or reason to the way these settings currently work.",Bitter Frustration
"I'm still getting the same problem in FBA 2012. There is no option in quick menu -> options for vertical mode. Under settings -> core there is an option for ""enable rotation."" I'm getting the same results with this turned either ON or OFF. settings -> core -> allow rotation ON:
![image](https://user-images.githubusercontent.com/12267058/55688480-342d4980-5936-11e9-8518-0d0c7cb97ac2.png)
settings -> core -> allow rotation OFF:
![image](https://user-images.githubusercontent.com/12267058/55688505-7060aa00-5936-11e9-9d88-ce8daffcca83.png)",Bitter Frustration
"as far as I can tell, the problem is the frontend based rotation not taking into account the aspect ratio change.
I don't think we need so many knobs in the cores to do this, it should be one option in the frontend and that's all.
I'm not even sure why there is a ROTATION ENV callback in the API",Bitter Frustration
there is a reason for it to be there clearly this isint a front end issue. It cant guess if you have rotated you monitor 90 degrees physically you need to deal with this as a core option if that is the case.,Bitter Frustration
"Agree w/fr500. In my opinion, a vertically-oriented game should look like this by default: ![image](https://user-images.githubusercontent.com/12267058/55689631-1961d180-5944-11e9-8bf0-d554f236ae08.png)
The user can then rotate the image using the advanced user option under settings -> video -> rotation
<strike>You could also add an option under video settings (""automatically rotate vertical games 90 degrees"" or something) to automatically set settings -> video -> rotation -> 90 degrees whenever a vertical oriented game is detected. But, I'm getting ahead of myself.</strike> EDIT, okay yeah this wouldn't work. But I still think that the core shouldn't be doing stuff to alter the output resolution (at least not by default) and that the user should do this with the frontend options for rotation and aspect ratio, or by manually selecting a core option to switch width/height for non-rotated displays.",
"> > > there is a reason for it to be there clearly this isint a front end issue. It cant guess if you have rotated you monitor 90 degrees physically you need to deal with this as a core option if that is the case.
The resolution/aspect ratio output by the emulator should remain unchanged by the core- see above. A vertical game that is 320x240 should be output as 320x240. The frontend options for video rotation and aspect ratio are more than sufficient and solve the problems related to the way things are currently handled, which are, to wit:
1. it's incorrect according to what the original hardware does
2. it's counter-intuitive
3. it results in scaling artifacts with integer scaling
If people want to play vertical games rotated 90 degrees on their monitors, then they should use the frontend for that. The emulator/core shouldn't alter the actual output.",Bitter Frustration
and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic,Bitter Frustration
"I'm saying that the front end options are sufficient and that the cores are currently doing something wrong/there's too much going on with the cores. Just make the cores output the original resolution, unaltered, and let the user use the frontend options for rotation and aspect ratio. That would solve the problem(s). In standalone FBA, you get this by default, which is as it should be: ![image](https://user-images.githubusercontent.com/12267058/55690013-449aef80-5949-11e9-872d-ea9cd88d04ce.png)
With ""Video -> rotate vertically aligned games -> enabled"" you get this, which is again as it should be:
![image](https://user-images.githubusercontent.com/12267058/55689969-ab6bd900-5948-11e9-8f7b-fcebec130e3c.png)",
ok with you so far you failing to mention the users monitors physical orientation in all this,Bitter Frustration
"I don't see why the monitor orientation is relevant to what resolution the core is outputting. Altering the height/width of the resolution so that the image is correctly oriented on a non-rotated monitor is altering the output resolution, and thus incorrect. Furthermore, in some cases you still have to rotate the image using the frontend options to get a properly oriented image even though the height/width are being switched; automatically switching the height/width doesn't solve the problem that it is intended to solve and is introducing more problems. You can see this in the above example with MAME 2003; TATE mode on or off still results in a sideways image so that the user either has to rotate it using a front end option or physically rotate their display. The core should just output the unaltered resolution. Altering the resolution, aspect ratio or video rotation is all stuff that should happen on the frontend or through a user option that has to be selected. At the very least, it would be preferable to stop enabling automatic rotation by default and have this be something that the user has to enable through a core option. The default should be the original unaltered image (see the example of standalone FBA, above). The user shouldn't have to dig through multiple menus of confusingly-named settings and apply a lot of trial and error in order to get the emulator to display the original unaltered image.",Bitter Frustration
"Also, as we can see, there's just a lot of weird stuff going on. TATE mode ON in MAME results in the same behavior as vertical mode OFF in FBA, and FBA 2012 won't display a correct image no matter what settings are applied, and I haven't even tested all the emulators yet. I understand the lack of standardization when it comes to core options, but this just seems somewhat ridiculous.",Bitter Frustration
im trying to understand where you are hitting this from the cores arent scaling the resolution RA scaless the resolution to your display.,Impatience
To what are you referring with this statement?,Impatience
"> and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic
If the front end can't detect whether a game is vertical or not, then it's the cores that are automatically switching the height/width for vertical games.",Bitter Frustration
yes they do people that dont rotate there monitors dont want to play there vertical games sideways or rotate there monitor by default. Tate mode wont rotated vertical games at all on mame2003 that what you requested. You have t do your part now and rotate your monitor physically to match the arcade.,Bitter Frustration
"This whole discussion is pointless.
> and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic
There is not need to guess anything, this is software development, there is an API, an implementation, and a frontend.
There is an API for this:
https://github.com/libretro/RetroArch/blob/master/libretro-common/include/libretro.h#L486
So the players are:
- core
- api
- frontend
Who knows the content needs rotation? The core does.
The environment callback is a set, which means it's telling the frontend to ""do something""
So what should happen could go two ways
a. The core tells the frontend: ""hey video is rotated, adjust aspect ratio from what I reported accordingly""
b. The core tells the frontend: ""hey this content requires rotation, do whatever you need to do so it works properly""
That's all. This is a frontend problem, but before anything can be do about it what we need is clarification from the API side so we can adjust both the frontend and the cores to do whatever needs to be done.
That's all.",Impatience
"there is no front end issue some arcades have different rotations not just 90 degrees. The core needs to work this out the user will need to be more specific from what he says he wants vertical games to display like this by default which is perfectly valid if you have a rotated monitor.
![fshark-190407-183227](https://user-images.githubusercontent.com/6128601/55691157-04605f00-5993-11e9-8e9b-f51fa4f66dde.png)
this is how it displays when you dont have a rotated monitor the user seems to think this is wrong
![fshark-190407-183154](https://user-images.githubusercontent.com/6128601/55691184-54d7bc80-5993-11e9-8fe8-57f884e15aa1.png)",
"Ok, you win, have fun arguing forever instead of proposing a solution.",Bitter Frustration
Ok you have a 4:3 monitor it draws as 4:3 like the fist picture you rotate it the monitor in the arcade 90 degrees what aspect ratio is it now for us to display on out normal monitor sitting on the tv stand? ill leave you two to it. Despite what the op said before fba mainline are set this way by default i just downloaded it. Its default rotate vertical games mame mainline and all our lr coresdo this by default afaik. However not all lr cores have the option not to rotate.,Impatience
"> This whole discussion is pointless.
> > > and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic
> > There is not need to guess anything, this is software development, there is an API, an implementation, and a frontend.
> > There is an API for this:
> https://github.com/libretro/RetroArch/blob/master/libretro-common/include/libretro.h#L486
> > So the players are:
> > * core
> > * api
> > * frontend
> > > Who knows the content needs rotation? The core does.
> The environment callback is a set, which means it's telling the frontend to ""do something""
> > So what should happen could go two ways
> > a. The core tells the frontend: ""hey video is rotated, adjust aspect ratio from what I reported accordingly""
> b. The core tells the frontend: ""hey this content requires rotation, do whatever you need to do so it works properly""
> > That's all. This is a frontend problem, but before anything can be do about it what we need is clarification from the API side so we can adjust both the frontend and the cores to do whatever needs to be done.
> That's all.
I don't understand all of this, but it sounds very reasonable. Yes, it does seem grant2258 and I have been going in circles with this. I think I've provided enough info on the problem as it currently stands to work towards a solution, but since I'm not a programmer, I've probably done all I can by this point.",
i agree with that @CharlesBukowski im sure @fr500 has some idea looking forward to seeing what he is going to do since he thinks there is an issue and what exactly is wrong as i cant see any issue at all with mame2003 or fba on libretro it both have the ability rotate or not rotate vertical games. It does it the same way as mainline fba and mame. So i wll digress im at a bit of a loss what you two seem to think the issue is is you want to maintain a 4:3 ratio you need to rotate the Monitor or physically force the aspect ratio to 4:3 when rotating and put up with the streched gfx,Bitter Frustration
"The issue is that the behavior isn't consistent across cores and it's confusing as hell what is actually going on. There is the separate issue of what the default behavior should be for vertical games, which you seem to be caught up on. Just having this stuff standardized across cores would be a huge improvement over the current situation, regardless of what the default behavior is set to. As of right now, MAME and FBA do the exact opposite thing with video_allow_rotate = true/false. And that's just two cores. Who knows what's going on with other cores; I haven't even had a chance to test them yet. TATE on and video_allow_rotate = true in MAME 2003 produces the same results as vertical mode on and video_allow_rotate = false in FBA. How does this make any sense? posting this for reference, read from this post down. https://forums.libretro.com/t/switch-video-width-height-when-video-is-rotated/21674/35?u=nesguy",Bitter Frustration
"> > > i agree with that @CharlesBukowski im sure @fr500 has some idea looking forward to seeing what he is going to do since he thinks there is an issue and what exactly is wrong as i cant see any issue at all with mame2003 or fba on libretro it both have the ability rotate or not rotate vertical games. It does it the same way as mainline fba and mame.
> > So i wll digress im at a bit of a loss what you two seem to think the issue is is you want to maintain a 4:3 ratio you need to rotate the Monitor or physically force the aspect ratio to 4:3 when rotating and put up with the streched gfx
1. automatically switching the width/height for vertical oriented games is altering the output resolution, which is incorrect. The emulator should just output the resolution completely unaltered. Altering what is output by the emulator should always be done by the frontend, or through an option that the user has to manually select, but maybe that's just my opinion. The default should just be whatever the emulator spits out before you start doing stuff to it. 2. rotating the game is something that can be easily handled in the frontend without all of the confusion that currently exists 3. 2 is something that can even be done automatically if desired by the user, through the frontend as explained by fr500.
4. the current method of automatically switching the height/width with vertical games will always result in scaling artifacts no matter what you set the aspect ratio to (unless it's a multiple of both 240 and 320). How can this be considered correct? 5. the current method of automatically switching the height/width of vertical games isn't even making things easier for the user in all cases, because video_allow_rotate = true/false isn't doing the same thing in all cores. The current method is just adding more confusion. My brain is tired. Hopefully the thread I linked to above sheds some additional light on the problem.",Bitter Frustration
"IMO Standalone FBA does things right.
See: https://github.com/libretro/RetroArch/issues/8551#issuecomment-480630788
By default it's sideways and 320x240, and there's a clearly labeled and easily accessed option to rotate video under the video menu. There should be a way to make RA work the same with all cores using one of the methods @fr500 described (it may need either a or b depending on what the core is doing). You could then add an easily accessed user option to automatically rotate vertical games. (or an option to disable automatic rotation if rotation is the default behavior)
Or we could just endlessly debate what the default behavior should be when launching RA for the first time.",Impatience
"Ok here is the screenshots with you information the core geometry is reporting the right resolution. I explained before above that the information you need. Fba by default comes with setup like rotated i installed it and checked. It does not display vertical none rotated by default. Just giving you information in one place here not debating at all
rotated
![Untitled](https://user-images.githubusercontent.com/6128601/55694766-85c4eb00-59ad-11e9-8938-fbe965c7f255.png)
tate mode
![Untitled1](https://user-images.githubusercontent.com/6128601/55695277-cd4c7680-59af-11e9-81de-deb700b117b2.png)
fba default install no setting changed
![fba default](https://user-images.githubusercontent.com/6128601/55695612-426c7b80-59b1-11e9-8938-b8717439adfc.png)
Ill add one more this is screen shot with fba pay attention to the width and height
![res](https://user-images.githubusercontent.com/6128601/55737000-526d7500-5a1c-11e9-9815-dc3a78be4ad0.png)",
"> Ok here is the screenshots with you information the core geometry is reporting the right resolution. I explained before above that the information you need. Fba by default comes with setup like rotated i installed it and checked. It does not display vertical none rotated by default. Just giving you information in one place here not debating at all
I'm trying to figure out the point you're trying to make. If you're still arguing over what the default behavior should be when launching RA for the first time, I'm done with that conversation.",Impatience
you rotate 320 x 240 image 90 degrees is becomes 240 x 320 thats as simple as you can describe it,Impatience
"> > > you rotate 320 x 240 image 90 degrees is becomes 240 x 320 thats as simple as you can describe it
Who is disputing that? What is the point of this?",Impatience
"```
With vertically oriented games, Retroarch switches the emulator-provided width and height. For example, if the width/height is supposed to be 320x240, RA will switch this to 240x320. This only occurs with vertically oriented games. I tested this in both MAME and FBA using the Windows 64 bit version of RA.
```",
"Yes, and how do you think what you just said contradicts this statement? I think there is a language barrier here that is preventing meaningful discussion.",
that was you that it in your first post your rotate an images the 90 degrees THE x /y swap,Bitter Frustration
"Furthermore, the conversation has progressed quite a bit beyond the initial post. If you had been keeping up with the conversation you'd realize that the real issue is a lack of consistency between cores when it comes to the options related to video rotation. I am 100% done debating with you what the default behavior related to video rotation should be. It doesn't matter what the default behavior is. There are pros and cons to having rotation on or off by default and ultimately it's just an arbitrary decision that has to be made. What matters now is getting consistent behavior across cores and removing unnecessary knobs/dials that just lead to more confusion. @fr500 already outlined the solution and what he needs to implement it.",Bitter Frustration
"> > > that was you that it in your first post your rotate an images the 90 degrees THE x /y swap
I literally can't even tell what you're trying to say, here. I'm sorry, I really don't want to offend but the language barrier is just too much to deal with. I feel like you just consistently fail to understand the point being made and respond with irrelevant information most of the time and now I'm just wasting a lot of time trying to clarify things to you. I appreciate that you're trying to help, but we haven't been getting anywhere for a while.",Bitter Frustration
"i literally cant even figure your problem out the games will show sideways if they arent rotated. mame fba and lr cores do this. So i guess i leave it at this not wasting more time on you back tracking it not a good default leaving vertical games sideways. Something you claimed fba done
https://github.com/libretro/RetroArch/issues/8551#issuecomment-480630788",Bitter Frustration
"> > > i literally cant even figure your problem out the games will show sideways if they arent rotated. mame fba and lr cores do this. did you even bother following the steps I listed to reproduce the bug? Or bother looking at the discussion at the libretro forums? There is most definitely something weird currently going on with video rotation and height/width being switched. It's not a problem present in all cores. Rotation behavior is not consistent across cores, for the nth time. That's probably the main problem. > So i guess i leave it at this not wasting more time on you back tracking it not a good default leaving vertical games sideways. Something you claimed fba done
FFS, the screenshots I provided prove it! Facepalm. Here, to make things even clearer:
![image](https://user-images.githubusercontent.com/12267058/55746327-a7969000-59f6-11e9-90c8-ed577e14f713.png)
![image](https://user-images.githubusercontent.com/12267058/55746399-cf85f380-59f6-11e9-8be5-e09bd8da31f1.png)",Bitter Frustration
you rotated the image what do you expect to happen? you screen shots prove nothing accept the video is rotated and not rotated. fba and mame2003 create the same images,Bitter Frustration
"> > > you rotated the image what do you expect to happen? you creen shots prove nothing accept the video is rotated fba and mame2003 create the same images
That's exactly what I expect to happen. Do you see that you're just not following what I'm saying very well?",Impatience
"> > > > you rotated the image what do you expect to happen? you creen shots prove nothing accept the video is rotated fba and mame2003 create the same images
> > That's exactly what I expect to happen. Do you see that you're just not following what I'm saying very well?
No they don't, and I've provided numerous examples demonstrating this.",Bitter Frustration
"I guess you dont get it when you rotate the video it becomes 3:4 proof is in the screen shot from fba rotated. it 240 x 230 as well not wasting any more time repeating this
![res](https://user-images.githubusercontent.com/6128601/55747495-4beaf780-5a34-11e9-9f22-acf658589866.png)",Bitter Frustration
"> > > I guess you dont get it when you rotate the bideo it becomes 4:3 proof is in the screen shot from fba rotated. it 240 x 230 as well not wasting any more time repeating this
> > ![res](https://user-images.githubusercontent.com/6128601/55747495-4beaf780-5a34-11e9-9f22-acf658589866.png)
Read this thread, from this post down to the end. Others clearly recognize that there is an issue. https://forums.libretro.com/t/switch-video-width-height-when-video-is-rotated/21674/21?u=nesguy
Please just go away. You aren't getting it and I've reached the limit of my patience with this.",Bitter Frustration
is fba wrong as well. Ive lost my patience with this as well.,Impatience
"> > > is fba wrong as well. Ive lost my patience with this as well.
From the thread I just linked to, above:
> BarbuDreadMonSenior Member
> 30m
> > @Nesguy My mistake about width/height being switched for vertical games in fbalpha. While there is no official statement about this, i also think the core should always report resolution before rotation, i changed this behavior in a commit today. Now that itâ€™s settled i would recommend allowing core rotation again and enabling vertical mode, itâ€™s now upscaling properly for me, and there doesnâ€™t seem to be issues with shaders either. One last thing about fbalpha : if you want proper aspect ratio, use core-provided aspect ratio + DAR in core options for fbalpha (actually iâ€™m thinking about removing this core option and always forcing DAR), aspect ratio is per-game and hardcoded in the emulator for each one.
So, yeah. It's not that you're wrong, it's just that you never quite understood what the problem was in the first place. Thankfully, the issue with FBA was recognized and corrected by BarbuDreadMon.",Bitter Frustration
sending the wrong geometry form the core to fix it i would say thats out of spec and a patch. im sure mark will patch mame2003 and + up to send wrong info so it works in mame2003 and plus. Its trivial to do never the less i wont be changing is as its out of spec to whats really happening. no matter what your little numbers say the resolution is 240 x 320 when rotated :). I wold suggest you use fba for now because every other core will have it the same way most likely. Ps current mame rotates fine its in the tab menu video options of course its wrong as well cause you said so,Bitter Frustration
"i can explain the reasoning behind TATE mode core option in mame2003:
the emulator sends two bits of information to the retroarch API; the games width and height, and how much it should be rotated:
so, for a typical arcade game, that would be:
**width:** 320
**height:** 240
**rotation:** 0 (none)
now, for a typical arcade vertical game (which were rendered sideways, and then the CRT was rotated when put in the cabinet), that information would be be:
**width:** 240
**height:** 320
**rotation:** 1/3 (90/270 degrees)
the issue is, retroarch uses the sent height and width regardless of whether it has been rotated or not. so, the emulator has to send the width and height of the game on the presumption that the rotation has happened. so, if you set `video_allow_rotate = false` you will get an unwanted result:
`video_allow_rotate = true`:
![centiped-190409-121359](https://user-images.githubusercontent.com/13054748/55795957-ce20fd80-5ac0-11e9-8bbc-10d01a5cf1e0.png)
`video_allow_rotate = false`:
![centiped-190409-120210](https://user-images.githubusercontent.com/13054748/55795769-71bdde00-5ac0-11e9-868c-f08684fff394.png)
so in this case, mame2003 is sending the core:
**width:** 240
**height:** 320
**rotation:** 1/3 (90/270 degrees)
but `video_allow_rotate = false` is saying ""I don't care about how much the emulator wants to rotate it"", so you are ending up with the game not rotated (fine), but the screen dimension are now wrong.
TATE mode was added so that it doesn't presume rotation. Ie, it instead would send:
**width:** 320
**height:** 240
**rotation:** 1/3 (90/270 degrees)
(the rotation is still sent - the presumption is that anyone with a TATE setup would also set `video_allow_rotate = false`)
![centiped-190409-122201](https://user-images.githubusercontent.com/13054748/55796687-7daa9f80-5ac2-11e9-9abb-dae5908ac2e9.png)
(PS, screenshots from behaviour of mame2003 from a year or so ago, and retroarch 1.3.6 - this is what i originally designed TATE mode on - someone else rewrote TATE mode in 2003 so it instead does the rotations within the core itself - personally i think it should all be handled in the front end)
so hopefully that shows the issue that TATE mode was trying to solve. IMO there's some better solutions:
1) the API lets the core find out what video_allow_rotate is set to. that way, it can send the appropriate height/width for the situation.
2) rotation of 1 (90) or 3 (270deg) also flips the height/width. this would be my preference.
i haven't really thought about the ramifications of these! :)",
"there is also some interplay with aspect_ratio that I've forgotten the details about.
EDIT: hmm actually it think TATE mode only changed the core provided aspect ratio. but anyway you can see the problem i had to solve.",
the front end is still doing the rotating in mame2003 the fix is trivial your just dont swap change the x/y (like you should be doing in the core geometry). rotate anything manually the geometory x/y in the menu doesnt change. https://github.com/libretro/mame2003-plus-libretro/compare/master...grant2258:out_of_spec?expand=1,Bitter Frustration
"> no matter what your little numbers say the resolution is 240 x 320 when rotated :). No one ever disputed that the correct resolution is 240x320 when rotated. What's being disputed is whether or not all the cores are reporting the correct resolution when the image is rotated. You really are failing to grasp what's going on, here. Numerous people have since recognized the issue I'm describing and the issue in FBA was both recognized and fixed. The fact that you're holding firm on this even in spite of this indicates either a willful stubbornness on your part to admit that you're wrong, or just a complete misunderstanding on your part. There are numerous ambiguities and equivocations occurring in this conversation which make it very difficult to have a meaningful discussion with you. In fact, on more than one occasion you seem to think that I'm arguing for the exact opposite of the point I'm trying to make.
The question of what the default rotation behavior should be for vertical games is almost trivial. It literally only matters the first time RA is launched, after which the user can set the behavior in the frontend. The MAIN issue is getting consistent behavior across all cores, regardless of what the default behavior should be. Now, if we're discussing the SEPARATE issue of what the default behavior should be or what is technically ""correct,"" I've already said what I'm going to say about that. You can either respond directly to the points I've made or continue ignoring them, either one is fine by me. Both standalone FBA and now also the FBA core do exactly what that they should be doing: by enabling rotation in the core options and enabling vertical mode in the quick menu core options, you get the correct image, which looks like this: ![image](https://user-images.githubusercontent.com/12267058/55835445-6b882b80-5ad9-11e9-95e5-767e36d2622c.png)",Bitter Frustration
@CharlesBukowski clearly in not debating with you and your screenshots anymore. you clearly dont understand geometry when a rotate happens. Im not wasting more time on it with you. No offence i posted a fix details for mame2003 and plus to send this bad geometry clearly you dont understand this at a level beyond screenshots.,Bitter Frustration
As for tate mode in mame2003 and plus is a ccw 90 degree rotate (it was renamed to what it is now from that) so if the arcade is 90 or 270 it will rotate the same way if the user has there screen rotated in a barcade/ servo/ stand setup . If you want a no rotate option youll need to post a github issue to get it added. intended use for this rotation is this https://www.youtube.com/watch?v=dShq_reM-84 if you have the ability to do it. So a no rotate would need to have a good reason,
to sum it up its to do with core rotating an image with ra the cores info updates the rotated geom. RA doesnt like it or update geometry even when you do a manual rotate itself. So you have to return the original orientation even when you rotate through RA with the core.,Bitter Frustration
all mame cores will need updates or ra should report rotated geometry.,Bitter Frustration
"@grant2258
You are completely hopeless. Several other devs agreed that thereâ€™s an issue, FBA dev already agreed that there is an issue and fixed it In FBA. To deny that there is an issue at this point is pretty much insane. Iâ€™m done talking to you.",Bitter Frustration
"@fr500, yeah I donâ€™t know why this is even still a discussion. Iâ€™m at a loss. I use custom aspect ratios with RA because I overscan some games that scale to 1120 on the y axis. Switching it to core provided didnâ€™t change what was going on with the reported width/height and rotation, as far as I could tell. Anyway, the issue in FBA was fixed yesterday so I think this issue can be closed; rotation behavior appears to be consistent, at least from the userâ€™s perspective. The issue of how to best handle rotation is really a separate topic that probably warrants its own issue.",
@CharlesBukowski ive already done the fix for 2003 and plus linked it and described the issue you are either stupid or argumentative or clueless about whats going on i think the latter :),Mocking
"@grant2258 Let's please keep this respectful at the very least. That last line was unnecessary and he shouldn't have been insulted like that.
People in this thread recommended that I close this thread so that is what I will do.",
"FreeRADIUS 3.0.19 doesn't build with libressl 2.8.3 any more, it shows the following build warnings:
src/main/tls.c:3286:33: warning: incompatible pointer types passing 'SSL_SESSION *(SSL *, unsigned char *, int, int *)' (aka 'struct ssl_session_st *(struct ssl_st *, unsigned char *, int, int *)') to parameter of type 'SSL_SESSION *(*)(struct ssl_st *, const unsigned char *, int, int *)' (aka 'struct ssl_session_st *(*)(struct ssl_st *, const unsigned char *, int, int *)') [-Wincompatible-pointer-types]
SSL_CTX_sess_set_get_cb(ctx, cbtls_get_session);
^~~~~~~~~~~~~~~~~ /usr/local/include/openssl/ssl.h:730:20: note: passing argument to parameter 'get_session_cb' here
SSL_SESSION *(*get_session_cb)(struct ssl_st *ssl,
^
src/main/tls.c:3383:3: warning: implicit declaration of function 'SSL_CTX_set_num_tickets' is invalid in C99 [-Wimplicit-function-declaration]
SSL_CTX_set_num_tickets(ctx, 1);
^
src/main/tls.c:3396:3: warning: implicit declaration of function 'SSL_CTX_set_num_tickets' is invalid in C99 [-Wimplicit-function-declaration]
SSL_CTX_set_num_tickets(ctx, 0);
^
This leads to this linker error:
LINK build/bin/radiusd
/usr/bin/ld: error: undefined symbol: SSL_CTX_set_num_tickets
>>> referenced by tls.c
>>> build/objs/src/main/tls.o:(tls_init_ctx)
See also [FreeBSD Bug 237216](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=237216)
The following simple patch fixes the build:
--- src/main/tls.c.orig 2019-04-12 06:47:56 UTC
+++ src/main/tls.c
@@ -1579,7 +1579,7 @@ done:
return 0;
}
-#if OPENSSL_VERSION_NUMBER < 0x10100000L || defined(LIBRESSL_VERSION_NUMBER)
+#if OPENSSL_VERSION_NUMBER < 0x10100000L
static SSL_SESSION *cbtls_get_session(SSL *ssl, unsigned char *data, int len, int *copy)
#else
static SSL_SESSION *cbtls_get_session(SSL *ssl, const unsigned char *data, int len, int *copy)
@@ -3379,14 +3379,14 @@ post_ca:
*/
SSL_CTX_sess_set_cache_size(ctx, conf->session_cache_size);
-#if OPENSSL_VERSION_NUMBER >= 0x10101000L
+#if OPENSSL_VERSION_NUMBER >= 0x10101000L && !defined(LIBRESSL_VERSION_NUMBER)
SSL_CTX_set_num_tickets(ctx, 1);
#endif
} else {
SSL_CTX_set_session_cache_mode(ctx, SSL_SESS_CACHE_OFF);
-#if OPENSSL_VERSION_NUMBER >= 0x10101000L
+#if OPENSSL_VERSION_NUMBER >= 0x10101000L && !defined(LIBRESSL_VERSION_NUMBER)
/*
* This controls the number of stateful or stateless tickets
* generated with TLS 1.3. In OpenSSL 1.1.1 it's also",
Please submit patches that specify specific libressl version numbers. libressl 2.9.0 appears to add API compatibility with OpenSSL 1.1.0 so just checking for the presence or absence of LIBRESSL_VERSION_NUMBER is not an accurate method of determining what functionality libressl supports.,
"It isn't that simple. The two checks I added are about OpenSSL 1.1.1, there is no libressl version yet that's compatible with that. So there's no way to do these checks for specific version numbers.
Leaves me with the one I removed; this could indeed be replaced with a specific version -- I assume libressl did that API change in version 2.8.0, but I'm not sure about it.",Bitter Frustration
"Then add a note against the preprocessor checks stating that OpenSSL 1.1.1 wasn't supported as of version 2.9.0. Be advised if you want to keep these checks in v3.0.x, you should also do the same for the existing uses of LIBRESSL_VERSION_NUMBER, otherwise, libressl support in v3.0.x will go the way it just did in master branch (9652affe38f41ba2484e013cf9d2c0bcb8c80d67).",Bitter Frustration
"So you took this issue as an opportunity for dropping libressl support? IMHO, using libressl version numbers isn't going to work out well, because libressl follows the OpenSSL API and defines the OPENSSL_VERSION_NUMBER macros accordingly -- incompatibilities still happen, and there might be OpenSSL API changes that aren't supported by libressl at all yet (as those for 1.1.1). What's typically done is adding just a few checks for LIBRESSL_VERSION_NUMBER being defined, so the *current stable* version of libressl is supported. This is normally enough for anyone building from source and for projects like OpenBSD or void linux that use libressl as system-wide replacement.
But of course, if you decide to drop libressl support, this renders this issue invalid.",Bitter Frustration
"I don't see your point. The current stable version of libressl is not fixed, it's going to change, and when it does, we will need to re-evaluate which APIs we use when linked with libressl. Without version numbers and comments we have absolutely no idea against which version of libressl this evaluation was last performed against, and no idea when we can strip out old compatibility code. It's not that we don't want to support libressl, it's that we resent the poor quality of patches which have been submitted so far to support it. Or, to be more explicit, continuing to support Theo de Raadt's little ego trip is fine, but it should not come at the expense of additional effort for our development team.",Bitter Frustration
"> So you took this issue as an opportunity for dropping libressl support? Don't be an asshole.
This is open source software. You're welcome to contribute. If you're too lazy to contribute a decent patch, then don't complain that we're too lazy to jump to your every whim.",Bitter Frustration
"> > So you took this issue as an opportunity for dropping libressl support?
> Don't be an asshole.
So *isn't* this exactly what happened? The instance I posted this issue, all checks for libressl were removed from master. Not much to interpret here.
Yes, this is opensource, and I don't *expect* you to support libressl. But if you do, supporting the latest stable version by introducing just a few checks where those for `OPENSSL_VERSION_NUMBER` aren't enough (although they should, as libressl is defining them) is the one sane thing to do, and it looks like you did in the past. If you feel this is too convoluted and/or requires too many changes, that's fine. But trying to determine exact version ranges where libressl claimed openssl x.y.z API support while getting feature ""foo"" wrong -- I guess you don't want to do this, and I don't want that either, for obvious reasons.",Bitter Frustration
"The commit message in master explicitly said, we would support libressl in master, if someone were to post decent patches that specified the libressl version numbers.",Bitter Frustration
"> So isn't this exactly what happened? The instance I posted this issue, all checks for libressl were removed from master. Not much to interpret here.
Yes, this is *exactly* what happened. You filed an issue about the v3.0.x branch. Then when we made changes to a *different* branch, you got confused, and complained.
I agree that there's nothing to interpret here. Your comments about the ""master"" branch were unreasonable and inappropriate. Hence my ""don't be an asshole"" comment.
If you don't want to be called out for bad behavior, don't engage in bad behavior.
We try to support libressl on the v3.0.x branch. But if it's too much work, we'll drop it. We don't owe you anything.",Insulting
"Oh, now we have *confused* **and** *asshole*. And you're probably trying to tell me the exact time of the commit to master is just coincidence. Well, I won't bother you any further with unwanted patches.",Bitter Frustration
"> And you're probably trying to tell me the exact time of the commit to master is just coincidence. Of course it's not a coincidence. The problem here is you wanting to get v3.0.x working (which is fine), but then bitching about a different branch, after not reading the ""master"" commit message.
> Well, I won't bother you any further with unwanted patches.
See? You're an asshole. We're happy to accept patches, as you well know. So this comment is just you doubling down on lies and poor behavior.",Insulting
"Many many, many users complains about RetroArch not adding files to their playlists because the CRC doesn't match the original dump.
It can be because of a translation patch, an SRAM patch, and so on...
So I suggest a very simple solution :
Just add an option in playlist menu that could be named '_Ignore CRC while scanning roms_' or '_Add unknown roms to playlists_', whatever.
The thing is that it'll replace that line
`""crc32"": XXXXXXXX | crc""`
By
`DETECT`
In order to easily display all roms on playlists, with a label **taken from rom filenames**, so that roms with a good labels would display thumbnails, and roms with another label (like T-eng 100% etc...) would not display cover (it's not a big deal).
RetroArch playlists are painful to use because of those unique CRC32, and it would be wonderful to simplify it in that way !
For now, I'm creating my Playlists with _RetroArch Playlists Manager_ on Windows, to add my SRAM patched GBA roms, as well as all my non-detected translated roms.
Thanks for reading, I really hope to see that option.
### Version
- RetroArch: 1.7.6
### Environment information
- OS: Windows, Android, Switch",Bitter Frustration
"Why not just use the playlist manager in the desktop menu? It doesn't use the scanner.
Part of the reason why we validate the checksum is because an equal many number of people have bad dumps that won't work and then blame RA or the core instead.",Bitter Frustration
"@bparker06 What is desktop menu ? Is it the menu that opens with F5 on Windows ?
I mainly manage my Playlists on my Nintendo Switch, I guess there isn't that menu ?
I understand your point, but you can add it as an hidden option that would appear if advanced settings are enable. Advanced settings for advanced users.
Yes there arr bad dumps, but most of players also have goods dumps + patched versions, and it's hard to have those beautiful playlists with all our collection.",Impatience
"One of the problems with this is that there's no way to associate database entries/metadata or thumbnails with playlist entries that don't match checksums, I would consider this a bad UX and defeating the point of working towards a ""beautiful playlist"".",Bitter Frustration
"I don't really understood your message, but, why can't RA act like RetroArch Playlists Manager ?
It takes the file name and put it as a displayed title, so that thumbnails can match that name.
e.g. Mega Man Zero (Europe).gba will be Mega Man Zero (Europe), and will read Mega Man Zero (Europe).PNG as a boxart.
Here, it will scan filenames and copy the name as a title in the .lpl file, and refers to file extension and/or current loaded core, to decide which console is concerned and save it as a playlist (e.g. GBA here)",
"It could, but not everyone's roms are already named properly, and they don't want to rename them. Matching based on checksum fixes that issue.
If the roms aren't named properly then thumbnails won't work. And when you scan without the database, even if the name is correct, it won't know which folder to look in for thumbnails.",
"> It could, but not everyone's roms are already named properly, and they don't want to rename them. Matching based on checksum fixes that issue.
That's why I suggested to make it an option and not forcing people to rename their roms. It's just adapting to multiple users.
> If the roms aren't named properly then thumbnails won't work.
Yes, but it's a small trade-off for having user friendly playlists IMO...
> And when you scan without the database, even if the name is correct, it won't know which folder to look in for thumbnails.
RA can't create a playlist based on the extension ?
For instance well named .sfc files would be added on 'Nintendo - Super Nintendo Entertainment System.lpl' with DETECT lines.",
"Thumbnails shouldn't work like this anyway.
For the packs or on-demand downloads (https://github.com/libretro/RetroArch/pull/8490) forcing filename is fine I guess.
But in the end the playlist format should be able to reference an arbitrary thumbnail filename.
That way on both GUIs we could have an option to link a thumbnail that is not on the thumbnails folder or isn't part of the packs.
The whole al filenames have to match convention was due to the fact that the playlist couldn't grow or store any more information because the format was hardcoded.
The new one has room to grow.
As for the OP.
I figure we could have an option that reads ""Add to playlist"" then you select a file and it would search the DBs for occurrences with that name, then the user would be able to select the best suited entry. Of course that's a lot more complex than it sounds
Bottom line, giving infinite hands anything can be done. But that's not the case on this project.",Bitter Frustration
"> > As for the OP.
> I figure we could have an option that reads ""Add to playlist"" then you select a file and it would search the DBs for occurrences with that name, then the user would be able to select the best suited entry.
Why not, but it'd be a hundred time longer than manually creating playlists on a PC.
I also thought about adding roms to playlists according to their unique header/informations so that an header can be linked to a name on the database. But I guess it'll require to recreate an entire database.",Bitter Frustration
"Here is a intense (but terrible) newflash:
CRCs are already ignored when scanning many many roms types - for serials. Among other things this means that contributing to the libretro-database romhacks CRCs is completely uselessÂ¹ and is most of the reason i'm hesitating to continue after contributing quite a lot and creating a tool to auto update my PRs and hacks.
If anything what i want is the exact opposite of this feature: give a option to ignore the serials and go full crc instead of pretending that a serial is a unique id (hilariously, even if you scan something like a snes rom, you get all of its 'versions' and 'dump variants' - and maybe even hacks don't even remember but it may be worse and they simply not be there). This is hardly unique, since console makers didn't really care often about editions or bugfixes bumping serials, though as i mentioned it gets kafkaesque once hacks come into the picture. I made plenty of suggestions to both make a CRC scanner faster (only do it optionally, only do it for (cd) formats with a inbuilt crc like chd, keep a separate hack database and only check when needed, keep the crcs of cue files for the 'canonical' dump sets etc). The RA project made it clear that the scanner is a afterthought with a rather impressive number of ways it can go wrong and no interest in doing anything to get data the original dump sets didn't provide (filtering mame split sets for only the main 'rom' for instance) so i stopped caring.
Â¹ in fact i'm about 95% suure that last time i checked, the 'crc' on the playlist files was often bogus now, and a artifact of the old format / code using crcs but the scanner changing to serial as primary key, so you have crcs being 'found' that are completely fictitious.",Bitter Frustration
"Please stop being so negative.
> The RA project made it clear that the scanner is a afterthought with a rather impressive number of ways it can go wrong and no interest in doing anything to get data the original dump sets didn't provide
Send us PRs to deal with these issues. The communication couldn't have been clearer. No walls of text are going to fix anything here on its own.
You might be mistaken into thinking we have infinite amounts of contributors, manpower and time to do all this - we don't. We depend on the generosity of coders to help the project forward, and we can't exactly rely on programs like Google Summer of Code either.",Bitter Frustration
"I opened the issues for about, oh 1 year? Possibly more. You yourself even mentioned 'uh maybe we shouldn't do this guys' on one of the worst examples and then someone marked the bug as 'minor'.
https://github.com/libretro/RetroArch/issues/7455#issuecomment-430884329
So no, i'm not going to do your features for you. I just thought it especially precious that people are so misinformed about this that they think CRCs are the 'problem that needs to be solved' when the scanner is a dumpster fire in many other ways because it abandoned unique keys or because there isn't a option to make the scanner stop trying to support everything under the sun and be bad at everything.
I'm not even opposed to fast scan of serials myself, only not when the 'serial' has a obvious other entry in the same database with a different crc. In that case, yes, i'd love the option to 'if it isn't a unique key, fallback to CRC plz'.",Bitter Frustration
"> I opened the issues for about, oh 1 year? Possibly more. You yourself even mentioned 'uh maybe we shouldn't do this guys' on one of the worst examples and then someone marked the bug as 'minor'.
> #7455 (comment)
>So no, i'm not going to do your features for you.
Stop being abrasive and show some respect. Last warning before I get Github moderators involved in here. Any kind of passive aggressive nonsense you can leave at the door from now on, keep it strictly focused on the factual and behave yourself. Nobody has time for this nonsense and this is supposed to be a drama-free developer zone. If you're not a developer, your ability to contribute to this project is limited to say the least, and walls of text don't amount to any code anybody can do anything with.
You say ""I'm not going to do your features for you"". The way I see it, YOU want us to do all this for you when I have indicated to you already it's not a fact of us wanting to do it or not, it's a matter of not having the necessary manpower for it and resources already being spread thin. Once again, stay polite while you're posting in these threads and don't cause a scene.
Being developers on open source projects is already enough of a burden and thankless job without having to deal with toxic personalities like the one you're portraying right now. Don't make our jobs even harder.",Bitter Frustration
"Here's the facts:
RA is using serials, which lead to duplicates. It's also ditching certain emulator specific configurations of roms, lke MAME split sets (which could be supported by just taking the crc of the main game archive and pretend the others are not important). So the project decided to 'canonize' some particular sets, sometimes, and fuzzify most times.
This is still not enough to both fuzzily 'identify' certain games (games are missing), so people are asking for even more fuzzy filenames. I disagree with this because if the 'game is missing' even with serial, there is something else seriously wrong (what? Probably the scanner is dying on a untested hill somewhere on a set. Misidentifying sets of a console for another is my guess, that whole mess of scanner looking at a cd dump 'magic bytes' to find out what type of console it's supposed to be, which is not very resistant to different types of fileformat).
It's also *misidentifying* game hacks, because naturally they don't bother changing serials.
Finally, RA after getting the first entry of a game list (or the last don't recall) puts its (possibly bogus) CRC on the playlist (not certain, but probably because there are features and code that needs a crc).
Any future feature that depends on crc for uniqueness (say, web data, autoconfigs, auto tweaks, netplay) is built on quicksand.",Bitter Frustration
"> I'm not even opposed to fast scan of serials myself, only not when the 'serial' has a obvious other entry in the same database with a different crc. In that case, yes, i'd love the option to 'if it isn't a unique key, fallback to CRC plz'.
My response to this is still going to be the same - make a Proof of Concept PR, if it's good, we can merge it. Our resources are already spread way too thin, and you are seeing armies of developers that are simply not there. This is the realities of an open source project - change happens at home, and if you want to see who is going to be leading that change, look in the mirror for the guy who could likely do it.",Bitter Frustration
"Alternatively, start a bounty and add some money to it so these issues can be incentivized so we can find a developer who would like to take this on.
I wish it were different and I could find a developer overnight who would take this on, but that is not the current-day reality, it was not last year, and it is not the case this year either.",Bitter Frustration
"I'm terribly sorry @andiandi13, but your issue was hijacked by two people in particular who really should have known better and who only seemed to participate in this thread to start drama and air grievances, and apparently caused them to piggyback off each other. To prevent this from spiralling out of control, the post in particular was removed and this thread locked. Please open a new issue and I'll make sure this does not happen again and your issue will be properly heard.
To @grant2258 and @i30817, don't do this again.",
"Just adding a link to the new issue for reference.
https://github.com/libretro/RetroArch/issues/8624",
as 'kick' is needlessly aggressive,Bitter Frustration
"It hasn't been considered aggressive for the decades it's been an IRC command, so why would it be now? Put a nice message in the reason field if you don't want to hurt their feelings. It's not like you're actually physically kicking them...",Mocking
"We have the option to be less aggressive, so let's go ahead and do that. There's no need to comply with the idiosyncratic norms of IRC culture in the labelling of UI features.",Bitter Frustration
"My main point was that it's not aggressive at all. Being ""kicked out"" of somewhere is a common term that is mutually understood to mean you were removed. Removing someone from a room against their will is going to potentially upset them regardless of what you call it. I don't really care what the UI says, but please keep the /command as ""/kick"" for the sake of familiarity and efficiency.",
"Um, excuse me, sweetie, this is 2019ï»¿...
'ban' as in 'banish' is also unnecessarily aggressive and totally inappropriate if you consider African American history. It should be renamed to 'Adorn Chastity Device' instead.",Mocking
"Aggressive terminology in the ""riot"" project, we can't have that, oh no no.",Mocking
"empirically weâ€™ve seen people unfamiliar to the app see the word â€œkick userâ€ and say â€œwtf? is this like poking them?â€ rather than extrapolating it to â€œkick the user outâ€.
so, as amusing as it is to see people getting so overexcited about such a trivial change, iâ€™m going to lock the issue to contributors. thanks!",Mocking
"Let's remove the wording ""X kicked X"" when someone is removed from a room and replace with ""X removed X""
The word ""kick"" can create very negative emotional responses from users and we have evidence of this.
Also, as Element is evolving with a wider variety of user base, it makes sense to use more appropriate and friendlier language throughout the product. FYI @ara4n",
"why the hell did you haphazardly change the initial value of properties in this commit???
https://github.com/gradle/gradle/commit/dbb85f50bb184cc128949d4eec039b9ed30fc170
why the hell you break my code programmed against gradle 4.10 in the **very first** release after that???
why didn't you respect you own mandates and added a new method `.emptyProperty(Class)` and deprecated the one you had instead of needlessly changing the damn semantics of the API? why was my code was intentionally broken on gradle 5.0 when it should have been good though at least gradle 6.0?
why didn't you even document this brain-dead breaking change in the 4.10 -> 5.0 migration guide here:
https://docs.gradle.org/current/userguide/upgrading_version_4.html#changes_5.0
and could you please at least document it now???
i can't believe you guys, this is completely irresponsible. and don't tell me you needed to do this to support property conventions, you didn't. what a waste of time.",Bitter Frustration
"I understand you are unhappy with the change in commit https://github.com/gradle/gradle/commit/dbb85f50bb184cc128949d4eec039b9ed30fc170. The change is part of the stabilizing process that every [Incubating API](https://docs.gradle.org/current/userguide/feature_lifecycle.html#sec:incubating_state) goes through. The summary of the contract is when an API is tagged as incubating, the users should be expecting breaking change without notice. You are welcome to submit a PR for this breaking change.
On another note, it would be preferable for your next issue to follow the issue template as well as reading over the [Code of Conduct](https://gradle.org/conduct/).",
"<!--
Hello ðŸ‘‹ Thank you for submitting an issue.
Before you start, please make sure your issue is understandable and reproducible.
To make your issue readable make sure you use valid Markdown syntax.
https://guides.github.com/features/mastering-markdown/
-->
**Describe the bug**
Currently it seems it's not possible to create relations with the Administrator model (one-way).
An example use case:
We have a plugin in which we want to create new entities of a certain type, but we also want to know which administrator created them, so we would like to save a relationship between this entity and administrator. This can be a one-way relationship, but it seems even that is not possible.
**Steps to reproduce the behavior**
1. Create a model
2. Create a relationship with administrator:
```
""created_by"": {
""model"": ""administrator"",
""plugin"": ""admin""
}
```
3. Run project
4. See error
**Expected behavior**
Ability to create relationships (even if one-way) to administrator
**Screenshots**
N/A
**Code snippets**
N/A
**System**
- Node.js version: v10.2.1
- NPM version: 5.6.0
- Strapi version: 3.0.0-beta.2
- Database: PostgreSQL
- Operating system: Archlinux
**Additional context**
N/A",
"Agreed, @lauriejim this is also related to what I mentioned to you in slack about overriding the admin model file",
"Hi,
I agree with you it would be a nice feature to have. Right now you can implement your usecase by using the user from the users-permissions plugin (I know it means having duplicates of your admins in the users) but this will at least allow you to implement until this feature is implemented somehow.
@derrickmehaffy @mdibaiee What kind of relations would you like to have ?",
"I have a similar issue with the new way the admin is configured.
## My use case
We have an in-house, agency calendar. Users must sign in to view the calendar. The sign in authentication is handled by Strapi.
Until recently, the calendar user's roles were either Strapi 'Administrators' or Strapi 'Authenticated'. There is no 'Public' access to the calendar. Users registered via a registration link (the registration process handled by Strapi) -- and a successful, email verified registration meant you were automatically set as 'Authenticated'.
If your Strapi user role is 'authenticated', you're only able to view the calendar. If, however, your Strapi user role is 'Administrator', you're able to view the calendar as well as make edits to the calendar via the Strapi admin back-end. A unit manager would say, hey, so-and-so needs to edit the calendar as well as view the calendar. When I got this request, I manually bumped the user's role from 'Authenticated' to 'Administrator' -- and then I'd send that new 'Administrator' user a link to the Strapi admin.
The point here is that both 'Authenticated' and 'Administrator' users used the same login process -- go to the registration link on the front-end, register, verify e-mail, and then login via the ```/auth/local```. The best part of this is that the user -- no matter if she were 'Authenticated' or 'Administrator' could use the same username and password for access to the back-end Strapi admin and the front-end calendar app (written in Vue)
## What's happening now
Now, 'Administrator' roles have to be authenticated via ```/admin/auth/local``` on the front-end whereas 'Authenticated` users authenticate via ```/auth/local```.
## The issue
So the issue right now is that those admin users can't simply login to the calendar anymore. The ```/auth/local``` route won't authenticate them. They need a seperate username/password to access the Strapi backend. So they'll have two username/password combos -- one to access the Strapi back-end, another to access the 'front-end'.
## Confusion
So, I'm confused now. I'd been using the single route to login. Now there's two routes. So either I need to create sepeate usernames and passwords -- or I ask the user up front: are you an admin? Are you a user? ... and then set the auth route accordingly.
My questions:
- How can I set an 'administrator' role so that she's able to log in to the Strapi back-end and my app front-end with the same username and password?
- I no longer see the 'administrator' role in my content type user relation. So, if say, a person is a Strapi admin and an authenticated Strapi user -- their username needs to be in both the 'Admin' role as well as the 'authenticated' role in order to see it in the user-relation dropdown on a content-type (and, say, add that user to a blog post or article or whatever.)
- Even if I create a brand new custom role -- and click every box for the permissions -- that new role isn't able to access the Strapi admin. I want a simple checkbox to say, okay, can this user access the Strapi admin? If yes -- click it and grant access. That's it. That seems like the simplest solution.
I understand the desire to seperate the strapi roles from the app roles -- but in my case, and (I suspect) many others -- the roles are identical. An 'admin' is an 'admin', 'root' is 'root', 'superAdmin' is 'superAdmin' -- so she should be able to log in to the strapi backend and login to the app front-end -- all with the same username and password.
The alternative, of course, is to create a brand new back-end UI -- essentially duplicating Strapi's admin -- and granting that custom user (or 'authenticated' user) access to my custom back-end UI -- and then just using Strapi as a way to handle GET/POST requests. But this backend UI and baked-in authentication was exactly the reason why I started using Strapi in the first place. :)",Bitter Frustration
"@alexandrebodin currently, our use case is just a one-way relationship to the administrator model.
We want our local plugins to be able to create models, which, as a metadata, include the Administrator which created them in their database record. So our table looks like this:
```
{
""connection"": ""default"",
""collectionName"": ""sms"",
""info"": {
""name"": ""sms"",
""description"": """"
},
""options"": {
""increments"": true,
""timestamps"": true,
""comment"": """"
},
""attributes"": {
""created_by"": {
""model"": ""administrator"",
""plugin"": ""admin""
},
""body"": {
""type"": ""string"",
""required"": true
},
""retailer"": {
""model"": ""retailer""
},
""users_count"": {
""type"": ""integer"",
""min"": 0
},
""created_at"": {
""required"": true,
""type"": ""date""
},
""updated_at"": {
""required"": true,
""type"": ""date""
},
""deleted_at"": {
""required"": false,
""type"": ""date""
}
}
}
```
If I understand correctly, the only change I'm looking for right now is the ability to specify `admin` as the `plugin` (or any other type of change) to use the model.
If you can point me to where the code for this resides, I might be able to create a pull-request myself.
As a next step (and I can foresee this requirement), I think we potentially will want to customize the administrator model as well.",
"Hi guys. Here is how I think the admin / user-permissions should work in the future so we can discuss the short term wins we can have:
## User-permissions
The idea is to make the `user-permissions` plugin a real `oauth` / `open id connect` server and use passportjs to allow for a lot of providers. ## Admin connection
In the same way I want the admin to use passport so people can configure it to connect to the users-permissions plugin if you want to. Use a ldap or any other passport providers.
What do you guys think about it ?",
For the fields/columns customization it is a much simpler usecase. You can already custmize it by make the updates in the model directly in your bootstrap function.,
"> For the fields/columns customization it is a much simpler usecase. You can already custmize it by make the updates in the model directly in your bootstrap function.
@alexandrebodin Probably should elaborate on this for over-riding the model.",
"You can add custom fields attributes (âš ï¸ not relations âš ï¸ ) by creating a custom hook.
Make sure to put run it first by updating the config in `config/hook.json`
```json
{
""timeout"": 3000,
""load"": {
""before"": [""custom-hook""],
""order"": [
""Define the hooks' load order by putting their names in this array in the right order""
],
""after"": []
}
}
```
`hooks/custom-hook/index.js`
```js
const _ = require('lodash');
module.exports = () => {
return {
initialize(cb) {
_.merge(strapi.admin.models.administrator, {
attributes: {
randomColum: {
type: 'string',
},
},
});
cb();
},
};
};
```
We either can allow modifiy the model via json like the other and in the interface but this will make the possbile errors too important in my opinion.
I would like to provide and fairly simple api to customize a model on startup.
example:
```js
strapi.models('administrator').extend({
atttributes: {
newField: {},
newRelation: {},
},
});
```",Bitter Frustration
@alexandrebodin what about modifying existing values? Such as these: https://github.com/strapi/strapi/blob/15daa6212c1323c1ccb12099df92d8a51a704ee1/packages/strapi-admin/models/Administrator.settings.json#L2-L6,
"And for example what if someone wants to modify their services/controllers of the admin package?
IE: https://github.com/strapi/strapi/blob/master/packages/strapi-admin/services/auth.js
I'm more for the opinion that the admin package needs to be split, one for the UI, the other should be an authentication plugin similar to `users-permissions`",
(or convert the whole admin package itself into a plugin),
"you can change any meta with the example I provided. At your own risks.
I really don't think people should modify the controllers or services. Extending the model makes sense. Providing some programtic extension points too but just changing the files is really the best way to allo people to break everything at anytime. It doesn't really scale.
If it is missing features I would rather have them start a conversation and make a PR so it helps not only them but the community. Alllow people to just overwritte any files kinda break the idea of making Strapi open source as no one would need to contribute back to it ðŸ¤” I guess it is a matter of opinion anyway.",Bitter Frustration
"@alexandrebodin > Alllow people to just overwritte any files kinda break the idea of making Strapi open source as no one would need to contribute back to it
I would like to point out that realistically Strapi should for easy customization in all aspects, this should include options for all packages as not everyone's use-case is going to follow ""the heard"", and there is going to be some niche cases. As an example extending the admin authentication to add certain password, username, email restrictions. Maybe specific password polices, the options are limitless and hence the reason I think that admin package needs to allow for this. If we are allowing users to modify the ""frontend"" of the adminUI, the backend should have the same respect.
![image](https://user-images.githubusercontent.com/8593673/59344633-57bc9980-8cc3-11e9-93a4-4235657eb9ab.png)",
"^^^ ""The API"" in the above image should be including the Admin Authentication API along with it's services, controllers, and routes.",Impatience
As long as we can create apis that respect the Open/closed principle sure. Otherwise Strapi becomes responsible for the bugs people create in the codebase. In this situation it's better to fork say a plugin than just change half of the codebase locally and still ask strapi to make sure everything works after upgrading. It's would be unrealistic to think that would be a maintainable ecosystem,
@alexandrebodin I would like to work on this since our product has a strict requirement for customizing the admin model. Can you please point me in a direction that matches your vision on this issue and I can get started on this?,
Hi @mdibaiee before starting we need to figure out what is going to be allowed to be modified and how. Does having the Model in the UI is necessary for example. Or can we just give a way to extend it programatically.,
"@alexandrebodin I agree. So, the way we think about it is that we want it to be a model just like any other model, very similar to how plugins are handled, we should be able to extend it and create relations with it. We would also like to manage administrators from the admin panel.
I think having it in the UI makes sense, if I try to think of a CMS, I can always imagine it having a panel for managing admin users (WordPress does that).
I also don't see a reason not to allow _extensions_ to the admin model, services and controllers. Though one may argue that we should not be able to remove anything or modify the default attributes from the model to avoid breaking the application, and I think that makes sense.
I'm interested to hear your opinions on this as well.",
"Any thoughts on this, @alexandrebodin ?",
I'd love a way to be able to associate an administrator with one of my content types.,
"> You can add custom fields attributes (âš ï¸ not relations âš ï¸ ) by creating a custom hook.
@alexandrebodin I've just tried your suggestion and unfortunately the field does not get saved in mongo. The field correctly appears in the UI and the value I set for it is send as part of the PUT request to update an existing administrator (`/content-manager/explorer/strapi::administrator/:id`). But the value never reaches mongo. Any idea what could be wrong?
I'm on 3.0.0-beta.19.3",Bitter Frustration
"> > You can add custom fields attributes (âš ï¸ not relations âš ï¸ ) by creating a custom hook.
> > @alexandrebodin I've just tried your suggestion and unfortunately the field does not get saved in mongo. The field correctly appears in the UI and the value I set for it is send as part of the PUT request to update an existing administrator (`/content-manager/explorer/strapi::administrator/:id`). But the value never reaches mongo. Any idea what could be wrong?
> > I'm on 3.0.0-beta.19.3
Hi @aefox
I follow this way override the strapi-admin package for adding the relation field 'role' to the administrator model.
First, I added one field to this file:
https://github.com/strapi/strapi/blob/master/packages/strapi-admin/models/Administrator.settings.json
```
""role"": {
""plugin"": ""users-permissions"",
""model"": ""role""
}
```
Then check the create and update functions on https://github.com/strapi/strapi/blob/master/packages/strapi-admin/controllers/Admin.js
```
const { email, username, password, blocked, ...otherProps } = ctx.request.body;
...
const user = {
email: email,
username: username,
blocked: blocked === true ? true : false,
password: await strapi.admin.services.auth.hashPassword(password),
...otherProps
};
```
And it worked!
Hope it helps,",
"Can i use the following approach. Im new to strapi
// helper/admin-overrides.js
const fs = require('fs');
let dir = './extensions/admin/models/Administrator.settings.json';
let dirDist = './node_modules/strapi-admin/models/Administrator.settings.json'
fs.copyFileSync(dir, dirDist);
dir = './extensions/admin/controllersx/Admin.js';
dirDist = './node_modules/strapi-admin/controllers/Admin.js'
fs.copyFileSync(dir, dirDist);
And use this in package.json
""scripts"": {
""develop"": ""npm run override-admin && strapi develop"",
""start"": ""npm run override-admin && strapi start"",
""override-admin"": ""node helper/admin-overrides.js"",",
"This issue has been mentioned on **Strapi Community**. There might be relevant details there:
https://forum.strapi.io/t/extend-user-admin-model/713/1",
"Due to the nature of how the admin plugin now works in v4, we currently do not plan to allow for this type of customization by default and there are other methods such as patch-package that do make it possible without needing support from us on this.
Marking as closed as discussed with @Aurelsicoko",
"Sorry to meddle but what is going on?
Why is the whole architecture of the program changing?
I'm far from an expert or even a proficient dev, and not part of the ""team"" but I still have some WIP things going on and these latest changes seem very concerning.
Is there any rationale or long term goals of rolling everything into retroarch.c?",Impatience
Here's an official answer: https://github.com/libretro/RetroArch/pull/8910#issuecomment-502708016,
"Thank you for responding.
In my opinion it's awful but like someone told me recently ""not my circus, not my monkeys""
I'll stick to the current codebase until things stabilize.",Bitter Frustration
"It's how the original SSNES/RetroArch was like, with most of the code that was split into separate driver files inside one big runloop file. This allows for better restructuring of the codebase by getting most global state in one central place, and less passing around of state everywhere.
Anyway, it's an iterative process, and it's done to make the program last and become more maintainable in the long run. We cannot get there with all these separate files in their current form with all sorts of global state all over the place. And as I keep maintaining, the majority of the code outside of retroarch.c and libretro-common should become 80 to 90% tasks, thread-safe and all. Anything global has to go into one centralized place, so that we can make order out of chaos. Not to mention it's more optimal this way as well. The code outside retroarch.c should become a lot more pure, no longer relying so much on global state everywhere, so that we can convert them easier into tasks. For this and other reasons, we need to do this drastic rebuilding.",Bitter Frustration
"> We cannot get there with all these separate files in their current form with all sorts of global state all across the place
Of course you can, you use a branch, do it piecemeal until you are done.
Anyway, as I said, I'm no expert.
On the few programming classes I had this was the complete opposite of what I was taught.
Tasks are fine for certain things but are a huge headache when you need things to run in order.
I'm sticking for the 1.7.7 codebase for the stuff I'm implementing.",Bitter Frustration
"Branches are a non-issue, the refactors are already done piecemeal. Anyway, I'm the leader of this project, and I am making this decision to do it this way. The code more or less stayed the same anyway, it's a simple matter of the driver files being moved into one big file, and more functions turned static.
> Tasks are fine for certain things but are a huge headache when you need things to run in order.
Of course they are a headache, but it eventually has to happen anyway. CPUs are only getting more cores instead of faster clock speeds so allowing for stuff to be put on multiple threads at the same time is a very definite help (for things that can be done out of sequence), the main UI cannot block, many of these tasks on the main thread do block, which is bad. It's bad for Android, iOS, it's bad for the A/V synchronization timer, etc. It's bad for platforms like UWP where synchronous file I/O is very heavily penalized. It's a simple case of dealing with the headache because this is the only way forward.",Bitter Frustration
"It was also an opportunity of getting rid of all these ctl functions (camera_driver_ctl, location_driver_ctl, audio_driver_ctl, etc). The ones that haven't already been gotten rid of, will be soon. There should be only one such function for all the driver stuff in retroarch.c, and that should be rarch_ctl. It was a bad design decision I made a few years ago to go for that approach, and themaister didn't like it either. So this is a way to correct that mistake.",Bitter Frustration
"It's just making it even harder for contributors to work in the codebase.
As I said before it's your call. I don't have any stake into this. Just my opinion.",Bitter Frustration
"I think keeping it the way it was actually does that. Sure, a near-500Kb file is not ideal and things are a lot bigger than they were in the SSNES days, but I'm sure the total code size can be made far smaller. And all this state passing around was not helping performance either for certain. You might even notice a speedup or two.
Anyway, it's part of a longer-term redesign. It will pan out well.",
"If it's in the name of performance, multithreading and also reducing code, it can't be that bad, all that global state had it coming anyways.
The maintainability might suffer a bit, but speaking as someone who just started contributing a few weeks ago, getting into it requires lots of debugging know-how anyways.
There's no easy ""reading into it"" (if you try starting with configuration.c, you will go bonkers), no ""it's about this, so it's probably there"" (for example: guess where the core init is called... retroarch.c? Nah, command.c), you basically only find stuff by starting with a log message and backtracking (good ol' printf debugging).
So in practical terms, putting more code in 1 place doesn't change much.
What I'm saying is: stay positive, if it turns out bad, it's not like we are now stuck with it forever but it'll give an idea of how to do it even better, and if it doesn't turn out bad, well, great!
Personally I think what's much more upsetting than the actual code changes are all these small commits, almost a whole page of them! Please aphex, squash more! :smile:",Impatience
"Thank you for the upbeat attitude @LazyBumHorse . And yeah you're right, the sourcecode is already not readable as is it for newcomers being splintered across so many files. It can only be improved really.",Bitter Frustration
I have had nothing but trouble with conda. Everytime I have to reconfigure my machine I basically have to take a day to get conda working. Is there a possibility to move away from it and just starting using pypi?,Bitter Frustration
I don't think it is realistically an option. Pip does not really support binary depandancies like oce. What is exactly the issue with conda? It works flawlessly on ci so I'm surprised that you are having difficulties with it. Would providing a docker image or a custom anaconda installer help?,Impatience
I think I have installed conda 5 times in total just to use cadquery. Out of those 5 times only once it worked out of the box. One of the times was on windows where it currently still isn't working. On my current OS conda doesn't work at all which means I currently run a Ubuntu VM just to be able to use cadquery. Conda to me is incredibly brittle software so I would really just prefer a normal environment.,Bitter Frustration
"@rowanG077 sorry to hear that. If you are willing to share what does not work I could maybe help. I had success following those instructions:
https://docs.anaconda.com/anaconda/install/silent-mode/",
"@adam-urbanczyk The problem with conda is that it pollutes the environment. I just removed it (again) because it was breaking everything else python-related on my machine in very pervert ways.
Docker might be A solution to this. It's should be much easier to use for non-python developers, too.",Bitter Frustration
"I'm working on getting a nix environment with cadquery and cq-editor going. Don't have it working yet though.
@adam-urbanczyk A docker image would really help I guess. But it just seems really to extreme to resort to package managers which take over your entire system or having to run an entire second system just to be able to run a piece of software.",Bitter Frustration
"@rowanG077 Feel free to look at my nixpkgs fork: https://github.com/gebner/nixpkgs/commits/cadquery CQ-editor doesn't work with the upstream pythonocc, so you might need to change the pythonocc-core sources to https://github.com/CadQuery/pythonocc-core/",
"@rowanG077 @ilya-epifanov assuming you are on Windows could you try the following way of installing miniconda:
```
start /wait """" Miniconda3-latest-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /S /D=%UserProfile%\Miniconda3
```
In general I use the `InstallationType=JustMe` and `RegisterPython=0` options and I have never experienced issues you are describing.",
"It runs fine at my Laptop (Win10 x64), too. The advantage of Conda is that you can create different environments (cqgui in the case of CQ-Editor) with different package versions.
I've created and `update.cmd` which I use to start CQ-Editor while keeping up-to-date with the sources:
```batch
@echo off
git fetch upstream
git checkout master
git merge upstream/master
conda env update -f cqgui_env.yml -n cqgui --prune
conda activate cqgui
python run.py
conda deactivate
```",
"@gebner Thank you! I got cadquery working in a nix environment now by using part of your commits as a reference. Are you planning on upstreaming? If not I will try to get cadquery and CQ-editor into nixpkgs.
@adam-urbanczyk Isn't it possible to package binary dependencies with pypi? I thought that stuff like python-opencv delivers the opencv binaries with the package itself. Wouldn't this be possible for cadquery as well?",Impatience
"@rowanG077 Not at the moment, feel free to submit a PR to nixpkgs.",
"I'm feeling like docker-on-conda is the right answer.
Conda works well when you want full control and full integration into your
environment, and for greenfield installation. Docker works well when you
want no muss and fuss just works install that disrupts nothing.
So I think the best option is the existing conda based packages, with an
additional docker container that has cq already installed (via conda of
course)
On Fri, Jun 21, 2019, 6:27 AM Gabriel Ebner <notifications@github.com>
wrote:
> @rowanG077 <https://github.com/rowanG077> Not at the moment, feel free to
> submit a PR to nixpkgs.
>
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/CadQuery/cadquery/issues/153?email_source=notifications&email_token=AAJ44A74ABS3SX5BEGBSUXLP3SUHLA5CNFSM4HY6ZHU2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYIC4NY#issuecomment-504376887>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAJ44AZSPQ3TAGTNZTYGIMTP3SUHLANCNFSM4HY6ZHUQ>
> .
>",
"@rowanG077 @ilya-epifanov any updates on trying the local install?
@rowanG077 yes it is indeed possible to include binary dependencies in a wheel. It is not really practical though, especially for a project with so complex dependencies. If you don't believe me, take a look inside of the wheel that the project you mentioned is providing.
@dcowden agreed. I'll close this issue and open a specific one for docker image. It would be great if someone from the community could pick it up - I don't have the bandwidth and experience.",
@jhol I propose to move the discussion here.,
"I recently found this project which might allow easy conversion of conda to pip packages https://github.com/regro/conda-press I still don't like this approach because it will result in embedding of all the non-python (OCCT, freeimage, ... ) deps in the package. Do we **_really_** want to go this way?",Bitter Frustration
"I have created nix expressions for cadquery and CQ-editor a couple of months back. I still use them.
This might also be an alternative instead of pip. Big advantage over conda is that it does not pollute the environment and actually works sanely. https://github.com/rowanG077/cadquery-nix",
"@rowanG077 Nice to see another nix user! I packaged cq-editor in nixpkgs, it's in 20.03 release. https://github.com/NixOS/nixpkgs/blob/20.03-beta/pkgs/applications/graphics/cq-editor/default.nix
Did you manage to get this working in a non-NixOS environment? I had a go at it but ran into problems connecting the nix installed cq-editor to the Ubuntu installed graphics driver. Here is a discussion on the same problem but for a different package: https://github.com/NixOS/nixpkgs/issues/9415
A solution would be to get the user to run cq-editor through this: https://github.com/guibou/nixGL but it's probably still convoluted enough to turn users away from the nix method and back to conda.",Bitter Frustration
"@marcus7070 A great! I was planning on getting it into nixpkgs but never got around to it!
I haven't tried it just using nix since I run NixOS. But I'd wager mine has the same problem.",
"I'm working on CQ-editor flatpak, that could be used for just CadQuery. I keep you informed here: https://github.com/CadQuery/CQ-editor/pull/76#issuecomment-626195409",
"Used command to install `CadQuery`, it installed everything into `/usr`, replaced system python with own and didn't wanted to uninstall it.
Decided to reinstall my system.
So I think this issue is actual.
> Pip does not really support binary depandancies like oce.
Can't any binary files just be copied using `setup.py`
> I'm working on CQ-editor flatpak
That would be awesome.
Use Linux Mint, Fedora, both have preinstalled and configured Flatpak.",Bitter Frustration
"> The advantage of Conda is that you can create different environments (cqgui in the case of CQ-Editor) with different package versions.
The disadvantage is that Anaconda never ever seems to work, whereas in my experience venv (managed by anything but conda), system interpreter en even pipenv work flawlessly every time. Example (this is me literally trying to get this to work on my system as I type this): Step 1: conda create -n cadquery ```
Fatal Python error: init_import_size: Failed to import the site module Python runtime state: initialized Traceback (most recent call last): File ""C:\Python39\lib\site.py"", line 73, in <module> import os File ""C:\Python39\lib\os.py"", line 29, in <module> from _collections_abc import _check_methods File ""C:\Python39\lib\_collections_abc.py"", line 12, in <module> GenericAlias = type(list[int]) TypeError: 'type' object is not subscriptable ``` Fixed the above problem. Let's try again:
```
Cannot set up a python SDK at Python 3.8 (dactyl-python) (C:\Users\xxx\.conda\envs\dactyl-python\python.exe).
The SDK seems invalid.
```
Wut? Why would it even...!? *uninstall and reinstall anaconda3 a few times to try again*
*uninstall anaconda3*
*try miniconda instead*
*manually set conda.exe env path, because that somehow didn't happen during install*
conda activate cadquery
``` CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'. ```
k? That's probably not supposed to happen *uninstalls miniconda*
*I realize environments are left behind in some directory, removes 6 GB of environments manually*
*install miniconda again. Installation mentions that it's recommended to start anaconda from start menu to set environment variables* *anaconda/miniconda does not appear in start menu*
*Google how to set miniconda in env path*
*Can't find conda.exe anywhere, reinstall minconda3 to let installation add miniconda to path, against installation recommendation yet according to documentation recommendation (they contract each other)*
Finally figured out what the problem was: I had Python 39 installed, but Conda can only handle up to Python 38 (which is unfortunate, because I would like to use dataclasses which are a Python 39 thing). Deleted all references to Python from environment variables (including Pythonhome/pythonpath), reinstalled minconda, chocolatey refreshenv, closed and reopened terminal and now the conda command works. On to the next challenge: set up Conda env in Pycharm and see if cadquery will work. Three hours later: can start coding. Day is over. Have to go to bed. Disclaimer, this is my personal opinion and I'm probably a n00b, but: I hate anaconda. Messing around with toolchains is my main frustration with coding. This is not the first time random conda crap makes my cry. I've made it work in the past, I've worked with it in the past, but I just can't get it to work now for the above reasons. It's unfortunate that this is not explained in docs that I found and even more unfortunate that Miniconda install doesn't detect that Python is already on the path and might cause problems (and I can't use system python now? I'm confused?). And this is even before all the potential environment and dependency mess. ugggggggggggg. I'm not trying to flame cadquery devs here, I love cadquery itself, it's awesome. My goal is to just give a report of ""a random day of a random dude who wants to use cadquery"" and the one and only massive struggle that I run into: anaconda.",Bitter Frustration
"YMMV but this works in the CI:
https://github.com/CadQuery/cadquery/blob/4c77c87eac9ec845a5a17cdd0acad745e938e4b9/appveyor.yml#L17
You might be also interested in using the prepacked version of CQ-editor (and CQ), for now available from here:
https://github.com/jmwright/CQ-editor/suites/1584433493/artifacts/28713792
It is not built daily ATM though.",
"> Finally figured out what the problem was: I had Python 39 installed, but Conda can only handle up to Python 38 (which is unfortunate, because I would like to use dataclasses which are a Python 39 thing). Deleted all references to Python from environment variables (including Pythonhome/pythonpath), reinstalled minconda, chocolatey refreshenv, closed and reopened terminal and now the conda command works.
I'm not sure what you mean by ""conda cannot handle Python3.9"", the following works for me:
```
conda create -c conda-forge python=3.9 -n py39-demo
```
Note though that OCP and CQ are not built for Python 3.9 yet.",
"> > I'm not sure what you mean by ""conda cannot handle Python3.9"", the following works for me:
> > ```
> conda create -c conda-forge python=3.9 -n py39-demo
> ```
> > Note though that OCP and CQ are not built for Python 3.9 yet.
I'm not sure why the miniconda docs say the following then? I couldn't even get Conda itself to run at all without errors as long as I didn't have Python 3.8 installed on the system (unless conda magically started working due to some other thing a fiddled with). ![image](https://user-images.githubusercontent.com/22872340/104819960-b1872d00-5831-11eb-911c-d289d0c25198.png)
Or is it that: miniconda needs system Python 38 for itself to run, but can run environments within itself that are Python 39? This was answered below: yes.",Bitter Frustration
That is the base env Python version. You can crate envs with 3.9 (using the above mentioned command).,
"Related question: can CadQuery run on PyPy? I can't get pypy to run in anaconda, so I'm not sure if it's worth it to keep trying.",Impatience
"No, you cannot run CQ in PyPy.
There are *conda distributions with PyPy: https://github.com/conda-forge/miniforge and you should be able to install pypy (https://anaconda.org/conda-forge/pypy3.7) into a new env in any conda distribution. But let's not make this a conda support line.",Impatience
"There are two alternatives to using Anaconda now, but they're situational and not a direct replacement for a PyPi package.
1. Install binary development versions of CQ-editor: https://github.com/jmwright/CQ-editor/actions?query=workflow%3Abuild
2. If all you need is to transform the results of a CadQuery script to some output format (STEP, STL and SVG are currently supported), there is cq-cli: https://github.com/CadQuery/cq-cli/releases/tag/v2.1.0",
"> Install binary development versions of CQ-editor
Very interesting! Is it possible to add the compiled binaries in [CQ-editor releases](https://github.com/CadQuery/CQ-editor/releases) artifacts?
Then add download links that points to artifacts of the last version in the readme. The installation will be easier! :)",
"I think that @adam-urbanczyk wants to adapt the builds for Azure pipelines first, but that is the general plan.",
"I'm using CD-editor for previewing my work but want to use a proper IDE (e.g. VS Code) for development - especially for code completion. Therefore I installed the pip package but now have realized that it's actually the wrong version...
For my specific use-case, having a pip package that doesn't include the binaries would already be pretty helpful!",
"I also feel that Conda as a requirement is really hurting cadquery, and I know I'm not alone because as I've been reading up on CQ I find many people complaining about this. Conda is a niche tool hated by many, and the entire python ecosystem is based around pypi. There are many, many python packages deployed via pypi that require C libraries (See numpy, scipy, tensorflow, etc...) so this has got to be possible, and I'm happy to help figure it out. Since the Open Cascade libs are already available as packages for Ubuntu then this might be as trivial as doing an apt install of the open cascade libs and then pip install of cq, which would be python only. When I clone the repo I'm able to directly run python in the base directory and import and use cadquery without a problem. This isn't an exhaustive test though, and I'm not sure what other dependencies there might be. Are the OCE libs the only C library dependencies? We can probably get the pure python release up quickly, and then I'm also 100% positive that the compiled libs can be directly included in the pypi package, because tensorflow does this and it includes binaries not only for the C library but also GPU shaders and more.
CadQuery is a tool I've been hoping for, and I'd love to help get it out to the larger python community. Once people can easily pip install and incorporate into their projects this could really take off.",
"@rosejn [OCP](https://github.com/CadQuery/OCP) is our set of bindings to the OCCT CAD kernel, and will have to be handled too.",
There is also a Conda dependency with Spyder.,
@roipoussiere I think that will only come into play with CQ-editor. Installing the core CadQuery library via pip shouldn't need it.,
"@rosejn I'm no expert on pypy, but wouldnt the right approach to making it just work be to build wheels for the various platforms?",
"> I'm using CD-editor for previewing my work but want to use a proper IDE (e.g. VS Code) for development - especially for code completion. Therefore I installed the pip package but now have realized that it's actually the wrong version...
> > For my specific use-case, having a pip package that doesn't include the binaries would already be pretty helpful!
@Hades32 you can use git directly with pip: `pip install git+https://github.com/CadQuery/cadquery.git`
> I also feel that Conda as a requirement is really hurting cadquery, and I know I'm not alone because as I've been reading up on CQ I find many people complaining about this. Conda is a niche tool hated by many, and the entire python ecosystem is based around pypi.
@rosejn let's agree to disagree about conda. From where I sit it is not niche. There are people complaining all the time, but somehow I haven't see anyone producing a working wheel yet. It is possible, the question is if it is practical and how much time will we spend on doing it and then supporting users in debugging a zillion of issues specific to different system configurations, incompatible compilation flags of OCCT etc. If you can contribute a build pipeline (preferably azure) for generating wheels for Win, Linux and OSX that work reliably then we'll be more than happy to merge it.
You can also consider building packages for your favorite distro - no blessing from the CQ team is needed to do that.
And BTW I hear that there is a deb package for miniconda nowadays. I haven't tried it myself, I always install to a local dir, never update `.bashrc` and I never had an issue. Same on Windows.",
"OK sure, I'm not here to get into a debate. I'm hoping to be constructive and to help get this project into the mainstream python ecosystem. If you go to scipy, numpy, tensorflow or virtually any other major python library you will find instructions to use pip or related tools like poetry, but nothing about Conda. I'm sure plenty of people use it and it does nice things, so I'm not at all trying to denigrate anyone's toolset of choice. I just know that I have fought with conda in the past, and myself and my team are all setup using poetry across many projects and I don't want to start requiring everyone to use conda.
I'm happy to try and get the ball rolling with OSX and Ubuntu compatible installs, as those are what I've got access to. Hopefully if those are working properly then some inspired windows developer on this thread can help to make it work there as well.
I've never used Azure, but I'm also happy to structure things as a build pipeline if they have some kind of standard setup to drop things into. For now I'll focus on just getting a local dev build working smoothly, and then I'll dig into the cloud automation once we know the key pieces to put in place.
So I've cloned cadquery, OCP, and pywrap. It seems that pywrap is more of a code analyzer that is generating C python bindings, and my assumption is that you've already got this integrated into your build pipeline, correct? It seems like a great goal to have that running 100% automatically on OCCT, and I applaud the effort to make that work. I've done this in the past and for a library as large as OCCT I can imagine that has been a real effort.
Now I'd like to build OCP and CadQuery, but I suspect the build instructions on the README page haven't been updated in a while. pywrap doesn't seem to be an executable for one, and this doesn't seem like a typical cmake setup where you would configure and build from inside of a build directory. Could someone please update the README, or outline the build steps here? Or if there is an azure build pipeline log that you could share which shows the steps maybe that will help.
Your work is greatly appreciated, and I will try to be a net positive rather than take a bunch of your time. I'm an experienced developer with grit so I won't sit around complaining and asking for help once I'm off and running.",
"If this can help you to achieve a pip module, [here is my attempt to create a CadQuery Editor flatpack](https://framagit.org/flatpaks/org.cadquery.cq-editor-occt/-/blob/master/org.cadquery.cq-editor-occt.yaml) (hence my confusion about Spyder), and also [OpenCascade](https://framagit.org/flatpaks/com.opencascade.occt/-/blob/master/com.opencascade.occt.yaml), where you can find some build steps.
I tried this, then focused on other projects ...so it's not finished. But maybe the build instructions could help you in a way.",
"@rosejn There are some build instructions in this [yaml file](https://github.com/CadQuery/OCP/blob/master/build-bindings-job.yml#L181), and there are a few discussions on the Google Group and GitHub on building OCP/Pywrap on FreeBSD, which might include some info that will be useful to you.
@roipoussiere Thanks for the link. I was not aware of your work on the flatpak.",
"> Now I'd like to build OCP and CadQuery, but I suspect the build instructions on the README page haven't been updated in a while. pywrap doesn't seem to be an executable for one, and this doesn't seem like a typical cmake setup where you would configure and build from inside of a build directory. Could someone please update the README, or outline the build steps here? Or if there is an azure build pipeline log that you could share which shows the steps maybe that will help.
Azure runs https://github.com/CadQuery/OCP/blob/master/azure-pipelines.yml, which in turn runs https://github.com/CadQuery/OCP/blob/master/build-bindings-job.yml. [Here is a recent build log](https://dev.azure.com/cadquery/OCP/_build/results?buildId=1260&view=logs&jobId=bfbe1bfd-e8c3-5173-6d5d-4afb5d0fac7a&j=bfbe1bfd-e8c3-5173-6d5d-4afb5d0fac7a&t=4ddd68ef-aa5b-58d6-fcfa-706b6db49807), you can get them through the check mark next to each commit on the OCP page.
If you plan to operate outside of Conda, you'll probably have to build OCCT too. See here for the receipe: https://github.com/conda-forge/occt-feedstock
You'll get frequent segfaults if you just use a default build, you need those flags and patches.
Some additional help might come from @greyltc's AUR pkgbuild https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=python-ocp
Are you sure you don't want to try Conda-press first?",
"> OK sure, I'm not here to get into a debate. I'm hoping to be constructive and to help get this project into the mainstream python ecosystem. If you go to scipy, numpy, tensorflow or virtually any other major python library you will find instructions to use pip or related tools like poetry, but nothing about Conda. I'm sure plenty of people use it and it does nice things, so I'm not at all trying to denigrate anyone's toolset of choice. I just know that I have fought with conda in the past, and myself and my team are all setup using poetry across many projects and I don't want to start requiring everyone to use conda.
You don't want to get into a debate and then you continue with it. Please read the official installation instructions of scipy: https://www.scipy.org/install.html#scientific-python-distributions-recommended . Is pip the recommended way of installation? No. Please don't spread misinformation. > Now I'd like to build OCP and CadQuery, but I suspect the build instructions on the README page haven't been updated in a while. pywrap doesn't seem to be an executable for one, and this doesn't seem like a typical cmake setup where you would configure and build from inside of a build directory. Could someone please update the README, or outline the build steps here? Or if there is an azure build pipeline log that you could share which shows the steps maybe that will help.
> This is the main repo you need to look at https://github.com/CadQuery/OCP . README is up to date (bottom part is of interest to you), azure build logs can be found behind the azure badge. As mentioned above there have been ppl that did it on AUR, NIX and FreeBSD.",Impatience
"There is an interesting output you can get from Conda with the command `conda create --dry-run --json -n dummy <package>`. Conda will print all the actions it will take to create an environment with *package*. In particular, there is a link section showing the runtime closure (all the deps required to run *package*).
Scipy has 30 packages, Tensorflow has 82 and OCP has 93. CadQuery is 119. Just for giggles, CQ-editor has 216.
Cleaned up package lists:
[scipy.txt](https://github.com/CadQuery/cadquery/files/6049053/scipy.txt)
[tensorflow.txt](https://github.com/CadQuery/cadquery/files/6049054/tensorflow.txt)
[ocp.txt](https://github.com/CadQuery/cadquery/files/6049055/ocp.txt)
[cadquery.txt](https://github.com/CadQuery/cadquery/files/6049137/cadquery.txt)
[cq-editor.txt](https://github.com/CadQuery/cadquery/files/6049052/cq-editor.txt)
If you were a perl or json whiz, I think you could also extract the downloaded size of the runtime closure as well.",
"We'd all *love* to see PyPI-hosted packages for CadQuery. No one sane is opposed to that. Unfortunately, the core issues here are all on PyPI's side: e.g.,
* [*Distributing large binary payloads as separate downloads*](https://github.com/pypa/warehouse/issues/7852).
* [*API to get dependencies without full download*](https://github.com/pypa/warehouse/issues/474).
Without significant movement from the official PyPI community on those issues (which seems unlikely, because there's no consensus as to at what the high-level solutions to those issues even are), it's unclear whether CadQuery can meaningfully do anything here.
## Explain It To Me Like I'm 28
Official Python packaging standards *basically* prevent sharing large low-level non-Python dependencies like OpenCASCADE (OCCT) and Compute Unified Device Architecture (CUDA) as binary Python wheels that can be safely reused by other Python packages. That's bad. Every Python package requiring those sorts of dependencies has to rebundle those dependencies inside *every* binary Python wheel they publish. That's even worse. PyPI has strict filesize requirements, because bandwidth funding is understandably scarce. [The CDN hosting PyPI downloads currently pays **$1.5 million USD** out-of-pocket to host those downloads](https://discuss.python.org/t/what-to-do-about-gpus-and-the-built-distributions-that-support-them), which is probably unsustainable for obvious reasons.
[The full OpenCASCADE distribution alone consumes **~1.5GB** in space](https://dev.opencascade.org/doc/overview/html). Now add pywrap + OCP + CadQuery itself + {whatever-else} to that. Since that math fails to add up on my primitive abacus, it rapidly becomes clear that CadQuery won't be publishing binary PyPI wheels anytime soon â€” however much everyone wishes it just would.
## Can't CadQuery Just Do What Do Other Packages Do?
Different Python packages circumvent those constraints in different ways.
The CUDA-based CuPy package, for example, [laboriously publishes **one PyPI-hosted binary wheel of CuPy for each supported version of CUDA**](https://docs.cupy.dev/en/stable/install.html#installing-cupy). Why? Because each version of CUDA is so grotesquely big, the only way CuPy can feasibly support all supported versions of CUDA is to isolate each CUDA version to its own version-specific CuPy wheel (e.g., `cupy-cuda112` for CUDA v11.2, `cupy-cuda111` for CUDA v11.1). Of course, that's blatantly insane. It makes the `pip` constraints solver (especially with respect to `pip install --upgrade`) useless, because `pip` thinks those are completely different packages. Moreover, it requires CuPy developers to continually go to the PyPI community with their hat in their hands and politely beg for increases in their filesize allotment. Why? Because CUDA ain't gettin' any smaller, folks. At some point in the all-too near dystopian future, PyPI maintainers will be forced to refuse their polite but gluttonous requests for increased bandwidth â€” at which time the CuPy userbase will be quietly required to use `conda` to install CuPy. That probably sounds familiar, doesn't it?
[Other packages like `nltk` provide a crazy `download()` function that you have to manually call after installing a negligibly small PyPI-hosted `nltk` package shim to *actually* install the package properly](https://github.com/pypa/warehouse/issues/7852#issuecomment-812948919). That's probably even worse than CuPY's kludge, because it breaks idempotency <sup>*don't forget to remove that `nltk.download()` call before deploying to production, folks!*</sup>, subverts sane packaging standards, and presumably fails to sanely interoperate with `pip`-based downloads caching. The burden of installation shouldn't be on the end user; it should be on the developer.
There's actually a heated discuss that unsurprisingly went nowhere about this exact topic on both [the official Python mailing list](https://discuss.python.org/t/what-to-do-about-gpus-and-the-built-distributions-that-support-them/7125) and [HackerNews](https://news.ycombinator.com/item?id=27205586). Because PyPI maintainers failed to reach a consensus agreement, the only certainty there is that PyPI (and thus `pip`) remains hostile to large binary dependencies (and thus Python packages requiring large binary dependencies).
That's us.
## So What Should CadQuery Just Do What Do Other Packages Do?
**Absolutely nothing.** The more pertinent question is: ""What should everyone *else* do?"" And the answer to that question is clear: ""Drop `pip` like hot radioactive lead.""
`pip` is great for managing pure-Python packages. `pip` is *okay* for managing non-pure-Python packages requiring small binary dependencies. `pip` is fundamentally unsuitable for managing managing non-pure-Python packages requiring large binary dependencies.
That's not going to change anytime soon... and that's not CadQuery's fault. It is what it is. :shrug:",Bitter Frustration
"That's a really good explanation! Off-topic rant: Also I would say the solution (to share large binary files within pip and their bandwidth cost problem) to me seems SUPER obvious and relatively simple to implement: the PyPI folks could build a torrent client into pip and download binaries and even pure Python packages that way. At least make the torrent thing an option for large binary files, besides the current way of doing things?
It would still allow them to pay crazy amounts to host (torrent seed) downloads so they remain fast (if they wanted to) + it would allow package distributors (**and package consumers**) to host/seed binaries and python packages. This would probably greatly decrease the hosting costs for the PyPI folks as well. Anybody who would like to support Python and pip could simply download or update a torrent magnet link and start seeding. On their server, on their phone or wherever they want. Aren't these issues basically the reason why the Torrent system was invented?",
"@as08gf9uw0tjawtegi Torrent means unpredictable bandwidth and availability. ""New and shiny"" stuff would be shared by a lot of seeders all over the world but older versions of packages that are not used by a lot of people may become unavailable or available at uncomfortably low download speeds. Not to mention the great effort of integrating torrent client with pypi and backward compatibility problems this radical change would bring. In any case if you still believe this is a good idea you should propose it at https://discuss.python.org/",Bitter Frustration
"> @as08gf9uw0tjawtegi Torrent means unpredictable bandwidth and availability. I don't understand this argument. How is a traditional webhost with x amount of bandwidth hosting a Python package more predictable and available than that exact same host seeding a torrent of the same package with the exact same amount of bandwidth?
Do you believe that hosting bandwidth is somehow magically ""wasted away"" into the torrent network when you seed a torrent vs host a file the traditional way?",Bitter Frustration
"> Do you believe that hosting bandwidth is somehow magically ""wasted away"" [..]
No magic involved. Just human nature. People tend to seed only what they use/watch/otherwise utilize now. Few seed something for the greater good. Even fewer do it from always-on server. Because of that some obscure and old packages would be hard or impossible to download. This is what I was talking about. This is not the place to discuss this initiative. CadQuery is not PyPI. If I'm wrong you probably should pitch this idea to pip maintainers at https://discuss.python.org/",Bitter Frustration
For anyone interested in @as08gf9uw0tjawtegi's torrent-based binary hosting for pip idea. I see they have started a topic at https://discuss.python.org/t/torrent-client-in-pypi-for-package-binary-distribution/9417,
"FWIW, it doesn't matter to me whether cadquery hosts on PyPI/supports pip install at all or not. I think every project should support the tooling that works best for them and their users. However I just wanted to clarify/point out a few things as a PyPI maintainer/admin!
Easiest/happiest thing first.
> * [_API to get dependencies without full download_](https://github.com/pypa/warehouse/issues/474).
This is coming, we've had a rough idea of what to do for awhile now, and we were just missing someone with the free time to go through and write it up as a proper PEP. That happened and [PEP 643](https://www.python.org/dev/peps/pep-0643/) is now accepted and is currently being implemented.
> * [_Distributing large binary payloads as separate downloads_](https://github.com/pypa/warehouse/issues/7852).
This is still a problem. In the linked thread under ""heated discussion"" (which I don't think was actually heated! but that's neither here nor there) Dustin (one of the PyPI maintainers/admins with me) laid out the problem with several possible solutions. We don't have a solution to this, but I'm pretty sure that everyone involved wants to solve this problem. Right now it's stuck in the ""discuss possible ideas"" phase, largely because nobody has stepped up to write, submit, and champion a PEP for a possible solution (not that that PEP would be guaranteed to be accepted, but that's how those projects make decisions).
The Venn diagram of people who have the ""large binary"" problem, people who care/want to work on pip/PyPI, and people who have the time to work on the above is pretty small, so it'll likely languish until someone gets more time, it bubbles up to be a much larger problem, or someone external champions an idea that gets successfully accepted.",
"I have CQ-editor working on my ""macOS Big Sur"" ""MacBook Air M1"" (Apple Silicon ARM based chip). I also use Windows 10 devices. I really can't remember how I installed CQ-editor (possibly the binary download?).
CQ-editor (and Jupyter-CadQuery) are all very nice but I'd like use a proper IDE (in my case Visual Studio) in order to better use and understand Cadquery. This has proved extremely difficult and hence, in my research, I found this thread. Sadly, even after reading everything above, I still don't have a working solution.
To increase adoption of cadquery, a key requirement is easy installation of it. That means that either installation should be a single operation or that there should be concise and clear documentation to follow (actually, there should always be such documentation).
I avoid using conda because I find it buggy, difficult to use and because it causes conflicts on my systems.
Should I ever manage to get this to work, I'll be documenting it.",Bitter Frustration
"FYI: Last few days I've been intermittently working on my own attempt at creating **Nix flakes** for **pywrap** and **OCP** all that while learning how to do it properly. I know that there were some tries to do it but all links in this issue I've opened so far led to repositories almost 2 years old + attempts in **nixpkgs** repo seem to also have been abandoned. Please correct me if I'm wrong.
Btw. my goals are following:
- pywrap, ocp and cadquery must be easily buildable locally from source code. No hackary like repackaging conda packages! Also use normal OCCT package (of a properly tested fixed version) from *nixkgs* as a source of headers not the ones added/hardcoded into the repos as submodule.
- Package **libclang**'s missing python bindings and provide them as an **nixpkgs** overlay in a flake until it's in **nixpkgs**.
- Each project should have nix flake inside its repo. One could then directly install them using their github urls. CQ maintainers would have to be OK with it of course.
- For each project also provide a development shell inside its flake so one can just fire up VSCode/ium inside it and work on the code.
- Finally provide an exemplary devShell where CadQuery is installed so one can open open VSCode/ium or whatever editor inside it and start drawing immediately.
- Also package CQ-editor and provide an exemplary devShell with both CadQuery and CQ-editor.
So far it's going good but I'm going to have to add a parameter into **pywrap** through which one can fully specify all clang's parameters. `--include` is not sufficient since Nix does not use UNIX-like filesystem hierarchy with global libraries and everything from sysroot to libcxx include paths (`-isystem` not `-I`) have to be specified. Not to mention a possibility of crosscompilation. Those flag are calculated automatically by nixpkgs's buiild infrastructure but have to be passed in when not using clang's wrapper shell script.",
"Nix is new to me. Can this package manager be used for non NixOS systems (Windows, MacOS)?
I'm with you about the local install without magic.",
"> Nix is new to me. Can this package manager be used for non NixOS systems (Windows, MacOS)?
> > I'm with you about the local install without magic.
It works on MacOS but not on Windows, at least not natively but perhaps it could work in WSL?",
"> It works on MacOS but not on Windows, at least not natively but perhaps it could work in WSL?
using WSL isn't for the faint hearted.",Irony
"@jchidley > To increase adoption of cadquery, a key requirement is easy installation of it. That means that either installation should be a single operation or that there should be concise and clear documentation to follow
What you say is misleading this is definitely provided but it involve conda. Anyway I see several solutions : 1. Install miniconda, this avoid most of the burden of Anaconda while retaining the easy install of binary package anaconda provides ( You can also look at mamba and micromamba that can also be a good replacement)
2. CQ-editor and jupyter-cadquery allow you to use an external editor, CQ-editor can refresh the view when you save a file (that can be outside the editor) jupyter-cadquery as a external viewer that run in your browser and gets updated when you call the show function in your script.
3. You can build OCP and the others cq dependencies from source but that would be a pain
PS : Using WSL is actually not very complicated",
"@Jojain I guess we have different ideas of what is easy. Anything that involves `conda` (or `miniconda` or `WSL`) isn't easy for the average user. If you want to grow the user base of this product beyond power users, it has to be trivially easy to install.
Having said that https://github.com/bernhard-42/jupyter-cadquery on Windows works well when using VS Code and the `jcv` viewer. I only found out about this option today.
As far as MacOS/Apple Silicon the only working solutions appear to be the binary download of CQ-editor and the Docker image for jupyter-cadquery. Or are there some instructions that I have missed?",Bitter Frustration
"@jchidley CQ-editor is easy to install and gives you something to start with and I think would fit most of the average newcomers, once they want more they can consider using conda/wsl or any otherway that exists to get a cadquery install.
That being said, answering your first question, have you tried activating this and using an external editor : ![image](https://user-images.githubusercontent.com/50221409/134774683-15d7bc4f-f4b5-4eea-85a9-e503c592ffac.png)",
"@Jojain using CQ-editor in this way means that it'll be working from the same python source file? In [another post](https://github.com/bernhard-42/jupyter-cadquery/issues/40#issuecomment-807625991) @wcraigtrader pointed out that CQ-editor isn't the most robust environment: it's good for smaller projects or code snippets. Given the choice, I'd rather go with the `jcv` viewer.",
"> @Jojain using CQ-editor in this way means that it'll be working from the same python source file?
Yes, you open your file in cq-editor and in your editor of choice, you make your change in your editor of choice, and each time you save your file, CQ-editor notice it and rerun your file.
If you have the choice I would also advice using `jcv` but we were talking on easy workaround here.",
"I had the same problems from within Code OSS. Cadquery installation went well with conda but I had to select the conda python interpreter to make it work from within VScode.
Just to rule this out :P",Irony
"I stumbled across this issue recently after reading a Hackaday article and I remember in the past (although this was a couple of years back) having issues with conda on windows (path related issues). I suspect it probably works better with a unix environment.
One thing I might try experiment with if I get time is the use of https://github.com/conda/constructor
The idea being you'd have 3 ways to install things
* The first being the usual way via conda
* The second being via a installable exe / bundle created with constructor (standalone install) with everything including cadquery included.
* The third being similar to approach two but just bundling the C++ dependencies cadquery needs, then installing cadquery / any python depends via pip. Then you'd just need some way for cadquery to find the installed binary libs / blobs, probably via an environment variable or something
Approach 2 would be the easiest / quickest for a windows install
Approach 3 would be best if you wanted to use a newer / local version of python
In theory it wouldn't be idea as you'd need to manually update the libs whenever cadquery was updated, but it would avoid issues with conda / miniconda installs for those that are non technical.
Also it may be possible to set this up via github workflows so that things are auto generated on new version tags.",
"@grbd do you think that your option (2) is significantly different than the zip bundle of CQ-editor that is currently available?
Regarding PATH issues on windows, you need to select certain options to get a fully portable install - we are updating the docs.",Impatience
"I would say option(2) would be very similar to the zip bundle of CQ-editor, although I did notice there was no python.exe in there unless I'm missing something.
The approach I've taken so far which seems to work fairly well under windows
Is install miniconda, untick any option for adding to the path etc to avoid it overriding an existing python install
Launch the Aconda shelll
```
# Update conda
conda update -n base -c defaults conda
# change directory to some place to install
cd C:\Apps\
# create a virtual conda environment
conda create -p cadquery
# Activate
conda activate C:\Apps\cadquery
# Install
conda install -c conda-forge -c cadquery cadquery=master
```
This gets things working in a directory, although I'd probably prefer option 3 personally.
I think also CQ-editor might be a bit behind cadquery at the time of writing in that I don't think it's marked for compatibility with python 3.10 yet",
"@grbd > I think also CQ-editor might be a bit behind cadquery at the time of writing in that I don't think it's marked for compatibility with python 3.10 yet
The [development packages](https://github.com/jmwright/CQ-editor/actions?query=workflow%3Abuild) mentioned in the [CQ-editor readme](https://github.com/CadQuery/CQ-editor#development-packages) are based on the current master branch, are built weekly automatically, and can be built on-demand when the community wants/needs it. Python 3.10 support was [merged into CadQuery 4 days ago](https://github.com/CadQuery/cadquery/pull/978) and the builds haven't been set up for 3.10 yet.
> I would say option(2) would be very similar to the zip bundle of CQ-editor, although I did notice there was no python.exe in there unless I'm missing something.
The Windows packages include a CQ-editor.exe file. The top level of the packages has a Windows command line script instead of the exe for two reasons.
1. Users didn't like searching through the CQ-editor directory to find the executable.
2. In some cases it's helpful to set environment variables before the exe is called.
We also have the capability of building a monolithic exe file, but the startup times for that format can be glacial.",
"I'd like to address some of practical points brought up in the rant(s) above, because they don't really make sense to me. It feels more like people arguing over vim & emacs rather than trying to come to a good solution to empower a community. I'm curious about a couple practical matters brought up here.
First, regarding the distribution of pre-compiled binaries. Who cares about making OCCT/OCP being available as a low-level dependency to other projects? That's not what the goal is here. Just package it up and make it part of cadquery, and if people want to use it like a library they can always depend on cadquery. I just checked, and the cadquery source itself is less than 600k, so this isn't a big deal at all. Many other python bindings to C (and C++) projects do exactly this, like tensorflow. As an alternative, the OCCT+OCP component could be packaged as a tarball, .deb, homebrew, etc., and then this would be installed separately before pip installing cadquery. In the same way that we need to download and install CUDA and CUDNN from nvidia before using tensorflow, this is a pattern that works just fine.
Second, the file size issue... is this really an issue? I just built opencascade 7.5 and the compiled libs + include headers are about 100 MB. I'm curious how this could become 1.5 GB with the addition of ""thin"" python wrappers, and whether all of the modules are even required. Maybe some of the challenge can be simplified by pruning the build output so that the end result is more manageable?
Last, as a final option, the NLTK example given above which requires a download() call (which means the size is irrelevant because it can just download from a github release link) sounds quite practical. What are the drawbacks of this approach for users? Clearly by the many people who care about this issue Conda is a constant source of pain, and in some cases just isn't an option. That download call can just look in .local/lib/CadQuery to see if it's installed already, and if not download it. This is simple, easy to verify, and not error prone. CadQuery is primarily a user tool, not something that will be deployed, but even if it is that could be setup in a dockerfile and done once on build, which is how most deployments happen these days anyways.
My prediction is that once cadquery is easily installable via pip so people can read a blog post and try it out in a moment of curiosity the user base, interest, projects and tools built using cadquery will all increase.",
"To begin with, I care about making OCP available as a package and about having sane binary dependency management for CQ. Everything you write is possible, but to me sounds like a hacky workaround (especially the NLTK example you give). Conda offers the best effort to result ratio for delivering something working and a huge number of relevant native packages that are (or could be) used by CQ. For some reasons some people hate it, but so far no one opened a PR with a clean pipeline building a wheel. If you don't want to spend time installing a portable conda distro, why would I spend time on building a hacky pip package? Given limited resources, it is more interesting and productive to develop new features.
With all that being said, there is an ongoing effort to build a wheel, but currently it has no ETA.",Bitter Frustration
"I haven't tried **miniforge** yet but with **conda** and **miniconda** I had following serious issues:
- The default installation modified my whole user environment.. edited .bashrc or .profile (perhaps both) and done that without asking as far as I remember. (This can be turned off in case of miniconda at least but my opinion is that it's malicious design.)
- Much more important issue is **global mutable state**:
Python's **venv** downloads everything into a folder inside your project (people usually use `.venv/` name) and does NOT modify anything outside of it and it is completely movable.
**(Mini)Conda** on the other hand assumes with insolence that you have an accessible home folder then downloads everything there and references/uses it from your project. It is detached from your project, cannot be moved with your project and leaves garbage for you to clean up from your home when you are done with the project.
This is unmaintainable, uncomposable, and hardly scriptable. Maybe it is OK for pure simple direct user but otherwise it is a huge DevOps antipattern aka *unnecessary global mutable state*.
That are my 2 reasons for not liking *Conda.",Bitter Frustration
"> Python's venv downloads everything into a folder [..] and it is completely movable
No, it is not movable. At least not easily and reliably. See for example https://stackoverflow.com/questions/32407365/can-i-move-a-virtualenv
In conda you can create as many environments as you like. Same as with venv. And you can remove them as soon as you are done with the project. Same as with venv. It is not worse than python + venv and not harder to manage. It is just different and takes some getting used to.",
"Conversion became too much off-topic - locking. If anyone manages to develop a working pipeline to build CQ wheels, please open a PR!",
"We're just waiting on PyPi to approve our size increase before we can make the cadquery-ocp wheel available, which will allow us to bring back `pip install cadquery`. Big thanks goes to @fpq473 and @roipoussiere for their work on the wheel builds.",
"@rowanG077 We have instructions in the README now for installing CadQuery via pip.
https://github.com/CadQuery/cadquery#cadquery-installation-via-pip
This addresses the OP's original request and so I am closing this issue. If there are other salient points in this issue, they can be broken out into new issues. To be clear, we are NOT dropping conda support for PyPI. Both will exist in parallel because conda has become an important part of our CI pipeline, and because some users will want/need to use conda and some will want/need to use PyPI.",
"<!-- For general questions about how to use Locust, use either the Slack link provided in the Readme or [ask a question on Stack Overflow](https://stackoverflow.com/questions/ask) tagged Locust.-->
### Description of issue
I have 1 large box to perform load testing. I have all of its file descriptors and memory to perform load testing. But locust never exceeds 100 rps.
### Expected behavior
I want to push locust to run at 10000 rps.
### Actual behavior
I can only do 100 rps. Why?
### Environment settings
- OS: RedHat 7.
- Python version: 2.7.12
- Locust version: 0.11.0
### Steps to reproduce (for bug reports)
```
# locustfile.py
class MainTaskSet(TaskSet):
@task
def get_root(self):
with self.client.get('/', catch_response=True, verify=False) as response:
try:
body = json.loads(response.content)
if len(body) <= 0:
response.failure(self.bad_json_message(response))
except Exception:
response.failure(self.bad_json_message(response))
class LocustTests(HttpLocust):
task_set = MainTaskSet
min_wait = 0
max_wait = 0
# bash
locust -f locustfile.py --no-web --host=http://remote.example.com -c 10000 -r 10000 --run-time 10m
```",Bitter Frustration
"> I can only do 100 rps. Why?
Are you monitoring your environment? Where's the bottleneck?",Impatience
"Using other load test tools, I've hit the limit of net.core.somaxconn:
$ sysctl net.core.somaxconn
net.core.somaxconn = 128
I kick that up to 1024 on both server and load-test machine.",
This definitely sounds like a server bottleneck to me.,
"I don't think the server is the bottleneck because on the same server, I could spawn 3000 rps using github.com/tsenart/vegeta. 5000 is not a big deal as well.",
"I had a similar problem here: https://github.com/locustio/locust/issues/1015...the most meaningful answer is ""remember, locust only runs in a single process, so it won't make use of multiple cpu cores unless you use multiple slaves.""",
"I like the whole idea of Locust overall; slick UI and master slaves architecture. However...
IMO, the whole hatch rate idea is too complicated and hard to understand. See `def hatch()` https://github.com/locustio/locust/blob/master/locust/runners.py#L102
Why not simplify and spawn everything all at once?",Impatience
"> IMO, the whole hatch rate idea is too complicated and hard to understand
that's subjective ... It's pretty straight forward to me.
> Why not simplify and spawn everything all at once?
because that's not always the workload we want to simulate. We need the ability to make virtual user arrival *not* happen all at once.... However, you are welcome to set your hatch rate to a level that essentially does what you are asking for. I'm not removing the hatch rate feature unless it's replaced by a better way to model increasing workloads.
there's nothing actionable in this issue... closing.",Impatience
"<!-- *Before creating an issue please make sure you are using the latest version of mongoose -->
**Do you want to request a *feature* or report a *bug*?**
> *feature*
**What is the current behavior?**
> `findOneAndUpdate` query `toJSON` doesn't return the aliased keys. Rather it returns the original keys.
**If the current behavior is a bug, please provide the steps to reproduce.**
<!-- If you can, provide a standalone script / gist to reproduce your issue -->
**What is the expected behavior?**
`toObject` or `toJSON` should return the aliased keys instead of the raw keys. Or a custom function like [`reverseTranslateAliases`](https://mongoosejs.com/docs/api.html#model_Model.translateAliases) can be implemented.
Related to #5184
**What are the versions of Node.js, Mongoose and MongoDB you are using? Note that ""latest"" is not a version.**
mongoose: 5.6.2
node: 12.0.0
npm: 6.9.0
<!-- You can print `mongoose.version` to get your current version of Mongoose: https://mongoosejs.com/docs/api.html#mongoose_Mongoose-version -->",
"Adding the following to the mongoose schema options solves the issue. It removes all the raw keys and only aliases are left.
```js
{
toJSON: {
virtuals: true,
transform: (doc: any, ret: any, opts: any) => {
Object.keys(doc.schema.obj)
.filter(k => doc.schema.obj[k].alias)
.forEach(ak => delete ret[ak]);
}
}
}
```
Maybe a plugin can help?",
"This is by design. The primary purpose of aliasing keys is to reduce storage in the database and network overhead between Node.js and MongoDB by shortening the keys when they're stored in the database, and converting the aliased keys back to long-form keys in Node.js.",
"If it's by design, then why doesn't it work as designed?",Bitter Frustration
"@klemensz if it doesn't work as designed, please open up a new issue and follow the issue template.",
*redacted*,
"https://github.com/kaminari/kaminari/blob/16c0d83c69d6e498c6fe11777d6757b3cf39222d/kaminari-core/lib/kaminari/models/page_scope_methods.rb#L29
I was using negative padding, and I need it. Please change this to be a warning at most, not a halting error.",Bitter Frustration
"Confirmed, my app works again after version-locking down to 0.17.0...
Here is the offending commit: https://github.com/kaminari/kaminari/commit/21baf16c0401dea1b6477a457aca5936237f4a56#diff-d90e018990d8862b642722642018c5af
This seems to have been done with no rational given (?)...
The usage case here is when I have 3 articles listed on the front page, but then I have links generated via link_to_next_page, and I want it to load 6 at a time, starting at the 4th article on the first click. So then I have a page size of 6 and and offset of NEGATIVE 3!!!",Bitter Frustration
"The whole context could be found here: https://github.com/kaminari/kaminari/issues/839. Have you looked at it? Also, this is not a bug but a feature request. It'd be greatly appreciated if you could send a PR.",Impatience
"That's all fine and dandy, that it can theoretically return no objects in some situations, but there's no reason you should have broken my completely working app. I mean, isn't that a perfectly fine ""fail silently"" or maybe a ""warn"" situation? I don't see the need for a halting error to be raised here. I'm version-locked to 0.17.0 because of this, now...
To me, it is a bug. The feature of being able to use negative padding has been broken.
raise ArgumentError, ""padding must not be negative"" if num < 0
should be changed to:
puts ""warning: pagination padding is negative""
or something like that.",Bitter Frustration
"Regardless of this ticket being a feature request or a bug report, any contribution or PR is greatly appreciated. However, just removing it does sound like a one-sided opinion given that [all the three DBs don't support negative `OFFSET` properly](https://github.com/kaminari/kaminari/issues/839#issuecomment-268211318) and we probably can not accept it. We would like a negative padding feature that consistently works with different databases and paginatable arrays.
Keep in mind that PostgreSQL and MySQL do not accept negative `OFFSET` values and SQLite just ignores it. We would also have to ensure compatibility with the built-in paginatable array and `kaminari-mongoid` if possible. The intension of raising an error is to prevent people from running into inconsistent behaviour (see [this comment for more details](https://github.com/kaminari/kaminari/issues/839#issuecomment-268211318)).",
"This doesn't really make sense to me. I have a perfectly working app and this change breaks it. It's a BUG. I'm not concerned with the fact that sometimes using negative padding can result in no rows being returned, in fact I would say that is the EXPECTED behavior, and that is perfectly normal when you use it *improperly*. I don't see why a warning would not suffice here.
I don't understand why you INSIST that my app must HALT AND STOP WORKING COMPLETELY because I used a negative number you didn't like. Good developers should give their users MORE options, instead of TAKING AWAY options for usability because they went slightly outside how the developer anticipated the software being used.
Why would I submit a PR to FIX your BROKEN pagination tool, after you've already said you won't accept it, for some reason which makes no sense to me.",Bitter Frustration
"Tell me, please, how can I have 3 results on the first page and then return pages in batches of 6, starting at with results 4-9 as page 2? It CANNOT BE DONE WITHOUT NEGATIVE INDEXES. You guys tried to fix an ""abberent"" behavior of negative indexes, but you ended up COMPLETELY BREAKING PAGINATION.",Bitter Frustration
"@Tectract Your comments are offensive and disrespectful. Caution, I may block you to keep a safe place next time.",Threat
https://github.com/kaminari/kaminari/pull/1004,
"I submitted the PR as requested. There is no need to halt people's application because they used a negative input variable, a warning here is just fine. My application works just fine, and indeed RELIES on this negative number FEATURE being available.",Bitter Frustration
"Imho, halting errors should not be raised unless there really is a completely breaking situation that cannot be resolved. A ""best guess"" and ""forgiving"" approach should be used instead.",
closing in favor of https://github.com/kaminari/kaminari/pull/1004#issuecomment-523909069.,
"I use negative padding, I need it for my application. It works for me with no errors. A warning is fine, no halting error is needed here.",
"Hmmmmm, I'm not sure how to fix this PR, or why it failed. I just put in one output to stderr statement.",Bitter Frustration
"@Tectract Did you read @yuki24's comment here? https://github.com/kaminari/kaminari/issues/1001#issuecomment-523680336
The reason we're dropping negative padding is not because we want to break your app (of course!) but because we couldn't implement a consistent and predictable behavior for that feature.
We understand that some previous versions of kaminari were luckily working on your app as you expected, but that same app may behave differently on a different setup. We don't want to ship such a fragile thing as our public API.
Kaminari is an abstraction layer for those who don't want to calculate and manipulate complex pagination logic. Or maybe we can call it a DSL.
We believe that a library for such purpose should never include something that behaves unpredictably. That is why we're carefully testing our code base against multiple versions of ruby, rails, and multiple kinds of database engines.
There are reasons behind APIs. And the reasons are written in the tests (and commit comments / discussions). We maintain tests so that we can make sure we're properly providing what we've designed.
So, when you're proposing to change an existing behavior, you will need to reflect your new design in the tests and make them all green for all test matrix.
Thank you for understanding!",
"I disagree with your decision to REMOVE a widely used feature, and BREAK peoples applications, for no good reason. Why can't you just allow this to be a warning instead? Why does your app need to raise errors, at all? It's a completely unecessary BREAKING change, that removes a FEATURE that your users are begging you to keep! Why are you doing this???",Bitter Frustration
"Instead of breaking people's applications, you should figure out a way to properly support negative padding. UNTIL THEN, you should allow this to be a warning. Just a suggestion.",Bitter Frustration
"@Tectract Again, did you read our comments?
We dropped that feature (if you call it a feature) because the behavior was unpredictable and so untestable.
I can't think of any good way that we can maintain tests for such a random behavior, but if you came up with a good approach, please implement your idea in Ruby and push it onto this PR.",Bitter Frustration
"Negative padding is already working just fine, returning no results in some instances is an expected behavior. I think the problem is actually with some user's interpretation of the behavior, not the actual behavior of the code.",
"Sadly I had to block this user Tectract entirely from the kaminari org. This user isn't just aggressive to us, but also has a history of ranting on other repos, most notably on [Diaspora](https://github.com/diaspora/diaspora/issues/7866#issuecomment-421593824), [React](https://github.com/facebook/react/issues/15432#issuecomment-484290354), [Rails](https://github.com/rails/rails/issues/33009#issuecomment-392961139), and [Ruby](https://github.com/ruby/bigdecimal/issues/98). I have reported this user to GitHub as well.",Bitter Frustration
"Anyone have any idea why ParamConverterListener is taking 5s on my local development environment? Actually, most of the loading process is slow. I'm not looking for someone to hold my hand and debug for days, I just wanted to know if this was common and, if so, what the fix is.
I'm on Windows 10 with Apache 2.4, PHP 7.3.5. The project is Symfony 4.3.3
![image](https://user-images.githubusercontent.com/2415447/63627293-0d19a080-c5d5-11e9-895a-e4c755386393.png)",Impatience
"What param converters do you have? What do your controllers look like that have those action params?
There is nothing we can do here for you. For support questions, please have a look at http://symfony.com/support",Impatience
"As a ""10x developer"" (https://htmlguy.com/about) this should be pretty easy for you to debug.",Mocking
"""There is nothing we can do here for you"". Then why have an issue type specifically for support?
Ah, the description below the support type says to see the link you shared.",Impatience
Love the personal attack too. Nice to know if I ever want to deal with pompous assholes I can just create a Symfony issue.,Mocking
"@Tobion congrats, you won an award for the best support of the month",Irony
"FYI, clearing var/cache/dev brought it down from 5 seconds to 1.",
"@eventhorizonpl It seems you missed the point again. We don't use github issues for support questions which should be very clear if you take the time to read https://github.com/symfony/symfony/issues/new/choose
And even as a support question, the report is not given any information that would be required to help. If you are better at answering this support question, why did you provide any help at all?",Bitter Frustration
"@Tobion Regardless of who is right or wrong, how about using some tact? I'm not looking for you to determine my exact problem. I don't want to waste anyone's time with that. No matter how experienced I am, I can't possibly keep up with all the changes you guys make to every component, or how it all works behind the scenes. Symfony documentation is designed for using the framework, not understanding the backbone. All I'm looking for is something like ""I've seen this before, it's just X. All you have to do is Y"" or ""If you have a lot of annotations, it's probably taking a long time to cache them prior to serving your request"". You guys have the inner-workings memorized. I don't. In an effort to save time, I thought it was worth posting a quick issue here. Sorry I didn't notice that the support topic told me to bugger off and pay for assistance. I know it's an environment issue because it's much slower on this dumb Windows machine than it was on my Mac (I'm stuck on Windows while my Macbook is in for the battery recall). So, in a way, it's not really Symfony, but then again, it is, because a static site loads very quickly.",Bitter Frustration
"@tobion ""It seems you missed the point again"" oh, so you know me well and you know how many times I missed a point. and it is now my fault. I'm sorry that you had a bad day...",Bitter Frustration
"We're all here to help one another. I have open source plugins that I give away for free and provide basic support. Regardless of Symfony's official stance on issues and assistance, is it really that hard for someone who's seen this before to jump in and give me a quick tip so I can save time? I could go to stack overflow, but...#shudders",Bitter Frustration
![image](https://user-images.githubusercontent.com/2415447/63629783-5d4b2f80-c5e2-11e9-83fb-df0fd2cba65d.png),
And so it's clear...I inherited this Windows machine with WAMP as a temporary solution while I wait for my Mac to come back. I'm not going to bother digging into this issue deeper because I'm 99% sure it'll be fixed when I setup Docker or Vagrant. I'm going that route tomorrow. Have a good night.,Impatience
"You can redirect back to previous url in controller, just like in Ruby On Rails:
```
def show
redirect :back
end
```
In symfony maybe can archive same way:
```
public function show(){
return $this->redirectBack();
}
```",
"Hello,
I will try to start if we need this feature :)",
How would this feature work? Because there's no reliable way of determining where to send back to as the referrer header is not foolproof,Impatience
@linaori we can get it from `$_SERVER['HTTP_REFERER']`.,
"That's not reliable. If you expect a return to the previous page, you'd need something reliable. That value can be spoofed or hidden.",Bitter Frustration
"We can save the previous URL in the session and, if `HTTP_REFERER` is empty, get it from there.",
"If you do this, make sure it's behind a feature flag, as it would make the session a requirement on each page, while it might not be necessary. It's also important to not save it on requests that had a non 2xx status.",
"the session is not working for that either: if you open 2 tabs, they will share the same server session, but not the same browser history.
Note that Rails (suggested as inspiration for the feature) implements it based on the Referer header, with a fallback location and an optional filtering of the allowed host for the Referer (using the fallback location if the filter rejects the referer).",
Closing as the corresponding PR has been rejected.,
"Are you kidding me? Laravel has that already. They even got a helper function for that, simple as `return back();`",Impatience
"[root@iZ25qp2d64mZ EHT]# python3 manage.py runserver_plus --cert server.crt 127.0.0.1:7000
Traceback (most recent call last):
File ""manage.py"", line 10, in <module>
execute_from_command_line(sys.argv)
File ""/usr/local/lib/python3.5/site-packages/django/core/management/__init__.py"", line 338, in execute_from_command_line
utility.execute()
File ""/usr/local/lib/python3.5/site-packages/django/core/management/__init__.py"", line 330, in execute
self.fetch_command(subcommand).run_from_argv(self.argv)
File ""/usr/local/lib/python3.5/site-packages/django/core/management/base.py"", line 390, in run_from_argv
self.execute(*args, **cmd_options)
File ""/usr/local/lib/python3.5/site-packages/django/core/management/base.py"", line 441, in execute
output = self.handle(*args, **options)
File ""/usr/local/lib/python3.5/site-packages/django_extensions/management/utils.py"", line 62, in inner
ret = func(self, *args, **kwargs)
File ""/usr/local/lib/python3.5/site-packages/django_extensions/management/commands/runserver_plus.py"", line 262, in handle
self.inner_run(options)
File ""/usr/local/lib/python3.5/site-packages/django_extensions/management/commands/runserver_plus.py"", line 305, in inner_run
self.check_migrations()
AttributeError: 'Command' object has no attribute 'check_migrations'",
PS:My django version is 1.8.2,
come on~ I need help~,Impatience
"Please look at; https://github.com/django-extensions/django-extensions#support
Ask politely and respectfully both for the people and their time.
`Django 1.8.2` is not supported anymore in `Django Extensions` and the `check_migrations` method is not available for commands in older `Django` versions. So this regretfully will not work.
Please upgrade your `Django` version.",
"Currently it is not possible to mount a filesystem to a mount point that contains blanks.
There are many problems in the ``states.mount.mounted`` code path, e.g. the mount command arguments are not quoted in the ``modules.mount.mount`` function, see https://github.com/saltstack/salt/blob/develop/salt/modules/mount.py#L1237. Code should look like this IMO:
```
cmd = 'mount {0} {1} {2} '.format(args, device, shlex.quote(name))
```
But this will fix only a small piece of the whole problem.
Another one is that ``states.mount.mounted`` does not detect correctly that the filesystem might be mounted already, i think it's because the key in the active table is not unquoted, so a comparison between
``/srv/dev-disk-by-label-My\040Passport\040Blue`` and the specified ``/srv/dev-disk-by-label-My Passport Blue`` fails.
To me it looks like the whole mount state and module is not able to handle blanks in device names and mount points properly.
Example SLS:
```
mount_fs_with_label:
mount.mounted:
- name: ""/srv/dev-disk-by-label-My Passport Blue""
- device: ""/dev/disk/by-label/My\\x20Passport\\x20Blue""
- fstype: ext4
- mkmnt: True
- persist: False
- mount: True
```
Result:
```
ID: mount_fs_with_label
Function: mount.mounted
Name: /srv/dev-disk-by-label-My Passport Blue
Result: False
Comment: mount: bad usage
Try 'mount --help' for more information.
Started: 08:31:10.286521
Duration: 181.307 ms
Changes: ```
```
# salt-call mount.active
...
/srv/dev-disk-by-label-My\040Passport\040Blue:
----------
alt_device:
None
device:
/dev/sda1
fstype:
ext4
opts:
- rw
- noexec
- relatime
- jqfmt=vfsv0
- usrjquota=aquota.user
- grpjquota=aquota.group
...
```
```
# ls -alh /dev/disk/by-label/
total 0
drwxr-xr-x 2 root root 60 Sep 17 08:29 .
drwxr-xr-x 7 root root 140 Sep 17 08:29 ..
lrwxrwxrwx 1 root root 10 Sep 17 08:29 'My\x20Passport\x20Blue' -> ../../sda1
```
```
# ls -alh /srv
total 28K
drwxr-xr-x 7 root root 4.0K Sep 17 08:07 .
drwxr-xr-x 21 root root 4.0K Sep 16 16:07 ..
drwxr-xr-x 4 root root 4.0K Sep 13 13:40 dev-disk-by-id-scsi-0QEMU_QEMU_HARDDISK_drive-scsi0-0-2-part1
drwxr-xr-x 2 root root 4.0K Sep 16 16:11 'dev-disk-by-label-My Passport Blue'
drwxr-xr-x 2 ftp nogroup 4.0K Sep 10 14:23 ftp
drwxr-xr-x 3 root root 4.0K Sep 16 16:07 pillar
drwxr-xr-x 5 root root 4.0K Sep 16 16:07 salt
```
```
# cat /etc/fstab
proc /proc proc defaults 0 0
UUID=90ee6298-385f-4841-bfdc-8b1e0e0ae5c1 / ext4 errors=remount-ro 0 1
# >>> [openmediavault]
/dev/disk/by-label/My\x20Passport\x20Blue	/srv/dev-disk-by-label-My\040Passport\040Blue	ext4	defaults,nofail,user_xattr,noexec,usrjquota=aquota.user,grpjquota=aquota.group,jqfmt=vfsv0,acl	0 2
# <<< [openmediavault]
```
```
# cat /proc/self/mountinfo
...
265 25 8:1 / /srv/dev-disk-by-label-My\040Passport\040Blue rw,noexec,relatime shared:148 - ext4 /dev/sda1 rw,jqfmt=vfsv0,usrjquota=aquota.user,grpjquota=aquota.group
...
```",Bitter Frustration
"Hello, @votdev I am trying to build myself home NAS with old Atom mini-ITX board... So i install OMV5, i plug in dad's old NTFS drive... and here we go...
Frankly, i wish Salt guys put the comments inside this source, listing all the bugs related to this module. So any hacker which for whatever reason would change it - would be instantly notifie on old pending bugs.
Salt seems extremely fragile here, probably no one else except for OMV5 uses it for partitions. Maybe OMV6 could do it outside Salt? Like good old UDEV rules or anything. I mean, before Salt porject might decide to drop this functionality that almost no one use, instead of burden of maintaining it for OMV5 alone....
Well, ranting aside, i am rather puzzled with your _device: ""/dev/disk/by-label/My\\x20Passport\\x20Blue""_
Where do you even get this hex substitution from???
Thing is, the whole mounting escaping is one uber-ancient legacy mess. Putting it here so maybe someone would use it. I spent like 3 hours googling around and experimenting with Python that i never used before. Tryied to google some standard about Posix/Linux/bash filename mangling/escaping.... and then Python module to undo it. To no avail.
Okay, so, to document it down.
- mtab/fstab and friends is one-of-a-kind ancient mess.
- it started with ancient BSD (not FreeBSD) function strunvis, which behaviour not documented. Probably that was OS-specific function (a la virtual methods). http://manpages.org/strunvis/3
- when Linux was mimicking good old BSd it only made ad hoc substitutions for 4 specific chars. There is no any systematic/generic pattern at all.
```
static inline void mangle(struct seq_file *m, const char *s)
{
seq_escape(m, s, "" \t\n\\"");
}
```
https://elixir.bootlin.com/linux/latest/source/fs/proc_namespace.c#L84
```
R(""\\"", '\\'),
R(""011"", '\t'),
R(""012"", '\n'),
R(""040"", ' '),
R(""134"", '\\')
```
https://sources.debian.org/src/sysvinit/2.96-6/src/fstab-decode.c/
So, whatever comes from Linux mounts information - should be de-mangled for those four special cases.
Every single space-separated column of every single line.
Ugly, and undocumented, but that is what it is. And, frankly, it is not that hard...
BUT, why do you want to compare with some arbitrary hex-escaped string? what can be a real use-case for that???
Linux kernel just does not have hex-escaping code for disk mounts.
Now, to be frank, even this would NOT be enough, because i can have multiple disks with the same partition label. Like many USB thumb drives with ""DATA"" partition. I can even have several partitions with the same name on singe disk!
Again, it can be fixed by detecting collisions and adding extra data, like counters or GUID or whatever, but...
What gonna OMV do if OMV's user has two drives with partitions having same labels, and then he hotplugs one disk, or another, or both in any order? Is it race condition now? Is it okay for OMV to have race condition?
Seems whatever use cases Salt imagined for them here is very different from what OMV users might face.",Bitter Frustration
"Output from Linux's mount
`/dev/sdb1 on /media/U:NTFS Disk type fuseblk (rw,relatime,user_id=0,group_id=0,allow_other,blksize=4096)`
Spaces are NOT escaped there!
Dunno how it is done on BSD/Darwin
And then we have this...
```
# salt-call mount.list_mounts
local:
----------
/:
/dev/sda1
......
/media/U:NTFS:
/dev/sdb1
/proc:
proc
........
```",Bitter Frustration
"@votdev re: escaping names for calling `mount` - i think that is what was intended to do so:
`""device"": device_name.replace(""\\040"", ""\\ ""),` inside `def _active_mountinfo()`
but that was only called when from `def active(extended=False)` then Extended is set to True, if ever
And similar code inside `def _resolve_user_group_names(opts):`
So it seems Salt prefers to keep space-containing names mangled, but mangled differently.
So, no escaping when calling `mount` or `umount` is needed,
---
I am not even sure that de-escaping mount point likes `xxx\040yyy` in Salt would be correct way to go.
There can be a point: since that module serves as abstraction layer and should hide UNIX-likes peculiarities from generic Salt modules, all IDs better be unmangled. But not sure. However IF to do this de-mangling, then quoting arguments for calling `mount` becomes required indeed.
But anyway, this line i believe should not had ended in /etc/fstab and whoever added it was at fault...
```
# >>> [openmediavault]
/dev/disk/by-label/My\x20Passport\x20Blue	```",Bitter Frustration
"@votdev i made quite many changes in that mounts.py - and now i am thinking about undoing almost all of them... Lack of any documentation...
I am coming to believe that, while never documented, the intention of that Salt module was to always use fstab-like escaped strings for all their IDs. If not, i would like to see specific calls into other Salt modules, which expect different convention for disk names.
I really did quite a number of changes to de-escape \040 and other special chars. And probably that was only breaking things.
Except for one place though, which i believe should be patched.
```
import pathlib def list_mounts(): # for debug
return _list_mounts()
def _list_mounts():
ret = {}
idx_mpoint = 2
# one cannot trust `mount` with space-containing paths
# at least on Linux - https://github.com/saltstack/salt/issues/54508
if __grains__[""kernel""] == ""Linux"":
idx_mpoint = 1
mounts = pathlib.Path('/proc/mounts').read_text()
elif __grains__[""os""] in [""MacOS"", ""Darwin""]:
mounts = __salt__[""cmd.run_stdout""](""mount"")
else:
mounts = __salt__[""cmd.run_stdout""](""mount -l"")
for line in mounts.split(""\n""):
comps = re.sub(r""\s+"", "" "", line).split()
if len(comps) > idx_mpoint:
## if __grains__[""kernel""] == ""Linux"":
## comps[0] = _Linux_fstab_unmangle( comps[0] )
## comps[idx_mpoint] = _Linux_fstab_unmangle( comps[idx_mpoint] )
ret[comps[idx_mpoint]] = comps[0]
return ret
```
Would you keep implementation non-patched and would you call `salt-call mount.list_mounts` - you would see the mount point broken, cut off on the first space. This was probably THE bug.
----------------
Okay, keeping my stolen Linux archeologist hat on
https://unix.stackexchange.com/questions/56291/what-causes-dev-disk-by-label-to-be-populated
```
mount -l .....
/dev/sdb1 on /media/U:NTFS Disk type fuseblk (rw,relatime,user_id=0,group_id=0,allow_other,blksize=4096) [U - Arch-2 Hitachi_2Tb_7200]
root@diskoteka:/media# ls /dev/disk/by-label/
'U\x20-\x20Arch-2\x20Hitachi_2Tb_7200'
root@diskoteka:/media# blkid -o udev -p /dev/sdb1
ID_FS_LABEL=U_-_Arch-2_Hitachi_2Tb_7200
ID_FS_LABEL_ENC=U\x20-\x20Arch-2\x20Hitachi_2Tb_7200
ID_FS_UUID=C6705D84705D7BDD
ID_FS_UUID_ENC=C6705D84705D7BDD
ID_FS_TYPE=ntfs
ID_FS_USAGE=filesystem
ID_PART_TABLE_TYPE=atari
ID_PART_ENTRY_SCHEME=dos
ID_PART_ENTRY_UUID=78fdd16a-01
ID_PART_ENTRY_TYPE=0x7
ID_PART_ENTRY_NUMBER=1
ID_PART_ENTRY_OFFSET=2048
ID_PART_ENTRY_SIZE=3907024896
ID_PART_ENTRY_DISK=8:16
```
So, it is UDEV or SYSTEMD which creates those weird hex-mangled names. Okay. Though putting them into /etc/fstab still feels wrong. `man mount` suggests against it and suggests using `UUID=...` and `LABEL=...` flags instead.
Now back to your
```
Example SLS:
mount_fs_with_label:
mount.mounted:
- name: ""/srv/dev-disk-by-label-My Passport Blue""
```
I don't know what it should mean in specific files/commands terms. But i feel this is the error on OMV part. And perhaps lack of documentation/understanding/forecasting on Salt part.
```
def mount(
name, device=False, mkmnt=False, fstype="""", opts=""defaults"", user=None, util=""mount""
):
.....
salt '*' mount.mount /mnt/foo /dev/sdz1 True
.....
if device:
cmd += ""{} {} {} "".format(args, device, name)
else:
cmd += ""{} "".format(name)
```
My inner archeologist says that the `name` AKA mount point AKA target directory is meant to be in bash-mangled format.
IOW OMV should had created ""\ "" containing fileneames:
```
mount.mounted:
- name: ""/srv/dev-disk-by-label-My\ Passport\ Blue""
```
Linux `man mount` also suggests against the second option due to ambiguity, where the single parameter is mount point name or device file name. I don't know if other UNIX-likes but Linux support those precision keys.
```
--source device
If only one argument for the mount command is given then the
argument might be interpreted as target (mountpoint) or source
(device). This option allows to explicitly define that the
argument is the mount source.
--target directory
If only one argument for the mount command is given then the
argument might be interpreted as target (mountpoint) or source
(device). This option allows to explicitly define that the
argument is the mount target.
```
So i think that part in `def mount` should better be written as
```
if device:
cmd += ""{} {} {} "".format(args, device, name)
else:
if __grains__[""kernel""] == ""Linux"":
cmd += ""--target ""
cmd += ""{} "".format(name)
```",Bitter Frustration
"@the-Arioch I don't think this is the right place to discuss OMV related things.
> Well, ranting aside, i am rather puzzled with your device: ""/dev/disk/by-label/My\x20Passport\x20Blue""
> Where do you even get this hex substitution from???
Escaping blanks is not my idea, it is used by every userland command that processes mount points, e.g. `mount`.
Either `systemd` want to have escaped paths in mount units too, there is a special command to convert
paths for you, see `systemd-escape`.
> Salt seems extremely fragile here, probably no one else except for OMV5 uses it for partitions. > Maybe OMV6 could do it outside Salt?
OMV already workarounds this issue, thus it is not affected by this reported issue here.
> I mean, before Salt porject might decide to drop this functionality that almost no one use, > instead of burden of maintaining it for OMV5 alone....
I don't think Salt will drop `mount.mounted` because it is a somewhat essential functionality of Linux systems.
> Now, to be frank, even this would NOT be enough, because i can have multiple disks with the > same partition label. Like many USB thumb drives with ""DATA"" partition. I can even have several > partitions with the same name on singe disk!
You can do that, but don't blame the software then. Using USB devices in a NAS is no good idea, but that's a different thing. IMO devices using in a NAS should be already connected to the NAS, no plug-and-play, this is not how a NAS is intended to work. If devices are always connected, then you will never run into the situation that duplicate labels might harm your system. This issue is user introduced and should be handled by them.
If you want to discuss this issue please open an issue in the OMV Git repository.",Impatience
"> So, it is UDEV or SYSTEMD which creates those weird hex-mangled names. Okay. Though putting them into /etc/fstab still feels wrong. I think it is ok to use systemd escaped paths in `/etc/fstab` since systemd handles filesystem mounting nowadays.",Bitter Frustration
"@votdev i meantioned systemd because of https://github.com/systemd/systemd/issues/12018
See... i know very little about Linux and nothing about Python, so i was googling everything i could think of :-)
But i am glad to hear from you. So, how can we scratch this itch, is Salt team is not with us on it...
Can you make some scripts demonstrting the alleged Salt bug that i could run from bash ? Also are there some hidden option in OMV5 to re-enable mounting space-containing partitions?",Impatience
"But i really am worried about potential race conditions in OMV when different partitions would have same label... IF you use label as ""primary key"" as persistent ID for all the other settings (user rights, sharing folders, etc), it might be quite a gotcha...",Bitter Frustration
"Some of the ""deep changes"" i mentioned above. I now think those are dead end, but just in case they would be useful to someone, maybe even us later.
Using ""Raw"" non-escaped string would probably be more proper design, but might really require deep refactoring of many Salt modules and then testing on many different systems. Horror...
```
def _Linux_fstab_unmangle(fs_str):
# rather ugly ad-hoc substitutions cosplaying ancient BSD's non-documented strunvis(...)
# https://sources.debian.org/src/sysvinit/2.96-6/src/fstab-decode.c/
# https://elixir.bootlin.com/linux/latest/source/fs/proc_namespace.c#L84
fs_str = fs_str.replace(r""\011"", ""\t"").replace(r""\012"", ""\n"")
fs_str = fs_str.replace(r""\040"", r"" "")
fs_str = fs_str.replace(r""\134"", ""\\"").replace(r""\\"", ""\\"")
return fs_str
def _Str_Nop(text):
return str(text)
def _fsfilter():
if __grains__[""kernel""] == ""Linux"":
return _Linux_fstab_unmangle
return _Str_Nop
```
and then
```
def _list_mounts():
ret = {}
idx_mpoint = 2
# one cannot trust `mount` with space-containing paths
# at least on Linux - https://github.com/saltstack/salt/issues/54508
if __grains__[""kernel""] == ""Linux"":
idx_mpoint = 1
mounts = pathlib.Path('/proc/mounts').read_text()
elif __grains__[""os""] in [""MacOS"", ""Darwin""]:
mounts = __salt__[""cmd.run_stdout""](""mount"")
else:
mounts = __salt__[""cmd.run_stdout""](""mount -l"")
for line in mounts.split(""\n""):
comps = re.sub(r""\s+"", "" "", line).split()
if len(comps) > idx_mpoint:
if __grains__[""kernel""] == ""Linux"":
comps[0] = _Linux_fstab_unmangle( comps[0] )
comps[idx_mpoint] = _Linux_fstab_unmangle( comps[idx_mpoint] )
ret[comps[idx_mpoint]] = comps[0]
return ret
def _active_mountinfo_linux(ret):
_list = _list_mounts()
_fi = _fsfilter()
filename = ""/proc/self/mountinfo""
if not os.access(filename, os.R_OK):
msg = ""File not readable {0}""
raise CommandExecutionError(msg.format(filename))
if ""disk.blkid"" not in __context__:
__context__[""disk.blkid""] = __salt__[""disk.blkid""]()
blkid_info = __context__[""disk.blkid""]
with salt.utils.files.fopen(filename) as ifile:
for line in ifile:
comps = salt.utils.stringutils.to_unicode(line).split()
device = comps[2].split("":"")
# each line can have any number of
# optional parameters, we use the
# location of the separator field to
# determine the location of the elements
# after it.
_sep = comps.index(""-"")
device_name = _fi(comps[_sep + 2])
device_uuid = None
device_label = None
if device_name:
device_uuid = blkid_info.get(device_name, {}).get(""UUID"")
device_uuid = device_uuid and device_uuid.lower()
device_label = blkid_info.get(device_name, {}).get(""LABEL"")
ret[_fi(comps[4])] = {
""mountid"": comps[0],
""parentid"": comps[1],
""major"": device[0],
""minor"": device[1],
""root"": _fi(comps[3]),
""opts"": _resolve_user_group_names(comps[5].split("","")),
""fstype"": comps[_sep + 1],
""device"": device_name, ## .replace(""\\040"", ""\\ ""),
""alt_device"": _list.get(_fi(comps[4]), None),
""superopts"": _resolve_user_group_names(comps[_sep + 3].split("","")),
""device_uuid"": device_uuid,
""device_label"": device_label,
}
return ret
def _active_mounts_linux(ret):
""""""
List active mounts on Linux systems
""""""
_list = _list_mounts()
_fi = _fsfilter()
filename = ""/proc/self/mounts""
if not os.access(filename, os.R_OK):
msg = ""File not readable {0}""
raise CommandExecutionError(msg.format(filename))
with salt.utils.files.fopen(filename) as ifile:
for line in ifile:
comps = salt.utils.stringutils.to_unicode(line).split()
ret[_fi(comps[1])] = {
""device"": _fi(comps[0]),
""alt_device"": _list.get(_fi(comps[1]), None),
""fstype"": comps[2],
""opts"": _resolve_user_group_names(comps[3].split("","")),
}
return ret
```",Bitter Frustration
"@votdev > OMV already workarounds this issue
by failing to mount the disk? because i can not mount disk in OMV5 or i would never learn about this issue.
failing to mount disk does not look like work-around at all.
let's think what we can do to make space-containing partitions mounted by OMV. It seems to be a kind of ""communication breakdown"" between Salt and OMV5, they expect and provide for mututally incompatible things.
here is minimally patched /usr/lib/python3/dist-packages/salt/modules/mount.py [mount.py.gz](https://github.com/saltstack/salt/files/6217180/mount.py.gz)
it makes space-containing mount point visible. If there still is something not working - i can not see what it is and how could i test it using `salt-call` scripts
```
# salt-call mount.list_mounts
local:
----------
/:
/dev/sda1
/dev:
udev
/dev/hugepages:
hugetlbfs
/dev/mqueue:
mqueue
/dev/pts:
devpts
/dev/shm:
tmpfs
/media/U:NTFS\040Disk:
/dev/sdb1
/proc:
proc
......
# salt-call mount.active
....
/media/U:NTFS\040Disk:
----------
alt_device:
/dev/sdb1
device:
/dev/sdb1
fstype:
fuseblk
opts:
- rw
- relatime
- user_id=0
- group_id=0
- allow_other
- blksize=4096
..........
# salt-call mount.active extended=true
......
/media/U:NTFS\040Disk:
----------
alt_device:
/dev/sdb1
device:
/dev/sdb1
device_label:
U - Arch-2 Hitachi_2Tb_7200
device_uuid:
c6705d84705d7bdd
fstype:
fuseblk
major:
8
minor:
17
mountid:
427
opts:
- rw
- relatime
parentid:
26
root:
/
superopts:
- rw
- user_id=0
- group_id=0
- allow_other
- blksize=4096
```
and also
```
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS Disk""
local:
False
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS\ Disk""
local:
False
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS\040Disk""local:
True
```",Bitter Frustration
"And now the most curious thing to me. I changed the partition label, then i mounted the disk from OMV Web UI and...
....and there is no any space-containing mountpoint path regardless of partition label.
The ""workaround"" seems to needlessly shoot down the perfectly working function!
Maybe it is only with MBR/NTFS disks, maybe GPT or XFS disks would use something else in `fstab`, dunno
```
# mount -l
....
/dev/sdb1 on /srv/dev-disk-by-uuid-C6705D84705D7BDD type fuseblk (rw,relatime,user_id=0,group_id=0,allow_other,blksize=4096) [U_-_Arch-2_Hitachi_2Tb_7200]
```
and
```
# salt-call mount.active
. . . .
/srv/dev-disk-by-uuid-C6705D84705D7BDD:
----------
alt_device:
/dev/sdb1
device:
/dev/sdb1
fstype:
fuseblk
opts:
- rw
- relatime
- user_id=0
- group_id=0
- allow_other
- blksize=4096
/sys:
----------
alt_device:
sysfs
```",Bitter Frustration
"@votdev it is sad how fast you were to say Salt is all wrong and how protective you fet about OMV.
You still try to push Salt to adhere to OMV data format, while common sense says it should be otherwise.
Salt users would not suffer from it. OMV users would.
Demanding PR from OMV users like me is funny when you did not make any PR to Salt, or maybe i am wrong and you did.
So, back to:
https://github.com/openmediavault/openmediavault/issues/566#issuecomment-809541057
The intention was and is to make OMV work with disks users insert. Without forcing them to go ssh sudo. So simple.
You make it look that making OMV ""just work"" is bad goal. > Why the hell should escapeshellarg be called here?
Because that woul be consistent with bash/Salt data format. But i alreeady said it was kneejerk impulse, so you eems to be crashing through door wide open.
> The function is doing exactly what you are suggesting, keep data raw/verbatim/unescaped within OMV
Some we are on the same page here. You blaze of ego is called for.
Since eysterday i was asking you to show me at the se
ems between OMV and Salt, the exact borderleines, didn't i?
I am glad you seem to did so above, https://github.com/openmediavault/openmediavault/issues/566#issuecomment-809529126
And when i showed those links, i commented upon them.
`Salt is based on Python, not PHP. The code you're ranting about never runs in the Salt context.`
I never said so. Both Salt and OMV are ""black boxes"" with some data exchange. And i was asking you to point me to the raw places of exchange and raw data being exchanged, didn't i?
Yesterday i spent hours looking into Salt code and patching it along your suggestions.
First i took your suggestions as correct and thought through. And just followed them. An then had to undo it all.
Now you imply it was your time wasted not mine.
That `Example SLS:` - many times from yesterday i asked you how can i reproduice this activity from bash command line.
For example above - https://github.com/saltstack/salt/issues/54508#issuecomment-808875885
You kind of answered by showing PHP code for SLS generation - after many requests and hours.
But you still not answered how to trigger that action from bash.
I asked you yesterday how to make OMV code trigger that action of Salt, allegedly buggy Salt.
And you refused to help me doing it.
https://github.com/openmediavault/openmediavault/issues/566#issuecomment-808955077
```
What can i patch in OMV5 to make this notification gone?
Why do you want to know that? What do you expect to improve?
```
You made me look into Linux kernel i am not familiar with, at the same time you are not very willing to point me to specific OMV code and Salt commands you are familiar with.
You are blocking any attempt to debug OMV and Salt interaction - and you demand perfectly polished PRs. It is not consistent. And it is would not help anyone. Not me, not you, not OMV users.",Bitter Frustration
"@garethgreenaway @waynew @sagetherage Please consider this fix to `Salt` above
> here is minimally patched /usr/lib/python3/dist-packages/salt/modules/mount.py
> mount.py.gz
https://github.com/saltstack/salt/issues/54508#issuecomment-808881089
That is a clear bug in `Salt` that can be reproduced on Linux box (and probably on other UNIX-likes) independently on OMV",Bitter Frustration
"@the-Arioch Please stop blaming and ranting me. This raised issue here has nothing to do with OMV.
@garethgreenaway please set this issue to read-only, I had to do the same on the OMV issues to stop these rants.",Bitter Frustration
"When a client is stopped, the tasks on that client are left running. When the client restarts, it goes through a restore process to get handles to all its tasks again. If a task fails or is removed while the client is shutdown, the client should be able to garbage collect any of its dangling resources (like alloc dirs) and restart the task. This is not happening with iptables.
Fortunately we ""tag"" all the iptables rules in one of two ways:
- Placing them in a chain named `CNI-xxxx`/`CNI-DN-xxxx`. I don't know what that `xxxx` is but it's not the alloc ID, container ID, or network namespace ID.
- Adding a comment in the form ` /* name: ""nomad"" id: ""<alloc ID>"" *`.
So it should be possible to identify ""Nomad owned"" rules and clean them up if they don't belong to an allocation we know about if we can figure out the naming for the CNI chains.
### Nomad version
```
Nomad v0.10.0-dev (e2761807a346c5e3afd577b7994cfc788700bb15+CHANGES)
```
(But probably any recent version.)
### Reproduction steps
1. Run Nomad under systemd.
2. Run our Consul Connect demo job: `nomad job run ./e2e/connect/input/demo.nomad`
3. Stop the job: `nomad job stop countdash`
4. Observe that the tasks and iptables are cleaned up properly.
- `docker ps`
- `sudo iptables -t nat -L -v -n`
5. Run the job again: `nomad job run ./e2e/connect/input/demo.nomad`
6. Stop the Nomad client with `sudo systemctl stop nomad`.
7. Observe that the tasks and iptables are still in place.
- `docker ps`
- `sudo iptables -t nat -L -v -n`
8. Remove the tasks: `docker rm -f $(docker ps -a)`
9. Restart Nomad: `sudo systemctl start nomad`
10. Observe that the tasks are started: `docker ps`
11. Stop the job cleanly: `nomad job stop countdash`
12. Observe that iptables are left behind: `sudo iptables -t nat -L -v -n`
### Logs
<details><summary>iptables after repro steps</summary>
```
vagrant@linux:/opt/gopath/src/github.com/hashicorp/nomad$ sudo iptables -t nat -L -v --line-numbers -n
Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes)
num pkts bytes target prot opt in out source destination
1 29 1276 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
2 20 880 CNI-HOSTPORT-DNAT all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
Chain INPUT (policy ACCEPT 0 packets, 0 bytes)
num pkts bytes target prot opt in out source destination
Chain OUTPUT (policy ACCEPT 6 packets, 360 bytes)
num pkts bytes target prot opt in out source destination
1 5 300 DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL
2 279 16740 CNI-HOSTPORT-DNAT all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
Chain POSTROUTING (policy ACCEPT 6 packets, 360 bytes)
num pkts bytes target prot opt in out source destination
1 348 20568 CNI-HOSTPORT-MASQ all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI portfwd requiring masquerade */
2 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0
3 0 0 CNI-6fcd2f53d5f720ec4eb5f04d all -- * * 172.26.64.102 0.0.0.0/0 /* name: ""nomad"" id: ""3e803d29-4f9d-ad8b-adb6-31456a39db69"" */
4 0 0 CNI-06d73cb6cdf7130196e2018a all -- * * 172.26.64.101 0.0.0.0/0 /* name: ""nomad"" id: ""ee25f5d7-dcb9-b336-fe3e-27e365aa5cd0"" */
Chain CNI-06d73cb6cdf7130196e2018a (1 references)
num pkts bytes target prot opt in out source destination
1 0 0 ACCEPT all -- * * 0.0.0.0/0 172.26.64.0/20 /* name: ""nomad"" id: ""ee25f5d7-dcb9-b336-fe3e-27e365aa5cd0"" */
2 0 0 MASQUERADE all -- * * 0.0.0.0/0 !224.0.0.0/4 /* name: ""nomad"" id: ""ee25f5d7-dcb9-b336-fe3e-27e365aa5cd0"" */
Chain CNI-6fcd2f53d5f720ec4eb5f04d (1 references)
num pkts bytes target prot opt in out source destination
1 0 0 ACCEPT all -- * * 0.0.0.0/0 172.26.64.0/20 /* name: ""nomad"" id: ""3e803d29-4f9d-ad8b-adb6-31456a39db69"" */
2 0 0 MASQUERADE all -- * * 0.0.0.0/0 !224.0.0.0/4 /* name: ""nomad"" id: ""3e803d29-4f9d-ad8b-adb6-31456a39db69"" */
Chain CNI-HOSTPORT-DNAT (2 references)
num pkts bytes target prot opt in out source destination
Chain CNI-HOSTPORT-MASQ (1 references)
num pkts bytes target prot opt in out source destination
1 59 3540 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 mark match 0x2000/0x2000
Chain CNI-HOSTPORT-SETMARK (0 references)
num pkts bytes target prot opt in out source destination
1 59 3540 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI portfwd masquerade mark */ MARK or 0x2000
Chain DOCKER (2 references)
num pkts bytes target prot opt in out source destination
1 0 0 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0
```
</details>
cc @davemay99 @angrycub as a heads up",
"Summary of the investigation at this point:
* When the client restarts, the network hook's `Prerun` fires and tries to recreate the network and setup the iptables via CNI.
* This fails because the netns already exists, as expected. So we tear down the task and start over.
* But in the next pass when we setup the iptables via CNI, we collide with the iptables left behind.
* To fix this we need the network namespace path (which is used by CNI as part of the handle for [`go-cni#Network.Remove`](https://godoc.org/github.com/containerd/go-cni#Network.Remove)).
* In the non-Docker case, Nomad controls the path to the netns and it's derived from the alloc ID, but in the Docker case (which includes all cases with Connect integration b/c of the Envoy container), Docker owns that path and derives it from the pause container name. So we can't use a deterministic name as a handle to clean up. But we can't get it from Docker either because at the point we need it that container has already been removed.
I've verified the following more common failure modes are handled correctly:
* Tasks recover fully when the client restarts (after a few PRs we landed in the current 0.10.0 release branch)
* There's no resource leak when the client restarts if the containers aren't removed.
* There's no resource leak when the client restarts as part of a node (machine) reboot.
Status:
* We could try to fix this by threading state about the network namespace from the allocation runner back into the state store, similar to how we deal with deployment health state. But this will always be subject to races between client failures and state syncs.
* We already have a PR open for 0.10.x to reconcile and GC Docker containers. Because all the rules we're creating are tagged with the string ""nomad"" and Nomad's alloc IDs, we can make a similar loop for iptable GC.
* Because I've verified that this leak doesn't happen in the common failure modes of a client or node reboot, we're not going to block the 0.10.0 release on this. We'll work up a PR for an out-of-band reconcile loop for 0.10.x.
Moving this issue out of the 0.10.0 milestone.",
"Noting this _isn't_ the same as #7537 - the repro steps here still leak rules, e.g.
<details>
<summary>after.txt</summary>
```
Chain PREROUTING (policy ACCEPT 12 packets, 5781 bytes)
pkts bytes target prot opt in out source destination 154 74057 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
118 56714 CNI-HOSTPORT-DNAT all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
118 56714 CNI-HOSTPORT-DNAT all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
Chain INPUT (policy ACCEPT 12 packets, 5781 bytes)
pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 32 packets, 3515 bytes)
pkts bytes target prot opt in out source destination 463 31262 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
264 17528 CNI-HOSTPORT-DNAT all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
245 16388 CNI-HOSTPORT-DNAT all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL
Chain POSTROUTING (policy ACCEPT 32 packets, 3515 bytes)
pkts bytes target prot opt in out source destination 448 40004 CNI-HOSTPORT-MASQ all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI portfwd requiring masquerade */
429 38864 CNI-HOSTPORT-MASQ all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI portfwd requiring masquerade */
10 720 MASQUERADE all -- * docker0 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match src-type LOCAL
0 0 MASQUERADE all -- * !docker0 172.30.254.0/24 0.0.0.0/0 0 0 CNI-11bedb6da1593ed3af43ef13 all -- * * 172.26.65.117 0.0.0.0/0 /* name: ""nomad"" id: ""2c0c32b5-2ea3-471f-378f-a740391bea60"" */
0 0 CNI-b5b13e7bdc4638b22a8a6e73 all -- * * 172.26.65.118 0.0.0.0/0 /* name: ""nomad"" id: ""6eb897ef-4de8-9a2c-22cc-5965fe282b19"" */
Chain CNI-11bedb6da1593ed3af43ef13 (1 references)
pkts bytes target prot opt in out source destination 0 0 ACCEPT all -- * * 0.0.0.0/0 172.26.64.0/20 /* name: ""nomad"" id: ""2c0c32b5-2ea3-471f-378f-a740391bea60"" */
0 0 MASQUERADE all -- * * 0.0.0.0/0 !224.0.0.0/4 /* name: ""nomad"" id: ""2c0c32b5-2ea3-471f-378f-a740391bea60"" */
Chain CNI-HOSTPORT-DNAT (4 references)
pkts bytes target prot opt in out source destination Chain CNI-HOSTPORT-MASQ (2 references)
pkts bytes target prot opt in out source destination 19 1140 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 mark match 0x2000/0x2000
0 0 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 mark match 0x2000/0x2000
Chain CNI-HOSTPORT-SETMARK (0 references)
pkts bytes target prot opt in out source destination 19 1140 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI portfwd masquerade mark */ MARK or 0x2000
19 1140 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI portfwd masquerade mark */ MARK or 0x2000
Chain CNI-b5b13e7bdc4638b22a8a6e73 (1 references)
pkts bytes target prot opt in out source destination 0 0 ACCEPT all -- * * 0.0.0.0/0 172.26.64.0/20 /* name: ""nomad"" id: ""6eb897ef-4de8-9a2c-22cc-5965fe282b19"" */
0 0 MASQUERADE all -- * * 0.0.0.0/0 !224.0.0.0/4 /* name: ""nomad"" id: ""6eb897ef-4de8-9a2c-22cc-5965fe282b19"" */
Chain DOCKER (2 references)
pkts bytes target prot opt in out source destination ```
</details>",
"We have a similar issue - iptables is a mess after some Nomad restarts.
We're on Nomad 1.0.4",
"We have hit this too. Having a few stale iptables rules is fine until new allocations are assigned with ports used by the stale rules.
```
CNI-DN-b545c28573a241d01dadd tcp -- 0.0.0.0/0 0.0.0.0/0 /* dnat name: ""nomad"" id: ""416a88b0-76fa-e270-6c80-7f102216ca13"" */ multiport dports 23674,23642
CNI-DN-b545c28573a241d01dadd udp -- 0.0.0.0/0 0.0.0.0/0 /* dnat name: ""nomad"" id: ""416a88b0-76fa-e270-6c80-7f102216ca13"" */ multiport dports 23674,23642
CNI-DN-7833d1be7094c0db0c99d tcp -- 0.0.0.0/0 0.0.0.0/0 /* dnat name: ""nomad"" id: ""aace5b7e-8e97-43d4-5705-06a94f326aee"" */ multiport dports 23674,22463,23641,21075
CNI-DN-7833d1be7094c0db0c99d udp -- 0.0.0.0/0 0.0.0.0/0 /* dnat name: ""nomad"" id: ""aace5b7e-8e97-43d4-5705-06a94f326aee"" */ multiport dports 23674,22463,23641,21075
```
In the above example, `416a88b0-76fa-e270-6c80-7f102216ca13` was removed while `aace5b7e-8e97-43d4-5705-06a94f326aee` is a new allocation. Clients of the service will get an error `Unable to establish connection to 10.133.67.138:23674` while trying to send requests to the running allocation. Even though we almost always drain nodes before manual nomad restart, sometimes nomad gets restarted automatically by systemd due to consul failure or other reasons, in which case we'd end up having stale iptables rules. So it'd be great if the iptables rule reconciliation can be implemented. ðŸ™ ðŸ¤ž",Bitter Frustration
"I'm seeing a slightly different trigger of this issue, but with the same root cause and end results. We use puppet to manage some iptables rules on the host[0]. When puppet makes a ruleset change, it triggers the new ruleset to be persisted (on EL, that's via iptables-save to `/etc/sysconfig/iptables`). This saved ruleset includes all the permanent rules we're managing via puppet, but also all the ""transient"" rules installed by nomad/cni plugins. Therefore the next time the ruleset is loaded (e.g. after host reboot), the iptables chains are pre-filled with stale rules from historic tasks. Currently nothing is cleaning those up, and since nomad is appending new rules, the saved rules are higher up in the NAT chains. This is causing particular pain where our ingress containers which listen on static ports 80, 443 get caught in the cached ruleset, because then after a reboot, the NAT rules redirect the traffic to a blackhole instead of the running ingress container. It also affects tasks that don't use static port numbers, but then the pain is deferred to when the port eventually gets reused and is harder to track down.
This is on nomad 1.1.5.
[0] One of the rules we're managing is `-A FORWARD -m conntrack --ctstate INVALID -j DROP` to drop packets which the conntrack module thinks are not valid, will not apply NAT. The kernel treats these as unsolicited packets, returning a TCP-RST. This tears down the connection between the container and the external service, causing disruption. There seem to be quite a few bug reports about this relating to docker, kubernetes etc, and this rule is the widely accepted workaround.",
"I've just re-read the upgrade guide for (in preparation for 1.2.0), and I think the changes in that 1.1.0 to append the CNI rules rather than insert at the top of the chain is what made this issue more noticeable (https://github.com/hashicorp/nomad/pull/10181). Previously, had transient rules been persisted, the next time an alloc was started, the new iptables rules would be inserted above the stale ones and thus take precedence. Now they are added below the stale rules, so traffic is matched and blackholed by the stale rules.",
"As noted in #11901, this affects us quite badly right now (though we're using nftables as opposed to iptables, the issue and result is the same). Whenever an unclean stop or agent restart has occurred for a job with static ports, those ports will (silently, no errors, local checks seem to succeed) fail to bind again until reboot or stale rules are manually removed.
While at first glance it looked like this was a regression caused by the priority-inversion in #10181 (as noted by @optiz0r ), that PR looks concerned only with the with `NOMAD-ADMIN` chain while in our case the issue is with stale rules blackholing dports under the `CNI-HOSTPORT-DNAT` chain (or maybe they're indeed the same after CNI does its magic?).",Bitter Frustration
"This is becoming a major security and stability issue as we are seeing allocations try to forward from ports that already have rules in iptables, and requests bound for them are getting forwarded based on the stale iptables rule.
Is there anything we can do to ensure this gets prioritized? Or can someone share a cleanup script they have been using?
Here is the error log around the time it fails to cleanup the stale allocation:
<details>
<summary>Logs</summary>
```
containerd[3148]: time=""2022-05-24T07:05:43.326040572Z"" level=info msg=""shim disconnected"" id=d13e150a783b3a72482e859901590f7d002b71c43c36ffe2f0d46aecca64e794
containerd[3148]: time=""2022-05-24T07:05:43.326107477Z"" level=error msg=""copy shim log"" error=""read /proc/self/fd/17: file already closed""
dockerd[3364]: time=""2022-05-24T07:05:43.326090953Z"" level=info msg=""ignoring event"" container=d13e150a783b3a72482e859901590f7d002b71c43c36ffe2f0d46aecca64e794 module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
consul[3318]: 2022-05-24T07:05:43.342Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.342Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.349Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.349Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.380Z [WARN] agent: Failed to deregister service: service=_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083-sidecar-proxy error=""Service ""_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>
consul[3318]: 2022-05-24T07:05:43.380Z [WARN] agent: Failed to deregister service: service=_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083-sidecar-proxy error=""Service ""_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>
kernel: docker0: port 1(vethc6029f5) entered disabled state
kernel: vethb5b69c3: renamed from eth0
kernel: docker0: port 1(vethc6029f5) entered disabled state
kernel: device vethc6029f5 left promiscuous mode
kernel: docker0: port 1(vethc6029f5) entered disabled state
systemd[1]: Stopping Nomad...
```
</details>",Impatience
"Hit this same issue today. Performed some manual iptables clean up on a problem client. Here are my notes in case this helps anyone else:
* Seems likely it was caused by some combination of quick nomad job allocation stop and re-deployment and/or nomad systemd service restarts, possibly before cleanup of a stopped allocation could be cleaned up.
* Symptom first noticed indicitive of a problem was of failed consul healthcheck, where the port that should be the listener on the host level and is properly bridged into the container as seen from the Nomad UI and nomad job config just isn't working.
* Tcpdump on the client machine to the port listener which doesn't appear to work shows SYN packet sent to a nomad bridge IP (`[S]`) and a RESET packet immediately returned (`[R.]`)
* Looking at the tcpdump output shows that the packet is actually being sent to the wrong nomad bridge IP, and further looking at the IP tables, shows that there are duplicate (or more) rules set up for port forwarding the listener into the Nomad bridge due to unclean allocation handling of an old allocation.
* On our OS, cleaning up the iptables can be done with a client reboot, but cleaning up by hand in these occurrences can be also be done.
* General procedure -- check iptables (command refs below). There will be iptables entries which show comments of allocation ID association. Check for non-existent allocation IDs being present and/or conflicting with existing allocation iptables rules. If such allocation IDs are seen, they will also be seen to be associated with user defined iptables chains starting with CNI-[a-f0-9] and CNI-DN-[a-f0-9]. These can all be purged with the example cmds below:
```
# In this case, old rules from a nomad bridge IP with no active allocation superseded the correct rules to a nomad bridge IP with an active allocation listener.
# Find references to obsolete iptables rules with missing allocation IDs (first cmds contains all info, addnl cmds are a bit more verbose)
iptables-save
iptables -t filter -L -v -n
iptables -t nat -L -v -n
# Delete rules specific to the bad allocation IP from the filter and NAT tables
iptables -t filter -D CNI-FORWARD -d 172.26.66.2/32 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
iptables -t filter -D CNI-FORWARD -s 172.26.66.2/32 -j ACCEPT
# Delete references to the user defined chains of the non-existent allocation from the filter and NAT tables
iptables -t nat -D POSTROUTING -s 172.26.66.2/32 -m comment --comment ""name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -j CNI-fe647aec064bf60036a312df
iptables -t nat -D CNI-HOSTPORT-DNAT -p tcp -m comment --comment ""dnat name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -m multiport --dports 8008,5432 -j CNI-DN-fe647aec064bf60036a31
iptables -t nat -D CNI-HOSTPORT-DNAT -p udp -m comment --comment ""dnat name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -m multiport --dports 8008,5432 -j CNI-DN-fe647aec064bf60036a31
# Delete rules from user defined chains of the non-existent allocation
iptables -t nat -F CNI-DN-fe647aec064bf60036a31
iptables -t nat -F CNI-fe647aec064bf60036a312df
# Delete the user defined chains from the non-existent allocation
iptables -t nat -X CNI-DN-fe647aec064bf60036a31
iptables -t nat -X CNI-fe647aec064bf60036a312df
```",
Any update on this issue? Facing it and it's causing very annoying stability issues on a select few hosts.,Impatience
"Please fix this or offer a proper solution, I don't care if we have to run a script to do it, but something that can be automated would be nice. We've positioned our whole infrastructure on Nomad, and this is killing us. We would prefer not to jump ship, but I'm still concerned how this isn't affecting other users?",Bitter Frustration
Affects us as well,
"Hey folks, we update issues when we're working on them. I can say this is on our roadmap but I can't really give a timeline.",
What are we supposed to do in the meantime?,Impatience
This issue is pretty devastating for the use case at my PoB. Is there a workaround that can be implemented until an official fix comes out? At the moment we have to do full reboots of Nomad and take our whole network offline when we run into it.,Bitter Frustration
@johnalotoski has posted a process above; if you were to run that as a periodic task (or just a cron job) that'd clean up the iptables.,
"Little bit difficult to script around that honestly, since you're also having to compare to what allocations exist, and what host they are on",
"Okay, we're moving on from this, we can't support our org with this",Bitter Frustration
"Hello, thanks for your help over the weekend (3025), you were right - there was 2 SSLs installed on machine. I have re-build platform (Slackware 14.1) and installed MySQL 5.0.24 from source, for freeradius 3,0.19 attempted built form source with configure and make errors. I attache configure and make logs. [configure.log](https://github.com/FreeRADIUS/freeradius-server/files/3695913/configure.log)
[make.log](https://github.com/FreeRADIUS/freeradius-server/files/3695914/make.log)
[config.log](https://github.com/FreeRADIUS/freeradius-server/files/3695915/config.log)
I do not know enough about freeradius yet to interpret logs. Thanks for your help.
Bev Clues",
"Questions belong on the freeradius-users mailing list. As the github template makes clear, github issues are only for actual issues.",
"Dear Alan, Thank you for your response all duly noted. I would like to put my point
of view whilst accepting that you can run your team and policy exactly
as you wish. For me I have to decide whether or not to join in. I am a
software developer with around 50 years of experience in small and large
projects. I am currently working on a secure email system which will
have a radius server as part of its back office services. For me, I
thought we had started a dialogue with my first problem, I acted on your
advice, reloaded linux, installed myql, tried to build freeradius and
there was a different kind of failure. In my view a failure of this kind
is usually a failure in the software, either configuration or build. As
such it was right to refer it back to the development team rather than
post to a users mailing list. I will not be doing this. I will fix the
problem myself or go to a different package. I will detach now. Best wishes with your excellent freeradius which I first found out about
in the O'Reilly book ""RADIUS"". Now on with my life. Bev
On 2019-10-07 12:25, Alan DeKok wrote:
> Questions belong on the freeradius-users mailing list. As the github template makes clear, github issues are only for actual issues. > > --
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub [1], or mute the thread [2].
Links:
------
[1]
https://github.com/FreeRADIUS/freeradius-server/issues/3029?email_source=notifications&amp;email_token=AHVY34RFLLE3KIJGSYBGJELQNMMEPA5CNFSM4I6ATUL2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAP6RDA#issuecomment-538962060
[2]
https://github.com/notifications/unsubscribe-auth/AHVY34XM44M2ULDOXSP5CFLQNMMEPANCNFSM4I6ATULQ",Bitter Frustration
"Please take a look at scripts/docker and you will see some Linux distributions. well, the FreeRADIUS work/build pretty well among of them.
Think about to use another Linux distributions with a better package manager (Debian, Ubuntu, Centos) that will help you to don't waste time fixing basic dependency issues.
Btw, It's 2019! why use a awful thing like Slackware? I've stopped to use _that_ in the Slackware 7.1 and all my memories are about a poor internal package manager.
think about that.",Bitter Frustration
"It's really quite simple. Your original issue was because you installed OpenSSL 0.9.7. A version which was first released in 2002. If you read _any_ of the documentation about that release, it all says ""do not use it or install it"". That issue was compounded by being unable to diagnose simple build issues. I was able to diagnose the issue with minimal information. Because that issue is extremely common, and trivial to debug and fix.
The next issue was to open another GitHub issue about more build errors. Not only that, but the logs you posted above were *truncated*. i.e. they showed absolutely nothing about any build errors. This failure made the issue entirely useless. It amounts to you opening the issue and saying _Stuff went wrong, but I'm not going to tell you what went wrong. Please fix it._
I expect more from someone with ""50 years of experience in small and large projects"".
The final issue is complaining:
> In my view a failure of this kind is usually a failure in the software, either configuration or build. Maybe. But since you failed to provide a build log which contained *errors*, it's impossible to tell.
> As such it was right to refer it back to the development team rather than post to a users mailing list. i.e. despite being told otherwise by the developers, you still think you're right.
Part of being a software developer is learning from experience. I suggest trying it. It's very handy!",Bitter Frustration
"I'm not sure that you'll like it, but I like to split code into dependencies instead of duplicating code in projects.
I've mentioned this change in https://github.com/ruby-i18n/i18n/pull/424#issuecomment-406851149.",
Thanks for the patch but I would rather keep the dependencies simple for this gem.,
"> Thanks for the patch but I would rather keep the dependencies simple for this gem.
Can you somehow to argument this point? It looks like ""Thank you, I'll not use I18n (or ActiveSupport, or Ruby on Rails, or Ruby), I'll write and maintain my own code instead"".",Mocking
I got out of OSS originally because of such petty arguments. Can we not?,Bitter Frustration
"> I got out of OSS
When? I see pretty green GitHub table, but it can't be filtered only with OSS.
> originally because of such petty arguments.
Can I look at discussions? I can read your opinion without repeating.
> Can we not? Yes.",Bitter Frustration
The table is green on Monday to Friday mainly. Hmmm perhaps I use GitHub for work?,Irony
"> Hmmm perhaps I use GitHub for work?
Probably, I didn't exclude this, but this is not an answer for any of two my questions. OK.",Impatience
"I have a control that's size is 128 wide and 16 tall. When I subscribe to the PointerPressed event, I am getting events from both x=128 and y=16 (which are outside of the control). Any X and Y coords larger do not fire the event. The event also will not fire if there is a different control directly next to this control. For example, if I have another control to the right of this control, then events with x=128 will not be sent to this control.",
"Found the problem: https://github.com/AvaloniaUI/Avalonia/blob/3b38aea2b9a5ddc4a15cadff7ba74c658c4ba2af/src/Avalonia.Visuals/Rect.cs#L228
I'll open a quick PR to fix it",
Just tried this on WPF and it's the same there: a button can be clicked one pixel to the right of its actual bounds.,Bitter Frustration
That's got to be a bug in WPF. There shouldn't be a goal to copy that functionality over to Avalonia when it just doesn't make sense.,Bitter Frustration
"> That's got to be a bug in WPF. There shouldn't be a goal to copy that functionality over to Avalonia when it just doesn't make sense.
Yes. Not to be rude but I have stopped all of my app development with Avalonia simply because there is basic functionality that remains broken for years because new features or ""compatibility"" is favored over deprecating or changing things for the better. I like Microsoft, and their acquisition of things usually makes things better, but this workflow I'm describing benefits nobody and it just stalls development for everyone for years.
Most of my bugs were filed in 2017-2019 and they still remain for the most part. I didn't even make reports for everything because they are so infuriating, or making a reproduction is just so time-consuming, or the issue is SO BASIC (ComboBoxes drive me insane in this) that I'm like ""surely they will find and fix this without my report"" so I just gave up. Whenever I NEED to update my stuff I end up spending the entire day diagnosing things that broke when updating (so the compatibility argument makes literally no sense since shit breaks anyway EVERY update without fail).
I have janky extension methods and fixes everywhere in every app because again, basic functionality just does not work and I'm waiting for things to get fixed, or events fire in different orders between updates. It's just impossible to justify my day doing this literally every time. I'm glad there is a ContainsExclusive now as I suggested in my PR but it should be used where possible ASAP. Not in 8 months. Then I can remove my extension method that does that in PointerPressed.
And people using my apps understand I have no motivation because of this. They see it's open source and want to help, then attempt to and go ""oh Avalonia with their 1000+ issues"" and try to port my stuff to GTK or something else since they also have issues with Avalonia. Again I don't want to be mean here but this is actual feedback you should consider seriously. You guys do hard work but the workflow is just not there where it matters. It's the hard truth and I wish it weren't since Avalonia can be so great, and should've been great a long time ago. But when ComboBoxes for example STILL don't perform as expected after 5+ years it really does hurt your users.
I pull every commit locally since I first found Avalonia, even to this day, and I read them all. But I see only new features and macOS low-level window fixes. Nothing that matters to me personally, and I get disappointed every now and then since I see the priorities. Anyway, that's just my rant and my opinion. I hope this new generation of users can develop with less headache than I did.",Bitter Frustration
"## Current behaviour
`composer.json`:
```json
""config"": {},
""autoload"": {
""psr-4"": {
""MyApp\\"": ""src/php/MyApp/""
}
},
```
Gives the following `vendor/composer/autoload_psr4.php`:
```php
<?php
// autoload_psr4.php @generated by Composer
$vendorDir = dirname(dirname(__FILE__));
$baseDir = dirname($vendorDir);
return array(
'MyApp\\' => array($baseDir . '/src/php/MyApp')
);
```
## Proposed behaviour
`composer.json`:
```json
""config"": {
""autoloader-root-dir"": ""src/php/""
},
""autoload"": {
""psr-4"": {
""MyApp\\"": ""MyApp/""
}
},
```
Would give the following `vendor/composer/autoload_psr4.php`:
```php
<?php
// autoload_psr4.php @generated by Composer
$vendorDir = dirname(dirname(__FILE__));
$baseDir = dirname($vendorDir);
return array(
'MyApp\\' => array($baseDir . '/MyApp')
);
```
I.e. the generated file would look *as if* `composer.json` were in the `src/php/` directory. Only descending the folder tree would be allowed, so you cannot do something like `""autoloader-root-dir"": ""../somewhereElse/""`. The name `autoloader-root-dir` is tentative.
## Motivation
### Built applications
In modern web development, codebases quite often use multiple languages, some of them compiled (typically something akin to TypeScript compiled to JavaScript). This means that the final application needs to be built. To cleanly separate the source files from the build artifacts, one would want to place all the source files into one directory (e. g. `src`) and have the built app in another one (e. g. `dist`). However, the `composer.json` file would typically be in the root of the repository - that is, *not* in the `src` folder. Under the current behaviour, there is no way to do this, not have the final php files somewhere like `dist/src/` and have a working autoloader. This RFC would enable that by setting `""autoloader-root-dir"": ""src/""`.
### Separating configuration files from the application itself
When an app is deployed, sometimes it is undesirable to deploy configuration files such as `.travis-ci.yml` or similar. With this RFC, one could put the actual application in a dedicated directory (e. g. `app/`), keep all the configuration files (including `composer.json`) in the root of the repo and still have a working autoloader by setting `""vendor-dir"": ""app/vendor""` and `""autoloader-root-dir"": ""app/""`.
This issue is a continuation of the recent debate in #5198. I would be willing to take a try at a PR, if the feature is deemed appropriate for inclusion in composer.",
"For PHP 5.6+, the relevant file for the autoloader is not `autoload_psr4.php` but `autoload_static.php` which makes such that no variables are ever used in the config array so that the array of paths can stay in the OPCache shared memory.
And IIRC, composer uses the shortest relative path possible for that. So with your own code in `app/MyApp` and the vendors in `app/vendor`, `autoload_static.php` may already have the expected `../MyApp` path (to be confirmed). So the second case my already be solved.",
"Thanks, I didn't know that.
For me the main use case is the first one, however the second one was mentioned by some people in the linked issue. If it turns out to be solved already, that's great.",
"What if you set vendor-dir to `src/vendor`? then the relative path should be ../MyApp, and if you then copy src/vendor to dist/vendor it should still work.
I'd rather avoid adding a new parameter for such a custom use case if it can be achieved with existing infrastructure.",
"Hmm yeah no sorry re-reading the description I guess this isn't gonna work. I am not sure I fully understand what you are trying to achieve though, more details would be good to really see what the point is and whether it's something we want to support if it isn't yet.",
"I find the arguments rather weak and lacking. In ""modern"" web development, one deploys build artifacts, not entire git repositories. So the structure of your git repository is irrelevant. This immediately solves both ""issues"" (separation of source and distribution files, and config files from build services and such).",Insulting
"> And IIRC, composer uses the shortest relative path possible for that. So with your own code in `app/MyApp` and the vendors in `app/vendor`, `autoload_static.php` may already have the expected `../MyApp` path (to be confirmed). So the second case my already be solved.
I've tried it and can confirm that that works. That leaves the first issue which I would like to solve
I have a git repo of the following:
```
â”œâ”€â”€ composer.php
â”œâ”€â”€ src
â”‚ â”œâ”€â”€ php
â”‚ â”‚ â”œâ”€â”€ index.php
â”‚ â”‚ â””â”€â”€ MyApp
â”‚ â”‚ â””â”€â”€ MyClass.php
â”‚ â””â”€â”€ js
â”‚ â””â”€â”€ ...
â”œâ”€â”€ vendor
â””â”€â”€ ...
```
I would like to create (with any build system, I don't really care) a `dist` directory which I could upload to a webhosting (it being a build artifact as @alcohol pointed out). I imagined it being of the sort:
```
â”œâ”€â”€ index.php
â”œâ”€â”€ MyApp
â”‚ â””â”€â”€ MyClass.php
â”œâ”€â”€ js
â””â”€â”€ ...
â””â”€â”€ vendor
â””â”€â”€ ...
```
But, by now, the path from `vendor/composer/autoload_static.php` to `MyApp/MyClass.php` has changed. So the autoloading would no longer work. But with the proposed new config parameter, I could set ```json
""config"": {
""autoloader-root-dir"": ""src/php/""
}
```
and then autoloading would work in the build artifact directory.
Why not just use `vendor-dir` then? Because that would require me putting the `vendor` dir in `src/php`. But the `vendor` dir **is not** source code for me - therefore, I wouldn't want it in the `src` directory.
Clearer now? I think the exact organization of the `dist` directory could be different a bit, but the overall idea would stay the same. I'd like to hear your thoughts on this.",
"What you need to configure here is your webhosting's webroot to point to `src/php/` so it can find index.php at the root, but vendors are *not* present within the webroot. This is a best practice as having vendors in the webroot mean people can easily access your dependencies which might reveal vulnerable software etc.",
"> This is a best practice as having vendors in the webroot mean people can easily access your dependencies which might reveal vulnerable software etc.
Thanks for the advice, I did not know that.
However, unfortunately, this is not a good solution on its own - I can't set the webroot to `src/php` as the files in `src` are sources - not build artifacts. And the web/app needs to run from the build artifacts, not the sources... Any other ideas? Solving this without needing to add anything would of course be preferable...",Bitter Frustration
"Then put the build artifacts wherever you need, and make sure that's the web root, and that the web root contains an index.php which includes whatever path is needed so it finds vendor dir etc, even tho that should be in a separate directory.
For example [Symfony's standard distro](https://github.com/symfony/symfony-standard) has a web dir with app.php being the index file/front controller, you can see what's in the web dir, it should be only things that should be accessible by user browsers. And the web root should be web/",Bitter Frustration
It baffles me this hasn't just been added yet. Just give people the flexibility instead of point towards web host configs.,Bitter Frustration
"Yes, well, luckily we don't have to make decisions based on what baffles some individuals and what not.",Mocking
"Updated from v92 to v110 and have flicker on the edges. I use PlaneBufferGeometry with null z values to represent unknown elevations(holes). The sparse grid shows a flicker where the nulls' edges are. Was not seen with flatShading: true or v92:
![image](https://user-images.githubusercontent.com/18248938/68339808-812af700-00a2-11ea-8f01-ec761d009580.png)
I know you are going to say NaN are not supported in geometries, but any chance you could fix this or point me in proper direction?
##### Three.js version
- [ ] Dev
- [x ] r110
- [ ] ...
##### Browser
- [x] All of them
- [ ] Chrome
- [ ] Firefox
- [ ] Internet Explorer
##### OS
- [ ] All of them
- [x] Windows
- [ ] macOS
- [ ] Linux
- [ ] Android
- [ ] iOS
##### Hardware Requirements (graphics card, VR Device, ...)",Impatience
"Its something that occurred between 103 and 104. 103 works, 104 flickers.... Looking at change log",Impatience
I'm afraid a screenshot is not sufficient to debug this issue. Please always provide live examples when reporting such problems.,
"thank you for your help; Flicker is seen when not moving.
https://jsfiddle.net/b4xzmqcr/",
"The issue gets worse when multiple 3js windows are open and active(open the fiddle in 2 windows). Also noticeably when ""lightShot"" screen capture was opened. Also noticed when whats app message received. I have a dev team of 4 guys finding inconsistencies of producing on different graphics cards with different results, but all see a flicker at different times",Bitter Frustration
"Sorry, but I can't see any flickering on my system. Tested with Chrome 78.0.3904.97, FF 70.0.1 and macOS 0.14.6. Are you on Windows?",
"video: Multiple instances of the window seem to be the issue, even with duplicate fiddles it appears.
[here](https://drive.google.com/open?id=1zgscsAoXE1DNdWyIW1qORhXgatCRhKL1)
windows version 1809 OS Build 17763.805
NVIDIA GeForce RTX 2080 SUPER
I have also tested on msi lapiop with NVIDIA GeForce GTX 980M and the problem is probably worse.
All other devs have windows machines, but we could check on a mac if that helps",
"looks like a Problem with the nvideo driver? i tested this on two Systems, with amd driver (linux) without any problem",
"I haven't gone through the diff of 103-104, but ideas on where to look that would make this happen?",
"Was able to replicate on Windows with NVidia GPU, but not on Mac with intel integrated GPU. Couldn't find anything on the change logs from `r103 -> r104` that would explain that. But, then again, you are making use of unsupported behavior, as you said it yourself.",Bitter Frustration
Seems very odd that separate instances of three are causing it. Even though unsupported seems like it could be a canary for a scoping issue?,
"why not just map NaNs to some big but finite values to move the triangles out of the view. that would mean removing the index attribute as well, however (rough idea: https://jsfiddle.net/uxwqyjvm/ )",
"In any event, I don't see a `three.js` bug here. I suppose this happens also with pure WebGL and with any other 3D engine so I think it's more correct to close the issue and move the dicussion to stackoverflow or the forum.",
"@makc We are constantly updating the surface with new known position. Might be worth testing but would take a considerable amount of work. If anything this would double the overhead(two geometries needed) of multi-million point grid geometries. Do you have any docs for ExplodeModifier? Also we have to have bufferGeometry. These are very large surface models.
@Mugen87 I do not think that is true because the issue is not seen in r103. There was a change in the code base that introduced this bug. If it never worked, then i would agree it is a webGL/GPU issue, but it was working up to r103 and we have it in production at r92 for a pretty long time(since r92 release)",Bitter Frustration
"So there were two shader changes in the revision in the build. EDIT
single change then adding that change in so this is messing it up.
sorry my github skills suck , we are svn:
it is here in light_pars_begin lines 1-40:
```
export default /* glsl */`
uniform vec3 ambientLightColor;
uniform vec3 lightProbe[ 9 ];
// get the irradiance (radiance convolved with cosine lobe) at the point 'normal' on the unit sphere
// source: https://graphics.stanford.edu/papers/envmap/envmap.pdf
vec3 shGetIrradianceAt( in vec3 normal, in vec3 shCoefficients[ 9 ] ) {
// normal is assumed to have unit length
float x = normal.x, y = normal.y, z = normal.z;
// band 0
vec3 result = shCoefficients[ 0 ] * 0.886227;
// band 1
result += shCoefficients[ 1 ] * 2.0 * 0.511664 * y;
result += shCoefficients[ 2 ] * 2.0 * 0.511664 * z;
result += shCoefficients[ 3 ] * 2.0 * 0.511664 * x;
// band 2
result += shCoefficients[ 4 ] * 2.0 * 0.429043 * x * y;
result += shCoefficients[ 5 ] * 2.0 * 0.429043 * y * z;
result += shCoefficients[ 6 ] * ( 0.743125 * z * z - 0.247708 );
result += shCoefficients[ 7 ] * 2.0 * 0.429043 * x * z;
result += shCoefficients[ 8 ] * 0.429043 * ( x * x - y * y );
return result;
}
vec3 getLightProbeIrradiance( const in vec3 lightProbe[ 9 ], const in GeometricContext geometry ) {
vec3 worldNormal = inverseTransformDirection( geometry.normal, viewMatrix );
vec3 irradiance = shGetIrradianceAt( worldNormal, lightProbe );
return irradiance;
}
```",
"looks like probe light is causing this issue @WestLangley @sciecode :
Added support for light probes. [#16223](https://github.com/mrdoob/three.js/pull/16223)
Edit as i walk through for future refrence:
1. Light probe is part of ambient light so removing ambient light fixes the issue. 2. light probe is a measure of ambient light not a child. So no ambient light no light probe?
3. Hemisphere lighting does not show flicker
4. Hemisphere white light is super flickery
5. Looks like the more white the light the worse the flicker, equally bad on hemisphere and ambient.",Bitter Frustration
"updated fiddle with super flicker:
https://jsfiddle.net/sxjemofg/1/
turn off ambient light for no flicker",
"RESOLVED
fixed it. Is this worth committing? Someone who knows get and three should commit this ðŸ‘ @WestLangley @Mugen87 ```
vec3 shGetIrradianceAt( in vec3 normal, in vec3 shCoefficients[ 9 ] ) {
// normal is assumed to have unit length
float x = normal.x, y = normal.y, z = normal.z;
// band 0
vec3 result = shCoefficients[ 0 ] * 0.886227;
//FIX
// in webgl 2.0 this could be replaced with isnan(val)
if(
(! ( x < 0.0 || x > 0.0 || x == 0.0 )) || (! ( y < 0.0 || y > 0.0 || y == 0.0 )) || (! ( z < 0.0 || z > 0.0 || z == 0.0 ))
){ return result; };
// response from dev who helped fix this:
// ""it is a bug because they should always account for NaN // whether it is intended or not ;)""
//END FIX
// band 1
result += shCoefficients[ 1 ] * 2.0 * 0.511664 * y;
result += shCoefficients[ 2 ] * 2.0 * 0.511664 * z;
result += shCoefficients[ 3 ] * 2.0 * 0.511664 * x;
// band 2
result += shCoefficients[ 4 ] * 2.0 * 0.429043 * x * y;
result += shCoefficients[ 5 ] * 2.0 * 0.429043 * y * z;
result += shCoefficients[ 6 ] * ( 0.743125 * z * z - 0.247708 );
result += shCoefficients[ 7 ] * 2.0 * 0.429043 * x * z;
result += shCoefficients[ 8 ] * 0.429043 * ( x * x - y * y );
return result;
}
```",
"> I use PlaneBufferGeometry with null z values to represent unknown elevations(holes). You have NaN's in your position data.
You also have NaN's in your vertex normals. Hence their length is undefined.
three.js does not support NaNs in data pushed to the GPU.",
"one should always account for NaN whether it is intended or not. And its a simple fix. We need this to work for our production and it doesnt harm anything having a check in. Could we please add, is there any harm? @WestLangley",Impatience
">one should always account for NaN whether it is intended or not.
No, YOU need to account for it in your app and pass valid data.",Bitter Frustration
"Yes, this is something that needs to be fixed on app level.
> three.js does not support NaNs in data pushed to the GPU.
NaN geometry data also corrupt operations on the CPU like intersection tests or the computation of bounding volumes. So again, this is no library bug.",
"We have a valid use case for putting NaN's in our data set that as far as I can tell can not be done in any other way, efficiently. In any real world application dealing with NaN's is just a fact of life .
@Mugen87 Bounding Volumes was actually my last post. It all stems from not handling NaN's properly. Just a simple test for isNaN would resolve most these. Why is there such push back and not a desire to resolve this issue?
Are there any suggestions for dealing with real world data that has NaN's? We have bandied around alpha mapping, but then we increase overhead with the map and need to access the alpha map and the vertex positions. Also what if the numbers are very large and overflows? Will that cause a NaN that might need to be dealt with.",Bitter Frustration
"> Why is there such push back and not a desire to resolve this issue?
`NaN` values is an app level problem from our point of view since the user is in some sense the data producer. Geometric data like position, normal or color data are meant to be numerical. `NaN` data are no numerical data.
Handling `NaN` values is use case specific since proper default values depend on what the application actually does.
This is not only an issue in graphics. When you process huge data in context of KDD and machine learning, data cleanup is one important preparation step before you execute the actual algorithm (which is not responsible for handling missing, undefined or corrupt values).",
"so this is a ""no"" on resolving? Is there a reason not to other than ""we don't support this""? Its a one line PR.
I guess we will be patching every update from here out.",Bitter Frustration
"three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.
Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix"".",Bitter Frustration
">I guess we will be patching every update from here out.
One way to handle your examples is to add an ""invalid"" attribute: 0 if valid; 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero.",Impatience
"> Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix"".
what case would it not fix? It fixes unknowns in position array from flickering. Could I add a switch to turn off probe lighting?
> One way to handle your examples is to add an ""invalid"" attribute: 0 if valid; 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero.
Where woudl you implement this. Seems alot like an alpha map but culling vertices else where?
Edit: thanks for the help. > three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.
its semantics(and not my project) but in my mind ""unknown"" != ""invalid"".",Bitter Frustration
"My [suggestion](https://github.com/mrdoob/three.js/issues/17882#issuecomment-551936446) is similar to alpha-testing and would require injecting code into the fragment shader. Without knowing the details of your use case, I can only offer it as a suggestion.
Please use the three.js forum if you need additional help.",
"@kpetrow > Just a simple test for isNaN would resolve most these. Why is there such push back and not a desire to resolve this issue?
You may have more luck by doing a PR with that ""simple test"" rather than asking us to do it for you.",Impatience
"@kpetrow,
> two geometries needed... ExplodeModifier
of course you could just create un-indexed buffer geometry to begin with, and not use ExplodeModifier or convert geometries otherwise.
> RESOLVED... Is this worth committing?
well, you know, a bunch of IF-s in the shader that almost noone would need any way... so, probably not.
> updated fiddle with super flicker
no flicker here at all",Irony
"> You may have more luck by doing a PR with that ""simple test"" rather than asking us to do it for you.
yes the debt is too big to resolve, so lets never fix it @mrdoob . Why not start here? If I do a PR to fix this issue will get any traction? Or if i add a switch to light probe to turn on/off like shadows and transparency? > well, you know, a bunch of IF-s in the shader that almost noone would need any way... so, probably not.
@makc Single if. You also turn off other features like shading, and transparency. Why not make this new undocumented featured(only example) that effect primary lighting switchable?
> no flicker here at all
@makc Well great let me just ship your computer around so everyone can use it. Or even better we will buy everyone a plane ticket and then can come stand in line to use your computer where the software does work.",Bitter Frustration
"**Describe the problem**
Currently you can enable multiple languages and can also prefer some over the other, but sonarr will ignore the prefered language if there is a better quality in another language. I would like a option (or standard) that other languages over the prefered ONLY get chosen when there is not prefered language.
**Describe any solutions you think might work**
**Currently behaivior**
Language selection
Upgrade until Language 1
Language 1 (higher is prefered)
Language 2
_Avaible Releases:_
Release language 1 720p
Release language 2 720p
Release language 1 480p
Release language 2 1080p
It will choose Release language 2 1080p, even there is a prefered language 1 720p.
I want it this way:
Suggestion:
Take language 1 720p
I think it would be clever to make this standard behaivior, but if you dont want to do that, then atleast give us an option to make it this way.",Bitter Frustration
"We don't have plans to support language over quality at this time, but will be looking at preventing downgrades of the language in order to upgrade the quality.",
"What is the idea behind the whole language thing then?
You can prefer a language, which doesnt get prefered.
I want to download it in english, if i cant get my mother language. But i cant enable it, because it will remove my mother language content in every series (like 90% or so). Because in english is higher quality avaible....",Bitter Frustration
"fyi. I grouped all 720p options into one 720p group. Im searching for English and German, German is the cutoff. That works perfectly in my testing, so not really necessary to implement anything !
Quality Profile is called ""720p - grouped""",
"HEy, can you post a screen from neccessary changes?
edit:
I dont think it works. It will pick 230p english over 100p german. Because someone thinks that graphic is more important then language xD (it doesnt prefer language over quality thats what this thread is about.. )
If all languages are the same quality it works, but thats not how non english releases work. Solltest du wissen.",Bitter Frustration
"Language Settings: ![Bildschirmfoto 2021-02-02 um 07 57 47](https://user-images.githubusercontent.com/39960884/106563692-73298780-652c-11eb-84fd-56aef6e4e020.png)
720p example:
![Bildschirmfoto 2021-02-02 um 07 59 03](https://user-images.githubusercontent.com/39960884/106563741-89374800-652c-11eb-96f6-3f42f7bb008f.png)
It is only working, when the quality that is existent, and you have found are equally. Not working for 720p English vs 480p German.
Wie du richtig erkannt hast, geht es nur bei gleicher QualitÃ¤t. Ich hab es gestern bei Badwomen probiert ohne Probleme. Supernatural genauso. Vorher hab ich die Serie in Englisch 720p laden lassen und wenn sie komplette war einfach auf Deutsch 1080p gestellt. Ne richtige LÃ¶sung wÃ¤re natÃ¼rlich geil, aber wenn das Ticket schon von der Grundidee abgelehnt wird, brauchst du auch keinen PR stellen, weil er eh abgelehnt wird.
Falls du noch Hilfe brauchst oder Fragen hast, schreib mir ne PM. Man kÃ¶nnte auch mal Indexer abgleichen, vllt wÃ¼rde das bei dir helfen.",Bitter Frustration
"We dont use sonarr to have a manual solution. It cant be that hard to make an option (or default) prio language over quality.
Die Variante ist wie ich es hatte, mittlerweile ist es nicht mehr so das er deutsch lÃ¶scht um besseres englisch zu laden (theoretisch, wobei ich mir da nicht sicher bin). Schreib dir wegen den Indexern.
@markus101 any news on this? We have a auto prio system, cant we just artificially add +1000 score to pref. language if we tick
[ X ] ""prio language over quality"" (with all due respect, this doesnt sound like a big problem?)",Impatience
"This would be really nice to have actually.
I prefer to have some animated series in danish, but if the episode is not available in danish, it should just grab the english one.
If my target is set to 1080p, i want it to download a 480p danish one over 1080p english, as language should be priority.
Only option right now is grouping all qualities together, but that also sucks as it'll just download whatever then and not prioritize the qualities... So it really should be checking language and then check quality when choosing releases. Not the other way around.",Bitter Frustration
"they dont understand or dont care
i guess its too easy to add and would help too much ppl xD",Bitter Frustration
I've run into the exact same issue. Current behaviour makes it impossible to prioritize Dual (English + Spanish) over quality which I would say sounds like a very sensible requirement,Bitter Frustration
"What about giving an arbitrary priority to quality so, at least, we can add a hardcoded value to hack the behaviour via the preferred words; If, let's say, quality is a +10, it's up to users to add higher values for the languages on the preferred words section. That would, at least, provide us some mechanism to automate our downloads",
i cant understand whats the problem is.,Impatience
"Folks - Markus was rather clear in his comment; that decision has not changed.
Repeatedly asking for this or `me too`ing an issue is simply noise and unproductive.
> [We don't have plans to support language over quality at this time](https://github.com/Sonarr/Sonarr/issues/3394#issuecomment-556944433)
There's 1 item to do from this GHI:
- preventing downgrades of the language in order to upgrade the quality.
If you want a workaround then group all your qualities together and handle quality preferences as preferred words. This would effectively remove quality from the equation as they'd all be equal.
https://wiki.servarr.com/sonarr/faq#how-are-possible-downloads-compared",Impatience
"> Folks - Markus was rather clear in his comment; that decision has not changed.
> Repeatedly asking for this or `me too`ing an issue is simply noise and unproductive.
> > > [We don't have plans to support language over quality at this time](https://github.com/Sonarr/Sonarr/issues/3394#issuecomment-556944433)
> > There's 1 item to do from this GHI:
> > * preventing downgrades of the language in order to upgrade the quality.
> > If you want a workaround then group all your qualities together and handle quality preferences as preferred words. This would effectively remove quality from the equation as they'd all be equal.
> https://wiki.servarr.com/sonarr/faq#how-are-possible-downloads-compared
Hi,
if there is really a working solution... could you please ellobrate it a little bit more and maybe show how to do it? maybe some screenshots? mabye add a faq or wiki speciall for this? Would help all non english speaking ppl in the world..",Impatience
"> if there is really a working solution... could you please ellobrate it a little bit more and maybe show how to do it? maybe some screenshots?
Not sure how well it works, but it does exist.
It was previously pictured here, which is just editing the groups: https://github.com/Sonarr/Sonarr/issues/3394#issuecomment-771416439
Please use one of the support channels: [forums](https://forums.sonarr.tv/), [subreddit](https://www.reddit.com/r/sonarr/), [discord ](https://discord.gg/Ex7FmFK), or [IRC ](https://web.libera.chat/?channels=#sonarr)for further discussion on this / for support/questions.
You may also check out the [wiki](https://wiki.servarr.com/sonarr)",Impatience
that simply does not work. It will always pick the higher quality if you have multiple languages... dont you understand? and english is in 99% the higher quality.,Bitter Frustration
"@bakerboy448 @markus101 I understand metooing isn't the solution but please also understand that you guys are kinda only focused on the use case for English speakers. It has been the same with Sonarr v2 and I ended up having to recompile a forked version of Sonarr so it could search for different languages properly (there was even a PR lying around that never got merged because it was being fixed in v3).
This is not a support request, this is a feature request that has been demanded many times. The decision you guys have made about quality over language makes no difference for anyone with only one language and it simply doesn't work for users with multiple ones: if you only have one language (i.e. English speakers) it won't matter if language comes first in the comparison as there's nothing to compare. On the other hand, if you have multiple languages, it makes sense that you prefer the cutoff ones over the rest. This would automatically implement the ""Prevent downgrading language during quality upgrade"" just by changing the order on the list of decisions you are applying. It's worth mentioning that the prevention mechanisms on their own won't solve much either as, for the first automated download, the highest quality language will be picked up, potentially never grabbing a release on the preferred language, so there wouldn't be any language downgrading.
In addition, the proposed workaround with quality groups is flaky, hacky, and it doesn't seem to work for all cases as using preferred words for quality selection is very verbose and seems error-prone (Sonarr has quite some logic to get that right). All we are asking is for a mechanism of inverting the order of these two lines: https://github.com/Sonarr/Sonarr/blob/develop/src/NzbDrone.Core/DecisionEngine/DownloadDecisionComparer.cs#L31-L32
I'm even happy with an if on an environment variable or any other hacky way that saves me from having to recompile that class and replace it the hard way.",Bitter Frustration
"> In addition, the proposed workaround with quality groups is flaky, hacky, and it doesn't seem to work for all cases as using preferred words for quality selection is very verbose and seems error-prone (Sonarr has quite some logic to get that right).
I dont even know why ppl say this. Its not working. I dont see a way to even with ""hacks"" get sonarr to prefer language over quality thats just a rumour. Its only working in a perfect world when e.g. english is the same quality as ""insert language here"", which is most likly not goin to happen.
I cant even understand why this was added when its literally not working. Like what is the intented use case for this feature? Also i dont understand why you changed my title which does now mean something complete different. Can we maybe fund a change? Like give us foreign users a cost point",Bitter Frustration
"@eihns, did you actually try merging the qualities? One of the biggest use cases for doing that is to *bypass* quality and implement your own solution to quality. Hop on Discord or make a post on reddit if you're having trouble understanding how to set that up, leave this feature request for the feature request instead of support.",Impatience
"> @eihns, did you actually try merging the qualities? One of the biggest use cases for doing that is to _bypass_ quality and implement your own solution to quality. Hop on Discord or make a post on reddit if you're having trouble understanding how to set that up, leave this feature request for the feature request instead of support.
Did you try? I dont try to _bypass_ anything i try to use _prefer_ language, which is not working. It does not prefer a language, it prefers the quality. Only if quality atleast match it will prefer the language.
The only working solution would be to only grab the worst quality and hope that its always in both languages available.
Also i startet the feature request and not a support. Why you flame me? There is no support needed its just not working or not intented to work that way. That seems to be the order.
CompareQuality,
CompareLanguage,
ComparePreferredWordScore,
CompareProtocol,
CompareEpisodeCount,
CompareEpisodeNumber,
CompareIndexerPriority,
ComparePeersIfTorrent,
CompareAgeIfUsenet,
CompareSize```",Bitter Frustration
"Please use one of the support channels: [forums](https://forums.sonarr.tv/), [subreddit](https://www.reddit.com/r/sonarr/), [discord ](https://discord.gg/Ex7FmFK), or [IRC](https://web.libera.chat/?channels=#sonarr) to discuss this further.
> The only working solution would be to only grab the worst quality and hope that its always in both languages available.
No, as mentioned previously you can use preferred words along with grouped qualities.
> That seems to be the order.
That's part of it, but not all of it.
As we've said previously, at this time we do not have plans to support prioritizing language (or anything else) over quality, we do have plans to improve quality upgrades when it comes to language downgrades.",Impatience
This should be achievable with Custom Formats in v4.,
"- [x] **I have created my request on the Product Board before I submitted this issue**
- [X] **I have looked at all the other requests on the Product Board before I submitted this issue**
About the GraphQL plugin:
If a model has a unique field defined in it, we should be able to filter for that field.
In this moment only IDs can be used for filtering
Example:
model Article with title, content and slug. Slug is required and unique
I would like to run this query
`
query {
article(slug: ""my-awesome-article"") {
title
id
content
}
}
`",
Thank you for this feedback!,
I'm also interested on this!,
Hi @nicecatch great suggestion. You can achieve that by overridding the query and the action to handle it quickly on your own :),
@lauriejim Maybe is a good example for another guide?,
"> Hi @nicecatch great suggestion. You can achieve that by overridding the query and the action to handle it quickly on your own :)
@alexandrebodin Where I can override the query? Thanks!",Impatience
"@abdonrd You can override your graphql creating a `schema.graphql.js` file in the `./api/{api}/config` folder
This would look something like that:
```js
const { sanitizeEntity } = require('strapi-utils');
module.exports = {
query: `
articleBySlug(id: ID slug: String): Address
`,
resolver: {
Query: {
articleBySlug: {
resolverOf: 'Article.findOne',
resolver(_, args) {
const entity = strapi.services.article.findOne(args);
return sanitizeEntity(entity, { model: strapi.models.article });
},
},
},
},
};
```",
"I just created the `api/competition/config/schema.graphql.js` file with:
```javascript
const { sanitizeEntity } = require('strapi-utils');
module.exports = {
query: `
competitionBySlug(id: ID slug: String): Competition
`,
resolver: {
Query: {
competitionBySlug: {
resolverOf: 'Competition.findOne',
resolver(_, args) {
const entity = strapi.services.competition.findOne(args);
return sanitizeEntity(entity, { model: strapi.models.competition });
}
}
}
}
};
```
But if I tried the query:
```graphql
{
competitionBySlug(slug: ""fms"") {
id
name
}
}
```
I get this error:
`""message"": ""Cannot return null for non-nullable field Competition.id.""`",Bitter Frustration
"Here is a working example. You need to await your service call and only pass the right params
```js
const { sanitizeEntity } = require('strapi-utils');
module.exports = {
query: `
articleBySlug(id: ID name: String): Article
`,
resolver: {
Query: {
articleBySlug: {
resolverOf: 'Article.findOne',
async resolver(_, { slug }) {
const entity = await strapi.services.article.findOne({ slug });
return sanitizeEntity(entity, { model: strapi.models.article });
},
},
},
},
};
```",
"Thanks @alexandrebodin! It works now! ðŸ‘ And how would it be with a plugin?
I've tried this, but it doesn't work for me.
`extensions/users-permissions/config/schema.graphql.js`
```javascript
const { sanitizeEntity } = require('strapi-utils');
module.exports = {
query: `
userByUsername(username: String): UsersPermissionsUser
`,
resolver: {
Query: {
userByUsername: {
resolverOf: 'User.findOne',
async resolver(_, { username }) {
const entity = await strapi.plugins['users-permissions'].services.user.findOne({ username });
return sanitizeEntity(entity, { model: strapi.plugins['users-permissions'].models.user });
},
},
},
},
};
```",
"@alexandrebodin it works with:
`extensions/users-permissions/config/schema.graphql.js`
```javascript
const { sanitizeEntity } = require('strapi-utils');
module.exports = {
query: `
userByUsername(username: String): UsersPermissionsUser
`,
resolver: {
Query: {
userByUsername: {
plugin: 'users-permissions',
resolverOf: 'User.findOne',
async resolver(_, { username }) {
const entity = await strapi.plugins['users-permissions'].services.user.fetch({ username });
return sanitizeEntity(entity, { model: strapi.plugins['users-permissions'].models.user });
},
},
},
},
};
```
It's the right way?",
"This can be done via customization, marking as closed. We have filters that would work just as easily also.",
"After a [twitter-exchange with Dries Vints](https://twitter.com/driesvints/status/1196780528302776320) it looked like there should be a way to easily handle ISO8601- and RFC3339-related datetimes containing fractions.
And after a bit of digging I realized that there already was a change done for RFC3339 by adding the microseconds. But the milliseconds where not yet part of the constants.
This PR tries to fix that by adding new constants:
* DATE_ATOM_FRACTION
* DATE_RFC3339_FRACTION
* DATE_ISO8601_FRACTION
* DateTimeInterface::ATOM_FRACTION
* DateTimeInterface::RFC3339_FRACTION
* DateTimeInterface::ISO8601_FRACTION
All these are based on their fraction-less counterparts and have added the "".u"" modifier after the ""s"" and before the offset-notation",
The test failure is legit; could you please fix it?,Impatience
Sure. Will do ASAP,
"Fractions are part of the original RFC3339, and are optional. Why does there need to be a separate constant for this?
Also, see [bug report][bug-79305].
[bug-79305]: https://bugs.php.net/bug.php?id=79305",Impatience
"@XedinUnknown because the new constants will provide the greatest datetime precision that's possible with ISO 8601. The current ones lack fractions. The new constant's value is one that's used regularly in, for example, api responses with datetime values.",
@XedinUnknown Because otherwise it will be a BC break.,
What's breaking about fixing support broken support for a format?,Bitter Frustration
@XedinUnknown an API may extract the result produced by your constant with some RegExp: `^\d+-\d+-\d+ \d+:\d+:\d+$` add the fraction and this extraction suddenly stop working. That's why it's safer to require the user to explicitly add the fraction by selecting the new constant.,Bitter Frustration
"You won't believe it but people are using those constants for some time now and wrote code that expects the output to be of a certain format. By modifying the existing constant this expectation is broken and therefore the code is not working anymore. So changing that constant is breaking backward compatibility.
That the current constant is not doing what you expect might be due to the fact that you expect something from the constant that is not even described by the standard. The ISO standard says that you *can* add fractions, but you do not *have* to add them. So the current constant does exactly that. And if you want to use fractions you can use the new constant.",
"The fact that you _can_ add fractions means that the standard supports fractions. And PHP claims to support the standard. In practice, however, an RFC3339-compliant timestamp that contains a fraction (i.e. is standards-compliant) cannot be parsed by PHP using the `DATE_ATOM` format, which means that support for it is broken. Fixing something that is broken does not constitute a BC break, because the specification does not change. If somebody has depended on _implementation_, instead of the specification, then they had made a mistake, and it is up to them to fix it. RFC3339 supports optional fraction, and if I made an expression that does not cater for that, then my expression does not support the RFC3339 standard. Same for the PHP `DateTime` implementation: if it does not cater for the optional fraction, then it does not support the standard. Adding support for the standard would simply fix that. It is not a BC break.",Bitter Frustration
"It means your implementation is broken if you relly on the DATE_ATOM constant. Passing an ATOM-Timestamp with fractions into `new DateTimeImmutable` works fairly well so the PHP-Implementation is not broken. But as a Constant can only hold **one** format and not contain an *either/or* along with the fact that PHP, until a certain version didn't support fractions at all, leads to the fact that we have a fraction-less Constant and (hopefully soon) a constant that contains fractions",Bitter Frustration
@XedinUnknown the fact it fixes a bug (which I disagree anyway) does not mean it's not a breaking change. Applications that properly work relying the current constant format will stop working on update. That's an obvious BC.,Bitter Frustration
"> Passing an ATOM-Timestamp with fractions into new DateTimeImmutable works fairly well so the PHP-Implementation is not broken
It does not, as I demonstrated in the bug report.
> But as a Constant can only hold one format and not contain an either/or
An RFC3339 timestamp with a fraction and without a fraction is _the same format_: the RFC3339 format.",Bitter Frustration
"Thanks everyone, further input will not be needed at this point in time. Please let @derickr review this change.",
"@heiglandreas, could you please rebase onto master and force-push, so we get the full CI runs?
@derickr, what do you think about these new constants?",
"First off, I love Chocolate Doom and how accurate and true it is to the original executables - right down to bugs.
Though one particular aspect that I feel may be good to be able to alter is the static limits of the engine, such as the visplane limits. Preferably this could be done as an extra command line argument or configuration setting. When not specified, the normal limit values can be used as a default.
These limits in the engine have no real affect on how the game is actually _played_, so I feel providing an optional way to adjust them would still be true to the _vanilla experience_ and not go against the philosophy of the project. It's akin to how all the networking changes _aren't_ 'true to the original' Doom, or how the Vanilla Save Game Limit or Vanilla Demo Limit are already optional changes you can make. All these changes are to give the _vanilla experience_ a greater range of access, just as they are all equally not 'true' and are not 'pure' to the original.
To make the code change as minimal as possible, it could be as simple as specifying numbers for the limits (i.e. visplane array size). One could simply set them to high values in order to run more intensive maps. Another possible and more elegant solution could be letting Chocolate Doom read the map data and set the array limits dynamically at map load time to match what is present.
I'm sure I'm not the only one who would love to use Chocolate Doom for SiGiL and other maps that are otherwise only incompatible due to these hard coded limits. It's quite annoying juggling multiple ports solely because of this single aspect alone, especially that it doesn't affect the _vanilla experience_ in any way - or at least much less so than the changes already put in and present in Chocolate Doom.",Bitter Frustration
"This is beyond the scope of Chocolate Doom, but you may be pleased to know that [Crispy Doom](https://www.chocolate-doom.org/wiki/index.php/Crispy_Doom) already accomplishes the desire to remove vanilla limits.",
"Thanks, though I already have tried Crispy. Not a fan unfortunately, there's too much changed with it and may as well use any given handful of ports at that point.
I'm curious though why it would be considered beyond the scope of the project. At face value, this seems to be pretty arbitrary considering the things already changed and different in Chocolate from the original game. Could I ask for you to shed a little more light on where the line is drawn at? It'd give at least some sort of resolve and explain the mindset. Edit: Thanks in advance!",Bitter Frustration
"Also I'd like to point out one of the core tiers from PHILOSOPHY.md: ""The surrounding aspects of the game that arenâ€™t part of the central gameplay experience can be extended as long as thereâ€™s a good reason and Vanilla behavior is respected."" What I'm proposing is being able to load countless maps and (more or less) an official Episode 5 for Doom, all of which are fully compatible with Vanilla Doom outside the limits. That seems to fall _precisely_ under that tier, does it not? :\ This is why I'm confused by you stating it is 'beyond the scope'.",Bitter Frustration
"It's important that Choco-doom glitches and crashes _exactly_ the same way that the original DOS executable does; otherwise it doesn't solve the problem of allowing people to test against vanilla _behaviour_. If choco-doom doesn't behave the same as doom.exe did in 1993 then it's not ""vanilla"" compatibility people are testing, it's ""chocolate"" compatibility! :)",Irony
"But wouldn't the optional extended formats offered by Chocolate go against what you just said? For example the Vanilla save game limit and Vanilla demo limit removal options. Those are not 1993 compatible and thus would go against the behavior.
And additionally this is why I said it should be an optional parameter to up the limits. So if you didn't specify to extend the limits, it would have the same limits and _would_ crash in the exact same manner as the 1993 version, and thus make the compatibility point moot.",
"> Thanks, though I already have tried Crispy. Not a fan unfortunately, there's too much changed with it
Yes? What exactly?
> I'm curious though why it would be considered beyond the scope of the project. Think about Chocolate Doom as a research project to replicate Vanilla Doom's behavior in the most precise, convenient and portable manner. It was never meant to play Sigil or any other limit breaking map or episode. And this won't change. You're asking at the wrong place. Also, I don't see why this would still be unclear after reading the philosophy document?",Bitter Frustration
"> Yes? What exactly?
I thought it would be self explanatory... but just open up Crispy and refer to the multiple pages of options.
> Think about Chocolate Doom as a research project to replicate Vanilla Doom's behavior in the most precise, convenient and portable manner. It was never meant to play Sigil or any other limit breaking map or episode. And this won't change. You're asking at the wrong place. Also, I don't see why this would still be unclear after reading the philosophy document?
Let me reiterate pillar 2 and what I said about it above. It makes it very unclear why this optional limit raise would be disallowed, as it falls directly within the pillar's requirements (or at least to the same degree the vanilla demo and save file breaking already allowed in Chocolate).
> Also I'd like to point out one of the core tiers from PHILOSOPHY.md: ""The surrounding aspects of the game that arenâ€™t part of the central gameplay experience can be extended as long as thereâ€™s a good reason and Vanilla behavior is respected.""
>
>What I'm proposing is being able to load countless maps and (more or less) an official Episode 5 for Doom, all of which are fully compatible with Vanilla Doom outside the limits. That seems to fall precisely under that tier, does it not? :\ This is why I'm confused by you stating it is 'beyond the scope'.
And if it is trying to be a research project specifically aimed to be most compatible, why do you allow breaking of vanilla limits for demos and save files then? That completely destroys the point to selectively allow non compatible aspects but not others.
If you allow for setting the array sizes of the static limits, you could allow raising limits while changing virtually nothing but a #define into a variable set by command line. If the values are left as default values, then you will have _exactly, 100%_ the same vanilla behavior. This would not change compatibility whatsoever, unless you _specifically_ tell it to do so - just as you can do with vanilla breaking save and demos Chocolate can produce. So my main question is, what is the defining difference between that vanilla breaking aspect and this proposed one when they are all optional and don't actually affect even how it crashes when disabled?",Bitter Frustration
"Heh, you can try Competiton Doom, it's Choco with limits removed and few additional tweaks which should not bother you.
https://github.com/fx02/cndoom/releases/tag/v3.0.0-alpha2",Mocking
"Thanks fx02. I'll have to give it a closer look, but it initially seems it's pretty close to what I'm looking for - though the future plans of bringing the Crispy HUD and potential inability to make vanilla compatible demos and saves is a bit worrisome. :\
Once I get a slice of time, I'm now planning to just fork Chocolate and make a quick hack to allow optionally specifying the limit sizes. Still will be 100% vanilla compatible and would be able to test 100% compatibility with vanilla map limits with the implementation I'm doing. Was sort of hoping I wouldn't need to do that, but it's apparent now that there really is just arbitrary lines drawn on what vanilla features are allowed to be optionally broken or not in Chocolate; and trying to find reason or explanation to this has gotten me nowhere. At least I know there are quite a few other people who'll enjoy it, based on the numerous posts I've come across also requesting this.",Bitter Frustration
"> What I'm proposing is being able to load countless maps and (more or less) an official Episode 5 for Doom, all of which are fully compatible with Vanilla Doom outside the limits. That seems to fall precisely under that tier, does it not? :\ This is why I'm confused by you stating it is 'beyond the scope'.
Hi @HollowedEmpire, I appreciate you feel really strongly about this. You aren't the first to ask for this feature and the decision was made some time ago. Maybe I can help to bring some clarity about the rationale.
I'd draw your attention to a contradiction that you've made yourself here in your comments - not because I'm trying to criticize, but just because it helps illustrate my point. You say that you dislike the fact that Crispy Doom has ""multiple pages of options"" and yet what you are asking for is another option for Choco (probably multiple options, because Vanilla Doom has many different limits).
I'm inferring from what you say that what you like about Choco is the fact that it has a sparse, minimalist set of options, and this is not a coincidence. It's a consequence of the fact that the project has a deliberately restrictive, minimalist philosophy that is intentionally designed to reject most feature requests. I made a technical decision when starting the project - it isn't going to play every Doom map ever made - and I've stuck with it.
Your request is perfectly reasonable in isolation but the problem is that there are many just like it - ""I love Chocolate Doom but I wish it just had *X*"" where X can be any one of: higher resolutions, uncapped framerate, removed limits, better networking, etc. etc. Plenty of other source ports already exist which do those other things - Choco doesn't. That's just how it is and what the project's about. If we added these features it would lose the property that both you like and I like - its minimalism and the fact that it's just Doom, nothing else.
I hope that makes things a bit clearer to you. In your last comment you describe it as an ""arbitrary line"" and I really would dispute that - I can tell you that both personally and as a team we have put a lot of thought and discussion into the [PHILOSOPHY](https://github.com/chocolate-doom/chocolate-doom/blob/master/PHILOSOPHY.md) document. I think you'll find that if you look through past discussions, very few things in this project are ever arbitrary.
One thing in particular I'd point you is that PHILOSOPHY is very careful to define what ""vanilla"" means - Vanilla Doom is DOS Doom v1.9. So for example when you say:
> I'm now planning to just fork Chocolate and make a quick hack to allow optionally specifying the limit sizes. Still will be 100% vanilla compatible
the definition of ""vanilla"" becomes muddled, since vanilla has limits. Mappers are clear when they make levels for Doom to distinguish between ""vanilla"" and ""limit removing"", and it's not true that SIGIL or any other limit-removing levels are vanilla compatible. And that perhaps cuts to the heart of the whole point I'm trying to make - it's not just an arbitrary line, vanilla Doom is the yardstick we measure against, and it defines the scope of what the project's about.",
"I guess with ""arbitrary line"" he meant that we already offer to remove the Vanilla savegame and demo size limits. And indeed, I find myself unable to give any good reasoning why these two limits for all things are removed but any others aren't.",Bitter Frustration
"For demo size the rationale is pretty simple - vanilla Doom has the `-maxdemo` command line argument, so it's equivalent to just setting that to an arbitrarily large number such that you never meet it.
In the case of the savegame limit I'll admit it's slightly arbitrary, and it's a feature that dates to the first couple of years of the project, when the philosophy was much less codified. I'm skeptical whether we'd add it today, though I'm also loathe to remove it at this point.",
"A complicated braindump of some extra thoughts around this so that it perhaps doesn't seem quite so arbitrary:
* Savegame limit removal ""breaks the rules"" but that doesn't mean any other feature request that breaks the rules should therefore be allowed.
* Why I say I'm ""loathe to remove it"" is a complicated matter that basically comes down to pragmatism, because if it was removed, there are a bunch of prominent ""vanilla"" (note quotes) map sets which would now become unplayable in Choco unless you never save, and I would be loathe to lose them. In other words I'd probably end up running and maintaining a fork of my own port.
* On the same theme, the feedback I've heard from is that mappers haven't historically taken the savegame limit as seriously as the other limits, so there's a large number of levels which are ""vanilla except for the savegame limit"".
* Even some pre-source release levels (ie. made for vanilla Doom) from map sets like Hell Revealed and Requiem break the savegame limit.
* You can make the argument that the savegame limit is different in substance to the other mapping limits since it doesn't determine what you can play, only whether you can save or not.
* Vanilla Doom can actually load huge savegames just fine, it's just saving them that it can't do.",
"> You can make the argument that the savegame limit is different in substance to the other mapping limits since it doesn't determine what you can play, only whether you can save or not.
I actually think this point is key. I think breaking free of the savegame limit is a quality of life measure not far removed from demos, windowed mode, or PNG screenshots.
Both it and the demo limits walk the gray-area of the question the philosophy document asks: ""Although this is Vanilla behavior, is there anyone who would want this?"" â€” the answer to both of them can be yes and no, depending on context. Chocolate Doom defaults to restricting save games to the same limits as vanilla, and this may be important to any users that frequently switch between Vanilla and Choco.",
"@fragglet Thanks for taking the time to explain this further, I do really appreciate it. Though unfortunately I still don't feel any closer to really understanding the rationale to let one optional vanilla break exist but not another. Well, besides the *very* thin 'keeping Chocolate closer to Vanilla by not introducing more stuff'.
I don't believe I've made any contradictions, or at least not in the way you were explaining I did. Maybe I didn't write it out adequately enough before, so I'll try to re-iterate it against the points you made to hopefully help clarify my point. The feature I'm suggesting would have absolutely no affect at all on the vanilla experience, outside of getting to actually *have the experience*. I definitely wouldn't side with any of the counter changes you suggested to mine. Increased frame rate, increased resolution, changed networking... those type of changes would affect the vanilla experience and definitely would be 100% against the philosophy document. Chocolate should only have changes that can give greater access and range to the vanilla experience, which I feel experiencing these otherwise vanilla compliant maps is perfectly acceptable; but *only* as something optionally! It *shouldn't* be the default behavior to accommodate limit breaking maps and I would fully agree with that. But I feel there is no reason there can't be an option to allow for using these maps so people can get the 'vanilla experience' with them. That's sort of what I feel is the entire point of Chocolate - bringing the vanilla experience into the present.
I'm not quite sure why you feel the simpler approach I suggested for the limit raise would suddenly make the vanilla compatibility muddled for Chocolate. An array is still an array. An array created by a variable with the value of 127 would be exactly the same size as an array was created by a #define with the value of 127. So if the array sizes are defaulted to vanilla limits, I can't see how that muddles things in any way at all. Of course, if you explicitly tell it to muddle the limits, then obviously that would - akin to the demo and save limits. Though the more complicated approach of auto setting the limits I suggested, while still possible to do as arrays and get precisely the same vanilla behavior with defaults, would introduce a lot more non vanilla code; so I can understand that being a potential thumbs down at the very least.
But, it seems at this point though that we will likely just have to agree to disagree. It is only opinion vs. opinion now, and we appear to be unable to see eye to eye on the topic. Likely we will only waste each other's time trying to continue on. I'll just have to take some time over a weekend to make a tiny hack in a fork, and that'll be that. It is unfortunate though that the fork is even necessary for something so incredibly minor. But at least this way everyone else who also requested this feature can still use Chocolate Doom's exceptional vanilla experience and have the option play otherwise fully compatible maps as intended.
I'll just close with thanking you again for taking the time to try and explain this further. And thank you and everyone else who worked on and created Chocolate. It really is quite exceptional and amazing software, even despite this hold back it insists to carry. I have used Chocolate as my choice Doom port for years, and is why I was particularly interested to try and understand why something I feel would fit it perfectly as a non-disruptive option, apparently cannot.",Bitter Frustration
"I agree that a fork is the right approach here and I just want to say no hard feelings. There are many different forks of Chocolate Doom at this point and I always welcome people to create more (indeed, the codebase is designed to be fork-friendly). Please do stay in touch and feel free to add a page for it on the website if you like.
Edit: one tiny thing:
> I don't believe I've made any contradictions, or at least not in the way you were explaining I did. My point was this: Chocolate Doom's philosophy seeks to avoid unnecessary options, and it does this by rejecting most feature requests that seek to add them. I'd encourage reading [Havoc Pennington's essay](https://ometer.com/free-software-ui.html) on this subject which I've always loved, specifically the section titled ""the question of preferences"".",
"> That's sort of what I feel is the entire point of Chocolate - bringing the vanilla experience into the present.
It is not necessarily the *experience* (you can have that similarly with other ports like e.g. Eternity or PrBoom+). Choco's aim is to *replicate the behavior* of DOOM.EXE on modern systems. Of course, the former follows from the latter.
That's why I brought in the research project comparison. The question is ""what would happen in DOOM.EXE?"", not ""how would it feel in DOOM.EXE?"". I hope you see the difference.",
"@fabiangreffrath Again, then why do you allow the vanilla breaking extended save file and demo formats still? Those *don't replicate the behavior of the DOOM.EXE whatsoever*. It's very hypocritical to try and justify not adding the *option* to increase the level limits for this reason, when you *already* provide options that *don't* replicate DOOM.EXE's behavior.",Bitter Frustration
@fragglet literally just explained that.,Impatience
"... He explained why they are *there*, but that doesn't automatically resolve them of being hypocritical. Having those features present makes Chocolate defy its own principles and philosophy. You would need to remove them in order to *actually* be true to Vanilla.
Also if you didn't notice, fragglet also *agreed* that it was somewhat arbitrary they are there and likely acknowledged that they are indeed against the philosophy of Chocolate.",Bitter Frustration
"So again, the reason of ""staying true to DOOM.EXE"" has no legs to stand on when you *already* defy it. Only after removing the other philosophy offending features would it make any sense to argue that; which I would be wholeheartedly for and would also then agree that this feature request would be totally invalid.
Basically, either please actually be true to DOOM.EXE and remove all non vanilla breaking aspects or don't arbitrarily deny options that don't change the game play and presentation of the game since there are already some present that do *exactly* that. I don't know why this seems to be such an ""out there"" concept to have consistency.",Bitter Frustration
"Hi @HollowedEmpire, in keeping with your previous comment:
> But, it seems at this point though that we will likely just have to agree to disagree. It is only opinion vs. opinion now, and we appear to be unable to see eye to eye on the topic. Likely we will only waste each other's time trying to continue on. I think we're just going round and round in circles in this point and nothing's likely to be said that hasn't already been said at length. With that in mind I'm going to lock the conversation since I don't think there's any more productive to be said. I'd rather avoid this devolving into a flamewar because I'd like us to stay on friendly terms so we can work together in future.",
"### Do not deprecate the `AbstractQuery::useResultCache`
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| New Feature | no
| RFC | no
| BC Break | no
#### Summary
Please do not deprecate the `AbstractQuery::useResultCache`. Without this method the ""chainability"" gets broken:
`
$em->createQuery()->setParameters()->useResultCache($debug)->getResult()
`
Without the method we have no way of doing the above and honestly I do not understand the need to remove it.",Bitter Frustration
"Encountered the same situation where the ""application context"" determines whether caching is enabled.
```php
$query->useResultCache($this->applicationContext->useCaching());
```
I do like the explicit enable and disable cache methods, but why deprecate the ""useResultCache()"" methods? They can all exist right?",Bitter Frustration
"Same here â˜¹ï¸ I have to refactor all my `useResultCache()` calls with an ugly `if`s:
```
ðŸ‘Œ Nice and smooth in ORM 2:
$res = $entityManager
->createQuery($dql)
->useResultCache($cache_enabled, $ttl)
->getResult();
----
ðŸ¤® Ugly in ORM 3:
$q = $entityManager->createQuery($dql);
if ($cache_enabled) {
$q->enableResultCache($ttl);
}
$res = $q->getResult();
```",Bitter Frustration
"The deprecation was introduced in #7701 , cc @someniatko .",
"@Ocramius it looks like people are complaining, milord
Should we revert the change?",Mocking
"Not really: conditionally using result cache is secondary, mostly for ORM wrappers than for end consumers, so I'd say that the new API is indeed preferable.
In practice, in development environments, replace the cache with a volatile version, rather than passing a `$debug` flag down by multiple layers.",
"> replace the cache with a volatile version
That's clever! @moxival @holtkamp @ilianiv are you satisifed with this? ```
ðŸ‘Œ Nice and smooth in ORM 2:
$res = $entityManager
->createQuery($dql)
->useResultCache($cache_enabled, $ttl)
->getResult();
----
âœ¨ðŸ‘Œâœ¨ Nicer and smoother in ORM 2:
// globally replace the cache with a volatile cache
// stop passing $cache_enabled through a bazillion layers
$res = $entityManager
->createQuery($dql)
->enableResultCache($ttl)
->getResult();
```",Mocking
"***EDIT*** _Lets make my comment less b**chy_
No I am not satisfied by this, when i submitted this issue I did not ask for a lecture in ""volatile cache"" I asked for you guys to leave a perfectly good method (that in no way will mess the ORM code).
If you can not leave the method by all means make us fill our code with ifs and elses for much more smoothness and volatile stuff ðŸ˜¸",Bitter Frustration
"Is this your way of telling us you don't understand what ""volatile cache"" means? If yes, there are nicer ways to do so. If not, then explain what it means to you, because when I read your last sentence , I'm under the impression that you don'tâ€¦ why would this cause lots of ifs and elses?
regarding your EDIT: at least you noticed you were not being very nice :P",Bitter Frustration
@Ocramius @greg0ire Lets say you want to implement an api that should handle GET parameter â€œnocacheâ€ that will make it hit the DB bypassing the cache. In this case replacing the cache is not so good. I think itâ€™ll cost you nothing to add a proxy method useResultCache() that will call one of the two new methods and everyone will be happy.,
"I think it would be simpler to have a cache decorator that is aware of the request stack (assuming Symfony context) and forwards calls to either a normal cache or a null cache (such as one of those: https://github.com/ThaDafinser/psr6-null-cache/blob/master/src/Adapter/ , but for doctrine/cache, we have not yet migrated to PSRs) depending on the value of this parameter. That way it works for the whole API just like you asked, and you did not modify n repositories with n calls to `useResultCache()` for that.",
"Dear @greg0ire this issue is not about me knowing what ""volatile cache"" is or isn't, I may know what volatile cache is (although in all honesty violate cache is just real fancy term) 'coz I was in the field for the last 15 years or I may have no idea coz I am 15 y.o. and I am just starting up with PHP . May i remind you **milord** that PHP does not constrain itself to Symfony nor decorators (even less to decorators that are aware of stuff). If it is too challenging for you to keep the `useResultCache` please just say so _my liege_ we would totally understand. I also understand that ORM 3 sounds exhilarating and you feel like you should make all your loyal subjects miserable and have them change every single bit of code but maybe, just maybe you would let us (the stupid ones that have no clue what ""volatile cache"" is or how to manipulate their request stack with decorators while trying to implement PSR- 35672 at no avail) this little twinkle star of hope and leave the `useResultCache` alone because there is literally **0** (_zero_) effort involved on your part in doing so ðŸ˜º (_please note the emoji - that means my comment is not mean but witty and hilarious filed with warm feelings and melting caches_).",Mocking
Closing and locking here: not worth discussing further with this tone.,Bitter Frustration
"### Bug Report
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| BC Break | yes
| Version | 2.10
#### Summary
Some changes in 2.10 make a BC break. On line 281 in OCI8Statement, the column is not valid in array, so throws a Notice: Undefined index: test.
Before upgrade to 2.10 the line looks like:
`$column = $this->_paramMap[$column] ?? $column;`
After upgrade do 2.10 is:
`$column = $this->_paramMap[$column];`
#### Current behaviour
Undefined index in array. The null coalesce operator was removed (WHY, who accept this ?)
![obraz](https://user-images.githubusercontent.com/16488888/69944505-c696e480-14e7-11ea-9e3f-421f9efc3e2d.png)
#### How to reproduce
The EntityManager must be of OCI8 instance.
```
$query = $this->entityManager->getConnection()->prepare($prepareSQL);
$query->bindValue('test', 'testvalue');
$query->execute();
```
#### Expected behaviour
Should be work by using default column.
Gentlemen. A bit of partolite work with doctrine. You have very poor standards, these things should not appear. Remember: If something works, don't move it.",Bitter Frustration
"> You have very poor standards
Uhm, no, the change was introduced because we do have good and also strict standards, and that was highlighted (although incorrectly) by analysis.
Meanwhile, closing as duplicate of #3738",Bitter Frustration
DAFUQ. So now will be throws exception ? Nice ... your standards.,Bitter Frustration
Nevermind.,Impatience
"Right, locking here: please refrain from opening further issues, if this is the level of discussion.",Bitter Frustration
"Seeing you are happy to close issues and never respond to replies in closed issues, I am forced to open new issue. I'm going to be brief this time.
In https://github.com/hashicorp/vagrant/commit/1b0148bc783298c7aa16a519e133bb26bcc1cc9f you removed function argument, which broke` vagrant-gatling-rsync` plugin. This is not a fault of plugin, but you breaking API compatibility. Please respect semver and do such changes in major version. Until then, please revert the BC break.
refs: #11229 #10974",Bitter Frustration
"Hi @ostrolucky - I would like to remind you that we are a small team of maintainers, and it has been less than or barely 24 hours since I responded to your issue yesterday. There are a lot of other folks that we are supporting at the same time as you, and we are responding as fast as we can. Please know that we aren't ignoring you, nor are we happy to close your issues and never respond. Please have patience with us while we try to assist you and others. Thanks :heart: I'm happy to make a temporary fix to support the two parameter method for this third party plugin using an internal library to Vagrant, but as I mentioned yesterday you will likely get more traction and a quicker fix if you open an issue on the plugin side and fix it there. Our Vagrant release cadence isn't too frequent, and there are lots of other issues and features that we're trying to get in for each release.
If you wish to get this moving faster, there is already a pull request on the plugin: https://github.com/smerrill/vagrant-gatling-rsync/pull/37 I recommend pinging the maintainer on there.
For fixing this today on your side, you should still be able to use the built in rsync synced folder feature, as this is the recommended way to use rsync and Vagrant. Alternatively, downgrading to Vagrant 2.2.5 and using that plugin until a fix has been met is also a possibility. You could also bring in the changes locally and run and install the plugin from source.
We can leave this open to track the issue for now in case others find it. If our next release time comes around and it's still not fixed on their side, I'll apply a patch that fixes this so that it works in 2.2.7. Thanks!",
"Reverts kangax/compat-table#1556
Until they can all be manually verified.",Impatience
Reverts kangax/compat-table#1557,
"At this point weâ€™re just going to go back and forth until one or both of us gets removed as collaborators.
Nothing lands in master that any collab blocks, this included.",Bitter Frustration
"Sorry, but you don't do anything for this project, only exhibit destructive activity, so your opinion does not matter here.",Insulting
"Thatâ€™s not your decision to make - weâ€™re all collaborators so all of our opinion matters equally, and without consensus, *nothing happens*.",Bitter Frustration
"Bad C code is generated with the following Nim code:
### Example
```nim
import locks
var l: Lock
echo l
```
### Current Output
```
CC: ltest.nim
Error: execution of an external compiler program 'gcc -c -w -I/home/tay/.choosenim/toolchains/nim-1.0.4/lib -I/home/tay -o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c.o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c' failed with exit code: 1
/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c: In function â€˜dollar___3Dq1hcGH7a2iNlnh4hnPbgâ€™:
/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c:219:50: error: â€˜pthread_mutex_tâ€™ {aka â€˜union <anonymous>â€™} has no member named â€˜abiâ€™
219 | addQuoted__N56rmvBL9a3xqnKq09cKdb3A((&result), x.abi);
| ^
```
### Expected Output
Compiler either compiles properly or errors before C code gen
### Possible Solution
* It looks like there might be an issue with the implementation of `$` for the Lock type
### Additional Information
* Fedora 31: `Linux localhost.localdomain 5.3.11-300.fc31.x86_64 #1 SMP Tue Nov 12 19:08:07 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux`
```
$ nim -v
Nim Compiler Version 1.0.4 [Linux: amd64]
Compiled at 2019-11-27
Copyright (c) 2006-2019 by Andreas Rumpf
git hash: c8998c498f5e2a0874846eb31309e1d1630faca6
active boot switches: -d:release
```",Bitter Frustration
Why close? The issue is still there,Impatience
"It's been several months and there was no feedback, I figured an issue wasn't necessary.",Impatience
"@3n-k1 that's not how it works in open-source in general - it just means that nobody worked on the issue yet, but as long as the bug is still there it should be open",
"I understand how FOSS works. Now, I would prefer if you didn't mention me again as I would prefer not to receive notifications for this.",
"It's been over half a year, and I get a bad taste in my mouth every time I see this. I'm closing this, and I will be blocking anyone who responds or re-opens; I really don't care if that makes me an ass.
Y'all have a minimal example and all the information necessary for this, feel free to open a ***new*** issue with a copy-paste of the initial issue I posted here.
This is me officially leaving the Nim community. So long, and thanks for all the fish.",Bitter Frustration
"> It's been over half a year, and I get a bad taste in my mouth every time I see this. I'm closing this, and I will be blocking anyone who responds or re-opens; I really don't care if that makes me an ass.
> > Y'all have a minimal example and all the information necessary for this, feel free to open a ***new*** issue with a copy-paste of the initial issue I posted here.
> > This is me officially leaving the Nim community. So long, and thanks for all the fish.
I'm okay with getting blocked by you - but really, ""leaving"" the community because of INCORRECT Nim code not compiling (even if it compiles it'll crash at best since you didn't initialize the lock, and for printing you can just try to use repr or cast to a pointer and repr) is not okay. Also this is open source, not some software you paid for, so you maybe should've tried fixing it yourself before? :)",Mocking
Just brigading so I can get my promised block. I certainly would not like you to use anything I contribute on.,Bitter Frustration
"@Yardanico I'm leaving the community because Araq has treated me like shit numerous times, but thanks for playing.",Bitter Frustration
The fact you need to bitch and moan about someone not wanting to be treated like a fucking piece of garbage just confirms this community is fucking garbage.,Bitter Frustration
"@beef331 Even if I *did* want to use any of your trash, it would be a bit difficult since you apparently can't bring yourself to write more than two sentences in a readme, assuming you write anything at all.
Nice website by the way, I really enjoy the gif of your superman64 clone playing as the site's background.
You wanna get shitty? Let's fuckin' go, kid. At least I actually contributed to Nim before Araq treated me like shit, unlike you.
Araq, be careful who you treat like garbage, you may lose one of your shitty niche language's biggest evangelists.",Bitter Frustration
"Am I ranting now? Sure the fuck am. I thought you'd all be able to be fucking adults about this, but I guess fucking not.",Bitter Frustration
"@Yardanico I tried to be as polite as possible when closing this, and you attempt to dogpile and bitch? Real fucking mature.
![image](https://user-images.githubusercontent.com/51172302/86258541-ecbd5980-bb88-11ea-8e54-477aaba9f3ef.png)",Bitter Frustration
"just gonna document why I'm leaving. You're all petulant children.
![image](https://user-images.githubusercontent.com/51172302/86258654-0ced1880-bb89-11ea-8517-36188fecf092.png)",Insulting
"Please do google ""inurl:irclogs.nim-lang.org simula"". Better yet, search ""from:simula#3097"" in the discord server. My last interaction is exactly the bullshit I'm talking about. Glad to see it hasn't ended.
![image](https://user-images.githubusercontent.com/51172302/86258722-22fad900-bb89-11ea-8ab6-a109557aad3d.png)",Bitter Frustration
"Hello, TSS edit on Details/Metric tab is not working (running V3.5-RC2X) although it was working properly in previous version. A number can be edited but when clicked outside of the box it goes to zero automatically. I made a search but did not find a similar topic.
Thanks.",
"TSS is being deprecated in v3.5, there is a compatibility metric named TSS to enable pre-existing charts to continue working but it has several limitations. I recommend to add a Data Field in preferences similar to your existing TSS but named BikeStress and use it for overrides, it will impact on your TSS dependent charts and it is a better solution for moving forward.",
"> TSS is being deprecated in v3.5, there is a compatibility metric named TSS to enable pre-existing charts to continue working but it has several limitations. I recommend to add a Data Field in preferences similar to your existing TSS but named BikeStress and use it for overrides, it will impact on your TSS dependent charts and it is a better solution for moving forward.
Many thanks for your response. I have manually edited BikeScore in the Details/Metrics tab of the activity, but it does not impact on TSS unfortunately. I searched under chart settings to add Bike Stress but there is no option. :(",Impatience
"BikeScore is an independent metric, the new name for TSS is BikeStress and you can add it to Details tab with an entry like this in Options/Preferences > Data Fields:
![BikeStressOverride](https://user-images.githubusercontent.com/1444784/71268127-f6891900-232a-11ea-8306-7352eea9f0f3.png)",
"> BikeScore is an independent metric, the new name for TSS is BikeStress and you can add it to Details tab with an entry like this in Options/Preferences > Data Fields:
> ![BikeStressOverride](https://user-images.githubusercontent.com/1444784/71268127-f6891900-232a-11ea-8306-7352eea9f0f3.png)
![GC options 2](https://user-images.githubusercontent.com/56796799/71310520-b787c000-241d-11ea-8ba9-b25d5dc13ed5.png)
Unfortunately there is no BikeStress here. Please see attached file.",Impatience
"For new users it is created automatically, existing users need to add it using the (+) button and the arrows to order.",
"> For new users it is created automatically, existing users need to add it using the (+) button and the arrows to order.
I click the (+) button but there is no choice to add anything. :( ![GC opt3](https://user-images.githubusercontent.com/56796799/71323124-71dbfd80-24d8-11ea-84a8-af8af4d5a685.png)",Impatience
"A new field was added at the first row, a simple click allows to change the Screen Tab or Field part, the arrows at the bottom left allows to position the selected field were you want.",
"> A new field was added at the first row, a simple click allows to change the Screen Tab or Field part, the arrows at the bottom left allows to position the selected field were you want.
Many thanks for your help, I appreciate it! I thought a drop down menu would appear, but the solution is to manually write ""BikeStress"" :)",Irony
"I tried this method a few weeks ago and could still not get it to work properly,does not give you a tick box next to bikestress in the edit metrics tab and does not overwrite imported details because of this. Have tried deleting the TSS field and just leaving bikestress there and that does not work either",Bitter Frustration
"> I tried this method a few weeks ago and could still not get it to work properly,does not give you a tick box next to bikestress in the edit metrics tab and does not overwrite imported details because of this. Have tried deleting the TSS field and just leaving bikestress there and that does not work either
Because itâ€™s BikeStress, not bikestress",Bitter Frustration
"Hi VS Code dev here :wave:
Open a python file, invoke a context menu in the editor and notice the following:
1) Run Current Test File command is there. When I execute it it says that I do not have testing enabled. I suggest to add this command only if testing is enabled in my workspace
2) Sort Imports -> shouldn't this be a part of the format document. I do not see why this command needs to be so promiment to take space in the context menu. Also there is already `Source Action...` command which shows the Sort Imports so this feels like duplication.
3) ""Run python file in terminal"" and ""Run selection line in python terminal"". I do not understand why one command says ""in terminal"" and the other ""in python terminal"". When I exectute them they just go to the terminal for me. Can we have some consistency in titles, or is it possible to drop one of those actions. Reason being, user can select the whole file and do Run selection, to cover the python file case
4) Run in Interactive window: I suggest to show this only if I have Jupter or whatever needed setup
5) Most of these actions do not make sense while you are debugging. I would also consider to hide them while the user is debugging (since the menu grows then additonally with vscode debug commands)
![Screenshot 2019-12-24 at 12 48 24](https://user-images.githubusercontent.com/1926584/71411983-d66b9980-264b-11ea-822c-a3375166d30c.png)
I believe VS Code plans to introduce context sub menus and then python could add all the actions to the submenu potentially.
fyi @qubitron @DonJayamanne",
"You also show these actions in the Explorer. I think the Explorer should be used for FS operations.
![Screenshot 2019-12-24 at 16 06 28](https://user-images.githubusercontent.com/1926584/71417755-5c490e00-2667-11ea-8c30-b2eb99d8c409.png)",
"Thank you for the suggestion! We have marked this issue as ""needs decision"" to make sure we have a conversation about your idea. We plan to leave this feature request open for at least a month to see how many ðŸ‘ votes the opening comment gets to help us make our decision.",
"@isidorn Yeah, the python/jupyter team has absolutely no regard for these issues. In particular, the [context menu bloat](https://github.com/microsoft/vscode-python/issues/15834) is even worse these days, and all the items are contributed by the jupyter extension. The summary of microsoft's view is what @claudiaregio said: > Just like any other VS Code menu item, *users do not have to use or click on any options/features made available to them if they do not wish to use them.* As I [said](https://github.com/microsoft/vscode/issues/9285#issuecomment-812277371), it's like saying: ## If you don't want candy crush, just don't click in the candy crush icon.",Bitter Frustration
"> @isidorn Yeah, the python/jupyter team has absolutely no regard for these issues.
> > In particular, the [context menu bloat](https://github.com/microsoft/vscode-python/issues/15834) is even worse these days, and all the items are contributed by the jupyter extension.
> > The summary of microsoft's view is what @claudiaregio said:
> > > Just like any other VS Code menu item, _users do not have to use or click on any options/features made available to them if they do not wish to use them._
> > As I [said](https://github.com/microsoft/vscode/issues/9285#issuecomment-812277371), it's like saying:
> > ## If you don't want candy crush, just don't click in the candy crush icon.
It's ""disruptive"" because it's true.",Bitter Frustration
"We no longer show testing related commands or the selection command in the menu:
![image](https://user-images.githubusercontent.com/13199757/208703250-76783034-9962-4d54-bacc-35c47bfe5eac.png)",
"CoffeeScript:
```coffeescript
class Cat
[@symbol]: =>
```
Output Javascript:
```javascript
class Cat {
constructor() {
this.[this.symbol] = this.[this.symbol].bind(this);
}
[this.symbol]() {}
}
```
But must:
```javascript
class Cat {
constructor() {
this[Cat.symbol] = this[Cat.symbol].bind(this);
}
[Cat.symbol]() {}
}
```
Two bugs:
===
1) `this` in constructor is not `this` in class body
2) not need dot after `this` keyword, `this.[exp]` is syntax error
Reason:
===
```coffeescript
class {
[EXP PROP NAME IN CLASS BODY NOT WORKING CORRECT]() {}
}
```",Bitter Frustration
"Hey, why you close it?",Impatience
"~Closes #752~ After some internal discussion, we're reverting this. See #1131.
To test this in your project, check out the branch and do two local installs from there:
* `pip install -e .` gets you `jinja`
* `pip install -e jinja2-compat` gets you a shim that redirects `jinja2` imports to `jinja` and emits a deprecation warning.
This seems to work in real code bases like Flask's tests and @indico. Once this is released, you'd only have to `pip install jinja2`, it depends on `jinja`. This should allow projects to gradually rename their imports and continue to use packages that still depend on `jinja2` and aren't updated.",
"However, some weird issues come up when you undo the changes to the tests so they all still import from `jinja2`.
First is that the tests won't run because there's a circular import issue with `from jinja2 import asyncfilters` getting imported before `asyncsupport`. This apparently already fails in master, but for some reason test collection only triggers it when going through the redirect.
If you exclude/remove the async tests, tests that expect exceptions begin failing with seemingly the correct exception. Apparently, something about the import internals causes `from jinja2.exceptions import TemplateNotFound` and `from jinja.exceptions import TemplateNotFound` to have the same repr but different ids.",Bitter Frustration
"I've tried experimenting with some other methods as well:
* Adding a special finder to `sys.meta_path` seems to have the same issues as above.
* Creating placeholder modules that do `from jinja.{name} import *` doesn't include anything that starts with `_`, which while not public could cause compatibility issues.
* Generating placeholders that literally import `*` and every `_name` works but ties compatibility to the imported private names remaining available into the future.",
"Switched to a new solution that doesn't have the problems I was seeing when messing with `sys.modules`. Now `jinja2` just has modules matching what's in `jinja`, with a special function that re-exports all the public and `_` names and emits a warning. Names starting with `__` (double underscore) are skipped as that seemed to be causing the issues.
The shim should continue working as long as the modules in `jinja` continue to match, and a quick release could be made to add more modules if we still want to maintain compatibility later.
All tests pass with `jinja` or `jinja2` imports.",
"Tentatively planning to have the jinja2 package depend on ""jinja<4.0"" so that developers have the entire 3.0 cycle (however long that is, there's no plans for 4.0 yet) to upgrade. Past that, I may update the package to extend the support, but not making any promises.
As mentioned above, the only way the shim becomes incompatible is if module names change, which is unlikely, but it has happened with, for example, Werkzeug to improve organization. So supporting the 3.0 cycle means not permanently moving anything if it happens at all until 4.0, which is sort of what a major version bump is for anyway.",
"The interesting stuff happens in `jinja2-compat/src/jinja2/__init__.py`, the rest is just renames.",Mocking
"Moved the Jinja2 compat package to a [separate repo](https://github.com/pallets/jinja2-compat). This makes the separation clearer, and makes it easier to make releases of the compat package independently of the state of the Jinja repo in case that's needed.",
"You can try installing Jinja2 2.11.0rc1 from https://test.pypi.org/project/Jinja2/2.11.0rc1/:
```
pip install -i https://test.pypi.org/simple/ --pre Jinja2
```
You should get Jinja 2.11.0rc1 pulled in as a dependency and both `jinja` and `jinja2` should be importable.",
We actually discussed about this on our Discord a few days ago and the rename will be 3.0 and not a point release as initially planned.,
"Hi,
**Describe the bug**
I can't access some data via GraphQL versus simple fetch.
**Steps to reproduce the behavior**
1- Create a new _avatar_ media field on users. 2- Try to get that avatar via the below GraphQL query:
````graphql
query {
me {
username
avatar {url}
}
}
````
3)- You'll get the below error:
_""Cannot query field \""avatar\"" on type \""UsersPermissionsMe\"".""_ with a _GRAPHQL_VALIDATION_FAILED_ code.
**Code snippets**
The below fetch if returning the avatar object:
http://localhost:1337/users/me/
````json
{
""id"": 20,
""username"": ""***"",
""email"": ""***@gmail.com"",
""provider"": ""local"",
""confirmed"": true,
""blocked"": null,
""created_at"": ""2019-12-26T19:17:36.847Z"",
""updated_at"": ""2020-01-01T20:11:46.181Z"",
""avatar"": {
""id"": 12,
""url"": ""/uploads/8dd5de998ffa4de09fc264c0fab924ab.jpg""
}
}
````
**System**
- Node.js version: 10.17.0
- NPM version: 6.11.3
- Strapi version: v3.0.0-beta.18.3
- Database: sqlite
- Operating system: macod
**Expected behavior**
I should be able to get the avatar data from GraphQL",
"> This is a templated message
Hello, please follow the issue template.
A proper issue submission let's us better understand the origin of your bug and therefore help you.
I will reopen your issue when we receive the issue following the template guidelines and properly fill out the template.
Please update the issue.
Thank you.
> A copy of the bug report template can be found below:
```
<!--
Hello ðŸ‘‹ Thank you for submitting an issue.
Before you start, please make sure your issue is understandable and reproducible.
To make your issue readable make sure you use valid Markdown syntax.
https://guides.github.com/features/mastering-markdown/
-->
**Describe the bug**
A clear and concise description of what the bug is.
**Steps to reproduce the behavior**
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error
**Expected behavior**
A clear and concise description of what you expected to happen.
**Screenshots**
If applicable, add screenshots to help explain your problem.
**Code snippets**
If applicable, add code samples to help explain your problem.
**System**
- Node.js version: <!-- Please ensure you are using the Node LTS version (v10) -->
- NPM version:
- Strapi version: <!-- Please make sure you are on the latest version -->
- Database:
- Operating system:
**Additional context**
Add any other context about the problem here.
```",
"@derrickmehaffy I just update the issue template, thanks for re-opening it :)",
"Thank you for reporting this issue.
The [definition UsersPermissionsMe](https://github.com/strapi/strapi/blob/632bf537ecf624019f2710f6b63dd2a93c6f74f0/packages/strapi-plugin-users-permissions/config/schema.graphql#L22) that is used for the `me` query is static.
We should make it a little bit more dynamic based on the User model structure.
A workaround is to use the [customization concept](https://strapi.io/documentation/3.0.0-beta.x/concepts/customization.html#plugin-extensions) to override the `schema.graphql` file of the **Users & Permissions** plugin.",
"Since I update to the latest version of Strapi, even the simple fetch doesn't return the avatar object.
I read the documentation about the customization concept but there is no `schema.graphql` in the extension folder. Grateful if you could share some hints.",Impatience
"I think my problem is on relation of this topic.
""users/me"" return only custom fields but not relationships, its for this reason avatar (files, images) is not listed.
Auth.callback (""auth/local"") return all datas correctly, using the same process for User.me (users/me) without the jwt outpout, this would be the solution ?",
Have the same problem.,Impatience
"I don't know if overriding the schema of the plugin works in a different way than overriding controllers or services but pasting `node-modules/strapi-plugin-users-permissions/config/schema.graphql` into `extensions/users-permisions/config` for editing the later one doesn't override it but tries to re-register it
```
[2020-02-07T09:15:41.870Z] error Error: There can be only one type named ""UsersPermissionsMe"".
Field ""UsersPermissionsMe.id"" can only be defined once.
Field ""UsersPermissionsMe.username"" can only be defined once.
Field ""UsersPermissionsMe.name"" can only be defined once.
Field ""UsersPermissionsMe.email"" can only be defined once.
Field ""UsersPermissionsMe.confirmed"" can only be defined once.
Field ""UsersPermissionsMe.blocked"" can only be defined once.
Field ""UsersPermissionsMe.role"" can only be defined once.
There can be only one type named ""UsersPermissionsMeRole"".
Field ""UsersPermissionsMeRole.id"" can only be defined once.
Field ""UsersPermissionsMeRole.name"" can only be defined once.
Field ""UsersPermissionsMeRole.description"" can only be defined once.
Field ""UsersPermissionsMeRole.type"" can only be defined once.
There can be only one type named ""UsersPermissionsLoginInput"".
Field ""UsersPermissionsLoginInput.identifier"" can only be defined once.
Field ""UsersPermissionsLoginInput.password"" can only be defined once.
Field ""UsersPermissionsLoginInput.provider"" can only be defined once.
There can be only one type named ""UsersPermissionsLoginPayload"".
Field ""UsersPermissionsLoginPayload.jwt"" can only be defined once.
Field ""UsersPermissionsLoginPayload.user"" can only be defined once.
Field ""Query.me"" can only be defined once.
Field ""Mutation.login"" can only be defined once.
Field ""Mutation.register"" can only be defined once.
at assertValidSDL (/home/ec2-user/environment/Molecula-dev/node_modules/graphql/validation/validate.js:108:11)
at Object.buildASTSchema (/home/ec2-user/environment/Molecula-dev/node_modules/graphql/utilities/buildASTSchema.js:71:34)
at Object.buildSchemaFromTypeDefinitions (/home/ec2-user/environment/Molecula-dev/node_modules/graphql-tools/dist/generate/buildSchemaFromTypeDefinitions.js:23:28)
at makeExecutableSchema (/home/ec2-user/environment/Molecula-dev/node_modules/graphql-tools/dist/makeExecutableSchema.js:26:29)
at Object.generateSchema (/home/ec2-user/environment/Molecula-dev/node_modules/strapi-plugin-graphql/services/Schema.js:300:22)
at Object.initialize (/home/ec2-user/environment/Molecula-dev/node_modules/strapi-plugin-graphql/hooks/graphql/index.js:59:50)
at Promise.resolve.then (/home/ec2-user/environment/Molecula-dev/node_modules/strapi/lib/hooks/index.js:37:28)
```
But editing the node-modules one doesn't work neither. I have just copied the type UsersPermissionsUser from the autogenerated file `exports/graphql/schema.graphql` into the type UsersPermissionsMe of `node-modules/strapi-plugin-users-permissions/config/schema.graphql` and it doesn't return any error but querying a field which wasn't there (related or not) renders a GraphQL validation error. I suppose this schema is not the one which is being used for the validation...",Bitter Frustration
Can you please share the content of the `schema.graphql` file.,
"Just one line added, the ""name: String"" in line 25, but name cannot be queried through the me modek in graphql
```
const _ = require('lodash');
const { ApolloError } = require('apollo-server-koa');
/**
* Throws an ApolloError if context body contains a bad request
* @param contextBody - body of the context object given to the resolver
* @throws ApolloError if the body is a bad request
*/
function checkBadRequest(contextBody) {
if (_.get(contextBody, 'output.payload.statusCode', 200) !== 200) {
const statusCode = _.get(contextBody, 'output.payload.statusCode', 400);
const message = _.get(contextBody, 'output.payload.message', 'Bad Request');
throw new ApolloError(message, statusCode, _.omit(contextBody, ['output']));
}
}
module.exports = {
type: {
UsersPermissionsPermission: false, // Make this type NOT queriable.
},
definition: `
type UsersPermissionsMe {
id: ID!
username: String!
name: String
email: String!
confirmed: Boolean
blocked: Boolean
role: UsersPermissionsMeRole
}
type UsersPermissionsMeRole {
id: ID!
name: String!
description: String
type: String
}
input UsersPermissionsLoginInput {
identifier: String!
password: String!
provider: String = ""local""
}
type UsersPermissionsLoginPayload {
jwt: String!
user: UsersPermissionsMe!
}
`,
query: `
me: UsersPermissionsMe
`,
mutation: `
login(input: UsersPermissionsLoginInput!): UsersPermissionsLoginPayload!
register(input: UserInput!): UsersPermissionsLoginPayload!
`,
resolver: {
Query: {
me: {
resolverOf: 'User.me',
resolver: {
plugin: 'users-permissions',
handler: 'User.me',
},
},
role: {
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.getRole',
resolver: async (obj, options, { context }) => {
context.params = {...context.params, ...options.input};
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.getRole(context);
return context.body.role;
},
},
roles: {
description: `Retrieve all the existing roles. You can't apply filters on this query.`,
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.getRoles', // Apply the `getRoles` permissions on the resolver.
resolver: async (obj, options, { context }) => {
context.params = {...context.params, ...options.input};
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.getRoles(context);
return context.body.roles;
},
},
},
Mutation: {
createRole: {
description: 'Create a new role',
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.createRole',
resolver: async (obj, options, { context }) => {
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.createRole(context);
return { ok: true };
},
},
updateRole: {
description: 'Update an existing role',
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.updateRole',
resolver: async (obj, options, { context }) => {
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.updateRole(
context.params,
context.body
);
return { ok: true };
},
},
deleteRole: {
description: 'Delete an existing role',
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.deleteRole',
resolver: async (obj, options, { context }) => {
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.deleteRole(context);
return { ok: true };
},
},
createUser: {
description: 'Create a new user',
plugin: 'users-permissions',
resolverOf: 'User.create',
resolver: async (obj, options, { context }) => {
context.params = _.toPlainObject(options.input.where);
context.request.body = _.toPlainObject(options.input.data);
await strapi.plugins['users-permissions'].controllers.user.create(
context
);
return {
user: context.body.toJSON ? context.body.toJSON() : context.body,
};
},
},
updateUser: {
description: 'Update an existing user',
plugin: 'users-permissions',
resolverOf: 'User.update',
resolver: async (obj, options, { context }) => {
context.params = _.toPlainObject(options.input.where);
context.request.body = _.toPlainObject(options.input.data);
await strapi.plugins['users-permissions'].controllers.user.update(
context
);
return {
user: context.body.toJSON ? context.body.toJSON() : context.body,
};
},
},
deleteUser: {
description: 'Delete an existing user',
plugin: 'users-permissions',
resolverOf: 'User.destroy',
resolver: async (obj, options, { context }) => {
// Set parameters to context.
context.params = _.toPlainObject(options.input.where);
context.request.body = _.toPlainObject(options.input.data);
// Retrieve user to be able to return it because
// Bookshelf doesn't return the row once deleted.
await strapi.plugins['users-permissions'].controllers.user.findOne(
context
);
// Assign result to user.
const user = context.body.toJSON
? context.body.toJSON()
: context.body;
// Run destroy query.
await strapi.plugins['users-permissions'].controllers.user.destroy(
context
);
return {
user,
};
}
},
register: {
description: 'Register a user',
plugin: 'users-permissions',
resolverOf: 'Auth.register',
resolver: async (obj, options, {context}) => {
context.request.body = _.toPlainObject(options.input);
await strapi.plugins['users-permissions'].controllers.auth.register(context);
let output = context.body.toJSON ? context.body.toJSON() : context.body;
checkBadRequest(output);
return {
user: output.user || output, jwt: output.jwt
};
}
},
login: {
resolverOf: 'Auth.callback',
plugin: 'users-permissions',
resolver: async (obj, options, {context}) => {
context.params = {...context.params, provider: options.input.provider};
context.request.body = _.toPlainObject(options.input);
await strapi.plugins['users-permissions'].controllers.auth.callback(context);
let output = context.body.toJSON ? context.body.toJSON() : context.body;
checkBadRequest(output);
return {
user: output.user || output, jwt: output.jwt
};
}
}
}
}
};
```",
"Hello! You don't need to copy the entire file.
```js
module.exports = {
definition: `
extend type UsersPermissionsMe {
name: String
}
`
}
```",
"Thanks for your comment @lauriejim, really helpful!",
"Now I reached the other problem, it works with plain fields but it just returns null with relationships
With this schema in `extensions/users-permisions/config`...
```
module.exports = {
definition: `
extend type UsersPermissionsMe {
name: String
messages(sort: String, limit: Int, start: Int, where: JSON): [Message]
}
`
}
```
... and this query ...
```
{
me{
name
messages{
_id
}
}
}
```
... I'm getting this response
```
{
""data"": {
""me"": {
""name"": ""Mig"",
""messages"": null
}
}
}
```",Bitter Frustration
You also have to add `messages` in your extension.,
"My fault, I copied it wrong but the code was well, I have edited the post.",
Ho yes I confirm!,Irony
"Same for me.
extensions/users-permisions/config/schema.graphql
```
module.exports = {
definition: `
extend type UsersPermissionsMe {
avatar: UploadFile
}
`
}
```
the query
```
# Write your query or mutation here
query {
me {
username
email
avatar {
url
}
}
}
```
the result
```
{
""data"": {
""me"": {
""username"": ""geo"",
""email"": ""email@.com"",
""avatar"": null
}
}
}
```
any workaround to get to get that value?",Impatience
"@geosigno Same for me. @lauriejim Please help
the query =>
query {
me {
id
username
user_info {
name
birthday
}
user_status {
securityLevel
}
}
}
its result =>
{
""data"": {
""me"": {
""id"": ""1"",
""username"": ""ImUser1"",
""user_info"": {
""name"": null,
""birthday"": null
},
""user_status"": {
""securityLevel"": null
}
}
}
}
my schema.graphql =>
module.exports = {
definition: `
extend type UsersPermissionsMe {
user_status: UserStatus
user_info: UserInfo
}
`
}
I am also trying another way, but it doesn't work for me.
https://github.com/strapi/strapi/issues/2627",Impatience
"I solved this issue with #2627 and below query is working well.
query{
me {
id
username
}
self {
user_info {
name
birthday
}
user_status {
securityLevel
}
}
}",
"I'm commenting here since #2627 is locked, and both issues are very related. I had to make a few adjustments to the fix mentioned by @unolee I wanted to query relationship linked to the authenticated user. I created a file named `extensions/users-permissions/config/schema.graphql.js` with this content:
```js
module.exports = {
query: `
self: UsersPermissionsUser
`,
resolver: {
Query: {
self: {
resolver: 'plugins::users-permissions.user.me'
},
},
}
};
```
Custom fields are now working fine, if you replace `me` by `self` in your queries.",
"@remi2j As @lauriejim said in https://github.com/strapi/strapi/issues/2627#issuecomment-568257332 you shouldn't use `self: UsersPermissionsUser` here out of security reasons.
Instead create a further Type with restricted fields, e.g.: ```
definition: `
type Self {
id: ID
username: String
email: String
additionalField1: String
additionalField2: String
...
}
`,
query: `
self: Self
`,
resolver: {
Query: {
self: {
resolver: 'plugins::users-permissions.user.me'
},
},
}
```
Or extend UsersPermissionsMe as seen in: https://github.com/strapi/strapi/issues/4878#issuecomment-584106410",
I confirm the workaround by replacing _me_ with _self_ is working fine.,
"This issue has been mentioned on **Strapi Community Forum**. There might be relevant details there:
https://forum.strapi.io/t/strapi-v4-cant-access-custom-fields-from-users-me-via-graphql/13744/1",
"This issue has been mentioned on **Strapi Community Forum**. There might be relevant details there:
https://forum.strapi.io/t/get-all-items-related-to-user/14159/1",
should be fixed in v4 I believe,
"Strava Sync funktioniert nicht mit Version 3.5 RC2X. Habe alles versucht, neu authentiziert, etc.
Token in GC ist nicht der gleiche wie in Strava
Fehler: ""ZeitÃ¼berschreitung bei Socketoperation""",Bitter Frustration
see https://groups.google.com/d/msg/golden-cheetah-users/TIXYuun6ntY/vkz5AIJ1BwAJ,
Dies funktioniert aber nicht !,Bitter Frustration
"Please, use the forum to get help.",
i think this is the forum,
Thereâ€™s a link to the forum in the 1st answer.,
"so i've done everything as described in the link and also reinstalled GC.
It is still not working, so it must be a false code!!!!",Bitter Frustration
"I just tried it and it works. Authorise with Strava via add account, then sync (make sure you set the date range to a range that includes some activity).
If the code was broken there would be a lot more reports of issues, believe me. We have a large user base. Make sure you are using the latest dev build from here:
https://github.com/GoldenCheetah/GoldenCheetah/releases/tag/V3.5-RC2X
Also, the full release is due in a week or so.",
"So i tried it again, nothing works. After login in strava via Facebook and allow Golden cheetah, the fault ""socket Operation (4)"" appears.",Bitter Frustration
How should i set a Data Range?,
"If you post in the users forum may be other user with the same problem can explain what he did i to solve it.
GitHub issues are used to track bugs and features only, as a FOSS we donâ€™t have the resources to give case by case support here.",
"In Windows gVim (not `win32unix`), `:Gwrite` doesn't work on files that are in symlinked directories (but `:Gstatus` works). It says that the file is _outside repository_.
In `win32unix` and `nvim` it works instead (it changes the buffer name to the real path).
For gVim (`win32`), I could fix it by changing https://github.com/tpope/vim-fugitive/blob/6bc345f6f16aee0dcc361f2f3bf80e4964f461af/autoload/fugitive.vim#L792 with
```vim
return FugitiveVimPath(empty(url) ? a:url : has('win32') ? resolve(url) : url)
```",Bitter Frustration
"Where is the symlink, where is it pointing, and where is the repository?",Impatience
"![Imgur](https://i.imgur.com/2DkGNXk.jpg)
Real path: D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin\buffer_history.vim
and repository in
D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin",
You said the symlink was a directory. I ask where the symlink is and you give me a file. I also asked where it's pointing and you did not answer. I can't help you if you won't answer my questions.,Bitter Frustration
"Sorry, it was late and I was sleepy. The symlinked directory is the same as the repo, I have:
```
~\vimfiles (not symlinked, real path)
~\vimfiles\plugin (symlink to D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin, that is also a repo)
(repo is also readable here -> ~\vimfiles\plugin\.git)
~\vimfiles\plugin\buffer_history.vim (:Gwrite here gives problems)
:Gwrite in the real file path (D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin\buffer_history.vim) works.
```
I understand that fighting with Windows paths is very annoying, just saying that fugitive isn't perfect in gVim on Windows, I had other issues as well, I didn't want to bother and I can also understand if you don't consider Windows a top priority. I just had an issue and I reported it, feel free to ingore it if you don't think it's worth wasting time on it.",Bitter Frustration
"On initialization, Fugitive checks if the current file is a symlink, and if so, calls `cd .`. On UNIX, and presumably `win32unix` since it's working there for you, this forces the current buffer to have its name resolved.
https://github.com/tpope/vim-fugitive/blob/08601f221a336d13a606b33714e5a1e8457b07f7/plugin/fugitive.vim#L212-L215
If you can find a way to force the buffer name to resolve on win32, we can do that as well. I would start with `exe 'cd' fnameescape(getcwd())`.
Throwing `resolve()` elsewhere is a game of whack-a-mole, plus it adds IO to things that are supposed to be fast. It's not a tenable solution.",Bitter Frustration
"Ok I'll try to do that, thanks.",
"What I could figure out is that in Windows something work differently with git.
I did some testing and I think that the problem lies in Git for Windows, that works differently, in that it doesn't realizes that the two files belong to the same repository. The command gives _the same error_ also if run from the terminal (cmd.exe).
I updated git, just in case, but the error persists. I think only a `resolve()` call in the `:Gwrite` command for `win32` can fix this.
These are the `cmd` generated in https://github.com/tpope/vim-fugitive/blob/c83355d5c52002f94d08267f1d14ca6d1a2763e9/autoload/fugitive.vim#L429
```
WLS Debian (neovim), also here $HOME/.vim/plugin is a symlink to the Dropbox directory,
:Gwrite works and the file path is changed to its resolved path
['-C', '/mnt/d/Gianmaria/Dropbox (Personale)/Linux/vimrc/plugin',
'--literal-pathspecs', 'add', '--',
'/home/gianmaria/.vim/plugin/buffer_history.vim']
``` ```
gVim Windows, also here the starting paths are different, but the command doesn't work
['-C', 'D:/Gianmaria/Dropbox (Personale)/Linux/vimrc/plugin',
'--literal-pathspecs', 'add', '--',
'C:/Users/Gianmaria/vimfiles/plugin/buffer_history.vim']
```
I also tested the same commands in the terminal (WSL and cmd.exe)
```
In WSL terminal, this works
$ git -C /mnt/d/Gianmaria/Dropbox\ \(Personale\)/Linux/vimrc/plugin --literal-pathspecs add -- /home/gianmaria/.vim/plugin/buffer_history.vim
In cmd.exe, this doesn't work (same error as with fugitive):
C:\Users\Gianmaria\vimfiles\plugin>git -C ""d:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin"" --literal-pathspecs add -- c:\Users\Gianmaria\vimfiles\plugin\buffer_history.vim
```",Bitter Frustration
"This is news to me, but I don't believe it changes anything. Fugitive only passes an absolute path if it can't figure out the appropriate relative path, because older Git versions didn't support absolute paths at all. The core problem remains: Fugitive expects your buffer name to not be a symlink.",
"This would fix it:
```
let file = s:winshell() ? resolve(file) : file
```
just before
https://github.com/tpope/vim-fugitive/blob/c83355d5c52002f94d08267f1d14ca6d1a2763e9/autoload/fugitive.vim#L4605-L4610",
"Since the problem is with Git for Windows (imho), I can't think of anything else.
I made a mapping to resolve current file, and it works with it:
nnoremap \f\ :exe 'file' resolve(@%)<cr>",
"The problem is not with Git for Windows. You pointed out the real problem in your initial post (emphasis my own).
> In `win32unix` and `nvim` it works instead (**it changes the buffer name to the real path**).
All of Fugitive is built around the assumption that the buffer name is the the real path. At first I ignored this, but then bugs cropped up when people called `:cd` and the buffer name changed from a symlink path to a real path mid flight, so my solution was to force a `:cd .` early, thus preventing symlink paths from ever entering the equation. What you've told me is that this `:cd .` doesn't work. Two solutions spring to mind:
1. Find a way to make it work (where ""make it work"" is ""resolve symlink in the buffer name"").
2. If there is no way to make it work, then the original bug is irrelevant. Stop calling `resolve()` on the Git dir and work tree and everything should be back to matched up.
The following solutions do not spring to mind:
1. Change the semantics of one public function by adding `resolve()` to it, and verify that the one command that is confirmed broken is fixed.
Please stop doubling down on the bad idea that I told you is bad, and try the thing I asked you to try:
> I would start with exe 'cd' fnameescape(getcwd()).
---
Ironically, after I started writing this you edited in a decent idea.
> I made a mapping to resolve current file, and it works with it:
> > ```
> nnoremap \f\ :exe 'file' resolve(@%)<cr>
> ```
See also https://github.com/tpope/vim-fugitive/pull/814#issuecomment-446767081 where I attempt to provide a more robust way to do this automatically. This is the sort of thing I would consider doing, but getting to work right requires calling `edit` to reload the file, and I worry that editing the file from `BufReadPost` will cause non-obvious bugs. I would encourage you to try it, at least until the issue is fixed.",Bitter Frustration
"I think I'll use that `Resolve` command of your post and chain it with my `Gwrite` mapping
```
nnoremap \gw :Resolve<cr>:Gwrite<cr>
```
and it should be ok, thanks.
About the issue not being with Git for Windows, I doubt it, because in Linux git commands autoresolve links, in Windows they do not. Because this works in Linux
```
$ git -C /mnt/d/Gianmaria/Dropbox\ \(Personale\)/Linux/vimrc/plugin --literal-pathspecs add -- /home/gianmaria/.vim/plugin/buffer_history.vim
```
and in Windows it does not. But it doesn't matter if I can get it working in one way or another.",Bitter Frustration
"Holy fuck dude, I spent literally an hour explaining the problem in *immense* detail and gave you one simple task to help me come up with a solution, and your response it to repeat the same exact red herring I debunked. I wrote the code, I know how it works. You can't even follow simple directions, I don't know why you think you're qualified to diagnose the problem.
If anyone else has this problem, please open a new issue, I'd love to get it solved.",Bitter Frustration
"<!--
######################################################################
WARNING!
IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE
######################################################################
-->
## Checklist
<!--
Carefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:
- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2020.01.15. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.
- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.
- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.
- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.
- Finally, put x into all relevant boxes (like this [x])
-->
- [x] I'm reporting a broken site support
- [x] I've verified that I'm running youtube-dl version **2020.01.15**
- [x] I've checked that all provided URLs are alive and playable in a browser
- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped
- [x] I've searched the bugtracker for similar issues including closed ones
## Verbose log
<!--
Provide the complete verbose output of youtube-dl that clearly demonstrates the problem.
Add the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2020.01.15
[debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
<more lines>
-->
```
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: ['--verbose', '-j', 'https://www.youtube.com/playlist?list=OLAK5uy_kj9o0LqqeGu3wJf_G1JqOJ-YHzexqptlM']
[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 3.8.1 (CPython) - Linux-5.5.0-1-MANJARO-x86_64-with-glibc2.2.5
[debug] exe versions: ffmpeg 4.2.2, ffprobe 4.2.2, rtmpdump 2.4
[debug] Proxy map: {}
ERROR: Signature extraction failed: Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(""Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/YoutubeDL.py"", line 796, in extract_info
ie_result = ie.extract(url)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/common.py"", line 530, in extract
ie_result = self._real_extract(url)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 2046, in _real_extract
signature = self._decrypt_signature(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1393, in _decrypt_signature
raise ExtractorError(
youtube_dl.utils.ExtractorError: Signature extraction failed: Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(""Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
```
## Description
<!--
Provide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.
If work on your issue requires account credentials please provide them or explain how one can obtain them.
-->
Trying to dump the Json information of a playlist seem to be broken for youtube.",Bitter Frustration
"I can confirm the the issue on many videos.
```
./youtube-dl -4 -v ""https://www.youtube.com/watch?v=pqIv3e5eBeo""
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: [u'-4', u'-v', u'https://www.youtube.com/watch?v=pqIv3e5eBeo']
[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 2.7.9 (CPython) - Linux-4.9.182-xxxx-std-ipv6-64-x86_64-with-Debian-7
[debug] exe versions: avconv 11.12-6, avprobe 11.12-6, ffmpeg N-48007-g62f8d27ef1-static
[debug] Proxy map: {}
[youtube] pqIv3e5eBeo: Downloading webpage
[youtube] pqIv3e5eBeo: Downloading video info webpage
[youtube] {18} signature length 109, html5 player vfl_PLd61
[youtube] pqIv3e5eBeo: Downloading player https://www.youtube.com/yts/jsbin/player_ias-vfl_PLd61/en_US/base.js
ERROR: Signature extraction failed: Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/YoutubeDL.py"", line 796, in extract_info
ie_result = ie.extract(url)
File ""./youtube-dl/youtube_dl/extractor/common.py"", line 530, in extract
ie_result = self._real_extract(url)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 2047, in _real_extract
encrypted_sig, video_id, player_url, age_gate)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1394, in _decrypt_signature
'Signature extraction failed: ' + tb, cause=e)
ExtractorError: Signature extraction failed: Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
```",Impatience
"I'm experiencing the same problem recently with a particular video. I'm also on **2020.01.15**, and the video works in-browser just fine.
```
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: [u'--verbose', u'--skip-download', u'--write-info-json', u'-o', u'i', u'https://www.youtube.com/watch?v=S79GcTt_8pc']
[debug] Encodings: locale UTF-8, fs UTF-8, out None, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 2.7.12 (CPython) - Linux-4.4.0-171-generic-x86_64-with-Ubuntu-16.04-xenial
[debug] exe versions: ffmpeg 2.8.15, ffprobe 2.8.15, rtmpdump 2.4
[debug] Proxy map: {}
[youtube] S79GcTt_8pc: Downloading webpage
[youtube] S79GcTt_8pc: Downloading video info webpage
[youtube] {18} signature length 109, html5 player vfl_PLd61
[youtube] S79GcTt_8pc: Downloading player https://www.youtube.com/yts/jsbin/player_ias-vfl_PLd61/en_US/base.js
ERROR: Signature extraction failed: Traceback (most recent call last):
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""/usr/local/bin/youtube-dl/youtube_dl/YoutubeDL.py"", line 796, in extract_info
ie_result = ie.extract(url)
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/common.py"", line 530, in extract
ie_result = self._real_extract(url)
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 2047, in _real_extract
encrypted_sig, video_id, player_url, age_gate)
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1394, in _decrypt_signature
'Signature extraction failed: ' + tb, cause=e)
ExtractorError: Signature extraction failed: Traceback (most recent call last):
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
```",Impatience
"### Steps to reproduce
<!-- (Guidelines for creating a bug report are [available
here](https://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#creating-a-bug-report)) -->
Follow the Docs: https://guides.rubyonrails.org/testing.html#using-separate-files
```ruby
# lib/test/authenticated_test_helper.rb
module AuthenticatedTestHelper
def sign_in(email, password)
post account_login_url(email: email, password: password)
end
def admin_sign_in(email, password)
post admin_session_url(email: email, password: password)
end
end
```
```ruby
# test/controllers/admin/samples_controller_test.rb
require 'test_helper'
require 'test/authenticated_test_helper' # << throws: `cannot load such file -- lib/test/authenticated_test_helper
module Admin
module SamplesControllerTest < ActionDispatch::IntegrationTest
...
...
end
end
```
### Expected behavior
<!-- Tell us what should happen -->
It should find the file since it's defined in EXACTLY the location that's recommended
### Actual behavior
<!-- Tell us what happens instead -->
```bash
30: from -e:1:in `<main>'
29: from -e:1:in `require'
28: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:54:in `load'
27: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:54:in `load'
26: from /home/samps/sites/cannabislims/bin/rails:9:in `<main>'
25: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'
24: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:30:in `require'
23: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require_with_bootsnap_lfi'
22: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/loaded_features_index.rb:92:in `register'
21: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `block in require_with_bootsnap_lfi'
20: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `require'
19: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/commands.rb:18:in `<main>'
18: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/command.rb:46:in `invoke'
17: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/command/base.rb:69:in `perform'
16: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor.rb:392:in `dispatch'
15: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor/invocation.rb:127:in `invoke_command'
14: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor/command.rb:27:in `run'
13: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/commands/test/test_command.rb:33:in `perform'
12: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:39:in `run'
11: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `load_tests'
10: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `each'
9: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `block in load_tests'
8: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'
7: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:30:in `require'
6: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require_with_bootsnap_lfi'
5: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/loaded_features_index.rb:92:in `register'
4: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `block in require_with_bootsnap_lfi'
3: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `require'
2: from /home/samps/sites/cannabislims/test/controllers/admin/samples_controller_tests/authenticated_test.rb:2:in `<main>'
1: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'
/home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:33:in `require': cannot load such file -- lib/test/authenticated_test_helper (LoadError)
```
### System configuration
**Rails version**: 6.0.2.1
**Ruby version**: 2.6.5p114 (2019-10-01 revision 67812) [x86_64-linux]",Impatience
"If i change to `require 'lib/test/authenticated_test_helper'` nothing changes
but changing to `require Rails.root.join('lib', 'test', 'authenticated_test_helper')` suddenly it works",Bitter Frustration
"Your test helpers should live in `test/` not in `lib/`. `lib` is not autoloaded, which is why `require 'lib/test/authenticated_test_helper'` doesn't work.
You are right though the docs are confusing. I believe they wrote a typo and meant write to store your helpers in `test/lib`, I usually put them in `test/test_helper/`. I have fixed the documentation in e7514128a6a5ca11385abfa7f699cc4fd9ceefd1 and 9082609a33531cf671c62231712d9a21a7facef6. The guides will be updated on the next release of Rails.
---
As an aside, you have more than one issue open on this repo that feels a bit like you're yelling at us. I get that you're new to Rails and are frustrated, but most of the folks here are volunteers and no one is purposefully making your life hard. â¤ï¸ We appreciate bug reports, and will fix things as soon as we can, but we're all human and we make mistakes.",Bitter Frustration
@eileencodes what's the point of the lib folder if I can't require files from it? that has always been the standard folder to place code that doesn't belong in `app`,Bitter Frustration
"@eileencodes I don't see how opening a separate issue when I find one is yelling at you. I have been using rails since rails 3. I don't see anything in either issue where I'm blaming anyone or getting angry, but i am starting to get sick of library maintainers taking bug reports as personal attacks",Bitter Frustration
"> but i am starting to get sick of library maintainers taking bug reports as personal attacks
If this happens a lot consider whether you're contributing to that? It's the all caps you're using on multiple issues I take issue with, not opening new issues. I asked you super nicely to chill a little, that's all. ðŸ˜•",Impatience
Capital letters are for **emphasis** not because I'm angry about it,Irony
"I wasn't upset even in the slightest until you decided to take this discussion off-topic over one capital word. I have *never* assumed someone was angry at me for trying to report a bug in any of the libraries I maintain, so I get irked when someones else's first response to someone trying to help improve the library is ""pull requests welcome"" (no shit it's open source. we know, we're just trying to make you aware of the issue) or the *assumption* that because we found an error we're mad about it or expect a free library to be perfect",Bitter Frustration
"## Description
This greater ensures the `HAVE_MENU_WIDGETS`, `HAVE_RGUI`, `HAVE_MATERIALUI`, `HAVE_OZONE` and `HAVE_XMB` will never be defined when `HAVE_MENU` is not.
## Related Issues
This allows only checking if the specific feature is enabled and not that `HAVE_MENU` is also defined with processor code and some clean up in `Makefile.common`.
## Related Pull Requests
I noticed this was not correctly fixed as I previously thought in commit https://github.com/libretro/RetroArch/commit/2be5ffe4e999757ba89a87b0012d1441ec78af77.",
"@twinaphex Please stop censoring users, its very unprofessional.",Bitter Frustration
"> My reason for not immediately merging this is for another reason - I want to make menu widgets independent of the menu code altogether. This includes also the menu animation code. That is why I don't want to immediately merge this PR overnight.
Also since you obviously do not understand what I even was submitting let me point out this PR would of only made that task easier in the future where you would of needed to remove one line instead of dozens as is the current state in the code, but good job throwing out the baby with the bathwater. :)",Mocking
"What options are supported by brewfile? I can't find anything after extensive googling. Even the main README mentions `unless` without describing it....
Can you point me to docs on the syntax of your DSL?",Impatience
"The DSL is defined at https://github.com/Homebrew/homebrew-bundle/blob/677f39d62a8aefdcbc091f193af1f7ddbee0052d/lib/bundle/dsl.rb
The individual components (`cask`, `brew`) are documented in their own repositories as bundle should be passing through any arguments or parameters as is.",
"Those aren't docs :(
But it's something at least. Thanks.
I think you should leave this issue open.",Bitter Frustration
"> Where are the docs?
In the README and `brew bundle --help`.
> Even the main README mentions `unless` without describing it....
`unless` is a Ruby keyword. `Brewfile` is a Ruby DSL.
> I think you should leave this issue open.
No, sorry, we don't leave issues open unless there's actionable work in them. As-is: there is not.",Impatience
"Yes, so where's the description of the DSL. I can write a DSL in python too, that doesn't mean it's going to have the same syntax, nor that I would expect my users to learn all of python in order to learn my dsl. This is highly actionable. There are zero docs, by your own admission. If you'd like to keep it that way, I guess that's up to you.",Bitter Frustration
"Here is the output of `brew bundle --help` in case you were curious:
```
Usage: brew bundle subcommand
Bundler for non-Ruby dependencies from Homebrew, Homebrew Cask and the Mac App
Store.
--file= Read the Brewfile from this file. Use
--file=- to pipe to stdin/stdout.
--global Read the Brewfile from ~/.Brewfile.
brew bundle [install] [-v|--verbose] [--no-upgrade]
[--file=path|--global]
Install or upgrade all dependencies in a Brewfile.
-v, --verbose Print the output from commands as they are
run.
--no-upgrade Don't run brew upgrade on outdated
dependencies. Note they may still be
upgraded by brew install if needed.
brew bundle dump [--force] [--describe] [--no-restart]
[--file=path|--global]
Write all installed casks/formulae/taps into a Brewfile.
--force Overwrite an existing Brewfile.
--describe Include a description comment above each
line, unless the dependency does not have a
description.
--no-restart Do not add restart_service to formula
lines.
brew bundle cleanup [--force] [--zap] [--file=path|--global]
Uninstall all dependencies not listed in a Brewfile.
--force Actually perform the cleanup operations.
--zap Remove casks using the zap command
instead of uninstall.
brew bundle check [--verbose] [--no-upgrade] [--file=path|--global]
Check if all dependencies are installed in a Brewfile.
-v, --verbose Print and check for all missing
dependencies.
--no-upgrade Ignore outdated dependencies.
brew bundle exec command
Run an external command in an isolated build environment.
brew bundle list [--all|--brews|--casks|--taps|--mas]
[--file=path|--global]
List all dependencies present in a Brewfile. By default, only Homebrew
dependencies are listed.
--all List all dependencies.
--brews List Homebrew dependencies.
--casks List Homebrew Cask dependencies.
--taps List tap dependencies.
--mas List Mac App Store dependencies.
```
No mention of the DSL, not even the subcommands allowed. I'm a big fan of brew-bundle, but it's not super useful for complex cases without docs. It's especially confusing because there are a number of outdated docs on the net for old iterations of brew bundle.",Bitter Frustration
But I guess I'll start learning about Ruby DSLs since documentation doesn't seem to be a priority for you *sigh*.,Bitter Frustration
"Perhaps you can answer this at least.
Is the following section of the man pages applicable to brew-bundle?
i.e. Can I define a custom external command and call it in a Brewfile?
```
CUSTOM EXTERNAL COMMANDS
Homebrew, like git(1), supports external commands. These are executable
scripts that reside somewhere in the PATH, named brew-cmdname or
brew-cmdname.rb, which can be invoked like brew cmdname. This allows
you to create your own commands without modifying Homebrew's internals.
Instructions for creating your own commands can be found in the docs:
https://docs.brew.sh/External-Commands
```",Impatience
Please reread our Code of Conduct and adjust your communication accordingly: https://github.com/Homebrew/.github/blob/master/CODE_OF_CONDUCT.md#code-of-conduct,
How have I violated that code of conduct? I'm sorry if I have.,
"I came here with an issue that you've acknowledged exists. I've done nothing but try to work with you to try to resolve that problem, respectfully and openly.
I will leave now. Good luck with whatever's bugging you.",Bitter Frustration
"> I'm sorry if I have.
I appreciate the apology.
> I've done nothing but try to work with you to try to resolve that problem, respectfully and openly.
I have not found your communication to be respectful. For example:
> Good luck with whatever's bugging you.
Yes, I agree leaving is probably for the best, sorry.",Bitter Frustration
"## Summary
As discussed in #4182, I'd like to request the addition of the ""override_ext"" front matter tag.
## Motivation
I'm using php code on my website, like many others, too. I can create a ""test.php"" file, and the generated file will have the "".php""-extension, all right. However, I want to use markdown for my blog posts, and these "".md"" files are automatically generated to "".html"" files! And because the posts have a .""php"" layout with php code, but the file ends up as html, the site does not work. OK, I could create a permalink as suggested in the issue linked above, but that's a bad workaround because I don't want to hardcode the file name and directory.
A similar request has been discussed already, but I find the [excuses](https://github.com/jekyll/jekyll/issues/5646#issuecomment-357329996) quite weak. As of now, it's hardcoded for markdown files to be converted to html files, and that's simply not right. Give us an option here!
Edit: OK, it's not really needed for other files where I can already set my custom extension manually, this would just add more confusion. But for all files which jekyll automatically converts into a hardcoded format, this should be changeable - most likely in config.yml.",Bitter Frustration
"This issue has been automatically marked as stale because it has not been commented on for at least two months.
The resources of the Jekyll team are limited, and so we are asking for your help.
If this is a **bug** and you can still reproduce this error on the latest <code>3.x-stable</code> or <code>master</code> branch, please reply with all of the information you have about it in order to keep the issue open.
If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced [hooks](http://jekyllrb.com/docs/plugins/#hooks) which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.
This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions.",
"This is already possible based on answers here.
https://stackoverflow.com/questions/14119772/how-to-change-extension-of-files-generated-by-jekyll
An arbitrary extension or no-extension file will be parsed with liquid if it has frontmatter and then jekyll outputs as .html
And if you set the permalink in the file you can force the extension to be .json, .php , etc.
Maybe set the permalink pattern globally in the config as a collection. You can set permalink for post but the response there was that it doesn't work in this case I think",
"Yeah, that's the problem: I don't want to hardcode the path with permalinks, I just want to be able to adjust the file extension...",
"Also, @DirtyF: It would be nice to at least give an explanation before just closing an issue, this way nothing will get resolved,",Impatience
"Can we please have a full reference of all available components & props? ðŸ˜˜
Currently the documentation is like a mishmash of practices, tutorials, and examples. I'm really missing a comprehensive reference. I have no way of knowing what I can import from the package, or what props might be supported on each component, without diving into its (minified!) sourcecode. Never mind what everything does ðŸ˜±",Bitter Frustration
"This is the plan:
- Utilize TypeScript typing and inline comments to generate API reference - Automate generation on build
- Normalize the format of the API reference with the look and feel of the rest of the docs
- Publish to docs site",
"Can you please give this some prioity?
I'm trying to use the `Link` component, of which there's not a single mention that it even exists, so that was a good guess, I suppose. But I have no way of knowing what I can/should pass to it.
Can I make it render as a different component like, say
```jsx
<Link as={FancyLink} />
```
I bet your jsdoc doesn't cover this.",Bitter Frustration
"The sample apps' styleguide page is intended to be a live reference of usage examples that you can actually run. See the link example:
https://github.com/Sitecore/jss/blob/dev/samples/react/src/components/Styleguide-FieldUsage-Link/index.js",
"I don't need examples. I know how to write code. I need a reference because I don't (or rather, *can't*) know what props I can throw at JSS.
Besides, people shouldn't have to dive into sourcecode to figure out how to use something. Sitecore is a huge commercial company, and not having proper documentation doesn't come across as particularly appealing.",Bitter Frustration
"I agree that an API reference will be helpful.
However, I think the following can help you in the meantime. > I have no way of knowing what I can import from the package, or what props might be supported on each component, without diving into its (minified!) sourcecode. Never mind what everything does ðŸ˜±
You donâ€™t have to try to decipher minified code. The JSS modules are written in TypeScript and are heavily commented. Just clone this repo locally and all the typed function signatures and props models will be available for you to reference.",Impatience
"We now provide API reference documentation as part of each release (starting with JSS version 18.0). You can find these on the release branch in the Github repository under `/ref-docs`. Here are the latest, for example: https://github.com/Sitecore/jss/tree/master/ref-docs",
"## PR Summary
Solution for issue #16631",
Two weeks without any manifestation of interest. Closing.,Impatience
"The tests are still not passing and none of the issues brought up in #16631 (the need to document the API changes, an attempt to move to the simpler fix, or a clear argument as to why IndexFormatter should not be deprecated). From my point of view we have been waiting on you.",Bitter Frustration
"It would also have helped if the issue title and description had more
information. I certainly could not tell what this PR was for just by
looking at this among all of the other PRs and issues.
On Tue, Mar 17, 2020 at 8:37 PM Thomas A Caswell <notifications@github.com>
wrote:
> The tests are still not passing and none of the issues brought up in
> #16631 <https://github.com/matplotlib/matplotlib/issues/16631> (the need
> to document the API changes, an attempt to move to the simpler fix, or a
> clear argument as to why IndexFormatter should not be deprecated). From my
> point of view we have been waiting on you.
>
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/matplotlib/matplotlib/pull/16634#issuecomment-600366626>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AACHF6DM2ANDPG462BOB3BLRIAJUTANCNFSM4K73BSUA>
> .
>",Impatience
"> The tests are still not passing
I will repeat for the last time : THE TEST REQUIRES THE BUG TO BE PRESENT.
The bug manifests itself by outputting ``Label 0`` for -1 < indexes < 0 instead of outputting an empty string ``""""``. The test is passed when the bug is present, that is when``Label 0`` is wrongly outputted. The test fails when the bug is NOT present, that is when ``IndexFormatter`` returns an empty string, ``""""``, AS IT SHOULD.
The solution --- which consists only of rounding the index value, ``round(i)`` instead of truncating the index value, ``int(i + 0.5)`` --- returns an empty string, ``""""``. The solution fails the test because the test is WRONG. The test is by itself A BUG.
> From my point of view we have been waiting on you.
I CANNOT CORRECT THE TEST. IT'S YOUR RESPONSIBILITY AND PREROGATIVE.
> the need to document the API changes
If you adopt the solution, THERE NO NEED TO CHANGE THE DOCUMENTATION NOR THE API
> why IndexFormatter should not be deprecated)
Because, WITH THE SOLUTION instead a deprecation :
1. IndexFormatter works. It simply works, as intended, without any bug.
2. ``IndexFormatter(x_labels)`` is far simpler than proposed convoluted alternatives like ``FuncFormatter(lambda x, _: dict(zip(range(len(x_labels)), x_labels)).get(x, """"))`` to achieve the same exact end result.
3. The solution doesn't break back compatibility in the middle of minor revision. The users don't have to change their programs. They don't have to find an alternative. 4. It follows the precepts of semantic versioning instead of adopting post hoc rationalisations (like ""others deprecated functionalities in the middle of minor revision so lets pretend it's okay for us to do the same"")
5. You don't have to spend time and energy changing the documentation. You don't have to teach the users alternative methods that are necessarily harder to implement and explain.
6. Apparently, the DEPRECATION is based, at least partially, on the false assumption that a decade of zero complaints from the users somehow means the users are unhappy with IndexFormatter and therefore don't use it. 7. THE DEPRECATION misses entirely the point of avoiding the vicious cycle of complexity. Increasing complexity to achieving the same exact functionality is a huge waste of your time. It only make you busier and busier, writing more convoluted code and spending more time managing harder to debug bugs.
I have wasted enough of my time trying to convince you. So be it.",Bitter Frustration
"@Wlodarski Saying that it is the responsibility of the person proposing a change to make the tests pass does _not_ mean that the new behavior cannot be adopted. You can fix the test to match desired behavior, and that is still the perogative of the implementor.
That said, changing the test is a _strong_ indicator that the API _has_ changed. You may see it as minor, you may see it as correcting to expected behavior. Nonetheless it is changed, which should be documented in accordance with the projects goals.
I am not speaking as representative of Matplotlib, only as a member of the broader open source community.
You seem to be focussing on your particular use case, and I don't blame you. You found something you thought unexpected, and found a rather simple fix for it. I applaud you for being willing to dive into a complicated codebase and find that solution.
That said, please acknowledge that the maintainers are acting in good faith here, that they are considering what you have pointed to, but are also looking out for the best interests of the userbase as a whole. You may disagree with them, and that's fair. But being silent for two weeks, then getting angry when they point to unresolved discussion points is not a productive disagreement.
The matplotlib project, being so core to how so many other people do their work, has very stringent expectations when it comes to behavioral changes, even small ones, by design, while balancing the desire to move forward, add features, clean up code, etc. And for future reference, two weeks is an extremely short time in open source software. Most of the people here are volunteers, working on their own free time. I myself have had PRs open for months (including one active one in this project) that have ultimately lead to being merged.
@tacaswell @WeatherGod Thank you for all that you do for the project.",
"@Wlodarski We expect everyone to engage in a professional manner consistent with the PSF code of conduct (https://www.python.org/psf/conduct/). I do not think your comments in this thread (or in #16631) reach that bar, please be more considerate of your words in the future.
I am going to close and lock this thread as I do not think this discussion will be productive if continued here. If anyone wants to pick up this work please open a new PR where we can start fresh.
-----
@ksunden Thank you for your kind and considered words :)",
"Refs: https://github.com/expressjs/discussions/issues/98
Refs: https://github.com/expressjs/discussions/issues/106#issuecomment-595342919
co-authored-by: Wes Todd @wesleytodd",
"@dougwilson , @wesleytodd : we are ready to land this, isn't it? what is blocking this?",Impatience
"@dougwilson / @wesleytodd - can you either remove the red-X or clarify what your concern is? If it is just that one is waiting for other's approval, please state so, so that we can move forward.
/cc @expressjs/express-tc - FYI, and important PR for defining project captains for repos. Please chime in if you have comments, or else complete review to help progress!",Impatience
"There is a simple issue here I was going to fix on merge, but didn't get a chance to this weekend with what is going on in my country. After work today I will at least comment on it so anyone can make the change if I cannot.",
"@dougwilson - I have addressed all the review comments, mostly following your suggestions `AS IS`. Please have a look and do the needful!",Impatience
it has been 11 days since the last update. can we take this forward?,Impatience
"Thanks @gireeshpunathil ! As we discussed on the TC meeting, it should be in the format as the current commit on `master` and our guide: the message on the first line, a blank line, a reference to the PR in the form ""`closes #PR`"". I can make the said changes if you need. LMK",
@dougwilson - made the changes to the commit message as per the guidelines; PTAL.,Impatience
"Ok... It still does not seem to be in the form from my comment... ? I can just update if you need, let me know. If I can better clarify how the message should be constructed let me know as well. I believe it was clear, and provided an example of a previous comment message, but if I can provide better guidance for the format, I certainly can.
I am circling around on this because I would like this to get merged, but I recall that you wanted to do the commit editing so I could just do the fast-forward merge, but it's not in the correct state for that. I can make those edits if you like, but don't want to force push on your branch without you being aware as per our previous conversation :)",Impatience
"@dougwilson - - thanks - yes, I prefer to be advised over edits if possible, towards improving my own knowledge on those areas.
this is the current commit message:
```
doc: add section for project captains
closes https://github.com/expressjs/discussions/issues/98
Co-Authored-By: Wes Todd <wes@wesleytodd.com>
```
this is the rule:
`the message on the first line, a blank line, a reference to the PR in the form ""closes #PR"".`
- the message is on the first line
- there is a blank line
- there is a reference to the PR against `closes` verb
the two additional things are:
- there is a subsystem (doc) in the message line (following the existing conventions)
- there is a co-author field (attribution of authorship, many of the original wordings came from Wes)
please advise where is the change required?",
"1. The subsystem is ""docs"" and not ""doc""
2. The closes should be `closes #4210`
3. We do not include co-author fields, but we can if desired to amend our guidelines, in which this commit would wait for that to happen
It should just be something like
```
docs: add project captains to contribution
closes #4210
```
Then as discussed in the TC meeting, the committer on the commit would need to be the person performing the merge itself (myself unless you would like to wait for another to do that).",
"@dougwilson - adjusted the commit message accordingly, PTAL!
@wesleytodd - I had earlier kept you as a `co-author` for this PR, but looks like we do not include that as per the current process. Can you please review and provide your concurrence? If you have concerns pls let me know, we would need to go the route of amending guidelines.",
"So based on the conversation going on in https://github.com/expressjs/discussions/issues/121 it seems like this is not ready to land as-is, as it seems that there may be either additional discussion to have on the goal here and/or updating to the wording in this pull request to reflect it. Specifically in regards to what falls under repo ownership.
It would come to reason to me that landing pull requests would be a ""day-to-day"" task for said project captains, and their particular merging strategies, commit message formats, etc. would serve the repo on a technical front, as outlined in this document.
I think part of the goal of project captains is to empower them to manage their repos, and commit messages seems to fall directly in that category to me. For example, if a project captain wants to use something like standard-release to automate releases and history file generation, they may need specific structure to commit messages. We should strive to empower the owners to do what they think will improve their workflow.
But maybe I'm misunderstanding on that point. Especially if I'm misunderstanding, that definately means we need greater clarity in the document, as we don't want to have everyone interpreting it differently, I presume. I'm happy to provide suggested edits on the wording, but likely need input from the original author @wesleytodd to sync on the interpretation so I can help clarify ðŸ‘",
"@dougwilson - it seems strange (and funny) to me that you closed https://github.com/expressjs/discussions/issues/121 with no action on it, and then quoting its content as a reason for further work here!",Mocking
Hi @gireeshpunathil that issue is orthogonal to this issue.,
"> But maybe I'm misunderstanding on that point. Especially if I'm misunderstanding, that definately means we need greater clarity in the document, as we don't want to have everyone interpreting it differently, I presume. here is the sequence of events, for better clarity for everyone:
- the commit message did not follow this repo's convention
- more specifically, I added `Co-authored-by` field, but not not an established practice in this repo
- @dougwilson pointed out that, I promptly removed it, but sought verbal consent from the co-author
- I opened an issue to discuss that: specifically, implementing a guideline pan-project
- that is closed as inappropriate in that repo, and to be dealt in individual repos.
- now back here, @dougwilson says it is causing misunderstanding!
This is becoming a surprise to me! where is the mis-understanding? If you assert that no org wide guidelines are present, so be it, and I will seek change proposal with captains, period! where is it that I interpreted this document at all? leave alone interpreted differently?
I suggest we take a step back; - retire the discussion in https://github.com/expressjs/discussions/issues/121 as it is already closed and I agreeing to your proposal to take anything further with individual repo captians
- and go ahead with this proposal",Bitter Frustration
"Hi @gireeshpunathil please let me know if there is some time in which you are available for a voice chat; I would like to proceed with landing this PR, but I am honestly unsure at this point how to interact with your pull requests that will not come off in bad faith, and would like a chance to have a discussion with you so we are on the same page so we can make progress landing.",Bitter Frustration
@dougwilson - I sent you a mail with my time preferences.,
"in general, this can have more reviews!",Impatience
"I don't have any real comments on this PR, but once the discussion is resolved and it lands, we should make sure the change is reflected on the website; I opened https://github.com/expressjs/expressjs.com/issues/1184 to track that.",
"I believe I have addressed all the outstanding comments. pinging @expressjs/express-tc , to seek some reviews or take it towards landing.",
I am locking this PR until the above issue with the interaction between myself and @gireeshpunathil is resolved.,Bitter Frustration
"Hi, Memcached team,
Recently, I revealed a buffer overflow vulnerability which may cause DOS attack. The exploit details can be found as following.
### Affect Version
memcached-1.6.0
memcached-1.6.1
### Root cause
file location: memcached.c:6156-6187
![image](https://user-images.githubusercontent.com/29170518/77297732-b15af580-6d24-11ea-8d4d-ca9befffeae5.png)
### Code Audit
```c
6178 char extbuf[sizeof(c->binary_header) + BIN_MAX_EXTLEN];
6179 memcpy(extbuf + sizeof(c->binary_header), c->rcurr + sizeof(c->binary_header), **extlen**);
```
in line 6179, since there is no mechanism to verify the parameter's length, in this case, the length of ""**extlen**"" when calling memcpy function, It will cause buffer overflow if large value assigned to the **extlen** variable.
### POC
```
0x80 0x01 [0x00 0x00] keylen
[0x30] extlen 0x00 0x00 x00
```
for the POC snippet, first, if I assign a large value to the variable **extlen**, on the other hand, in order to bypass the validation of data packet which sent in following code snippet,
`6156 if (c->rbytes < keylen + extlen + sizeof(c->binary_header))`
we can construct a very large data packet and send it to the server running **memcached** 1.6.0 or 1.6.1 anonymously. After that, the program will crash because of the issue mentioned above.
Note: Please confirm this issue ASAP. Besides, just letting you know, I am gonna submit this issue to CVE mitre.
Please let me if you have any questions.
Sincerely,
Icejl",Impatience
"In future, it may be wise to message the maintainer privately first and let them confirm the issue and give them the standard period before releasing either way.
I've messaged the maintainer on IRC, waiting for an update.",Impatience
"> In future, it may be wise to message the maintainer privately first and let them confirm the issue and give them the standard period before releasing either way.
Will memcached users be afforded the same consideration by attackers?",Irony
"I applaude Icejl here because now people who don't want to get DOSed can disable/remove the buggy memcached version (or fix it and recompile; since it's such a small change anyway). Without that information they may otherwise run vulnerable code, so I am 100% in support of Icejl and completely against thesamesam's suggestion to be silent. ""Standard period"" means unknown silence and no fixes in that time, so it is a misnomer.",
"I didn't come up with the idea of responsible disclosure, I was just suggesting. I don't intend to debate on the merits of it - that's for the wider community to decide. Just letting the OP know that it's a thing.
This happens in a lot of places, and I'm not the person to take it out on if you disagree with it.",
Locking this off-topic discussion.,
"1.6.2 released with fix: https://github.com/memcached/memcached/wiki/ReleaseNotes162
specifically: https://github.com/memcached/memcached/commit/02c6a2b62ddcb6fa4569a591d3461a156a636305
and fwiw, I've been responsive to security reports (or even report them myself) and give credit happily when due for over ten years. Don't waste my good will, please.",Bitter Frustration
"#### Problem Description
mitmproxy --mode socks5 --listen-port 1080 --ssl-insecure
""Error in HTTP connection: Certificate verification error for None: self signed certificate in certificate chain (errno: 19, depth: 2)
clientdisconnect
clientconnect
Certificate verification error for None: self signed certificate in certificate chain (errno: 19, depth: 2)
Invalid certificate, closing connection. Pass --ssl-insecure to disable validation.""
#### Steps to reproduce the behavior:
Just open anything with self-signed cert, socks5 mode.
#### System Information
> Paste the output of ""mitmproxy --version"" here.
Mitmproxy: 5.0.1
Python: 3.8.2
OpenSSL: OpenSSL 1.1.1f 31 Mar 2020
Platform: macOS-10.15.3-x86_64-i386-64bit",
"I can't reproduce this.
```
Î» curl -k --socks5-hostname localhost:1080 https://self-signed.badssl.com/
```
works fine. Here's how I invoke mitmproxy:
```
Î» mitmdump --mode socks5 --listen-port 1080 --ssl-insecure
Proxy server listening at http://*:1080
127.0.0.1:52598: clientconnect
::ffff:127.0.0.1:52598: Certificate verification error for self-signed.badssl.com: self signed certificate (errno: 18, d
epth: 0)
::ffff:127.0.0.1:52598: Ignoring server verification error, continuing with connection
127.0.0.1:52598: GET https://self-signed.badssl.com/
<< 200 OK 502b
127.0.0.1:52598: clientdisconnect
```",
"You kidding me, right? There is a bunch of reports **of the same issue** in your issue tracker. I've posted an exact output of the log. If you can't reproduce, than this **is solely your problem**, but it doesn't meant bug doesn't exist. Perhaps you should follow what I have wrote, that it's on MACOS.
Also why don't you try to access a naked ip host with self signed cert? Did I mention it has to be domain?
I used ssl split and it worked fine, comparing to mitmproxy. Bye!",Bitter Frustration
"Whoever will find this issue and gets pissed off, because author doesn't bother to fix it for years, **ditch mitmproxy and use SSL SPLIT**",Bitter Frustration
I'm glad you have found something that works for you! Closing this as a duplicate of #3865.,
"Doing a manual projection with a user defined matrix [[1,0,0,0],[0,1,0,0],[0,0,0,0],[0,0,0,1]]
results in a 2d projection onto the xy plane.
Requesting a linear extrude of this 2d object residing on the xy plane results in an error extruding a 3d object when no 3d object exists.",Bitter Frustration
"Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.",Mocking
"As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module [`projection()`](https://en.wikibooks.org/wiki/OpenSCAD_User_Manual/Using_the_2D_Subsystem#3D_to_2D_Projection).",
"So you are saying that there are two kinds of 2d objects in openscad.Â The first has zero dimensions along one axis and the second has zero directions along one axis and has special magical properties.Â Unacceptable.Â 11.04.2020, 07:29, ""Torsten Paul"" <notifications@github.com>:
As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module projection().
â€”You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.
If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.",
"OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.",
"Manifolds are only defined in 3 or more dimensions. Â So, ya. Â Using a matrix to project a 3d object onto a 2d surface necessarily means not having a manifold.Â The projection function also does not return a manifold since it returns a 2d object.Â There is no such thing as a 2-manifold.Â ""Lots of self intersections"" -Â  That is your problem, not mine.Â If you provide a multmatrix function you should support any matrix, not just an unspecifiedÂ subset of them.Â Â 11.04.2020, 05:40, ""Chris"" <notifications@github.com>:
Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.
â€”You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"Â If you provide a multmatrix function then you should support all matricies, not just an unspecified subset of them.Â What other matrix operations don't you support.Â Â 12.04.2020, 14:05, ""Torsten Paul"" <notifications@github.com>:
OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.
â€”You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",Impatience
"Maybe you should actually write that down somewhere. Â You know, like in the documentation. Â Especially in the Multmatrix section of the documentation to make it clear that this program is incapable of managing Â  all matrix operations.Â LOLÂ It's half baked.Â Â 12.04.2020, 13:58, ""Chris"" <notifications@github.com>:
No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.
If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.
â€”You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",Mocking
"Ok, I think that is enough.",
"The matrix operations are fine but just like polyhedron they are GIGO. You can make an infinite number of 3D objects that CGAL will not accept because they are not manifolds. OpenSCAD provides projection() to turn a 3D object into a 2D object. It doesn't simply set all the Z coordinates to 0. It creates a new polygon outline like a shadow and removes all the internal edges that would collapse on top of each making a polygon with lots of self intersections.
On the other hand multmatrix does a matrix multiplication on all the vertices. It doesn't add or remove any of them or change the edges.",Bitter Frustration
"- Reworked the ConsumerDispatcher to improve scenarios around dispose and OnDisconnect.
- Fixed the deadlocked that happened on a Fail-over combined with Dispose of consumer from the - InternalConsumer. The scheduled dispose messages was removed from the queue OnDisconnect and the AutoResetEvent was never signaled in the InternalConsumer Dispose.
- Added Cancel to HandleModelShutdown in the BasicConsumer, so subscriber of consumer event will know if the consumer is shutdown by the model",
Recreated the PR. @zidad @Pliner @luigiberrettini could you look through this ? It is a quite nasty dead lock we have when one of our rabbit nodes losses network connection ðŸ‘Ž,Bitter Frustration
@zidad @Pliner @sungam3r i made an alternative version without the two concurrent queues here. https://gist.github.com/Rotvig/98940a4b6bbd30c48a79732b610f04a6 would you prefer that solution ?,
"Regarding https://gist.github.com/Rotvig/98940a4b6bbd30c48a79732b610f04a6 :
`while (!disposed && queue.TryTake(out var consumerAction, -1))` -1 means infinite.
Also, I do not like the complete queue iteration requeue. These actions will have side effects, especially if the reconnection happens rather quickly.",Bitter Frustration
@Pliner @sungam3r i added the sync behavior of the InternalConsumer Dispose. But we should consider bumping the package to netstandard 2.1 and use IAsyncDisposable instead or implement our own async await AutoResetEvent and create an Async Dispose :),
"> Regarding https://gist.github.com/Rotvig/98940a4b6bbd30c48a79732b610f04a6 :
> > `while (!disposed && queue.TryTake(out var consumerAction, -1))` -1 means infinite.
> > Also, I do not like the complete queue iteration requeue. These actions will have side effects, especially if the reconnection happens rather quickly.
So you prefer the implementation with the two queues? :)",Mocking
"> @Pliner @sungam3r i added the sync behavior of the InternalConsumer Dispose. But we should consider bumping the package to netstandard 2.1 and use IAsyncDisposable instead or implement our own async await AutoResetEvent and create an Async Dispose :)
Let's postpone this discussion to the develop branch release.",
"> So you prefer the implementation with the two queues?
Yes.
> But we should consider bumping the package to netstandard 2.1 and use IAsyncDisposable instead or implement our own async await AutoResetEvent and create an Async Dispose This is too heavy hammer for such a little issue. Migration to netstandard2.1 is a serious step, and if done, it should be justified by significant changes in the internal organization of the code. Not worth it yet.",Bitter Frustration
@micdenny would you have the chance of reviewing this PR anytime soon ? :),Impatience
@Rotvig @sungam3r @zidad @micdenny Could you please review my latest commit?,
"So, we didn't agree about the implementation ðŸ˜ž, but it's important to release a fix for this issue(not only for @Rotvig, but in general).
Let's discuss in Slack how can we redesign this part of our system, maybe remove completely :wink:
FYI @Rotvig @zidad @sungam3r @micdenny",Irony
We can do better here.,Bitter Frustration
"Apparently `text/javascript` is the new hotness now...? https://datatracker.ietf.org/doc/draft-ietf-dispatch-javascript-mjs/
I first noticed this on MDN's [Common MIME Types](https://wiki.developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types) page, which now shows `text/javascript`.
@mathiasbynens, @domenic: Can you provide insight into what's going on at the standards body level? When do you expect to see the IANA registry updated? What about mime.types for Apache or NGINX? (mime-db pulls from all three of these places.)",Impatience
"Thanks for the ping!
This is not really ""new""; the HTML Standard has included [a willful violation of RFC 4329](https://html.spec.whatwg.org/multipage/#dependencies:willful-violation) for a long time:
> The term ""JavaScript"" is used to refer to ECMA-262, rather than the official term ECMAScript, since the term JavaScript is more widely known. Similarly, the MIME type used to refer to JavaScript in this specification is `text/javascript`, since that is the most commonly used type, despite it being an officially obsoleted type according to RFC 4329. [RFC4329]
[And](https://html.spec.whatwg.org/multipage/scripting.html#scriptingLanguages):
> Servers should use `text/javascript` for JavaScript resources. Servers should not use other JavaScript MIME types for JavaScript resources, and must not use non-JavaScript MIME types.
RFC 4329 does not match reality. The HTML Standard does (in this particular case), and https://datatracker.ietf.org/doc/draft-ietf-dispatch-javascript-mjs/ is in the final stages of being standardized so that RFC 4329 is officially deprecated at the IETF/IANA level.
There's a bit of a chicken-egg problem in that server software often has a policy of only considering IETF/IANA even when those standards are clearly out of touch with reality, like in this case. This leads to ridiculous situations like Python's built-in `SimpleHTTPServer` adding support for `.mjs` with the wrong `application/javascript` MIME type. ðŸ¤·â€â™‚ï¸
I hope you consider changing changing `mime-db`'s without waiting for https://datatracker.ietf.org/doc/draft-ietf-dispatch-javascript-mjs/ to be finalized.",Bitter Frustration
"Ultimately this module is not a MIME registry service and not in the business of handling such disputes. It is an aggregation of the sources in the readme, which is where these types of changes need to be made and this module will passively update the records to reflect changes in the source registries.",
"@dougwilson I completely agree with you, but the least we can do is add `""extensions"": [""js""]` in `text/javascript` key until the dispute is resolved.
It really breaks all my validations since there is no way to get the extension of `text/javascript` files which keeps getting uploaded from clients.
For now, I am doing regex matching to bypass this.",Bitter Frustration
"@cybersupernova this module is not here to resolve ""disputes"" -- it is here to provide a aggragate json database of the types as defined from the three sources at the top of the readme.",
"@dougwilson I am not asking to resolve any dispute, but giving no extension for so common MIME `text/javascript` makes no sense.
I am using the database but still, I have to return extension for this MIME with extra `if` condition. I cannot overlook this in my app since it's a commonly used file type.
Anyways, thanks for your time.",Bitter Frustration
"Hi @cybersupernova I definately understand, but this module is here to provide a aggragate json database of the types as defined from the three sources at the top of the readme. This really means you are saying that the issue is that none of those three sources are providing an extension for that MIME type, is that correct? If we have an issue where it actually is in one of those sources and we are not populating in this database, that is definately an issue. If you are saying that our upstream sources do not provide that mapping, then it can always be corrected. The IANA source provides a method to add them, noted in our README: http://tools.ietf.org/html/rfc6838#section-5",
"@dougwilson According to [this tweet](https://twitter.com/bradleymeck/status/1480198522737733632) (by @bmeck), it seems like maybe [the RFC](https://datatracker.ietf.org/doc/draft-ietf-dispatch-javascript-mjs/) has now received official approval? I may be misunderstanding the status, so apologies if so.
But if it's indeed been ""approved"", can this library now move forward with updating to ""text/javascript""? If not, what else is necessary to update?
//cc @broofa",Impatience
"The RFC-to-be's current IESG status is ""Approved-announcement to be sent"". The draft has been formally approved, but there are some administrative things left to take care of before it gets officially published as an RFC.
I don't think we should wait for that to happen to take action here, though.",Impatience
My 2 cents is not to jump the gun... drafts are drafts for a reason... it's something that can be achieved when it's actually published. While it would be nice to have it more standardized patience would be a good thing to make sure the draft doesn't expire at its expiry date without a resolution.,Impatience
"First of all, this has been literally years in the making... it's not like they drafted this last week. It widely represents web reality for at least 5+ years now.
Moreover:
""The draft has been formally approved, but there are some administrative things...""
This doesn't at all sound like a draft that's in jeopardy of being abandoned. Let's not deal in generalities here, but focus on the specifics of this RFC and the reality of it.
----
This database drives libraries that many widely used static file servers use to determine and serve mime types for JS files. Every minute, 10s or 100s of thousands of these files are being served up to clients, most with ""incorrect"" headers.
In my case, the incorrect headers were causing problems with my PWA not being properly/fully validated by tools in the chrome ecosystem like Lighthouse. So I had to fork the static file server and add logic to override the mime-type. I've had to maintain that fork for 3+ years now. Quite frankly, that's a huge security risk even if it wasn't also a giant pain.
Point being: there are real costs to being slow to adopt/reflect the reality of this change.
If the change was still in debate/discussion, I would definitely understand the hesitation. But the period for that has now passed. I think it's time to act.",Bitter Frustration
"## Summary
In some scenarios it can be easier/safer to pass JWT tokens to clients through cookies (secure, sameSite, httpOnly, etc) so that they don't store those in localStorage/have to add those themselves to requests to the back-end.
Having CouchDB be able of extracting a JWT from a Cookie in addition/as an alternative to the Authorization header would be beneficial for such scenarios.
## Desired Behaviour
Being able to configure CouchDB to extract JWT tokens from cookies (in addition or as an alternative to the Authorization header).
## Additional context
My project uses a fairly classical high level architecture:
* Back-end system exposing a RESTful API
* Front-end single-page application
The back-end RESTful API issues self-signed JWT tokens and provides those to the SPA through secure/samesite/httpOnly cookies so as to simplify management and keep a good level of security. This works fine in our scenario since there's a single origin. When the SPA issues requests against the RESTful API, the cookie containing the JWT token is attached transparently by the user agent.
If that same JWT token could also be trusted by CouchDB (which is now possible with 3.1.0 as far as I understand) and if CouchDB could extract it from the cookie, then our SPA clients could seamlessly authenticate against CouchDB.
For the longer term, OAuth support would indeed be nice, but I can imagine that it is already somewhere on CouchDB's roadmap.",
"**I second this motion.** Parsing HTTP Cookies is trivial. CouchDB would simply need to specify the key, or the name, of the cookie that contains the JWT.
**I recommend cookie shall have a configurable key value which can be set under the `[chttpd]` section. This delivers maximum flexibility.**
It is said to be best practice to issue JWT's as HTTPOnly cookies meaning that they are not available to client/browser JavaScript. In other words, _HTTPOnly_ cookies make it impossible to build a custom header aligning with CouchDB's interface, namely `Authorization: Bearer <JWT token>`
https://docs.couchdb.org/en/stable/api/server/authn.html#jwt-authentication
RFC 6265, HTTP State Management Mechanism
> [Section 4.1.2.6. The HttpOnly Attribute](https://datatracker.ietf.org/doc/html/rfc6265#section-4.1.2.6)
> The HttpOnly attribute limits the scope of the cookie to HTTP
> requests. In particular, the attribute instructs the user agent to
> omit the cookie when providing access to cookies via ""non-HTTP"" APIs
> (such as a web browser API that exposes cookies to scripts).",
"Hi,
you can use JWT tokens without requesting sessions via `_/session` API.",
I need to be able to have CouchDB read cookies in order for me to not expose the JWT to client side JavaScript.,
What are you trying to achieve? Session cookies and JWT tokens are seperate things. A client can request a session with username/pasword or can authenticate with a JWT token.,
"I want to authenticate with JWT tokens. I have it working.
Couch requires the JWT be passed in a specific _client request_ HTTP header, namely `Authorization: Bearer <JWT token>`. Right?
How can I set this _client request header_ on the server?",Impatience
"> How can I set this _client request header_ on the server?
Which server do you mean, the CouchDB server?",Impatience
"No the web server. In my case Node.js, or PHP, any web server.",
"To summarize this and get this correct. Your problem is to add this `Authorization: Bearer <JWT token>` header to your client request, because CouchDB expects this as a http request header to authenticate via JWT token?",Impatience
"Precisely. The JWT is stored in an _HHTPOnly_ cookie on the browser/client. So, how do I set the clients HTTP `Authorization` header server side?",Impatience
"You can't. You can, however, set client HTTP cookies server side. So, we need Couch to look for the JWT in an HTTP cookie named _`<configurable>`_.
The idea is that it is a security measure to _not_ expose the JWT to client side scripting.",
"> Precisely. The JWT is stored in an _HHTPOnly_ cookie on the browser/client. So, how do I set the clients HTTP `Authorization` header server side?
The JWT token has nothing to do with a session or a cookie. You send it with your request to get access (if correct). How you (securly) store them on your client is a completely different question.",
"Add support for authentication/authorization using cookies containing JWTs is the issue.
We need Couch to look for JWT's in a cookie.
How to securely store JWT's on your client includes using HTTPOnly cookies which makes it impossible to authenticate to Couch (unless you expose the JWT to client side scripting).",Bitter Frustration
"> We need Couch to look for JWT's in a cookie.
How should this work? The server can't send the client the credentials to obtain access. It's the same as when the server would sends the client a username/password combination. The client needs to provide the server credentials to authenticate and not the server.
If you want a cookie (httponly - with a JWT token) from the server, how do you authenticate against it and/or with what credentials?",Impatience
"CouchDB does not issue JWT's. CouchDB _verifies_ JWTs (vis a vis the `[jwt-keys]` that it is configured with.
CouchDB supports Authentication via JWT so long as the JWT is sent in an HTTP header titled `Authorization`.
CouchDB SHOULD support authentication via JWT sent as an HTTP cookie.
Why? Bcs cookies are more secure in that they can be stored on a client as _HTTPOnly_.",
"> CouchDB does not issue JWT's. CouchDB _verifies_ JWTs (vis a vis the `[jwt-keys]` that it is configured with.
> I know ðŸ˜‰
> CouchDB supports Authentication via JWT so long as the JWT is sent in an HTTP header titled `Authorization`.
> Correct.
> CouchDB SHOULD support authentication via JWT sent as an HTTP cookie.
> Ok, and how do you request that cookie?",Impatience
"CouchDB does request cookies. CouchDB receives cookies.
Another server issues the cookie to the client/browser. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies#define_where_cookies_are_sent",
"> CouchDB does request cookies. CouchDB receives cookies.
> I know, you didnâ€™t answer my question. How do you request your cookie?",Impatience
"Cookies are issued to browsers/clients by servers. Cookies are not requested, they are received.
A server sends a browser a cookie with a JWT as the value. The browser includes that cookie (that JWT) in _every_ HTTP call to all servers on a domain or by path (configurable)
<img width=""1160"" alt=""image"" src=""https://github.com/apache/couchdb/assets/12751611/1b23ae80-0bda-4966-bdda-12afedd6177e"">",
"CouchDB sends a cookie if you request a session. To get a session, you need to provide a username/password. If you are authenticated successfully, you will get this session cookie (httponly), which can be used for further requests.
If you have an JWT token, you can send this with a header. If this token is correct, you get access to the system. You have to send this token with every request.
If you have a username/password you donâ€™t need a JWT token for requests or vice-versa.",
This issue seems elusive to you. I'll let someone else run with it.,Mocking
"> This issue seems elusive to you.
No, but you donâ€™t tell me how you want request your cookie (with your token), because you need an authorization before to get the cookie, but when you have an authorization before you donâ€™t need a JWT token anymore, because you can already authenticate against CouchDB.
> I'll let someone else run with it.
ðŸ€",Bitter Frustration
"How I get my JWT is not the issue and has nothing to do with CouchDB. I think this is where you might be missing me?
![image](https://github.com/apache/couchdb/assets/12751611/4990bc20-4c3e-456e-b2ee-ee1b3f41cbc2)",Bitter Frustration
Maybe [proxy auth](https://docs.couchdb.org/en/stable/api/server/authn.html#proxy-authentication) is more suitable for you?,
"Proxy auth does not remedy this issue and is not related to this issue.
We don't want to expose JWT's to client/browser JavaScript. CouchDB accepting JWT via cookies is the remedy.",
"RFC 6750, Â§ 5.3,
> Don't store bearer tokens in cookies: Implementations MUST NOT store
bearer tokens within cookies that can be sent in the clear (which
is the default transmission mode for cookies). Implementations
that do store bearer tokens in cookies MUST take precautions
against cross-site request forgery.",
Unless someone can point to an RFC or other authoritative source I don't think CouchDB will support receiving JWT's in a cookie. We require them to be sent as bearer tokens.,
"CouchDB itself uses authentication tokens as _HTTPOnly_ cookies. The `AuthSession` cookie below is from Couch.
Somebody on the CouchDB development team understands the need for implementing accepting JWT's via cookies.
<img width=""706"" alt=""image"" src=""https://github.com/apache/couchdb/assets/12751611/5cbdb113-ca9a-47b4-ab58-8b8cc9ba1899"">",
"There is no ""need"" for us to do so. We are well-versed in the various cookie properties, including HttpOnly.",
I'm not understanding why you refuse to allow app developers to take advantage of http cookies as bearers of auth tokens just like Couch does?,Bitter Frustration
"""CouchDB supports cookie authentication"" [[source](https://docs.couchdb.org/en/stable/intro/security.html#cookie-authentication)] **_unless you use JWT authentication_**. SHOULD be added to documentation.",Impatience
Such documentation clarifies that CouchDB team understands the use case and benefit(s) for cookie authentication and explicitly refuses to extend these benefits to the JWT handler [Issue# 2873](https://github.com/apache/couchdb/issues/2873).,Bitter Frustration
enough of this.,Bitter Frustration
"I find it's hard to find suitable IDE to work with your platform. I end up using Visual Studio 2019 with C++ and now including emscripten.h causes 627 errors on Intellisense I tried to ignore them by setting CAExecludePaths but none of them working.
If you really care about future about WASM then please fix this issue otherwise many developers will leave WASM eco system.",Bitter Frustration
Actually CAExecludePaths appear to be working but with it 627 errors and without it 958 errors. This is absurd ![download](https://user-images.githubusercontent.com/60860426/81945672-03512680-95c4-11ea-8247-b44fee4e2da2.png),Bitter Frustration
"Given that you have the intellisense at hand, perhaps you'd be able to provide a PR that cleans this up?
> If you really care about future about WASM then please fix this issue otherwise many developers will leave WASM eco system.
> This is absurd
Appreciate the ultimatum, but these kinds of messages do not really mean anything. Different people use different sets of tools and workflows. I think you have stumbled to discover that not many people use Visual Studio intellisense on Emscripten Wasm projects. Perhaps you would be able to champion improvements for the support there?",Irony
Maybe you can recommend me alternative to intellisense perhaps that one that many people use.,Impatience
"It order to make such things work in an IDE you will probably need to teach it about the emcc compiler. Things like what its default include paths are.
You might have more luck with Visual Stdio Code, its seems a lot more flexible. But you will still need to configure it yourself. Perhaps write the mailing to the ""emscripten-discuss"" mailing list to see if anyone else has done this yet. If you end up with a working solution I'm sure we would be happy to add the configuration instructions to the docs.",
@sbc100 I did set include paths and the EMCC build option.,
Oh come on. Development is not a casino and I'm not here to try my luck . Already spent 5 hours on this. This has to be more smooth than this.,Bitter Frustration
"This thread reminded me of [an excellent article that Rich Hickey wrote a couple of years ago about open source development interaction around the Clojure programming language and community](https://gist.github.com/richhickey/1563cddea1002958f96e7ba9519972d9). Old, but seems to resonate quite well with this conversation thread.
When we write that maybe you'd like to champion such support, we are dead serious. There are currently no Emscripten developers that would be spending their free time implementing Visual Studio integration, and there are unfortunately no companies that would have this close at heart either, so someone will have to step up for that to happen.",Bitter Frustration
"It is sad to see a developer that with 5 hours of frustration already concludes that WASM may have a ruined future.
I have been learning Emscripten and everything related to WASM for weeks, it is hard but I learned many things that I did not know, I am missing a long road of problems but here we go.
I see that you are used to Visual Studio where everything is configured and you do not need to place anything, and just click on the play button. That is the bad thing when everything is provided by your mom.
I use Visual Studio Code and it works great, intellisense, include dirs, etc.
My **c_cpp_properties.json** file in VSCode, for me and my friend Evilsan.
```
{
""configurations"": [
{
""name"": ""EmscriptenConfigJoma"",
""includePath"": [
""${workspaceFolder}"",
""Include"",
""I:/WebAssembly/emsdk/upstream/emscripten/system/include/**""
],
""defines"": [
""_DEBUG"",
""UNICODE"",
""_UNICODE""
],
""windowsSdkVersion"": ""10.0.18362.0"",
""compilerPath"": ""I:/WebAssembly/emsdk/upstream/emscripten/em++.bat"",
""cStandard"": ""c11"",
""cppStandard"": ""c++17"",
""intelliSenseMode"": ""clang-x64""
},
{
""name"": ""EmscriptenEvilsan"",
""includePath"": [
""${workspaceFolder}/**"",
""Include"",
""A:/emscripten/emsdk/upstream/emscripten/system/include/**""
],
""defines"": [
""_DEBUG"",
""UNICODE"",
""_UNICODE""
],
""windowsSdkVersion"": ""10.0.18362.0"",
""compilerPath"": ""A:/emscripten/emsdk/upstream/emscripten/em++.bat"",
""cStandard"": ""c11"",
""cppStandard"": ""c++17"",
""intelliSenseMode"": ""clang-x64""
}
],
""version"": 4
}
```
My **Compile.ps1** script
```
Clear-Host
Write-Host ""â–ˆ Compiling VModule.js/.wasm - $([datetime]::Now.ToShortDateString()) $([datetime]::Now.ToShortTimeString())"" -ForegroundColor Blue
Write-Host
em++.bat `
""main.cpp"" `
""Vx.cpp"" `
""Lib\Ins-Emscripten-llvm-BitCode\lib\Ins.bc"" `
-o ""VModule.js"" `
-I ""Include"" `
-std=c++17 `
-O2 `
--bind `
--pre-js ""Pre.js"" `
--post-js ""Post.js"" `
-s WASM=1 `
-s SINGLE_FILE=0 `
-s ASYNCIFY=1 `
-s NO_EXIT_RUNTIME=1 `
-s EXTRA_EXPORTED_RUNTIME_METHODS=[`'ccall`',`'cwrap`',`'lengthBytesUTF8`',`'stringToUTF8`'] `
-s EXPORT_NAME=`'VModule`' `
-s MODULARIZE=1 `
-s DISABLE_EXCEPTION_CATCHING=0 `
-s ASYNCIFY_IMPORTS=[`'EM_JS_GetIP`'] `
-s EXPORTED_FUNCTIONS=[`'_main`',`'_C_GetProperties`'] `
-s VERBOSE=0 `
-s ENVIRONMENT=web,worker `
-s USE_PTHREADS=1 `
# -s ASSERTIONS=1
# -s FETCH=1 `
#--closure 1 `
Write-Host
Write-Host ""â–ˆ Ended"" -ForegroundColor DarkGreen
```
Getting to have those configuration files and that everything works beautifully, it may seem silly but it has taken me a long time but the reward is great since I have integrated my WASM with my web project in Blazor with C#.
I appreciate the work taken by all those who carry out and continually improve this project so that others like me enjoy it.
Long live the emscripten and WASM lords.",
"> It is sad to see a developer that with 5 hours of frustration already concludes that WASM may have a ruined future.
> > I have been learning Emscripten and everything related to WASM for weeks, it is hard but I learned many things that I did not know, I am missing a long road of problems but here we go.
> > I see that you are used to Visual Studio where everything is configured and you do not need to place anything, and just click on the play button. That is the bad thing when everything is provided by your mom.
> > I use Visual Studio Code and it works great, intellisense, include dirs, etc.
> > My **c_cpp_properties.json** file in VSCode, for me and my friend Evilsan.
> > ```
> {
> ""configurations"": [
> {
> ""name"": ""EmscriptenConfigJoma"",
> ""includePath"": [
> ""${workspaceFolder}"",
> ""Include"",
> ""I:/WebAssembly/emsdk/upstream/emscripten/system/include/**""
> ],
> ""defines"": [
> ""_DEBUG"",
> ""UNICODE"",
> ""_UNICODE""
> ],
> ""windowsSdkVersion"": ""10.0.18362.0"",
> ""compilerPath"": ""I:/WebAssembly/emsdk/upstream/emscripten/em++.bat"",
> ""cStandard"": ""c11"",
> ""cppStandard"": ""c++17"",
> ""intelliSenseMode"": ""clang-x64""
> },
> {
> ""name"": ""EmscriptenEvilsan"",
> ""includePath"": [
> ""${workspaceFolder}/**"",
> ""Include"",
> ""A:/emscripten/emsdk/upstream/emscripten/system/include/**""
> ],
> ""defines"": [
> ""_DEBUG"",
> ""UNICODE"",
> ""_UNICODE""
> ],
> ""windowsSdkVersion"": ""10.0.18362.0"",
> ""compilerPath"": ""A:/emscripten/emsdk/upstream/emscripten/em++.bat"",
> ""cStandard"": ""c11"",
> ""cppStandard"": ""c++17"",
> ""intelliSenseMode"": ""clang-x64""
> }
> ],
> ""version"": 4
> }
> ```
> > My **Compile.ps1** script
> > ```
> Clear-Host
> Write-Host ""â–ˆ Compiling VModule.js/.wasm - $([datetime]::Now.ToShortDateString()) $([datetime]::Now.ToShortTimeString())"" -ForegroundColor Blue
> Write-Host
> em++.bat `
> ""main.cpp"" `
> ""Vx.cpp"" `
> ""Lib\Ins-Emscripten-llvm-BitCode\lib\Ins.bc"" `
> -o ""VModule.js"" `
> -I ""Include"" `
> -std=c++17 `
> -O2 `
> --bind `
> --pre-js ""Pre.js"" `
> --post-js ""Post.js"" `
> -s WASM=1 `
> -s SINGLE_FILE=0 `
> -s ASYNCIFY=1 `
> -s NO_EXIT_RUNTIME=1 `
> -s EXTRA_EXPORTED_RUNTIME_METHODS=[`'ccall`',`'cwrap`',`'lengthBytesUTF8`',`'stringToUTF8`'] `
> -s EXPORT_NAME=`'VModule`' `
> -s MODULARIZE=1 `
> -s DISABLE_EXCEPTION_CATCHING=0 `
> -s ASYNCIFY_IMPORTS=[`'EM_JS_GetIP`'] `
> -s EXPORTED_FUNCTIONS=[`'_main`',`'_C_GetProperties`'] `
> -s VERBOSE=0 `
> -s ENVIRONMENT=web,worker `
> -s USE_PTHREADS=1 `
> # -s ASSERTIONS=1
> # -s FETCH=1 `
> #--closure 1 `
> > Write-Host
> Write-Host ""â–ˆ Ended"" -ForegroundColor DarkGreen
> ```
> > Getting to have those configuration files and that everything works beautifully, it may seem silly but it has taken me a long time but the reward is great since I have integrated my WASM with my web project in Blazor with C#.
> > I appreciate the work taken by all those who carry out and continually improve this project so that others like me enjoy it.
> > Long live the emscripten and WASM lords.
VS Code is not the answer for everything. It's a text editor not even a proper IDE. I'm not building hobby projects. Enterprise projects need a proper IDE.
It's not a bad thing when everything provided by mom. it increase productivity and I can force my brain cells to work on a problem. You should build an OS based on your mom argument because it provide you **great** reward while others getting frustrated.
Emscripten is not Haskell !!!",Bitter Frustration
"Alright, it looks like this topic is right about over for now. Let's take a breather. Contributions are super-welcome, looking forward to all the PRs that add VS and VS Code workflows for the community's benefit.",
"I removed old Inflector, tried laravel inflector, tried to copy Symfony/Component/String/EnglishInflector manually, and even CakePhp inflector, or my own inflector...
There's no inflector who returns
singular('Congress') -> ""Congress"" | Actual result is ""Congres""
singular('Status') -> ""Status"" | Actual result is ""Statu""
Of course i wrapped with ""irregular words""...
Maybe there's not a bug, because word is already singularized when passed to inflector.
Then there's expected something like ""isSingular()"" or ""isPlural()"" when function is not idempotent...",Bitter Frustration
"IIRC, the method does not claim to be idempotent. So regarding the ""congress"" example, I wouldn't classify this issue as bug because you're passing a singular form to a function that expects a plural.
Regarding ""status"" however, it's a different story because ""status"" is in fact a valid plural form of ""status"". So in that case, ""status"" would be the expected result.",
"How do you provide if i pass from console arguments the ""word"", i dont know is it a plural or singular. I think i able to pass it to plural or singular function.
Maybe just update the docs with yellow block with this warning about idempotence, prevents the people for searching ""the good one"" (i mean - spending time)
About status - same story. Status is in singular form ""status"", trying to pass it into singular() gives incorrect ""statu"", when pass to plural -> ""statuses"" as expected. (Status is plural form of status? First time see the point)",Bitter Frustration
"This doesn't look like a bug to me. `singularize()` expects a plural, `pluralize()` expects a singular (I am not even sure we can reliably determine if a word is singular or plural).",Bitter Frustration
"> This doesn't look like a bug to me. `singularize()` expects a plural, `pluralize()` expects a singular
Exactly. That's how the methods are specified. The component uses heuristics to determine singular/plural forms __under the assumption that the input is correct__.
NLP is hard and I don't think this component aims to solve NLP problems. :wink:",Irony
It doesn't look like there is actually anything to fix. So I am going to close this issue as explained before. Thank you for understanding.,
"We show the movie how the user problem converts into *we already have best quality, dont be stupid* as a result some lalala and issue gone, noone understand, noone try. Ok no problem",Bitter Frustration
"I'm sorry that this issue was not resolved the way you expected it to be. What you've presented might be a valid use-case, it's just not one that this component tries to solve.
That being said, I'd like to ask you to use a respectful language when commenting or filing issues on this tracker.",
"Ohh, if we use on our project ""respectful language"" we never push anything.
Class theory means ""expect respectful language from different class, not from your employees"". Thats the quality recept... sorry me for unexpected recommendations. There's only two classes: worker and seller. If you're worker you should expect respectful language from sellers. I do not want to sell your work. I want same as you - make it better.
If you expect respect from guys who wants to improve you - you never improve. As the fact.
===
Again, the problem is: user pass to me unknown plural/singular form as a word. Forcing user to pass both forms made Pluralizer useless. The library can solve it, but *there's already good library* phenomenon found.
And also some likes and agreements for you because of idol love.",Bitter Frustration
"Please note that human language is very difficult to grasp in programming logic. Developers usually are no language experts. I think it's quite amazing that there is programming logic that is able to singularize/pluralize words.
If you think it's possible to detect if a word is singular and plural, please change the code and create a pull request. That's the greatness of open source: combining the knowledge of the every developer that is happy to share their knowledge.
At last, I want to make you aware of the [Symfony Code of Conduct](https://symfony.com/coc). That's how the Symfony community has decided to communicate with eachother (as every developer lives in very different cultures, it's important to have one standard code of conduct for the community).",
"I never did that before, but it seems like sometimes you'll have to do everything for the first time. If using a respectful language stops you from being productive, I am sure you will be able to create your own open source project where this is the way to progress. But I am tired of spending (a massive amount of) my free time improving software that is given for free to everyone who wants to use it and then wasting my time reading things like that. That's why I am locking the conversation here as there's nothing to add.",Bitter Frustration
"**Describe the bug**
If the waiting room is on, attendees can't be promoted to moderator
**To Reproduce**
Steps to reproduce the behavior:
1. Go to demo.bigbluebutton.org
2. Turn on 'Require moderator approval before joining'
3. Start the room, and admit an attendee
4. Click on user to bring up menu and click 'Promote to moderator'
**Expected behavior**
The user becomes a moderator
**Actual behavior**
Nothing happens
**BBB version (optional):**
BigBlueButton Server 2.2.11 (1944)
Also tested on demo.bigbluebutton.org
**Desktop (please complete the following information):**
- OS: Linux
- Browser: Firefox
- Version: 68.7 ESR",
"Can confirm this, we also had this issue with one of our customers today.
If you turn of the ""Require moderator approval"", a guest can be promoted to moderator role.",
Good catch.,
Duplicated https://github.com/bigbluebutton/bigbluebutton/issues/7460,Impatience
"Pedro you proposed to hide the promotion to guests. In my opinion, the solution should be to make the guest promotion work even if the room requires approval. Am I wrong?",
"By design, `guests` cannot be promoted in bigbluebutton.
I agree with you that it shouldn't be like that but to make such change right now could impact other users that already adopt the guest feature as it is. I hope to see the change you asked (and myself asked https://github.com/bigbluebutton/bigbluebutton/issues/7460) in some future release.",
"Pedro, when the room does not require approval, guest users can be promoted without any problem, that works fine. That's why I understand the right solution is to behave the same way. Are you sure that is ""a feature"" that without approval you can promote them, and with approval you can't? I would say no. In fact that is confusing! Just my humble opinion.",Bitter Frustration
"Can you explain a little more about what is a room that does not require approval? What are you considering to be a guest?
This is the server-side control on user's promotion:
https://github.com/bigbluebutton/bigbluebutton/blob/da9489c9dde1e1d0a3da942b95d95def66dd6105/akka-bbb-apps/src/main/scala/org/bigbluebutton/core/apps/users/ChangeUserRoleCmdMsgHdlr.scala#L32-L43
Promotions are only enabled to non-guest users.",
"@pedrobmarin Apologies if I didn't use the right terminology, I'm not so experienced with the project yet. The behavior I see (and others reported in the google group as well) is when the room was created with the guestPolicy=ALWAYS_ACCEPT , then you can promote any user as moderator. But if you created the room with guestPolicy=ASK_MODERATOR , then the menu option is shown but the user is not promoted. In both cases, the users are VIEWERS. Not sure if the code you are showing is connected to this issue.",
"Nothing will change on that behalf. If you could promote someone before, you will still be able to do so.
There is a wild role entitled `guest` that is used in some particular cases. You can identify guest users by checking at the users' list, they will be labled as _Guest_. Those users can't be promoted. The thing we tried to manage on that commit was to simple not show the promote option for something moderators won't be able to execute whatsoever.",
"I can understand that from a technical point of view, but not from a user's perspective. Is there a way we can request the feature to have the same behavior for any kind of user (guest or not)? I see at least 5 guys reporting this as a bug in the google group (https://groups.google.com/forum/#!topic/bigbluebutton-dev/W56HWasv5z8), I guess many more could be interested. Thanks!!! @ffdixon @pedrobmarin",
"This is the point where we go full circle as I agreed with you before that's the behaviour I would expect from the guest feature. Since the changes to make it work as we want lies under a design level of the product, it's more complicated to modify and guarantee nobody loses from it. I hope this is something we could add to bbb core in future releases but right now we can only make sure the UI is in sync on what an user is able to do or not.",
"Hi Pedro, no full circle at all. I understood (but not agreed) what is the current behavior of the guest feature. I'm just asking if there is a process for the community to request changes or new features, because I see I'm not the only one interested. I would like to follow the right process and not to leave it in the comments of a closed ticket as now, if possible. Thanks again for your help!",
"I was the one who agreed with you and the person who took the time to answer all your questions. All I said was that I was one of the interested people on the change you proposed.
If you try to open a new issue you will probably see there is an option for `Feature request`. That's usually the place people can ask for new things.
Have a great evening. No problem at all!",
"Whats wrong with retroarch? Everything is wrong! Its terrible, hujnya ebanaya karoche!
btw its my pack. No all borders/icons/whatever , simple shader which improves graphic out of box
https://github.com/noiseshaade/RetroArch-vape
No stupid descriptions, interface fits in screen
![image](https://user-images.githubusercontent.com/61594968/83236221-27cf0600-a161-11ea-822d-83149d470924.png)
Out of box, retroarch is extremely complex even for advanced users. Who thought that F1 is great idea for menu? Why not escape like in usual games/emulators?
The only advantage of ra vs standalone emulators is that it support shader and TECHNICALLY it could be better, but on practice its 100mb of shader folder. Whats wrong with syncronization, why snes has sound crackling out of box? U link syncronization with monitors sync rate but it seems like it changes. Why you cant do like previous old emulators did and somehow link emulation with stable generator? Its not very bad if u sync game with time, not display
Why sound should lag in 2020?
Why you linked sound control with DB and pression +- returns random volume?
For example, just scaled image with bilinear filter looks extremely clean like this
![image](https://user-images.githubusercontent.com/61594968/83236335-5e0c8580-a161-11ea-952e-9696308c42dc.png)
Color and feelings are not natural
Its possible to fix with some YIQ, blur, black/white point correction, noise for texturization, it gives this effect
![image](https://user-images.githubusercontent.com/61594968/83236413-7ed4db00-a161-11ea-9163-7d076ec77254.png)
Mario looks like
![image](https://user-images.githubusercontent.com/61594968/83236448-88f6d980-a161-11ea-8039-190ef1c86d9c.png)
Not like extra clear
![image](https://user-images.githubusercontent.com/61594968/83236780-13d7d400-a162-11ea-9532-d47b1e38261b.png)
Its also problem that LCD provides unnatural, too juicy color. May be this mario looks more natural, i reduced saturation
![image](https://user-images.githubusercontent.com/61594968/83236857-38cc4700-a162-11ea-9715-e1fd050c8f87.png)
U provide users extremely complex shader pack with AAAAAAA effects for tik tok
Why, if games created for playing, not playing in emulators? I promise that you software makes people VERY angry
A lot of hotkeys assigned on alphabet part of keyboard, why? Stock keyboard buttons sucks. Is it bad if u want just play from keyboard?
as for me, i use
WASD for arrows
QE - start/select
IO-/\O
KL-[]X
JU-L1L2
;P-R1R2
Why does its so necessary to load ffmeg library and crash if u remove this? It needs a lot of memory and huge size of emulator part. Are u really sure that so much users want to record the game or stream on twich?
May be there is some simple fork of retroarch for normal people? Why u cant make simple and ready for use, not so terrible?",Bitter Frustration
"These CRT shaders how does they works? They are very complex but at least out of box they provide ugly dark scanlines, no noise, no blur, thats all
![image](https://user-images.githubusercontent.com/61594968/83237679-4b934b80-a163-11ea-826d-bbab8fabdbe3.png)
Look and ducktales gradients, do u really wanna say that crt method is 'narural'?
![image](https://user-images.githubusercontent.com/61594968/83237790-767d9f80-a163-11ea-96e2-07a4a0f72afe.png)",Mocking
"Your project exist about 10 years and there is still not usable interface out of box, even no guide which explain how make this shit normal, explain what do u need to restore graphic etc",Bitter Frustration
"U are so mad about graphic sync and ideal sync, that forgot about sound interpolation
If u link this with some stable realtime generator, it could give some frames offset like 60.4hz vs 59.8hz, but who cares on PC ~_~
As for me, i changed emulation speed on nestopia to 150% and its okay for fun. And if somebody so geeky that wants ideal sync, he can create custom resolution with 59.94hz",Mocking
"I am sure you didn't read the contributing guidelines before posting...
As for every open source and free projects, maintained by people on there spare time, you are free to contribute, or use something else.",Bitter Frustration
![medecin-famille-masque_23-2148168455](https://user-images.githubusercontent.com/6266038/83276270-a44cfd80-a1d0-11ea-93d2-dc5004f4d889.jpg),
"What's up with the release notes still missing for 1.0.4? It has been four days since the release. This is unprofessional.
https://github.com/pandas-dev/pandas/releases/tag/v1.0.4 links to https://pandas.pydata.org/docs/whatsnew/v1.0.4.html but the whatsnew page has been missing. Shouldn't it be made available no later than two hours after a release? People look for it and can't find it. A release shouldn't be published if its notes are not ready to go.
@simonjayhawkins you seem responsible.",Bitter Frustration
"@impredicative check your tone please: things like ""This is unprofessional"" and ""you seem responsible"" are not kind.
See https://github.com/pandas-dev/pandas/issues/33300#issuecomment-635932128.",Insulting
try https://pandas.pydata.org/pandas-docs/version/1.0.4/whatsnew/v1.0.4.html in the meantime.,
"@TomAugspurger I am stating the facts, and you don't like the facts. Do you know what else is not kind? People wasting their time clicking on a link that doesn't work, waiting days, and it still doesn't work. It's unkind and it's a waste of time. This is a routine problem with Pandas releases. Also, why did you close this issue considering it's not resolved yet? Who is your manager? I want to report your dysfunction to them.",Bitter Frustration
"> @TomAugspurger I am stating the facts, and you don't like the facts. Do you know what else is not kind? People wasting their time clicking on a link that doesn't work, waiting days, and it still doesn't work. It's a waste of time. This is a routine problem with Pandas releases. Also, why did you close this issue considering it's not resolved yet? Who is your manager? I want to report your dysfunction to them.
@impredicative pandas is an open source project, which is ALL VOLUNTEER. If you want to help great. Otherwise, please stop comments that are not helpful at all.",Bitter Frustration
"## Issue Summary
As it has become the standard in RAGL and proven its value, it might be a good idea to replace it everywhere for better balancing.
What is ERCC? ERCC provides more exits for harvesters (north exit) which makes harvesting from all sides more balanced. Furthermore, the outline of the refinery got changed to square to alleviate map-inconveniences. TODO: More detailed guide by widow about ERCC.",
"It this has a footprint
=+x
+=+
x==
while the current one has this
\_X\_
xxx
X==
\===
it looks like
![Screen Shot 2020-06-05 at 14 18 10](https://user-images.githubusercontent.com/37534529/83871007-ebd70a80-a737-11ea-9e09-6e61d8529322.png)
while the OG looks like
![Screen Shot 2020-06-05 at 14 24 34](https://user-images.githubusercontent.com/37534529/83871195-4f613800-a738-11ea-8ee9-de5d4111230b.png)",
ERCC currently suffers from #18232,Bitter Frustration
Please also include screenshots that show how the harvester clips through solid walls and the roof for a fair comparison.,Impatience
"It works most of the time, but if you do these exact paths it clips
![ERCC4](https://user-images.githubusercontent.com/37534529/83871616-08c00d80-a739-11ea-9668-d47c2d36851c.gif)
![ERCC1](https://user-images.githubusercontent.com/37534529/83871625-0b226780-a739-11ea-8af6-99c820c4a31d.gif)
![ERCC3](https://user-images.githubusercontent.com/37534529/83871628-0bbafe00-a739-11ea-9f66-a492a9b27f56.gif)
![ERCC2](https://user-images.githubusercontent.com/37534529/83871630-0c539480-a739-11ea-853d-66bd42b7185b.gif)
One of the problems is that the harvester art is bigger than once cell
![Harv](https://user-images.githubusercontent.com/37534529/83871836-75d3a300-a739-11ea-93f7-94b332bd69a0.png)
that also causes problems with the current ref
![REF1](https://user-images.githubusercontent.com/37534529/83871862-88e67300-a739-11ea-9828-d546cd89e71d.png)
![REF3](https://user-images.githubusercontent.com/37534529/83871884-94399e80-a739-11ea-9c23-dc0f8aef6a83.png)
but these don't exist with ERCC",Bitter Frustration
"> pchote: it would be much better if it used a new design that was not a space-warping bodge of the original refinery
> pchote: get rid of the overhang completely
> pchote: it is never going to be accepted as a replacement for the original refinery, but if you want to be added as a distinct thing that could be optionally enabled then it needs to make sense as its own thing",Bitter Frustration
"One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO.",Bitter Frustration
"I can get behind what @pchote is saying. I think this already has a nice place in map-mods. And if ruleset-only mods are ever implemented then the ERCC (and other balance mods and tests) could be easily applied to any map.
And even if the ERCC gets accepted to the core game then it should be optional and not the default.",
The mutator thing should be mentioned here if it is raised as an issue somewhere.,
"> The mutator thing should be mentioned here if it is raised as an issue somewhere.
#13629 / #14325 / #9422",
"> One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO.
I understand that but it's missleading. Fact is that majority of above avarage skilled players prefer playing with ERCC refineries. OpenRA supposted to improve gameplay of the originals, so players that who actually understand openra's gameplay are forced to play on a mod map doesn't give it a good look
I'm not saying that ERCC has to be implemented in its current form, after all it was merelly an experiment to see if refineries could be fixed. I'm just saying that it's a very relevant issue that needs a solution",Bitter Frustration
"Additionally the person responsible for ERCC is not interested in refining it further, so it falls on the shoulders of someone else",Bitter Frustration
This could also be solved using `PlaceBuildingVariants` and inverted artwork.,
"> > ERCC currently suffers from #18232
> I tested it and that problem doe snot (really) exist with ERCC @Punsho . The only thing I observed is that you have to click a bit ""below"" the husk which is probably due to the logical layout/overlay structure. But you can certainly recover husks from the ERCC refinery.
There's more than one place where a harvester can die, i had a game where 2 harvesters were dead on a refinery. I was able to revive one by the method you just mentioned, but the other was completelly unclickable. This is how this bug was discovered",
"This could be in the form of an tickbox at lobby, probably it would be default off.",
"> This could be in the form of an tickbox at lobby, probably it would be default off.
That's doesn't scale well. A dropdown to choose mutators from would be better.",
"I am officially drawing a line under this request. ERCC will never be merged upstream while I remain involved with OpenRA. At this point it has nothing to do with any of the in-game aspects, it is entirely about the behaviour of the people championing it.
While most of the competitive RA community are good people, there is a rotten core of toxic entitlement that manifests as abuse and belittlement, often focused around ERCC, and usually focused personally against me. This behaviour has forever soured ERCC, and integrating it into the upstream RA mod now would validate that behaviour as an effective strategy for influencing OpenRA's development.
Why now? Two recent incidents targeting me (from N/A in the community discord, and Longely/Widow in the competitive discord - the two main personalities behind ERCC) were incredibly offensive and belittling, and are the straw that has broken the camels back when added on top of many other generally toxic events this year. People in the competitive community were having fun meming about ""us vs the devs"" earlier this year, but at that time I was seriously considering quitting OpenRA completely due to how unhappy and embarrased the behaviour of some people in the community (and the general tolerance of that behaviour as acceptable by everybody else) was making me. Ultimately, I decided that I enjoy working on the project, and making things better for everybody else in the community. I *do not* enjoy interacting with the assholes, and intend to be increasingly blunt in dismissing their toxic behaviour. This is the first concrete action towards that.
I can certainly appreciate the desire for solving directional map imbalance, to the point where I fully implemented the code for one solution (rotatable structures) and developed a working prototype for a second (allowing harvesters to dock from any direction, visually jumping docking point). I think there is scope for some ERCC alternative, but I suggest that the competitive community as a whole put some effort into solving its attitude problem before they try to revisit such ideas here.",Bitter Frustration
"Eric Hodel publicly attacked and was very hostile against Bozhidar Batsov on Twitter:
https://twitter.com/bbatsov/status/1269490401054019584
I do not think this person would represent Ruby community and would be removed from rubygems organization.
As an autistic person and non-western person I don't feel being welcome and safe with person like Eric Holder in Ruby community.",Bitter Frustration
"Calm down, Please.",Impatience
"<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->
<!--- Provide a general summary of your changes in the Title above -->
<!---
Documentation on ZFS Buildbot options can be found at
https://openzfs.github.io/openzfs-docs/Developer%20Resources/Buildbot%20Options.html
-->
### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
The horrible effects of human slavery continue to impact society. The
casual use of the term ""slave"" in computer software is an unnecessary
reference to a painful human experience.
### Description
<!--- Describe your changes in detail -->
This commit removes all possible references to the term ""slave"".
Implementation notes:
The zpool.d/slaves script is renamed to dm-deps, which uses the same
terminology as `dmsetup deps`.
References to the `/sys/class/block/$dev/slaves` directory remain. This
directory name is determined by the Linux kernel. Although
`dmsetup deps` provides the same information, it unfortunately requires
elevated privileges, whereas the `/sys/...` directory is world-readable.
### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->
Manual testing of `dm-deps` script and manpage.
Ran the test suite.
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [x] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [x] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).",
"There is also:
` CC [M] /var/lib/buildbot/slaves/zfs/Amazon_2_x86_64__BUILD_/build/zfs/module/icp/algs/aes/aes_impl.o
`
But not very visible.",Bitter Frustration
"**Is your feature request related to a problem? Please describe.**
Lot of software developers are discussing on twitter to rename default branches for their projects from ""master"" to ""main"" or equivalent https://twitter.com/search?q=master%20branch&src=typed_query
The primary reason being master-slave an oppressive metaphor.
**Describe the solution you'd like**
Node.js core follows the trend to change the industry standard, and renames default branch from ""master"" to ""main"" or similar
**Describe alternatives you've considered**
Sticking with existing master branch name for the default
EDIT: Updated ""renaming master to main"" to ""renaming default branch name from 'master' to 'main'""",
This is not a technically difficult task (https://www.hanselman.com/blog/EasilyRenameYourGitDefaultBranchFromMasterToMain.aspx) but it might break some things. I definitely think we should try to change it.,
Does this mean we would have to change the cluster API (which includes master in its vocabulary) as well in a semver-major?,Impatience
"This is getting silly already. No way it makes any sense.
UPD: How is this offtopic? Please stop bringing politics into development world. That's just mindblowingly silly.",Bitter Frustration
"That is hilarious. From defacing nodejs.org with blm propaganda, to renaming master branch to avoid similarity with master-slave metaphor. What is the next requirement OpenJS will ask for? To remove the **test** directory to avoid similarity with **test**icles? Please keep technical aspects of the software free of politics and globalistic propaganda.",Mocking
"@ronag i don't think that cluster was brought up in this thread, though it has been discussed on other occasions.",
"I'm -0 to the change itself. I don't think that changing it because it is ""industry standard"" is a valid argument, at least not yet. If we are changing it because we think it is a loaded/inappropriate word, then I think we should remove all occurrences to remain consistent with that decision.",
"I don't feel like this is something people have actually complained about. When we've made these changes in the past (for example in child_worker.suicide) it was guided by someone acting in good faith feeling strongly about the terminology. If someone from the project's base does feel strongly about it - I suggest we rename it, though `master` is the default git branch name so I would caution we pick our battles.
I vote we:
- Don't change the name from master to something else currently.
- Change it when a contributor/collaborator feels strongly enough about this.
Of course, if you @trivikr personally feel strongly about this then I support you :]",
"> Of course, if you @trivikr personally feel strongly about this then I support you :]
I don't feel strongly against using master branch name. I proposed it as I plan to follow it in my personal projects and work projects, and wanted to ask Node.js community.
Should we add this to tsc-agenda?
> though `master` is the default git branch name so I would caution we pick our battles.
Is there an equivalent ask in git repo?
If not, we should create one.",
"Also, to all the people making the drive by comments: please keep discussion civil and remember Node.js has a [code of conduct](https://github.com/nodejs/admin/blob/master/CODE_OF_CONDUCT.md). If you can't be polite here you really don't have to comment. It's fine to either support or object to the proposed change (or any proposed change) as long as you are [civil](https://github.com/nodejs/admin/blob/master/CODE_OF_CONDUCT.md#our-standards) - **we do not tolerate abuse** towards the project and its members here.
To collaborators: kind reminder that you are [allowed to ban users](https://github.com/nodejs/admin/blob/master/Moderation-Policy.md#non-collaborator-posts) that make these sort of comments and to hide said comments - but please update the project (as explained there) with what actions you took so that any moderation actions taken are done in full transparency.",
"> Is there an equivalent ask in git repo?
I'm not sure where the git repo is - but I recommend trying the mailing list and asking there https://git-scm.com/community I think ""doing whatever git does and bringing the issue to their attention"" is a viable strategy - but again, I don't feel particularly strongly about the use of ""master"" and if someone else does - sure.
> Should we add this to tsc-agenda?
I'm not entirely sure why?",
"> > Should we add this to tsc-agenda?
>
> I'm not entirely sure why?
To let tsc make a call on this request.
Other option would be to keep this issue open for a week or so, and see if it gathers more feedback or support.",
"> To let tsc make a call on this request.
As far as I understand it that's not how our governance works. Any collaborator may add the `tsc-agenda` label for an issue so it gets TSC eyes on it - but that should only be done if consensus seeking fails. It's an escape hatch for when we need to _force a vote_ which is pretty rare. At least that is my understanding of [the process](https://github.com/nodejs/node/blob/master/GOVERNANCE.md).
So far everyone here seems to be pretty in sync (no one is opposed but no one is particularly in favor). We're not even in disagreement ðŸ˜…",
"> I'm not sure where the git repo is - but I recommend trying the mailing list and asking there git-scm.com/community
I've sent an email to Git Community mailing list, and will update here once they come up with a decision.",
"Bear in mind that URLs linking to `master` will not redirect to the new default branch name (and for repos where it matters, which isn't this one, github-pages only works on the default branch when it's named `master`). It may be worth waiting for Github to fix these discrepancies before making the switch.
(to be clear; i'm in favor of making the change, and ""main"" seems as good as anything else, but the disruption caused by Github's incomplete support for a non-master default branch are significant)",Bitter Frustration
+1 to main but I do want to see what lead GitHub takes in making this easier,Impatience
"I personally feel very strongly that we should change this.
+1 to `main`",
I would be -1 untill a plan is drawn for the changes. This could cause a lot of issues with our build ci which would need to be accounted for.,Impatience
"Ok, it looks like there are people in favor in the org who feel strongly that we should change this. So far no objections and everyone in the conversation is +0 -0 or +1.
Does anyone object to changing this (just the main branch name from `master` to `main`)?
It looks like it's [not particularly hard](https://github.com/nodejs/node/issues/33864#issuecomment-643656288) technically (+ an update to the collaborator guide and policy). We probably need to address [the links](https://github.com/nodejs/node/issues/33864#issuecomment-643672588) as well.",
"> So far no objections and everyone in the conversation is +0 -0 or +1.
What about https://github.com/nodejs/node/issues/33864#issuecomment-643674646?
Also, I think GitHub might pick `trunk` instead of `main`. https://github.com/cli/cli/issues/929...
> Does anyone object to changing this (just the main branch name from master to main)?
I don't object... but I don't think it's a good idea to rush this...",Impatience
"Fwiw there is quite a lot of work for us to do in order to make sure we do this in a way that is not disruptive.
To @AshCripps point, we definitely need to do a large audit and preparation before moving forward.
I was putting together some notes yesterday outlining steps to take and what to consider before making a change like this
I'd like to suggest that we pause discussion until Monday and I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption",
@ronag that message was posted after I started writing mine so I did not see it. Sorry for the (timing) confusion. Fwiw https://github.com/nodejs/node/issues/33864#issuecomment-643674646 isn't a conceptual -1 it's a -1 until a plan is drawn for how we make the changes. I thought that not making the change quickly without discussing this or laying out how (clearly) is a given.,Bitter Frustration
"Needlessly censoring words does nothing to resolve the social issues surrounding them. The reasoning behind this is the same used when changing the gun emoji to a squirt gun. But in the end, such efforts only take away words and symbols we can use to easily describe concepts, such as the hierarchical and control structures we work with every day. It's destructive at worst and pointless virtue signalling at best.",Bitter Frustration
"Anyway, -1 to this. Not worth the effort.",Bitter Frustration
I am -1 on renaming/changing the branch.,Bitter Frustration
-1 on this.,Bitter Frustration
Putting this on the tsc agenda for discussion,
-1 on renaming the branch,Bitter Frustration
"I would recommend we escalate this (to a vote for example) when:
- We have a clear plan on how to make this change addressing the raised issues (GH links, build infrastructure etc).
- We have an individual or group willing to champion those changes.
Can either of the collaborators -1ing (@gireeshpunathil / @mscdex ) speak up regarding what in particular they are objecting to? (The process of changing it? the name `main` itself?)",
"Also, this issue is locked because it has received a large number of abuse comments. Like the website change - changes with this flavor tend to get a lot of attention.",
"> I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption
@MylesBorins Also think about how to roll back when things go wrong. Auditing Jenkins jobs is the kind of mind-numbing tedium that makes human error more likely than not.",Bitter Frustration
"@benjamingr I don't think such a change is necessary and by making such a change it would be creating a lot of technical issues. I've never heard anyone seriously say they found the branch name offensive since git's inception, much like I've never heard anyone seriously find similar terminology offensive elsewhere in computing in the many decades it's been in use.
I believe attaching a single context to such technical terminology (that involves inanimate things) is not useful. Instead, everyone should be focusing their time and effort on things in the world that are *actually* and *obviously* offensive.",Bitter Frustration
"@mscdex
> much like I've never heard anyone seriously find similar terminology offensive...
That's literally what you're seeing here. Keep in mind that what you might consider ""*actually*"" and ""*obviously*"" offensive will differ significantly from what others may find ""*actually*"" and ""*obviously*"" offensive.",Irony
There are definitely some very real and difficult technical challenges to address here. My suggestion would be to make this a strategic initiative with the first step being to establish precisely how to make this change with the least amount of disruption.,
"As a community that has pledged for inclusivity, it is natural to be sensitive and sympathetic to the current situations. However, I guess taking a step back and looking at things from a more wider perspective, I would ask these questions myself, in order:
- Is Node.js community well represented by race, gender, ...? any process change required for the membership criteria for better inclusion of under-represented groups?
- Is Node.js leadership well represented by race, gender, ...? again, any process change required?
- Is Node.js process and practices designed to be inclusive, and is it working well? a retrospective session with commitment to acting on the results?
- Is Node.js nomenclature, words used etc. free from offensive terms? (items such as this)
Doing the naming change first - easy, but causes turbulence in the eco-system, plus gives an impression that we are good with the rest (it is so possible that we are doing good there, but we haven't inspected). Addressing the other things - difficult and time consuming, but addresses the root of the issue from systemic perspectives.
I am willing, and eager to taking part, and / or driving such initiatives, if there is a consensus on `Let us do these things first, and attack this later!!!`",
"> That's literally what you're seeing here.
Are you referring to the OP? If so, that didn't exactly strike me as someone saying they personally find the term offensive. To me the issue text read more like ""hey, other people are making this change in other projects, should we do it too?"".
> Keep in mind that what you might consider ""_actually_"" and ""_obviously_"" offensive will differ significantly from what others may find ""_actually_"" and ""_obviously_"" offensive.
Here ""actually"" and ""obviously"" was meant to describe things that a greater majority of people can agree are offensive.",Irony
"To make it explicit, I am strongly +1 on doing this. This is not mutually exclusive with other initiatives in any way. As for the technical issues, we can either wait to see if something comes from Github, or implement tooling ourselves that would keep the current branch name synced/as an alias, for example.",
"I'd like to echo the sentiment from @addaleax that changing the branch name need not be mutually exclusive from other changes which our project likely needs to do to be more welcoming / inclusive. We should always be examining our process and thinking about what we can do to improve and make small iterative changes towards where we want to be.
The term `master` for the main tracking branch is something that has always bothered me. To push back against any narrative that might claim that changing the name of the branch is a fad I'd like to point to https://github.com/mylesborins/node-osc which is one of my more popular modules. I have been using the name `current` rather than `master` for over a year. The only reason why I had not brought up making this change at the project was because I didn't think it would have the momentum to be successful, not because I didn't believe that this change should be made.
@mscdex do you have specific technical failures you are concerned will happen? Would an in depth transfer that speaks to all of those changes put your mind at ease? If there was a way to ensure that anyone attempting to push or work with `master` in the future would be gracefully redirected by GitHub help to ease those concerns (this is a feature I'm poking at internally to see if it can be offered). > I believe attaching a single context to such technical terminology (that involves inanimate things) is not useful. Instead, everyone should be focusing their time and effort on things in the world that are actually and obviously offensive.
We debated extensively about naming API surfaces, why should this be any different? We accepted a default and those helping to create that default are saying ""hmm, maybe we should reconsider this"". Even if you don't find it useful to attach that context does that fact that others do, and in turn it makes them less productive at what they do, not resonate with you?
> Doing the naming change first - easy, but causes turbulence in the eco-system, plus gives an impression that we are good with the rest (it is so possible that we are doing good there, but we haven't inspected). Addressing the other things - difficult and time consuming, but addresses the root of the issue from systemic perspectives.
@gireeshpunathil I'm having a really hard time parsing the logic here. It seems like you are saying that because making this change will be easier than other changes will imply there is no other work to be done? I don't think anyone is saying that at all. What I do see people saying is that this is a small incremental change, that will take quite a bit of work to do properly... but one that I think will be meaningful to various members of our community.
FWIW I've been thinking quite a bit about our governance model, and more specifically consensus seeking, to examine if it is the best governance model for an inclusive environment. I proposed a [openjs world summit session](https://github.com/openjs-foundation/summit/issues/297) to discuss it as well!
I truly think that making positive social change is an ongoing effort that requires constant small change in the right direction. In fact I would argue that choosing to not make smaller, more obvious, iterative changes (such as changing our default branch) would have the inverse effect of telling people that Node.js is an environment that does not care about this.",
"Also some interesting references for this discussion
[Original Discussion on git mailing list](https://public-inbox.org/git/7265e8cd-6166-1da6-c1b9-85d5d591bfd7@gmail.com/T/#ma9a4da070ddf8ff87729960bb31ce2a8776c73c1)
[Proposed change to git to allow overriding the default branch](https://public-inbox.org/git/nycvar.QRO.7.76.6.2006111610000.56@tvgsbejvaqbjf.bet/T/#mb6bb1a9ea6133b2829d9d47a6ecfa3c909e15b05)
[A contributor from bitbucket stating the github + bitbucket + git are all working on this together](https://twitter.com/seanjregan/status/1271515685353426946)
[A great twitter thread explaining why making this change is the right choice](https://twitter.com/mislav/status/1270388510684598272)
[A statement from a GitHub employee that `main` is likely to be the new default](https://twitter.com/billygriffin22/status/1271274332396773378)
[A checklist used by the github cli to change default branch](https://github.com/desktop/desktop/issues/6478)
[A tool by @gr2m to automate the process of changing the branch](https://github.com/gr2m/octokit-plugin-rename-branch)
[Discussion about changing defaults in GitLab](https://gitlab.com/gitlab-org/gitlab/-/issues/221164)",
"> It seems like you are saying that because making this change will be easier than other changes will imply there is no other work to be done?
@MylesBorins - that was an addendum. My main point is that the other items that I listed are more tangible, personal and direct to the people / group who are subjected to the theme in question, and hence present a natural order to address.
I now see your submission in collab summit, and acknowledge it as part of a constant attempt towards improving our community, in the context of diversity and inclusivity! thank you!!",
"Not a TSC member, but I am a +1 to this change, echoing @addaleax and @MylesBorins sentiments that these do not need to be synchronous changes but can be done simultaneously as we do more intensive work.
For full transparency, I've also brought up the possibility of doing this with all repos under the @nodejs/community-committee.",
"I'm -0 to this change, Many open source project are considering this change but I'm very fearful of all the changes to scripts/automation. As @MylesBorins mentioned, we will need to do a full audit before considering this change. I'd also suggest pushing out a blog post/tweets detailing the changes several weeks before we make the switch in order to give developers a chance to migrate any tooling that they're written.",
I do want to see what lead GitHub takes in making this easier,
"I found a great guide on ways to handle the branch-rename as well as paths towards gradual migration. Will likely be experimenting with some of this stuff on a personal repo over the next two weeks.
https://github.com/chancancode/branch-rename/#gradual-migration",
"Just a note with some tooling I'm seeing come up around this:
- @gr2m (former CommComm member and long-term ecosystem ecosystem member) is working on a GitHub bot. He's talked a bit about it [on Twitter](https://twitter.com/gr2m/status/1273297040231948288?s=20) and has [a repo](https://github.com/rename-master-branch/github-app/pull/1).
- [github-default-branch](https://www.npmjs.com/package/github-default-branch) allows you to change the default branch for one or many repos via CLI.
- [retarget_prs](https://www.npmjs.com/package/retarget_prs) allows you to retarget PRs from one branch to another. It was built by one of the PMs on npm / maintainers of libgit2.",
"> Just a note with some tooling I'm seeing come up around this:
What would really help is a way to have `master` automatically mirror (as in both read and write) `main` for a transitional and possibly long amount of time and point new users and tools towards `main` while gradually and at a very comfortable pace pace migrating all the existing infrastructure, docs, guides and tooling.
I'm not sure if that's possible (or hard) as I'm far from an expert or tooling - but such a tool doesn't sound particularly hard to make conceptually. I think the [guide](https://github.com/chancancode/branch-rename/#gradual-migration) Myles linked to provides one such way (via GitHub actions) that might be appropriate.
Links on GitHub would also probably need to automatically redirect somehow.",
"So we can change any links referring to master to refer to HEAD and they will ""just work"" with whatever our default branch is. The branch-rename repo I pointed at has an action for mirroring master / main
https://github.com/chancancode/branch-rename/#phase-1-mirror-master-and-main",
"GitHub has released a repo with official guidance
https://github.com/github/renaming
It mentions a new redirect feature that launched today
> Now: supporting early movers ðŸšš
Some projects on GitHub have already renamed their default branch. As a result, any links to those projects that contained the old branch name would previously have been broken.
>
>So, our first change, shipped on July 17th, updates GitHub.com to redirect links that contain a deleted branch name to the corresponding link in the repository's default branch.
>
>This change supports projects that have already moved. If you havenâ€™t moved yet, we recommend not moving right now, and waiting until later this year. Weâ€™re investing in tools to make the renaming the default branch of an existing repository a seamless experience for both maintainers and contributors.",
"That's good news, but also means we have to wait ""until later this year"".",Impatience
"We can run a test, but when I changed a repo on Friday the redirect was working.
We could test this on another repo in the org first.
I can also follow up with that team internally and see if we can get it turned on for us.",
"I've seen comments that indicate that blob links redirect but tree links don't (or the reverse); so it may not be a complete feature yet.
Given the large number of open PRs on this repo, it seems like ""silently retargeting open PRs"" would be a pretty helpful feature to have too.",Impatience
"@MylesBorins I doubt CommComm folks would have any issue changing some of our repos, if you'd like to go with that.",
"@bnb if ya'll wanna pick a repo we can work from there.
Maybe nodejs.dev?",Impatience
nodejs/community-committee or nodejs/examples are both low-risk repos that we could try. nodejs.dev is a more complex repo that could also work. I'm up for helping out with any.,
"I've just finished renaming CITGM. Steps included.
* Create main branch
- `git checkout -b main`
* Push main branch
- `git push upstream main`
* changing default branch in https://github.com/nodejs/citgm/settings/branches
* Using the [retarget_prs](https://github.com/ethomson/retarget_prs) utility to update all open pull-requests targeting master
- `npx retarget_prs --token LOLNOPE https://github.com/nodejs/citgm master main`
- I generated a temporary personal access token with all repo permissions for this
* Delete master branch
- `git push upstream :master`
* [Open a PR](https://github.com/nodejs/citgm/pull/819) to update documentation
So far there do not seem to be issues. Some highlights
- All old URLs are automatically getting redirected.
- e.g. https://github.com/nodejs/citgm/blob/master/.gitattributes redirects to https://github.com/nodejs/citgm/blob/main/.gitattributes
- retargeting prs appears to have worked without issue
- all existing integrations with actions + codecov seem to still work
[Tracking issue](https://github.com/nodejs/citgm/issues/812)",
"I also updated nodejs/examples. Here's the steps - they're slightly different than the ones Myles took since I just used the web UI:
- Create `main` from `master` in web UI (dropdown on the repo's main page)
- Change default branch from `master` to `main` in the repo's settings
- Manually retargeted PRs (there were only two, pretty easy to just do this through the PRs in the web UI)
- Delete `master` (can be restored if needed)",
see also: https://github.com/nodejs/admin/issues/543 regarding changing the default for new repos,
Related: https://github.com/github/renaming#renaming-existing-branches,
"List of non-archived repos, along with whether they need to be updated or not.
Generated with:
```JavaScript
const repos = require('./noderepos.json');
for (let repo of repos) {
if (!repo.archived) {
let done = ' ';
if (repo.default_branch !== 'master') {
done = 'x';
}
console.log('- [' + done + '] ' + repo.name + ':' + repo.default_branch);
}
}
```
on output from
`npx repos nodejs noderepos.json`
- [x] Gzemnid:master
- [x] Release:master
- [x] TSC:main
- [x] abi-stable-node:doc
- [x] admin:master - PR - https://github.com/nodejs/admin/issues/589
- [x] badges:master
- [x] bot-love:master
- [x] branch-diff:master
- [ ] build:master - https://github.com/nodejs/build/issues/2761
- [x] build-toolchain-next:main
- [x] changelog-maker:master
- [x] ci-config-github-actions:master
- [x] ci-config-travis:master
- [x] citgm:main
- [x] code-and-learn:master
- [x] commit-stream:master
- [x] community-committee:main
- [x] core-validate-commit:master
- [x] corepack:main
- [x] create-node-meeting-artifacts:master
- [x] diagnostics:main
- [x] docker-node:master
- [x] education:master
- [x] email:master
- [x] examples:main
- [x] getting-started:master
- [x] github-bot:master
- [x] gyp-next:master
- [x] hardware:master
- [x] help:master
- [x] http-next:main
- [x] http-parser:master
- [x] i18n:master - https://github.com/nodejs/i18n/issues/502
- [x] installer:master
- [x] llhttp:master - https://github.com/nodejs/llhttp/issues/115
- [x] llnode:master
- [x] llparse:master
- [x] llparse-test-fixture:master
- [x] lts-schedule:master
- [x] make-node-meeting:master
- [x] meeting-picker:master
- [x] mentorship:master
- [x] modules:main
- [x] nan:master - https://github.com/nodejs/nan/issues/920
- [x] napi-test-suite:master
- [x] next-10:master
- [ ] node:master
- [x] node-addon-api:main
- [x] node-addon-examples:main
- [x] node-api-headers:main
- [x] node-auto-test:master
- [x] node-code-ide-configs:master
- [x] node-core-utils:master
- [ ] node-gyp:master - https://github.com/nodejs/node-gyp/issues/2495
- [x] node-inspect:master
- [x] node-meeting-agenda:master
- [x] node-report:master
- [x] node-review:master
- [x] node-v8:canary
- [x] node-version-jenkins-plugin:master
- [x] node.js.org:gh-pages
- [x] nodejs-ar:master
- [x] nodejs-bg:master
- [x] nodejs-bn:master
- [x] nodejs-collection:master
- [x] nodejs-cs:master
- [x] nodejs-da:master
- [x] nodejs-de:master
- [x] nodejs-dist-indexer:master
- [x] nodejs-el:master
- [x] nodejs-es:master
- [x] nodejs-fa:master
- [x] nodejs-fi:master (archived)
- [x] nodejs-fr:master
- [x] nodejs-he:master (archived)
- [x] nodejs-hi:master
- [x] nodejs-hu:master
- [x] nodejs-id:master
- [x] nodejs-it:master
- [x] nodejs-ja:master
- [x] nodejs-ka:master
- [x] nodejs-ko:master
- [x] nodejs-latest-linker:master
- [x] nodejs-nightly-builder:master
- [x] nodejs-nl:master
- [x] nodejs-pl:master
- [x] nodejs-pt:master
- [x] nodejs-ru:master
- [x] nodejs-sv:master
- [x] nodejs-sw:master
- [x] nodejs-ta:master
- [x] nodejs-tr:master
- [x] nodejs-uk:master
- [x] nodejs-vi:master
- [x] nodejs-zh-CN:gh-pages
- [x] nodejs-zh-TW:master
- [x] nodejs.dev:master
- [X] nodejs.org:master - https://github.com/nodejs/nodejs.org/issues/3761
- [x] nodetogether:master
- [X] official-images:master - https://github.com/nodejs/docker-node/issues/1563
- [x] outreach:master
- [x] package-compliant:master
- [x] package-maintenance:main
- [x] post-mortem:master
- [x] promises-debugging:master
- [x] readable-stream:master - https://github.com/nodejs/readable-stream/issues/461
- [x] reliability:master
- [x] remark-preset-lint-node:master
- [x] repl:master
- [x] security-advisories:master
- [x] security-wg:master
- [x] snap:master - https://github.com/nodejs/snap/pull/17
- [x] social-media-delegates:master
- [x] string_decoder:master
- [x] tap2junit:master
- [x] tooling:master
- [x] tweet:main
- [x] undici:master
- [x] unofficial-builds:master - https://github.com/nodejs/unofficial-builds/issues/35
- [x] uvwasi:master
- [x] version-management:master
- [x] web-server-frameworks:master
- [x] webidl-napi:main
- [x] whatwg-stream:master
- [X] ~~worker:master~~ archived instead",
"> post-mortem:master
Suprised this one hasn't been archived.",Impatience
"@richardlau it should've been, good call",Impatience
Today I renamed the branch on 22 repositories and opened a pull requests to update workflows where necessary.,
"@targos, did you update - https://github.com/nodejs/node/issues/33864#issuecomment-773576351 ? just want to make sure we have a good view on what's left.",Impatience
"@mhdawson yes, I did.",
"Went through an opened issues in a number of internationalization repos to give people a heads up/ask if there are any concerns.
nodejs-ar through nodejs-ko:master. Unless there are concerns by Friday I think we should be ok to go ahead and rename those. Any help for doing that after Friday would be good :)",
Updated a few of the inactive/historical CommComm initiative repos and checked them off.,
"When through an updated the internationalization ones that I'd opened issues on last time.
Created issues to FYI/ask for concerns to rename master->main in the remaining internationalization related ones.",
"Also opened issue in https://github.com/nodejs/education
https://github.com/nodejs/Gzemnid
https://github.com/nodejs/ci-config-github-actions
https://github.com/nodejs/ci-config-travis
https://github.com/nodejs/github-bot/
https://github.com/nodejs/security-advisories
https://github.com/nodejs/security-wg/",
"@mhdawson I already did education, apparently clicking the checkbox didn't take",Impatience
"@bnb, not sure how I missed that it had already been done, thanks.",
"Rename about 8 more today, opened a few more FYI issues in advance of rename",
"For the remaining repositories, could it be enough to rename the branch and then create a new `master` branch and set up a simple bot that copies commits from `main` to `master`? I believe we are only concerned about tools reading from the old `master` branch, not writing to it.",
@tniessen that would prevent github from setting up the very helpful redirects.,Impatience
"Oh, right, good point. Thanks.",
"In the TSC meeting, @gireeshpunathil asked if it's possible to create a master branch after it has been renamed to main. I tested and the answer is ""yes"" even with branch protection rules. @targos suggested that if we create a blank/minimal branch and then protect it, that might work.",
"There are concerns about repos like `nodejs/node` where we land PRs manually, that after the rename, collaborators can still push to `master` by mistake and recreate the branch by doing so.
We can mitigate this with a branch protection rule that prevents pushes, but according to @Trott's tests, the rules do not prevent branch creation (first push after the rename).
Maybe it would work if we push an empty `master` branch and protect it? I don't know what would happen with GitHub automatic redirects in this case...",
"Definitely do not create a blank master branch, that will break the redirects from master to main that GitHub sets up.",
@ljharb do you have an idea to prevent mistakes? Node.js collaborators are used to `git push upstream master` to land pull request and I'm pretty sure a mistake will happen if we don't have something in place to block it.,Impatience
"The options I see are:
1. no change, deal with it as it happens
1. create a branch and protect it, but this breaks usability for everyone who's not a collaborator
1. make a bot that listens for master branch creations and deletes them
1. wait for github.com to support serverside git hooks and reject any branch named master (no idea how long this will take)
1. teach collaborators a different workflow",Bitter Frustration
"> teach collaborators a different workflow
perhaps it's time we begin merging rather than `git push upstream master`? I've heard people mention this for years - perhaps this could be a catalyst.",
Renamed in another 4 repos,Impatience
Opened some more issues on the remaining repos. There are now issues open for all remaining repos except for nodejs/node. Once we get the others done we can tackle that one.,
"A few more done, down to 9 left.",
"2 more, down to 7 left.",Impatience
"> There are concerns about repos like nodejs/node where we land PRs manually, that after the rename, collaborators can still push to master by mistake and recreate the branch by doing so.
>
> We can mitigate this with a branch protection rule that prevents pushes, but according to @Trott's tests, the rules do not prevent branch creation (first push after the rename).
Maybe it would work if we push an empty master branch and protect it? I don't know what would happen with GitHub automatic redirects in this case...
IIUC the concern is only until the migration happens (i.e. until when both `master` and `main` exist and are in sync in nodejs/node), correct? We can plan a time for the migration (similarly to what we do with security releases and CI upgrades), communicate it, and then during that time we restrict push to this repo to those involved in the migration. Once the migration is completed, we restore push to `@nodejs/collaborators`, and ensure the branch protections for `master` only let the bot push to it. I _think_ we can even add a git hook for push to give collaborators a message telling them to use `main` instead of `master` when pushing (not entirely sure).
Another idea I have (which I've been thinking for a different repository/organization, is to use the `post-checkout` git hook to communicate every time someone checks out to master (and try to identify if it was ran on our CI etc). That would help us identify when usage of master branch drops enough for us to safely discontinue it.",
"If both `master` and `main` exist, though, then github won't redirect people from the former to the latter - it'd be better to keep `master` entirely deleted (altho i think branch protections could still prevent it being pushed to)",
Are there any cases where GitHub redirect wouldn't work? Can people (or Jenkins jobs or other forms of automation) still checkout to it etc?,
"@mmarchini I'm not sure, but I've not heard of any issues after switching over any of the other repos.
I do think treating it along the lines of a security release make makes sense. A pre-planned block of time where we restrict access makes sense.",
And I'm along the same thought as @ljharb that we should avoid a period where both are used and just do a rename and try to prevent a push of master after that.,
One of the things we could do is to prepare `node-core-utils` so that it refuses to land a commit if the branch is `master`.,
@targos and I guess a loud FYI for people to update as well.,Impatience
"This list is slightly outdated btw: It's missing `full-icu-npm`, `icu4c-data-npm`, and `full-icu-test`. @srl295 offered to update those <3",
... and @srl295 finished before I could even comment lmao,Mocking
@bnb good that they are already addressed. Once we believe we are complete we can re-run the commands I used to double check we are finished.,
"> One of the things we could do is to prepare `node-core-utils` so that it refuses to land a commit if the branch is `master`.
https://github.com/nodejs/node-core-utils/pull/586",
"What is missing to rename the nodejs/node repo? Are we tracking down what needs to be done?
Maybe @bnb could you take a stab at this?",Impatience
"One thing to check is - in the prompt to do the renaming, it tells you how many PRs it will migrate. If that number doesnâ€™t match the number of open ones, then those not included will be irrecoverably closed. This may be fine, maybe not, but itâ€™s something to be cautious of. (it usually applies when the PR author has not checked â€œallow editsâ€, or, when theyâ€™ve deleted their fork). The person doing the migration wonâ€™t get notifications of the PR closures, but other watchers of them will.",
![image](https://user-images.githubusercontent.com/2352663/154302447-0b4eda97-71c4-4f37-819f-77d339004712.png),
That's not bad! Only 3 will end up being closed. Sadly there's no easy way to find those without looking through every open PR.,Bitter Frustration
"> Maybe @bnb could you take a stab at this?
any specific ideas on what needs to be done here? Happy to help of course.
I think the big thing is that we'll all want to be ready to help out when _something_ inevitably breaks because `master` is hardcoded. Looking through a [very unsophisticated GitHub Search](https://github.com/nodejs/node/search?q=master), we do have a lot of `master` in the codebase and some of them definitely seem to be referring to branches, including in python and Markdown. I think Markdown should be an easy fix in the future, but it's something that will need to be done.",
"> any specific ideas on what needs to be done here? Happy to help of course.
No idea tbh :(. We really need a champion for this.",Bitter Frustration
"I worked through most of the repositories so that we now have all but 6 out of over 100 moved over - see https://github.com/nodejs/node/issues/33864#issuecomment-773576351 for the checklist.
The remaining ones are more complicated in that there are external dependencies. I know that @richardlau has volunteered to look at some of them.
I've not had any cycles lately to loop back and see where we have made process/try to unblock. @bnb that is where a champion who can take a look at the overall picture, do want's need for the remaining repos or find volunteers for the remaining 6 could really help to close it out.",Impatience
"These are the repos that still need a rename:
- [x] build:master - https://github.com/nodejs/build/issues/2761
- [x] i18n:master - https://github.com/nodejs/i18n/issues/502
- [x] node:master
- [x] node-gyp:master - https://github.com/nodejs/node-gyp/issues/2495
- [x] snap:master - https://github.com/nodejs/snap/pull/17
- [x] unofficial-builds:master - https://github.com/nodejs/unofficial-builds/issues/35",
"Poked the issues for i18n, node-gyp, and unofficial-builds.",
"https://github.blog/changelog/2022-05-05-block-creation-of-branches-that-have-matching-names/
ðŸŽ‰ (2)",
"While https://github.blog/changelog/2022-05-05-block-creation-of-branches-that-have-matching-names/ doesn't restrict admins/owners, requiring signed commits should block accidental branch recreation, as not all of the commits in the history are signed. I checked that it works on https://github.com/nodejs/Gzemnid/settings/branch_protection_rules/25690377
![Screenshot_20220511_163952](https://user-images.githubusercontent.com/291301/167852075-6e78b50e-85ae-4ad8-94c0-d4a3528355e4.png)",
Nice workaround!,Irony
"@nodejs/node FYI we are planning to rename the primary branch for nodejs/node to main on Wednesday June 15 ~ 10 ET. This will require that people do the following with respect to local clones. The info from GitHub:
```
Your members will have to manually update their local environments. We'll let them know when they visit the repository, or you can share the following commands.
git branch -m master main
git fetch origin
git branch -u origin/main main
git remote set-head origin -a
```",
"> @nodejs/node FYI we are planning to rename the primary branch for nodejs/node to main on Wednesday June 15 ~ 10 ET. This will require that people do the following with respect to local clones. The info from GitHub:
> > ```
> Your members will have to manually update their local environments. We'll let them know when they visit the repository, or you can share the following commands.
> git branch -m master main
> git fetch origin
> git branch -u origin/main main
> git remote set-head origin -a
> ```
cc @nodejs/collaborators",
"wow, this has been a long time coming, congrats on the perseverance!",
@richardlau thanks for adding the at mention :),
Just noticed that we don't have a calendar event for the renaming (was trying to find the exact time in the calendar). It probably would be somewhat useful to create an event in the project calendar for this.,
"BTW for those who uses node-core-utils, you can use `ncu-config` to update your local config once the renaming happens:
```
$ ncu-config set branch main
```",
@joyeecheung added to the project calendar. For everybody it's this Wednesday June 15th.,
About to do the rename,
"We went through and updated ~100 jobs, now testing a few key ones.",
"Re-ran steps to check all repos in the node.js org above (https://github.com/nodejs/node/issues/33864#issuecomment-773576351) and they look good:
[midawson@midawson rename]$ node doit.js - [x] .github:main
- [x] Gzemnid:main
- [x] Release:main
- [x] TSC:main
- [x] abi-stable-node:doc
- [x] admin:main
- [x] bot-love:main
- [x] branch-diff:main
- [x] build:main
- [x] build-toolchain-next:main
- [x] changelog-maker:main
- [x] ci-config-github-actions:main
- [x] ci-config-travis:main
- [x] citgm:main
- [x] cjs-module-lexer:main
- [x] code-and-learn:main
- [x] commit-stream:main
- [x] core-validate-commit:main
- [x] corepack:main
- [x] create-node-meeting-artifacts:main
- [x] devcontainer:main
- [x] diagnostics:main
- [x] docker-node:main
- [x] email:main
- [x] eslint-plugin-nodejs-internal:main
- [x] examples:main
- [x] full-icu-npm:main
- [x] full-icu-test:main
- [x] getting-started:main
- [x] github-bot:main
- [x] gyp-next:main
- [x] hardware:main
- [x] help:main
- [x] http-next:main
- [x] http-parser:main
- [x] i18n:main
- [x] icu4c-data-npm:main
- [x] js-native-api-test:main
- [x] llhttp:main
- [x] llnode:main
- [x] llparse:main
- [x] llparse-test-fixture:main
- [x] loaders:main
- [x] loaders-test:main
- [x] lts-schedule:main
- [x] make-node-meeting:main
- [x] meeting-picker:main
- [x] modules:main
- [x] nan:main
- [x] next-10:main
- [x] node:main
- [x] node-addon-api:main
- [x] node-addon-examples:main
- [x] node-api-headers:main
- [x] node-auto-test:main
- [x] node-code-ide-configs:main
- [x] node-core-test:main
- [x] node-core-utils:main
- [x] node-gyp:main
- [x] node-meeting-agenda:main
- [x] node-pr-labeler:main
- [x] node-review:main
- [x] node-v8:main
- [x] node-version-jenkins-plugin:main
- [x] node.js.org:gh-pages
- [x] nodejs-collection:main
- [x] nodejs-dist-indexer:main
- [x] nodejs-ko:main
- [x] nodejs-latest-linker:main
- [x] nodejs-nightly-builder:main
- [x] nodejs.dev:main
- [x] nodejs.org:main
- [x] official-images:main
- [x] package-maintenance:main
- [x] post-mortem:main
- [x] promises-debugging:main
- [x] readable-stream:main
- [x] release-keys:main
- [x] reliability:main
- [x] remark-preset-lint-node:main
- [x] repl:main
- [x] security-wg:main
- [x] snap:main
- [x] social-media-delegates:main
- [x] social-team:main
- [x] string_decoder:main
- [x] tap2junit:main
- [x] tooling:main
- [x] tweet:main
- [x] undici:main
- [x] unofficial-builds:main
- [x] uvwasi:main
- [x] version-management:main
- [x] web-server-frameworks:main
- [x] webidl-napi:main
[midawson@midawson rename]$",
"Good on the pkgjs branch as well
midawson@midawson rename]$ node doit.js - [x] .github:main
- [x] action:main
- [x] action-import-blocklist:main
- [x] create:main
- [x] create-package-json:main
- [x] dependents:main
- [x] design:main
- [x] detect-node-support:main
- [x] meet:main
- [x] membership-updater:main
- [x] nv:main
- [x] parseargs:main
- [x] statusboard:main
- [x] support:main
- [x] support-separate-repo:main
- [x] triagebot:main
- [x] wiby:main
[midawson@midawson rename]$",
"AMAZING!!!
<img width=""487"" alt=""Screen Shot 2022-06-15 at 12 14 01 PM"" src=""https://user-images.githubusercontent.com/498775/173876093-16209cbf-25d9-4f47-92a8-e8c7eccd84c1.png"">",Mocking
ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰,
"ðŸ™Œ ðŸ™Œ ðŸ™Œ
Can we unlock this issue?
@mhdawson can provide a public update and close it!",Impatience
"I donâ€™t think unlocking the issue is necessary at this point. If the branch rename is causing an issue, Iâ€™d recommend opening a new issue instead. it feels good to click on the â€œClose as completedâ€ button for this one, thanks to everyone who worked on making it happen ðŸš€",
"We've completed running some tests and all looks ok at this point. We are not aware of any issues at this point.
This was a long an incremental effort (almost 2 years) and I'd like to thank @richardlau and @sxa who helped me do some of the tricker/more complicated repos. It was easier to do those as a group than it would have been for any one person on their own. Also thank to all of the other collaborators who supported in ways from finding a way to prevent accidental pushes of the old branch to quickly responding to issues suggesting the rename in repos across the nodejs org.
I'd also like to thank Red Hat management who early in the larger discussion on renaming primary repos across the GitHub ecosystem not only supported but asked teams to help open source communities make the change.",
Added fix to [node-stress-single-stress](https://ci.nodejs.org/view/Stress/job/node-stress-single-test/) today which had a reference to the old name hiding in an advanced panel in the jenkins UI :-),Irony
Have to add that given the change size and my initial skepticism about it being a smooth transition - Â±20 days later it has been a pretty smooth transition from a collaborator's PoV (at least mine) so props to whomever ensured that and made sure that this change could happen without interruptions.,
"against stupid political agitation that has no place in opensource world. Whole world use master/slave terminology in almost all technical sciences and there is no reason to change it. As I am not communist, I am doing regular commit and PR and not rewriting the (git) history by force ...
### Motivation and Context
Removes political agitation from the code. Makes code much more readable and clean. ### Description
Only clean revert of previous weird commit. No additional changes.
### How Has This Been Tested?
Didn't make any tests.
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ x] Documentation (a change to man pages or other documentation)",Bitter Frustration
"I disagree that reverting to the previous language makes the code more readable and clear. In fact, it does the opposite. The patch adopts the ""dependency"" terminology used by `dmsetup(8)` in order to be as precise as possible when describing the functionality.",
"Words such as ""wh\*telist"" or ""b\*acklist"" are racist. We cannot have those absolutely disgusting derogatory words in our codebase. Therefore, we need to change them to race-neutral words.
There are more references to racial supremacy throughout the code. These should be discussed.
Should this macro be renamed to `skip_blankspace`?
https://github.com/openzfs/zfs/blob/master/module/nvpair/nvpair.c#L46
Also, in the Lua module, we mark objects as ""wh\*te"" or ""b\*ack"". I suggest changing the colors to some non-offensive ones. I propose indigo (`#4B0082`) and forest green (`#228B22`).
https://github.com/openzfs/zfs/blob/master/module/lua/lgc.c
### Motivation and Context
Following other open-source projects, we should strive to keep the ZFS code base clean of any vulgar, racist or other unnecessary remarks and to allow for friendly and welcoming environment for people of all kinds.
### How Has This Been Tested?
It compiles - it's just basic refactoring.
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [ ] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [ ] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).",Identify Attack/Name Calling
Indigo children are against use of color indigo to color memory.,Irony
I identify as a blockhead and find this PR extremely offensive.,Mocking
"Who put this garbage into your head? Should we also ban traffic lights because there are red and yellow lights (sorry, I can't even guess anything for green)?",Mocking
"Neither ""whitelist"" nor ""blacklist"" are racist. Please check the history and use of terms before
jumping to conclusions.",Impatience
When this functionality was added more descriptive and clear names could have been used. I see no reason not to address that now. PR 10457 makes a very similar change to this with slightly different terms (blocked -> excluded).,
"<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->
<!--- Provide a general summary of your changes in the Title above -->
<!---
Documentation on ZFS Buildbot options can be found at
https://openzfs.github.io/openzfs-docs/Developer%20Resources/Buildbot%20Options.html
-->
### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
These terms reinforce the incorrect notion that black is bad and white
is good.
### Description
<!--- Describe your changes in detail -->
Replace this language with more specific terms which are also more clear
and don't rely on metaphor. Specifically:
* When vdevs are specified on the command line, they are the ""selected""
vdevs.
* Entries in /dev/ which should not be considered as possible disks are
""excluded"" devices.
### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->
compiles
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [ ] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [x] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).",
"as I just posted an issue as a person of colour asking to have my thoughts considered, it was closed to discussion and locked as **too heated**.
this is a pretty blatant indicator that you didn't care about race relations at all.
please stop locking all the comment sections for these IMPORTANT ISSUES you supposedly want to further and support.",Bitter Frustration
because black comments matter..,Identify Attack/Name Calling
"As I indicated in https://github.com/openzfs/zfs/issues/10458, I'm open to a discussion on this. However, we have seen that when comments are open to anyone, the discussion quickly becomes dominated by people who are not involved in ZFS development, and too heated to be productive. (e.g. reddit, ars technica).
I believe that as the OP you should be able to continue to post to locked issues. Members of the OpenZFS github organization can also continue to post to locked issues.",
"I determined that even the OP can't post on locked issues (unless they are a member of the organization). I didn't hear from you privately, so I'm going to unlock the other issue and continue the discussion there.",
"There is currently a strong debate about whether the standard branch should be given a name that has less negative implications than ""master"". The starting point was apparently this [thread on the Git Mailing List](https://lore.kernel.org/git/CAOAHyQwyXC1Z3v7BZAC+Bq6JBaM7FvBenA-1fcqeDV==apdWDg@mail.gmail.com/), which, as far as I know, brought the topic up for the first time. It is criticized that the term ""master"" might have a negative connotation for people who were directly or indirectly affected by slavery. I think that especially in the light of the fact that the Rust Community is known for its openness towards all ethics and minorities, serious consideration should be given to renaming the master branch correspondingly. The GitHub CEO himself has already expressed his support for the idea (see [this tweet](https://twitter.com/natfriedman/status/1271253144442253312?s=19)), and the official GitHub cli repository has also already been changed accordingly.
Currently, it seems that the following two are the most common alternatives to the term ""master"":
- trunk
- main
GitHub has chosen the term trunk for the cli repository. Whether the name of the master branch of this repository should be changed accordingly should be discussed. I'm open to any constructive opinions on this topic.",
"I vote for `trunk`, `main` is very confusing as a german speaking hu**man**.",
Or just don't change anything because this is a dumb idea and noone has complained before because it's not related to racism in any way and the 2 accounts above are just alts.,Bitter Frustration
"What is an alt? Do you mean the german word for `old`? You're name doesn't seem very old, it seems like a minecraft name.",Mocking
Alt is alternate account or new account and yes i do play minecraft,
"oh good to hear, great game",Irony
"Hi, This user has been deleted. Beyond that, major changes need to go through the RFC process or the compiler team's major change process. As such, I'm going to give this issue a close, as this isn't the right way to pursue this change. Thanks.",
"I met some 12yo guy and he played in Megaman, it was very hard for him. Not only megaman, even Duck Tales
And... Out of box your software is totally unusable, especially for kids
These AI, wtf is this? Why even cheap old chinese consoles with shitty hardware can just launch games and play without fucking with settings, but your software can not?
You have tons of shaders, but no shader preview. With paint.net u can preview changes, in your AAA software u cant. Very slow navigation, it does not remember position
Extremely huge interface
My config is this
![image](https://user-images.githubusercontent.com/61594968/85333850-4e563780-b4a8-11ea-97f7-b730b2c5f262.png)
Your is this
![image](https://user-images.githubusercontent.com/61594968/85333927-70e85080-b4a8-11ea-86c7-f9e45bac2f7f.png)
Why you describe wtf does 'audio' means? Can u make it fit in screen
Ozone has no scrollbar, no mouse support, if u try to scroll, actually its equal to up/down button
People dont wait that software does not work with mouse
U cant explain even default configuration, what button u need to press for some effect
![image](https://user-images.githubusercontent.com/61594968/85334229-e0f6d680-b4a8-11ea-8ee7-44cdc1c33a8b.png)
U can draw something like normal games if gamepad is not connected like 'L - ok', 'O - cancel'
How to reset settings with start button u dont descibe too
Why do u draw licence? It takes much space
![image](https://user-images.githubusercontent.com/61594968/85334410-3501bb00-b4a9-11ea-962d-8b9ed949ca50.png)
Why not organize cores like
NES\
Snes\
U just give people huge list of all cores. What difference between then - there is no description. But u can see that lisense is GPL v3, very important information haha
Your software with 'release' has >13000 of files, size is bigger than 600mb and out of box it even does not have any core
Alphabet keys are assigned on emulator functions. If u wanna make keyboard gamepad like
wasd iokl it will conflikt, and no highlight. WTF, why F1 calls menu? Do u know that many people use laptop, they can assign something on F keys by default
And they have to press Fn+F1 to call menu, who wait that?
Your bloatwared AAAAAA software is extremely glitchy, unusable for NORMAL people
Even your buttons numeration
![image](https://user-images.githubusercontent.com/61594968/85335010-48615600-b4aa-11ea-83fc-caddb6418469.png)
Like BY...some keys...AX is wtf
On 1st place u put very strange settings like bind timeout, duty cycle
![image](https://user-images.githubusercontent.com/61594968/85335123-747cd700-b4aa-11ea-8d85-c3affaf220d2.png)
If user wanna change settings, on 1st place u need to have Port1, just tune control
I dont know how after 10 years of development u still have so glitchy and unusable software",Bitter Frustration
"And ra eats cpu even if emulation is on pause. nestopia or epsxe do not do this, it continues to render and heat laptop even if it in fullscreen and u switch from ra to windows",Bitter Frustration
"For normal people on windows u can make small version without these bloatwared things, simple structure of folders, no tons of files in root, some cores are included out of box",Mocking
"As for me, to make interface usable, i just reduce scale to 0.6x and disable sublabels
If I need some description, i can press select button, btw font size in this is very small
![image](https://user-images.githubusercontent.com/61594968/85336082-105b1280-b4ac-11ea-9a05-117d092eacb4.png)",Bitter Frustration
"For normal view as for me i need some calibration, its possible to make it very simple
From this
![image](https://user-images.githubusercontent.com/61594968/85336237-49938280-b4ac-11ea-9db4-30048a378ff7.png)
To this
![image](https://user-images.githubusercontent.com/61594968/85336254-53b58100-b4ac-11ea-8f56-0c2f357b90fb.png)
YIQ boost, levels correction, some blur (coz CRT TV does not provide clean lines), some noise for texturization
Its not correct to just draw black lines or crt subpixels, coz when u played from tv, it was no huge subpixels from typical distance
These crt shaders have too strong settings and its a parody",Bitter Frustration
"I appreciate you taking the time to assemble these suggestions and pics. I deleted the pic of your friend, as we don't want any pictures of people (esp anyone 12 or under) for various international liabilities.
While I/we don't agree with all of your suggestions (or with how you've expressed them lol), some of them are valid and helpful, so thanks for that. As long as you want to keep your suggestions in this thread, I won't close/lock it.",Irony
vashe pofig,
"But u should know that for normal people, not crazy linuxoids, especially for kids, your software is nightmare in everything what u did. Any normal guy i thing can tune classic emulator, but retroarch looks like u hate people and kidding them",Mocking
your ranting is misplaced. You should know this is not the place of such.,Bitter Frustration
"Chill out men!! kids under 10 can use retroarch confirmed, aside from that, your attitude it's so bad, you could read retroarch docs to how to make a plesant UI, also you could use kiosk mode to hide advanced settings, you can customize retroarch to your own flavor, but have respect to the devs, also you could make a PR to contribute, instead of just spittings scorpions from your keyboard, this is an open source project, help them, not sink them.",Bitter Frustration
"Oh okay where is place :)?
Its just my thoughts
People says that u need to be correct, but actually retroarch is not correct of anything. It has some logic, ok, but this logic works only for developers and geeks
Who wait from gaming software that F1 calls menu? Or there is mouse, but actually mouse does not work like normal mouse. Does making scrollbars or mouse support is more difficult that AI discord twitch?
Why am I see that video button changes 'video output settings'
Does user so stupid that he does not know what video means? But this stupid user must know that F1 calls menu
Base mission of emulators is providing people play in games. Just install and play, whats wrong with this logic? May be it needs some delays, but retroarch as i know is crazy about strange ultra super zero latency. Sound could lag on not super PC, but its okay, coz there is super zero latency is activated
For most cases laptop is enough. ps1 emulators worked on pentium 3
psp with 333mhz cpu and 32mb ram has emulators of snes gba, whatever, fullspeed no lags even with some delays buffer size whatever
But retroarch...",Bitter Frustration
"As non-native english speaker, I recognice that I have taken a lot of time to be able to express myself in a proper, polite way, I think that @noiseshaade is a non-english native, so I recognize his writing as my own of the first years, and I feel it not like a rant, but as an struggle to express his point of view. I found that @hizzlekizzle was very cool and polite to accept it. Retroarch is an incredible piece of software, powerful but complex to master. To be constructive, I would suggest that maybe all @noiseshaade frustration, and probably many others could be solved with an option to launch a ""first time launch wizard"" that allows less savvy users to get retroarch up and running with a few clicks here and there: Just basically downloading a few cores depending on the system you want to emulate, setting up the gamepad, enable kiosk mode, and you are good to go. With that you cover the bare basics to have it up and running and you can later prettify or tweak the output to your liking. I also found nice the shader preview suggestion (and also a notification of ""resource intensive shader"" would be good). I had some struggles finding out , by example, which was the highest level of XBRz shader my HTPC could handle without getting sluggish. Sometimes it got so slow that I had to kill retroarch because it was faster to relaunch it than waiting to reach the shader menu again.
Not everyone will agree with me, that's just my two cents.",
"Its poweful... But powerful for who?
What 'zero latency' means?
For example, u can reduce audio buffer size from 64ms to 1ms. Latency is 64x time less, but actually normally u will not see any difference. And it needs as i know extremely powerful hardware
On normal hardware it will be extremely laggy as i know
Its possible to make these 'ultra small latencies' so small that comfortable SNES emulation needs intel i9 10900k 6GHz DDR6 128Gb ram quantum pc neural AAA networks whatever, but its not fun",Bitter Frustration
"For example, people can buy low-end laptops if they just need some internet
Modern games are AAA shit, overloaded with special effects, its not magic, its not fun, they just eat resources. People or kids do no wanna play in 'modern' games
its nothing wrong if by default retroarch does not eat many resources, if somebody needs these zero latency or what these guys use, he can activate it
retroarch focused on mobile phones, but actually playing in old games from touchscreen is awful
there is some cool things for phones which turn them to psp like console
https://www.youtube.com/watch?v=b3-Ua8ZQc84
but retroarch 'shop' sells t-shirts, iphone cases... https://teespring.com/stores/retroarch ... very very strange",Bitter Frustration
"@Zephyr-Battiassi Yes, I agree that an optional first-run wizard would be nice. Unfortunately, it's one of many things that are very difficult for us to do within our UI vs with a single-platform native GUI toolkit.
@noiseshaade we don't have anything invested in the teespring merchandise. It's just a free service that sells stuff with your logo/designs for a cut of the proceeds. In contrast, it's very expensive and labor-intensive to produce, market and sell your own electronics hardware. It's something we've wanted to do for a very long time, but it has very high up-front costs (upwards of $20k for the controllers we were looking at a few years ago) and we would have to store the stock, mail out orders (internationally), deal with refunds, etc.",
"> Extremely huge interface
`./configure --disable-menu`
Problem solved!",Irony
"oh for me 0.6x scale and disable sublabels solve interface problems, even no real scrolling is no critical
Instead of this
![image](https://user-images.githubusercontent.com/61594968/85346749-6cca2c00-b4c4-11ea-9c37-5b78b2e7f189.png)
when gamepad is not connected, it could be something like this
![image](https://user-images.githubusercontent.com/61594968/85346774-7d7aa200-b4c4-11ea-8341-6820bf9917b8.png)
If u just launch emulator on PC, u dont know what this means
![image](https://user-images.githubusercontent.com/61594968/85346827-a8fd8c80-b4c4-11ea-9266-1762bb8ed733.png)",Impatience
"I dont know how retroarch is popular on mobile phones or consoles, may be a lot of people use it, but actually they do not have much choice
If there is regular windows user
For example u wanna just play one old game like megaman
Download nestopia, ntsc filter and set up controller fastly
With retroarch
Download 200mb installer with thousands of files, unpack them. download core, forget about previous user experience, even calling menu
And after it... Same thing, but after tons of manipulations and time Even somebody wanna try discord/twich/ai achivements whatever, they have to spend A LOT of time and energy. And mental health",Bitter Frustration
"> Chill out men!! kids under 10 can use retroarch confirmed, aside from that, your attitude it's so bad, you could read retroarch docs to how to make a plesant UI, also you could use kiosk mode to hide advanced settings, you can customize retroarch to your own flavor, but have respect to the devs, also you could make a PR to contribute, instead of just spittings scorpions from your keyboard, this is an open source project, help them, not sink them.
Hmm. I do not need 'pleasant ui'
Project exist 10 years. Classic emulator like nestopia snes9x was created with very small team, or with only one person
Or, for example, there is zsnes it has its own 'driver' like retroarch does, but its intuitive
![image](https://user-images.githubusercontent.com/61594968/85352867-eff37e00-b4d4-11ea-8f27-e45e43a1a67b.png)
only one 1 file, size is 580kb
It just works. Its not 'ideal', but its very easy to understand.
Ntsc 'shader' exists too
![image](https://user-images.githubusercontent.com/61594968/85353582-95f3b800-b4d6-11ea-854a-7c129d5f45e1.png)
Simple sliders
![image](https://user-images.githubusercontent.com/61594968/85353636-ba4f9480-b4d6-11ea-9f52-2611cbb05f7c.png)
Retroarch has **54** crt shader presets. No preview. No remember position. How does they work... Usually they are very strong or complex, some of them can lag nes on intel hd4000. No sliders or any visualisation
![image](https://user-images.githubusercontent.com/61594968/85353837-30ec9200-b4d7-11ea-914f-5bfb68c2a67c.png)
After 10 years of development with huge team...
For me as 'normal' user retroarch has only 1 advantage, i can calibrate image
I dont know all these ntsc crt things, just some colors changing, noise, blur...",Bitter Frustration
"I mean, Retroarch is more complex than single emulators. Retroarch is a flexible front-end to these emulators, not a 1:1 representation of them. It's a bit like that multi-emulator from Byuu.",
"Man
There is a lot of chinese multi emulator handheld consoles google it on youtube
https://www.youtube.com/watch?v=9mSvczcgS9c
https://www.youtube.com/watch?v=TX7igfC8wgs
They emulates ps1, i think that there is no much ram or not great CPU
U can put them in pocket
Just launch game and play it.
Whats wrong with retroarch on PC?",
"Rk2020 3.5 ips screeen It emulates psp well
https://www.youtube.com/watch?v=CsxWApj5xmw
RockChip RK3326 (4cores ARM Cortex-A35 1.3 GHz)
In **2020** year i still have troubles on 2.6Ghz intel i5 to emulate ps1, oh no. its after 10 years of retroarch development...",Bitter Frustration
"Do it by yourself and stop complaining, pay to new developers to make it like you want, all of your issues have of somekind of hate, stop being pasive-aggresive with your comments, and mainly agressive.",Bitter Frustration
"I mean... I think its huge team. Huge team which do nothing. They just promise that they fix something... But they cant do this. If they make ready out of box software, there is no growth... No growth means degeneration, people say. They should growth. Bigger size. More icon pack. More borders. More shaders. More system requments. More cores. They can write 100 of cores for every platform. Its 'job'
For example, Microsoft has >151000 of people, Apple >137000
What does these people do actually? Win 10 vs Win 7 has no much difference
In modern world its normal situation that billions of people do nothing. And retroarch team...",Bitter Frustration
do nothing geez. can we see what you've done aside from rambling? did you pay anything? were you forced to pay if so? stop rambling and get a life.,Bitter Frustration
"U should know that now we live in the 'end of time'. All these events ðŸ˜¼. There is no school, no job or predictions anymore :)",Irony
"8)
![image](https://user-images.githubusercontent.com/61594968/85360995-b3ca1880-b4e8-11ea-9511-189c695159c8.png)
![image](https://user-images.githubusercontent.com/61594968/85361005-b7f63600-b4e8-11ea-95d7-1ded806d5615.png)",Mocking
">There is a lot of chinese multi emulator handheld consoles google it on youtube
Yes and some of them use RetroArch lol
> huge team
There are quite a few people who contribute to libretro as a whole (cores, cheats, etc.), but for RetroArch itself, there's usually only a handful. I think the most active contributors at one time was like 6 and it's currently ... 2.",
"I agree with you on at least one thing : Ozone should have a proper touch interface, for :
- Scrolling
- Going backward without having to touch the tiny ""Back"" button on the bottom righ hand corner (e.g. using the back button of the phone, make sense.)",
"Oh some guys make core. There is a lot of cores for snes for example
In old days people who made core - they make interface too. Its art work too, by the way. Snes9x has its own interface, zsnes, nestopia
Its nothing wrong if people who make cores make usable interface. Or they can share retroarch build with included core
People actually have their own taste. If somebody likes classic emulators, he never make build with such huge scale for pc and dont assign function on alphabet keys. Or something like F1 calls menu
Its possible to make build for yourself or your friend",
"When it was classic emulators, developers of cores took responsibility how emulators look and works
Its a shame, if u turn compact menu (it takes 22px of height)
![image](https://user-images.githubusercontent.com/61594968/85429199-6384a200-b54c-11ea-9ffe-d1b27ca17a47.png)
To this huge menu (102 px of height with stupid) with no scrollbars ![image](https://user-images.githubusercontent.com/61594968/85428881-dd685b80-b54b-11ea-8085-1c81dfbfdd84.png)",Bitter Frustration
its time for you to stop rambling and use something else. no one is forcing you to keep using the most awful software ever.,Bitter Frustration
"Yeh, but in my opinion retroarch destroys whole emulation party :). For example, u wanna play in game and enjoy it, but u cant enjoy it, coz of
I spent about 4 month to tune retroarch
For my side it looks like
Play in usual emulator
When boring, launch retroarch
Catch another AAHAH WTF IS THIS
Return to normal emulator
I just know that i can calibrate picture using shaders, for me advantages of ra vs epsxe-like emulators is only framerate counter - it means i can generate dynamic noise. And there is gaussian blur preset, im not shader coder
Then it can be used on nes or any console
I had some practice with shader on videoplayers like potplayer and in retroarch its very glitchy
![image](https://user-images.githubusercontent.com/61594968/85433773-ed376e00-b552-11ea-9371-98a9fd97b136.png)
For players its like 'before scaling' and 'after scaling', it possible to use fullscreen as surface. It allows u hide (apply levels correction) black borders and apply noise for them too
Ra has strange behaviour. For example if u wanna work with screen as surface, u have to use bilinear filter. Imho if u use just blur instead of bilinear filter, it gives better results
For example i wanted to try
Its original
![image](https://user-images.githubusercontent.com/61594968/85434412-07258080-b554-11ea-8dd7-286e23c670b2.png)
Its bilinear
![image](https://user-images.githubusercontent.com/61594968/85434616-523f9380-b554-11ea-838e-5d9ceeb74aa2.png)
Its nearest algo + gaussian blur
![image](https://user-images.githubusercontent.com/61594968/85434653-5e2b5580-b554-11ea-9ad9-6d40daaf10a8.png)
But u cant get 'clean' pixels for nearest algo, it should works like integer upscale roundup(displayres/consoleres), then bilinear or whatever downscale to fit in windows
But its not possible, u have to apply blur after bilinear filter... Okay, it works, but less accurate",Bitter Frustration
"Shaders is like 'killer feature' of ra, but actually...
No preview, no remember position, tons of strange effects, no visualization (sliders) like reshade
There is a tons of ra features but they do not work well. They spend a lot of time for documentation, porting, web site, youtube preview, whatever. No usability",Bitter Frustration
"Maybe its user feedback. Or 'its open source, anybody can fix it'
But retroarch has reputation of ugly unusable bloatwared software
They share people 200mb installer for windows with thouthands of files
Usual people do not approve it, if they have fun with classic emulators
For third part developers its no much reason to help retroarch, they continue share this 200mb installer with 190mb debug exe, tons of files
Its 'universal' politics of universal gui with same scale on phones, PC, whatever, universal borders and pngs
Normal people who stay on PC i think know about retroarch, but users and coders who can fix something should hate retroarch",Bitter Frustration
"Alright, you've moved from constructive feedback to pointless complaining, so I think we're done here.",Bitter Frustration
EOM,
Pre-emptively locking the issue. I'm letting people know i'll be making this change. The change itself is not up for debate.,Entitlement
"Similar to the reasoning behind GitHub's recent statement that the default branch on new repos will be changed to not use ""master"" can we change the name of the default branch on this repo to something else? Perhaps ""main""?",
Master is not a problematic word.,Irony
"This crap has gotten way out of control, give em an inch and they'll try to take a mile. Enough is enough already, rather than people getting offended by a word that is not offensive in this context, grow some thicker skin and calm down. I agree with @spock123 above, here the word Master is not a problematic word",Bitter Frustration
"@CodeLiam I guess my `Masters Degree` now has to renamed to `Main Degree` as well.. What people don't grasp is that in this context, the Master is like the Master in an audio recording. It's THE version. That's the context and the only context. I can't believe the insanity that's going on.",Bitter Frustration
"I concur @spock123 I am of the opinion that this is not even an issue with regards to Angular, it's functionality, and if it works properly and should therefore simply be closed. Regardless I hope you are well and have an awesome day!!!",
"Simply stupid. There's no other way to put it. Stupid, like all this progressive shit destroying everything it touches.",Bitter Frustration
"Hey @jermowery, thanks for starting the discussion. This is something we've been talking about. As you mentioned GitHub will be likely [taking actions](https://twitter.com/natfriedman/status/1271253144442253312?lang=en) in the future and we'd want to coordinate the efforts.
I'll lock this thread for now. If you're interested in further reading on the motivation for such a change, I recently got pointed to a [similar discussion](https://lkml.org/lkml/2020/7/4/229) in the Linux community.",
"The framework, CLI and components repo have recently switched to using `main` as default branch.",
"Currently IPython uses `~/.ipython` which just clutters people's home directories and makes things like backing up configuration files much more hassle than it should be. This was last discussed more than five years ago and should be discussed again. Points made at that point in time frankly, don't really hold up. It is more consistent to follow platform specs, people learn where to look first. Secondly, for support reasons, you don't really have to ask what platform they're on, just mention the three paths in one sentence, it's actually rather easy.",Bitter Frustration
"For those looking for the previous discussion, it can be found [here](https://github.com/ipython/ipython/pull/4457).",
"I believe if you move your folders in XDG compliant places it should work, though updating all the code and documentation all over the internet, plus the code to do the right thing is a lot of work; and XDG spec IIRC did not completely match or was not clear for all the types of files that could be present. There might be packages on PyPI that may help with ~/.config /User/.../Libraries/AppData. %appdata% depending on OS, and this will also likely need to touch all the jupyter ecosystem (ipykernel, jupyter_client, traitlets, etc, so I doubt there'll be an effort from core dev to push that forward.
THough if you have issues when files _exists_ in XDG placed and not found I""ll be happy to get fixes in.",Bitter Frustration
"@Carreau You're being a bit facetious. There's no need to update ""all the code and documentation all over the internet"", nobody has every done that and never will. Neither would the updating even be necessary, the official documentation should contain the location of the configuration file, that's it. The third silly thing is that you're acting like IPython doesn't have any changes that break some pieces of code or tutorials out there. All in all it makes your comment seem more like irrational stubbornness than actual concerns.
Not to mention, if it's _that_ hard to change the folder, then that hardcoding seems like a massive code smell anyways and should be fixed anyways, and the choice to set the location of the folder should be up to the user.",Bitter Frustration
"Hey, I was there for the first or changing default configuration directory, and all the issues it created; And I'm one of the person who has to deal both with public and private requests about the bits of documentation that users have tried on the internet and does not work because its inaccurate, either when it was written or because IPython changed, so yes I'm well aware of the consequences of changing. And so yes the official documentation is one of the last place users looks for that information, and yes sometime they look at the latest docs for 5 years old IPython version.
Yes `~/.ipython` is not optimal, and no it's not a code smell we removed all the complex logic because it was brittle and was super confusing, and because every time we were teaching software carpentry there was section with all the combinaisons of linux/mac/windows IPython version x.y.z
So yes I was of your opinion 10 years ago, and no to this days I still don't know the 3 path on each of the platform, even on Mac which I'm using everyday and which now hides ~/Library in the finder.
You can change the code location by setting environment variables, and I told you that if it was not working with configuration in xdg dir I would gladly accept patches for that so that you have the choice. Now if it's to criticize the code without proper understanding of the technical and social reason of why it is the way it is, and bring XDG zealotry into the mix, I'm even less likely to make an effort.
PR still accepted but discussion closed.",Bitter Frustration
"Saw there are places where the terms blacklist/white list are used. Consider replacing these. Didn't do an exhaustive search of other terms that could be offensive, but see https://twitter.com/TwitterEng/status/1278733305190342656",
"I agree, words matter.
How about contributing a pull request to make the changes?",
"There is not one place where the term blacklist was used in this project code! Only once in a comment (how to change the comment???). Also a quick search for the term ""sanity"", ""dummy"" and so on did not result in any code line affected. Yeah sure... rename your ""master"" branch to ""primary"" or so... The lazyness from OP stinks as it is easier to . Close this thread, as this is not stopping supposed racism but some person feeling better by yakshaving instead of actually solving issues.",Bitter Frustration
https://github.com/sinonjs/nise/pull/167/commits/bbfc55a9a5e6061cb8234cd65b7983a7713813c1 The yak is mostly shaved now. But your vitriol is misplaced; much like my original suggestion may have been ignorantly misplaced not knowing how the code was divided in sub-projects. There may be other subprojects that could be improved.,Bitter Frustration
"Nice, so you can close this Issue as there was no issue.",Mocking
@Uzlopak Your comments are not constructive and the tone is not very respectful. Please read our [Contributor Covenant Code of Conduct](https://github.com/sinonjs/sinon/blob/master/CODE_OF_CONDUCT.md).,Insulting
"@mantoni Consider my post as freedom of speech under Article 10 of the European Convention on Human Rights, which overrides your CoC (Stichwort: mittelbare Drittwirkung von Grundrechten). If you disagree:
When will you rename the master-branch to primary-branch?",Entitlement
"This is how you can rename master to anything you want, which ""improves language to respect less privileged populations""
https://stackoverflow.com/questions/8762601/how-do-i-rename-my-git-master-branch-to-release",Irony
@rgeerts Maybe renaming the master branch is something you should also tell everyone in their projects?,Impatience
@rgeerts thank you for your contribution ðŸ’¯,
I also thank rgeerts for his extensive work on this project.,
"Thank you for raising this issue.
In our opinion, in the context of software repositories, ""master"" takes on the meaning of [""an original version of something from which copies can be made""](https://dictionary.cambridge.org/dictionary/english/master).
We are choosing to follow git conventions. When those conventions change, we will make appropriate changes to the Sinon repositories.",
"Saw there are places where the term master is used. Consider replacing these.
Didn't do an exhaustive search of other terms that could be offensive, but see https://twitter.com/TwitterEng/status/1278733305190342656
also see:
https://stackoverflow.com/questions/8762601/how-do-i-rename-my-git-master-branch-to-release",
"Hello!
Umm, we don't really care for icons and web animations... Instead, when are you going to add REAL support for streaming?? Meaning, Dolby ATMOS, DTS:X, Dolby Vision, etc?
Microsoft beat you to it, with their new Edge browser. And to be honest, I love it MORE than Chrome now!! It's awesome!!!!!!!!!!!!!!!!!!!!
Oh, and not to forget: give us the option to switch internally between the Java Script Player and HTML5 Player, for streaming as well? Hmm? When?
I am dead serious, WHEN??? You are not adding/doing anything useful to it, nothing beneficial for the end user.
Please DO actual relevant and useful stuff for/with it! I'm almost 40, and I bet I'm gonna die BEFORE you even get the idea to implement all of this!!! Pfffff................ðŸ˜­ðŸ˜­ðŸ˜­ðŸ’”ðŸ’”",Bitter Frustration
"No reason to open a duplicate. You will need to provide more info if you want to see this fixed. I also suggest toning down your language a bit, most people working on Rust do this in their free time, and it isn't exactly motivating to have people complain like that.",Bitter Frustration
"""most people working on Rust do this in their free time""
And again that crap: ""I'm doing it for free so don't expect any quality from it.""
I don't care if you fix it or not. I tell you what. Leave it and don't fix it. It is not my soft... I don't care...",Bitter Frustration
https://github.com/rust-lang/rust/issues/33674<< Still happening in 2020!!!,Bitter Frustration
"@storeilly, @kraney, @maallyn I updated the code on the feature_singlemixserver branch with the following functionality:
- The master ""director"" client defines the mode, i.e. if mono/stereo and the audio quality.
- The slave clients have to use the same mono/stereo and audio quality settings as the director, otherwise their client will show ""TRYING TO CONNECT"" all the time until it automatically disconnects after a while.
- Only the master ""director"" sees the full audio mixer board faders and can control the audio mix. All slave clients will see an empty audio mixer board.
It would be great to get feedback from you:
- Is the source code stable?
- Does it work according to the above specification?
- How many clients does a fast server PC can serve in that mode (I would recommend to use Mono mode to get the most number of clients)?",
"Will each client be able to balance their own input in the mix?, if not I can imagine one of two scenarios. 1 some singers will turn their gain too high to hear themselves against the mix and the director turning their fader back down to compensate ending with the singer clipping.
2 some singers turning their gain too low to reduce themselves and the director doing the reverse. I'm not convinced this use case will be popular or workable sorry",Bitter Frustration
"I really like this feature since it has a huge potential. If really only the ""master client"" can do the mix I don't see any problem @storeilly?
What I see as a problem is that the clients just see ""TRYING TO CONNECT"". I'd prefer (at least for newer clients) to show ""Please set the audio quality/... as follows: [...]"" or even a server message which sets them automatically. If you don't do that it would probably rise quite some support questions and problems.",Impatience
"I don't like it. Sorry. Consider human nature and the fact that you are
removing an element of control from them. If they can't adjust their own
perception of their volume in the mix without affecting how others hear it,
they will compensate and use the control that they have which does. They
will sing too loud or too quiet (depending on personality type). Any
attempt by an outside force (the director) to balance the mix will force
them to increase the compensation to detrimental levels.
On Sat 19 Sep 2020, 13:16 ann0see, <notifications@github.com> wrote:
> I really like this feature. If really only the ""master client"" can do the
> mix I don't see any problem @storeilly <https://github.com/storeilly>?
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/corrados/jamulus/issues/599#issuecomment-695206116>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABIJSQLRZPBGA6P3VVZFURLSGSOI7ANCNFSM4RS3AX3Q>
> .
>",Bitter Frustration
"Is there any way what's done could be tuned to having each participant (apart from the director) see only their own channel control? This wouldn't save server side processing - you'd still have n mixes to create. But it retains simplicity in the UI whilst preserving control.
The levels would be set by the director, apart from a participant's own level, which they could set for themselves. Perhaps it would still need a ""balance"" so you could raise your own level against the rest of the mix, or in this mode, your own slider becomes ""balance"" -- at the bottom, you only hear everyone else, at the top, you only hear yourself, in the middle you hear the director's mix.
(I'd also think a ""list participants"" option would be needed. Otherwise I'd be wondering ""who's here?"" all the time.)
----
Of course, my other question is -- how does this affect the jam recorder? Is the emit AudioFrame still done before the mix? This seems like it would answer one of my questions on the thread for recording a pre-mixed file - the director's mix is what you'd record. (Though I'd still advise a new signal/slot, it needs the filename handled properly and the file not included in the projects.)",
"To me the scenario that leads to clipping sounds pretty passive-aggressive and like it could best be resolved by the director mentioning out loud that heâ€™s having to dial down (or up) someoneâ€™s volume. It doesnâ€™t seem like an indictment of the entire feature.
I havenâ€™t had a chance to try it yet for time zone reasons but I should be able to run it during this weekend.
I have a little concern with the â€œfirst to connectâ€ solution for selecting a director only because, supposing it goes wrong and someone accidentally joins before the director, how can it be resolved? Everyone has to leave and then wait for confirmation by the director via some other channel before rejoining? However I understand the desire to minimize the disruption to the code.
One other idea for selection comes to mind - maybe the director is the first one to join who chooses a baton as his instrument?",
"> If they can't adjust their own perception of their volume in the mix without affecting how others hear it, they will compensate and use the control that they have which does. They will sing too loud or too quiet (depending on personality type). Any attempt by an outside force (the director) to balance the mix will force them to increase the compensation to detrimental levels.
That may or may not be. I think this strongly depends on how the singers can adapt to the mix. For some it may be simple to adapt, for some it may not be possible to sing. But this can only be evaluated on a real test with a lot of singers, I guess.
> I'd prefer (at least for newer clients) to show ""Please set the audio quality/... as follows: [...]"" or even a server message which sets them automatically.
Sure, a lot things could be done. The current code implements the basic functionality. If it turns out that this mode is useless, all additional effort would be wasted. So the first step is to verify that this mode works with a lot of singers (which are all instructed before entering the server what they have to set in the Jamulus client). If that was successful, more time could be spent to improve that server mode.
> Is there any way what's done could be tuned to having each participant (apart from the director) see only their own channel control? This wouldn't save server side processing - you'd still have n mixes to create.
Yes, exactly. If you want any individual mix, you have to use the normal mode.
> I'd also think a ""list participants"" option would be needed. Otherwise I'd be wondering ""who's here?"" all the time.
The server could send a Chat message when a new client enters the server. But as I wrote above, before any more time is spent, it has to be verified that this new mode works in a real scenario and turns out to be useful.
> Of course, my other question is -- how does this affect the jam recorder?
The jam recorder is not affected. It works the same as in the normal server mode.
> I have a little concern with the â€œfirst to connectâ€ solution for selecting a director only because, supposing it goes wrong and someone accidentally joins before the director, how can it be resolved?
No problem. You can simply swap the places at the server. The one who currently has control leaves the server. Then the director leaves the server and immediately re-connects to the server. Now the director has the control.",
"The swap sounds simple for a small group, but really problematic with a group of 60 people who are all joining a rehearsal that is starting, each according to his own timing.
But I do want to say I really appreciate the very fast turnaround on this, and the approach to try with the simplest change first then refine once the principle is demonstrated.",Bitter Frustration
"I ran a test using the current master first to serve as a baseline. For my test I connect once using a normal client I can listen to, then add a bunch of headless clients that are just silent and produce no audio out. (Connected to a jack dummy audio.) Because of this I can't be 100% sure that they work equivalent to a normal client. However I did check that if one of those is ""master"", I can connect successfully with a normal client and get an empty mixer, but hear audio. I'm using 2vCPUs on a cloud server. Intel Skylake architecture I believe.
In this test I was able to connect 62 clients, but the last few didn't go smoothly. The user name took a long time to appear after the channel appeared. The sound quality degraded somewhat. CPU usage was around 75-80% out of 100 on a two core system. That is, about 160% usage out of 200 in the linux way of reporting it.
I then ran a test using singlemix. With this, I was able to connect 80 clients with no immediate issues. Sound quality was good. CPU usage was around 46% out of 100, or about 92% / 200 in the linux way of reporting it.
However, it only stayed stable for about a minute. After time passed, without any new clients, it shifted into a very degraded mode. Sound came in short, regular choppy bursts, about half on and half off, like 120 bpm pulses. CPU usage dropped to about 13%. Stopping the synthetic clients does not immediately resolve it, but it does go back to normal once the server times the clients out of its list.
This was in normal quality, 128 byte buffer, Mono.",
"There's one longer term use case I'd like to support that might suggest some requirements for this mode. I'm looking into the ability to re-transmit the Jamulus audio into a YouTube Live or Facebook Live stream. It seems pretty straightforward to do this using SoundFlower on a mac so you can capture what is coming from Jamulus.
In such a situation, the director might likely want to send a click track to the band members to control the tempo and dynamics. So it would be useful to have a second mix not tied to the director one, where that one click track can be muted for the audio that will be retransmitted.",
"Folks:
I just did a compile of the feature-singlemixserver pulled as of about 10 PM Pacific U.S. Time on Monday, Sept 21, 2020. It is on my server newark-music.allyn.com, which is a four CPU dedicated server at Linode in Newark, N.J. I did confirm that only the first connect has the sliders and others dont.
However, I did notice the following warnings on the compile output:
=====================================================================
usr/include/x86_64-linux-gnu/qt5/QtWidgets -isystem /usr/include/x86_64-linux-gnu/qt5/QtGui -isystem /usr/include/x86_64-linux-gnu/qt5/QtNetwork -isystem /usr/include/x86_64-linux-gnu/qt5/QtXml -isystem /usr/include/x86_64-linux-gnu/qt5/QtConcurrent -isystem /usr/include/x86_64-linux-gnu/qt5/QtCore -I. -I. -I/usr/lib/x86_64-linux-gnu/qt5/mkspecs/linux-g++-64 -o clientdlg.o src/clientdlg.cpp
src/clientdlg.cpp:738:66: warning: unused parameter â€˜strVersionâ€™ [-Wunused-parameter]
QString strVersion )
^
src/clientdlg.cpp:751:68: warning: unused parameter â€˜strVersionâ€™ [-Wunused-parameter]
QString strVersion )
^
g++ -c -m64 -pipe -O2 -std=c++0x -D_REENTRANT -Wall -W -fPIC -DAPP_VERSION=\""3.5.10git\"" -DOPUS_BUILD -DUSE_ALLOCA -DCUSTOM_MODES -D_REENTRANT -DHAVE_LRINTF -DHAVE_STDINT_H -DQT_NO_DEBUG -DQT_WIDGETS_LIB -DQT_GUI_LIB -DQT_NETWORK_LIB -DQT_XML_LIB -DQT_CONCURRENT_LIB -DQT_CORE_LIB -I. -Isrc -Ilibs/opus/include -Ilibs/opus/celt -Ilibs/opus/silk -Ilibs/opus/silk/float -Ilibs/opus/silk/fixed -isystem /usr/include/x86_64-linux-gnu/qt5 -isystem /usr/include/x86_64-linux-gnu/qt5/QtWidgets -isystem /usr/include/x86_64-linux-gnu/qt5/QtGui -isystem /usr/include/x86_64-linux-gnu/qt5/QtNetwork -isystem /usr/include/x86_64-linux-gnu/qt5/QtXml -isystem /usr/include/x86_64-linux-gnu/qt5/QtConcurrent -isystem /usr/include/x86_64-linux-gnu/qt5/QtCore -I. -I. -I/usr/lib/x86_64-linux-gnu/qt5/mkspecs/linux-g++-64 -o serverdlg.o src/serverdlg.cpp
src/serverdlg.cpp:647:68: warning: unused parameter â€˜strVersionâ€™ [-Wunused-parameter]
QString strVersion )
^
g++ -c -m64 -pipe -O2 -std=c++0x -D_REENTRANT -Wall -W -fPIC -DAPP_VERSION=\""3.5.10git\"" -DOPUS_BUILD -DUSE_ALLOCA -DCUSTOM_MODES -D_REENTRANT -DHAVE_LRINTF -DHAVE_STDINT_H -DQT_NO_DEBUG -DQT_WIDGETS_LIB -DQT_GUI_LIB -DQT_
==============================================================
The chat message does show the git log as well as a warning that this server will have faders only on the first connection.
This server is on the central server listing for All Genres.
Mark",
"I spent some time poking around with the audio mix code and I've found some changes that improve the maximum scale even without resorting to the singlemix option.
Specifically
* There is a 1ms or 2ms budget for completion of the mix code (depending on buffer size) before the next timer already fires before it finishes. This limits the number of mixes operations you can do in a thread. The number of mix options per channel grows, therefore the number of mix channels per block must shrink, as the number of channels grows.
* The Qt default thread pool defaults to ""optimal"" which is one thread per CPU core. So on a 4 core system, you get 4 threads no matter how many the mix code tries to launch. This puts a ceiling on scale unless it's changed. About 2 threads per core seems to be the limit I was able to use before you exceed the time budget anyway due to contention.
With tweaks to those items, I have been able to scale to 65 channels on an 8-core system with no notable audio issues; none of the quirkiness I was noticing in prior tests. That's with
* Mono-in / Stereo-out
* 128 buffers
(all clients)
and uses 345% CPU.
Note that I only seem to be able to use about 40% of each CPU core on this system before I exceed the time budget for mixing. It might actually be possible to improve the results by doing the mix for half of the channels at a 0.5ms time offset from the other half, at least for this CPU architecture. That is, don't roll all of the threads into the same FutureSynchronizer.
I also tried adding an indexed lookup for channels by hostname in hopes of making PutAudioData more efficient, but I don't believe it made a significant difference. PutAudioData is not the bottleneck.
```
diff --git a/src/server.cpp b/src/server.cpp
index bbdcd949..4e4ab03e 100755
--- a/src/server.cpp
+++ b/src/server.cpp
@@ -427,6 +427,8 @@ CServer::CServer ( const int iNewMaxNumChan,
vecChannels[i].SetEnable ( true );
}
+ QThreadPool::globalInstance()->setMaxThreadCount(QThread::idealThreadCount()*4);
+
// Connections -------------------------------------------------------------
// connect timer timeout signal
@@ -694,6 +696,7 @@ void CServer::OnCLDisconnection ( CHostAddress InetAddr )
if ( iCurChanID != INVALID_CHANNEL_ID )
{
vecChannels[iCurChanID].Disconnect();
+ hashChannelIndex.remove(InetAddr);
}
}
@@ -1042,8 +1045,11 @@ static CTimingMeas JitterMeas ( 1000, ""test2.dat"" ); JitterMeas.Measure(); // TE
// processing with multithreading
if ( bUseMultithreading )
{
-// TODO optimization of the MTBlockSize value
- const int iMTBlockSize = 20; // every 20 users a new thread is created
+ QFutureSynchronizer<void> FutureSynchronizer;
+ // Each thread must complete within the 1 or 2ms time budget for the timer.
+ const int iMaximumMixOpsInTimeBudget = 500; // Approximate limit as observed on GCP e2-standard instance
+ // TODO - determine at startup by running a small benchmark
+ const int iMTBlockSize = iMaximumMixOpsInTimeBudget / iNumClients; // number of ops = block size * total number of clients
const int iNumBlocks = static_cast<int> ( std::ceil ( static_cast<double> ( iNumClients ) / iMTBlockSize ) );
for ( int iBlockCnt = 0; iBlockCnt < iNumBlocks; iBlockCnt++ )
@@ -1065,7 +1071,6 @@ static CTimingMeas JitterMeas ( 1000, ""test2.dat"" ); JitterMeas.Measure(); // TE
// make sure all concurrent run threads have finished when we leave this function
FutureSynchronizer.waitForFinished();
- FutureSynchronizer.clearFutures();
}
}
else
@@ -1480,21 +1485,8 @@ int CServer::GetNumberOfConnectedClients()
int CServer::FindChannel ( const CHostAddress& CheckAddr )
{
- CHostAddress InetAddr;
-
- // check for all possible channels if IP is already in use
- for ( int i = 0; i < iMaxNumChannels; i++ )
- {
- // the ""GetAddress"" gives a valid address and returns true if the
- // channel is connected
- if ( vecChannels[i].GetAddress ( InetAddr ) )
- {
- // IP found, return channel number
- if ( InetAddr == CheckAddr )
- {
- return i;
- }
- }
+ if (hashChannelIndex.contains(CheckAddr)) {
+ return hashChannelIndex[CheckAddr];
}
// IP not found, return invalid ID
@@ -1592,6 +1584,8 @@ bool CServer::PutAudioData ( const CVector<uint8_t>& vecbyRecBuf,
{
// in case we have a new connection return this information
bNewConnection = true;
+ // also remember in the index
+ hashChannelIndex[HostAdr] = iCurChanID;
}
}
diff --git a/src/server.h b/src/server.h
index 15644c8f..acf96ab0 100755
--- a/src/server.h
+++ b/src/server.h
@@ -324,7 +324,6 @@ protected:
// variables needed for multithreading support
bool bUseMultithreading;
- QFutureSynchronizer<void> FutureSynchronizer;
bool CreateLevelsForAllConChannels ( const int iNumClients,
const CVector<int>& vecNumAudioChannels,
@@ -334,6 +333,7 @@ protected:
// do not use the vector class since CChannel does not have appropriate
// copy constructor/operator
CChannel vecChannels[MAX_NUM_CHANNELS];
+ QHash<CHostAddress,int> hashChannelIndex;
int iMaxNumChannels;
CProtocol ConnLessProtocol;
QMutex Mutex;
diff --git a/src/util.h b/src/util.h
index a8d7b3b3..88ee1117 100755
--- a/src/util.h
+++ b/src/util.h
@@ -834,6 +834,10 @@ public:
quint16 iPort;
};
+inline uint qHash(const CHostAddress& adr, uint seed) {
+ return qHash(adr.InetAddr, seed) + qHash(adr.iPort, seed);
+}
+
// Instrument picture data base ------------------------------------------------
// this is a pure static class
```",
"Better yet, donâ€™t use a FutureSynchronizer at all and have a separate timer loop per channel block. Then
you might not need to expand the thread pool. Slightly tricky to adjust the block size in that case though.",
"Sounds interesting @kraney. I also consider the OnTimer() triggering is critical to the process (https://github.com/corrados/jamulus/issues/455#issuecomment-692926704), as it occurs asynchronous to the audio processing stuff, and I think timer exhaustion (ie decompress+mix+compress loop not finished before next timer triggers) should be logged somehow to get a sense if the system isn't introducing audio artifacts due to overload. Did you implement any probe to detect that case?
Anyways, looking at Qt docs (https://doc.qt.io/qtforpython/PySide2/QtCore/QTimer.html#accuracy-and-timer-resolution) I'm unsure how Jamulus reacts if OnTimer retriggers while still active in the processing loop due to overload.
`All timer types may time out later than expected if the system is busy or unable to provide the requested accuracy. In such a case of timeout overrun, Qt will emit timeout() only once, even if multiple timeouts have expired, and then will resume the original interval.`",
"> With tweaks to those items, I have been able to scale to 65 channels on an 8-core system with no notable audio issues; none of the quirkiness I was noticing in prior tests. That's with Mono-in / Stereo-out,128 buffers.
That is interesting. With your settings (stereo, 128 samples) we already had a report of supporting 100 clients, see https://github.com/corrados/jamulus/issues/455#issuecomment-685692054. The question is what was different when brynalf did his test. Certainly, his CPU is faster but 65 vs 100 clients is a big difference.
> Better yet, donâ€™t use a FutureSynchronizer at all and have a separate timer loop per channel block.
I would say that this is no practical way. pljones wrote a nice picture about the threading situation: https://github.com/corrados/jamulus/issues/455#issuecomment-670172250. So we have one part which decodes all OPUS packets and then we have multiple threads which work on that data. If you have different timers, how to deal with the common part at the beginning since each client needs the decoded audio data from all other clients.",
"Looking at #455, brynalf is on a 32-core CPU which would hide the main things I ran into with a smaller number of cores. idealThreadCount would be 32 already, so it would probably not be a limiting factor. I could run with more cores but it's not really practical for a cloud-hosted instance; it would get rather expensive. And really, the capabilities of the 8-core instance I was using are greatly underutilized.
For the common decoded audio data - I would say it seems like it would make sense to preserve the decoded data in a circular buffer. Each independent OnTimer would ""top it up"" with the latest new data and expire an equal amount of the oldest data, then work on the result.
It looks like #455 would be the more appropriate place to continue the discussion - sorry to hijack this ticket.",
"Or another thought, why preserve the incoming encoded packets at all? Why not decode them on the way in, and store only the decoded data?",
It would help if at the very least the next decode task could start before the mix tasks from the prior timer have completed. That would let CPU utilization get closer to 100% per core.,Impatience
"See here:
- https://github.com/corrados/jamulus/issues/547#issuecomment-692877664
- https://github.com/corrados/jamulus/issues/547#issuecomment-695171942
- https://github.com/corrados/jamulus/pull/535#issuecomment-679365410
A simple approach with only little changes in the Jamulus server code is wanted. Specification:
Adding a new command line argument to the server like --singlemix:
- No multithreading (since we only have one encoding and mixing so no multithreading needed)
- Only 128 sample frame size support
- Only Mono support (gives us the most possible number of connected clients which is what this modification is all about)
- The first connected client on that server is the ""director"", all other clients which connect afterwards get his mix. So you just have to make sure that the director is already connecting to the server before your session begins (this requirement should be very easily to be fulfilled).
There is a vecvecbyCodedData buffer which is used for both, encoding and decoding. I'll introduce a separate buffer so that I can re-use the output buffer of the first client for all other clients. So instead of calling MixEncodeTransmitData for all the other clients, they simply get vecChannels[iCurChanID].PrepAndSendPacket ( &Socket, vecvecbyCodedData[iDirectorID], iCeltNumCodedBytes );. I just did a quick hack: If I modify CreateChannelList so that no client is added, the audio mixer panel is just empty. This would be the case for the slave clients. But then they do not see how many clients are currently connected which is not a big issue. If ""--singlemix"" is given, ""-F"" and ""-T"" is deactivated and a warning is shown that these cannot be combined. In the OnNetTranspPropsReceived function we can check that the client uses 128 samples, if not, refuse to connect.
There is a new branch were the implementation is done:
https://github.com/corrados/jamulus/tree/feature_singlemixserver",
"Question on the implementation for this: After the Leader has joined the server, do all other users see no faders, or faders that are inactive, or what? I think they shouldn't see faders that appear to be active, but should still see Profile Names of connected users.",
"To @storeilly's concerns above, I still think there's huge value for choral groups in this feature. Imagine an in-person rehearsal:
- Everyone's standing close
- Everyone's likely unamplified
- Everyone's listening to everyone else
- Everyone's balancing their output (singing volume) to fit in the group sound
- The Leader is telling anyone out of balance to adjust
There's no individual mixer controls - everyone hears the group sound ""unmixed"", then adjusts their individual output to fit. The Jamulus equivalent would be to send unmixed sound to all users (or to ignore their fader settings), then have individuals adjust their gain + singing volume to compensate, which this addresses.
That doesn't solve for the user who says ""I can't hear myself well enough"" or ""The group isn't loud enough"", but that's not a problem that Jamulus can solve. Jamulus' faders are not a control for System Volume - they're a control for ""Jamulus within the bounds of System Volume"". So if System Volume = 50%, then setting a Jamulus fader to 50% is equivalent to setting System Volume = 25%, and setting a Jamulus fader to 100% is equivalent to setting System Volume to 50%. If the user isn't getting enough volume from a ""flat"" mix, they need to increase their System Volume (which could be PC sound card or audio interface output), add an in-line mini amp to headphones, etc.",
"> There's no individual mixer controls - everyone hears the group sound ""unmixed"", then adjusts their individual output to fit.
Isn't that partially self-contradictory? Each individual has their own mix: it's what reaches their ears, just like in Jamulus. They can control their own input level - no one else has control over that, just like in Jamulus.
> ignore their fader settings
That's right. And it's generally how I play. I expect people in the group to use their ears and adjust their input levels so that the group is hearing a good mix.
It's strange the difference in etiquette between NINJAM and Jamulus. In NINJAM, it's very much seen as ""your problem"" if you're too loud or quiet. You're affecting _everyone_ else in the group, so it's down to you to fix it. In Jamulus, there's much more acceptance that everyone has control over what they hear and is expected to handle whatever someone else does -- even though it's affecting everyone in the group.",
"> They can control their own input level - no one else has control over that, just like in Jamulus.
Presumably you mean their own *output* level, i.e. singing volume?",Irony
"No, I mean their input level in terms of Jamulus, i.e. their singing volume.",
"OK, I see - it's an input to Jamulus, but an output from the person :-)",Irony
"Given what's been said in this thread I do indeed think that just forcing all joiners to have no control over their mix opens a can of worms on compensating on how well you hear yourself in a mix.
I imagine it would be more complicated, but an interesting feature would be to voluntarily delegate your fader mix to another user - e.g. if you can identify a ""director"" just check a box to follow that person's mix, rather than set your own. Could it be possible to toggle between the option this feature sets up, and regular Jamulus behaviour?",
That's exactly what I was going to suggest - delegation sounds like a much more flexible design to me.,
"It's also one (or more) less mixes to calculate on the server for every user that activates delegation, if I understand the jamulus workings correctly. So a few individual mixes + a majority delegated would be good. Notably if you have a choral group with a few instrumentalists. The instruments are very likely to want a different mix than the singers.",
"Yeah - this would also open up possibilities for mix engineers working on ""stage"" mixes and ""front of house"" mixes for live streams.",
"I was happy to find that branch (feature_singlemixserver). For me, it is working quite well. We are a choir with about 40 members and are going to introduce Jamulus in the next weeks. The single mixing possibility is for us the best solution and in my opinion only solution. But I ran in the following problem:
Server: running feature_singlemixserver branch (headless) = 3.5.10git
Client1 = mixing client: Mac with Jamulus 3.6.0/3.6.2
Client2: PC with Linux Jamulus 3.6.2
Client3: Raspberry with JackTrip image running Jamulus 3.5.12git
If Client1 (mixing client) runs version 3.6.0, Client3 plays audio (I can hear Client 1 + 2 + 3)
If Client1 runs version 3.6.2, Client 3 doesn't play audio any more, but on Client 1 + 2, I can hear audio from Client3
Was there a change in the audio protocol, which makes Version 3.6.2 somehow incompatible with 3.5.12 (but only for receiving signals)?
Would it make sense to rebase feature_singlemixserver to the maser branch - is it even possible?
I'm concerned sticking with this branch on an older Jamulus version getting the dependency to use the same (old) version for client + server.",
"> Was there a change in the audio protocol, which makes Version 3.6.2 somehow incompatible with 3.5.12 (but only for receiving signals)?
No, there wasn't. I don't see a reason why the 3.5.12 server should be incompatible to 3.6.2. There are way older Jamulus servers still in use, see, e.g., https://explorer.jamulus.io. There you can see the versions of the servers.
Maybe there is some other issue in your setup which prevents this problematic client not to have any audio.",
"That was my hope, but I didn't find an issue in my setup.
The strange thing is, if Client3 connects to the idle server, then I hear myself, but if I'm the second or later, I don't hear anything.
The special thing with this branch is that the first client becomes the ""director"", which defines the mode (mono/stereo and the audio quality). If Client3 is in ""director"" mode, audio is present, otherwise not.
If the ""director"" is a 3.6.0 client (Client1), Client3 audio is received, in case of 3.6.2 no audio at Client3.
I got the same issue with 3.6.0 when ""director"" mode settings are different to Client2/3. settings.
Maybe that additional info helps ...
Do you need any additional info's? Is there a possibility to verify the mode settings (is it sent over the protocol)? Debug via wireshark?",
"> The special thing with this branch is that the first client becomes the ""director"", which defines the mode (mono/stereo and the audio quality)
Exactly. I assume that is your problem. All clients must use exactly the same settings (mono/stereo mode and quality), otherwise you will not hear any sound.",Bitter Frustration
"That is, what I try to explain, same hardware setup, same client config, the only difference: if the ""director"" is version 3.6.0, Client3 works as expected, if version 3.6.2, then I receive no audio on Client3 :-(
Is it possible to enable some ""detailed"" logging to see, which mode is transferred to the server?
I'd like to dig into more detail, but maybe you can give me some hints. Thanks.",Impatience
I think I know what the issue is. In 3.6.0 we introduced a packet counter (#619) which changes the length of the network packet. So it seems you have two choices: Either all of your clients are < 3.6.0 or all your clients must be >= 3.6.0. Why are you using an old version on the Raspberry Pi?,Impatience
"Thanks you so much for your recommendation!
> Either all of your clients are < 3.6.0 or all your clients must be >= 3.6.0. As this branch is based on 3.5.10, I have to use clients < 3.6.0.
Or do you see a chance to rebase this branch to the master?
> Why are you using an old version on the Raspberry Pi?
I use the Raspberry JackTripFoundation-Image and they offer a really easy web interface to handle the login. I can register my (private) server and invite all users. Only the mic gain and output level can be adjusted, no much possibility for mistakes...
Or do you know another easy to use Jamulus web gui for the RasPi?",Impatience
"Hello and thanks for Jamulus in general!
We are ""ramping up"" Jamulus in our 23-people choire, too. We had a test run with 5 clients today, and already experienced vast differences in levels (& delays).
I like the ""delegation"" idea a lot, should that become a separate ticket? In my dreams, this would include downstreaming the slider positions, so I can derive my personal from the conductor's mix. (edited:) I just found that is just another step from there:
#756 > Either all of your clients are < 3.6.0 or all your clients must be >= 3.6.0.
All clients including or excluding the serverinstance?",
"> Or do you see a chance to rebase this branch to the master?
I have just merge the latest code into the feature_singlemixserver. Can you please test if it now works fine for you?",Impatience
"I've just tested singlemixserver branch after merge with master and it works the following way:
1) If ""director"" client is <= 3.6.0, all other clients need to have versions <= 3.6.0
2) if ""director"" client is > 3.6.0, all other clients need to have version > 3.6.0
This behavior is independent of the version of the server instance, works with server version 3.5.10 and 3.6.2.
Is there a way, to overcome this restriction in the singlemixserver branch, because this does not apply to master branch?",Impatience
"Hi fellow musicians!
I created a [new rudimentary solution](https://github.com/jamulussoftware/jamulus/compare/master...cwerling:master) to the `--singlemix` mode cherry-picking a few of Volker's original commits as well as some of my own changes.
* The **first joiner** to the server will be the master and will control **everybody's gain and pan**
* Everybody's gain and pan will be the master's **except for their own channel which is muted**
* Everybody will see the usual controls still (in order to see the connected clients)
* All the buttons (except from ""GRP"" assignments) are useless for the non-masters
* This fork is based on the latest master (as of writing)
I implemented this for our mixed choir of around 25 people. We start our weekly rehearsals with a soundcheck where previously everybody had to create a good mix (which is hard and time-consuming). Now our conductor can do it for all of us which should improve the sound quality a lot. It's paramount to us that we don't hear Jamulus monitoring. Most of us don't use either no monitoring or local monitoring (from their interface).
Would be glad to have some people test this! You don't need custom clients, just a custom server build.
Next steps to make this feature-proof:
- [x] As a client: Find out that the server uses `--singlemix` and maybe even identify the 'master' client (implicitly through index `0` or explicitly through server broadcast)
- [x] Show any hint in the UI that this server has a central mix (e.g. by disabling all controls except GRP)
- [ ] Test test test
Happy to hear any feedback!",
"Great thing, I'm happy that there are some new ideas and implementations for the chorus branch of Jamulus! I'm using the singlemixserver branch for rehearsals in my chorus since two month - it works great. We are around 40 singers - it works stable and smooth. But in our setup, all participants - except the master - are using a Raspberry PI with a VirtualStudio - image from JackTrip. It has a very basic Web-UI - perfect for singers with basic PC knowledge and for all not to be deviated from the sheet music.
As I'm the one, who controls the mixing console, I'm missing the feature of a ""solo"" or ""PFL"" of the individual channels only for the ""master"", but all others shall still listen themselves. This feature would be really helpful when trying to find a channel, which is noisy or has a bad connection during the rehearsal.
If I press the ""solo"" button in the current implementation, all others also hear the ""solo""-ed channel only, so it is useless in the singlemixserver context.",
"I need your input! I am still not sure what the most ""Jamulus way"" is to tackle the Jamulus-routed monitoring:
**Method 1**: Always mute people's own channel in the single mixes (this is currently implemented on my fork)
_Pro_: Fully disabled Mixer UI, easy to understand
_Con_: Nobody can have Jamulus-routed monitoring
**Method 2**: Let non-masters control at least their own channel in the mix
_Pro_: People can decide if and how loud they wanna hear themselves through Jamulus-routed monitoring
_Con_: Partially disabled mixer board, more complex logic, Method 2 gets even more nasty when you think about this: Who controls the pan/gain of the master herself on the common mix? If the master controls it for everybody, she effectively cannot mute herself (which wouldn't work for our choir's conductor).
But if the client controls the master's gain/pan, the UI gets even more complicated: For non-masters, all controls will be disabled _except for both one's own channel as well as the master's channel_. This is pretty complex for a user to understand imho.
In terms of KISS I would prefer Method 1. Also, in our choir use case nobody relies on Jamulus-routed monitoring.
What do you guys think?",
"The problem is that the ""official"" rule is to listen to the signal from the server. What about adding a Parameter to the single mix Parameter which enables/disables the own signal for all singers?",
"I read in some threads that Jamulus monitoring is a must to be able to guess the delay and to hear my voice with the same delay as all others. Otherwise, I get the impression, that I'm faster than the others if I switch off Jamulus monitoring. It is mentioned as a source of slow down the speed.
That makes sense to me and in my chorus, nobody has the possibility to switch monitoring off. It is a matter of getting accustomed to that kind of reverb and it is only remarkable it you speak or sing alone, in the chorus sound, I cannot recognize it!
My opinion with some experience !",
"> The problem is that the ""official"" rule is to listen to the signal from the server.
Can you elaborate on that one?
> What about adding a Parameter to the single mix Parameter which enables/disables the own signal for all singers?
That's a good extension to **Method 1** I think. We could have `--singlemix` and `--singlemix-nomonitoring` as server flags. The distinction would be client-agnostic.
> I read in some threads that Jamulus monitoring is a must to be able to guess the delay and to hear my voice with the same delay as all others. Otherwise, I get the impression, that I'm faster than the others if I switch off Jamulus monitoring. In my personal (choral singing) experience, this is not an issue at all. Rather, I am barely able to _speak_ a few sentences while hearing myself with the overall delay (of let's say 50ms) without going crazy. :)
But use cases might especially differ for non-vocal instruments and so I don't want to just rule our use case for everybody (i.e. never use Jamulus-routed monitoring).
So as @ann0see suggested: Have `--singlemix` and `--singlemix-nomonitoring`, where the latter disables one's own channel for everybody?",
"Better have `--singlemix ""monitor|no-monitor""`.
For the official rule see the getting started page on jamulus.io and Volkers' paper: https://jamulus.io/PerformingBandRehearsalsontheInternetWithJamulus.pdf",Impatience
"I'm done with the GUI changes and created a (to-be-reviewed) PR here: https://github.com/jamulussoftware/jamulus/pull/1381
Suggest to continue the discussion over there :)",
Locking this issue now so as to keep discussions in fewer places. See also https://github.com/jamulussoftware/jamulus/discussions/1380,
"Hi, Sorry for the maintenance noise here. Iâ€™m just into triaging issues.
Since this issue is locked, and another discussion is linked by gilgongo it doesn't make sense to leave this issue open. Therefore, I'll close it.
Please continue any related discussion in the other discussion.
Note to the topic: See the feature branch: https://github.com/cwerling/jamulus-mastermix/tree/feature_singlemix_reloaded which any interested person can take up if needed. Please open a new issue if you plan to improve it.",
"@marciseli Which version are you using? which errors are you facing? Other than that, have you read the issue template? https://github.com/FreeRADIUS/freeradius-server/blob/master/.github/issue_template.md",Impatience
What an utterly useless comment.,Bitter Frustration
rlm_sql_mongo driver is missing.,Impatience
Increase the log level to 1 or higher. It's expected for level debug to log deprecated calls.,
I had the same problem.,
Debug message -> expected.,Impatience
"### How to use GitHub
* Please use the ðŸ‘ [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.
* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.
* Subscribe to receive notifications on status change and new comments. ### Steps to reproduce
I keep getting the following message in my logs:
```
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	2020-09-25T14:36:30+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	2020-09-25T14:36:29+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	2020-09-25T14:36:29+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	2020-09-25T14:36:29+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	2020-09-25T14:36:29+0200
```
### Server configuration
**Database:**
phpMyAdmin
**PHP version:**
7.3.16
**Nextcloud version:** (see Nextcloud admin page)
20.0.0 RC2
App:
Files sharing
1.12.0",
"Core apps really shouldn't trigger deprecation messages, but get fixed (arguably prior to a release).
Or other way round: if the implementation of files_sharing is considered stable and future proof, NC shouldn't mark it as deprecated.",Bitter Frustration
"@andreas-p could you check if this green button is blocked for you?
![Bildschirmfoto von 2020-11-04 15-57-22](https://user-images.githubusercontent.com/1374172/98127040-876ed180-1eb6-11eb-8321-6f4ad2175135.png)
/s",Irony
"@ChristophWurst No it's not. If you're suggesting I should create a PR for this (officially closed!) issue: For my debugging purposes, I commented out the deprecation message in lib/private/legacy/OC_App.php. files_sharing/appinfo/app.php looks different from what I'm familiar with, so I refrained from digging there.",Bitter Frustration
"As a user my pull request would simply be disabling the message altogether. But that might be useful for developers. Idea is.. if your ""expected debug message"" takes a dump on the UI which is user-facing then your ""feature"" is as useful as this guy's facial tattoo:
![useful facial tattoo](https://i.pinimg.com/736x/11/8f/e6/118fe6cec3fbfc8ead6eea47f7dc15cc--worlds-worst-tattoos-the-ojays.jpg)
Maybe another solution would be to keep deprecation messages away from the UI. Or give the user a way to use filters?
Otherwise this is how much fun you can have as a user trying to debug why the email app chokes on ssl connections:
[much fun - 37MB gif](https://bugout.regex.cf/dumbestdesignaward2020.gif)",Mocking
"> As a user
So as a user you're complaining about a *debug log output*. Seriously, why do you worry about this so much? You enable debug mode for a few secs to gather some info, then you disable it again.
Now tell me: would you rather have a silent log where we, the maintainers, can't help you solve an issue due to the lack of information?",Mocking
"> you're complaining about a _debug log output_
No, actually I'm not complaining about a debug log output, and I've already conceded that deprecation notices might be useful for developers. What I'm specifically stating is that **deprecation notices** at a rate of 3/second is a **useless thing in the UI**.
> Now tell me: would you rather have a silent log where we, the maintainers, can't help you solve an issue due to the lack of information?
I thought deprecation notices are more of a reminder to later tackle the phased out code. Am I wrong?",Bitter Frustration
With the stateless nature of php it's hard to debounce this. Every request will trigger it.,Bitter Frustration
"Okay, understood. Thank you.",
"In particular, if you're using the Logging app with the auto-refresh option checked in the web UI, the requests generated by the auto-refresh itself will trigger this very frequently. I ran into this myself recently and if you turn that off and then tail the actual log file on the command line, you'll see it settles down.",
"TBH I saw no harm in commenting out that line and that's how I managed to get a usable UI. But it's a good tip, thank you.",
FYI: Updated from NC 19.0.5.2 to 20.0.2.2 and loglevel was set from 1 to 0 in config.php during update. Machine went totally crazy having Nextcloud.log rotated (cut and moved at 100 MB size) automatically few times. I guess update isnÂ´t reliable yet...,Bitter Frustration
"I do not want to continue this hate-thread but I fear during development for the cookbook app this issue caused a regression as we were not able to test accordingly. Currently, we are recovering but the main issue can be brought down to this issue.
In fact, I was not able to see the live error logs during testing/debugging the app. So I missed the warning of PHP that a certain set of functions/fields are undefined (typos) and other similar effects. The log was just cluttered with these deprecation warnings (which are good in general). But as these are generated on each invocation (also the update of the logs) this ends in an infinite recursion.
For now, I am able to see some useful information by disabling the `files_sharing` app but this is no permanent solution, especially as we are planning to allow for the sharing of recipes in the app. At most that moment in time, we need to allow the sharing again, which will clutter the logs again.
I tried to create a patch in [this PR](https://github.com/nextcloud/server/pull/24689) but I am not able to get things right, I am missing the knowledge of the internal structure of the NC core.
I am asking @ChristophWurst to reconsider closing this issue as I can only second the general statement/issue here: *A component uses a deprecated method and that should be replaced by a newer structure.* The fact that it is a core component just makes it hard for other contributors to contribute a ""quick fix"".
If this is on the agenda for a near-future update, perfect, just give a short statement and I am going to keep waiting.",Bitter Frustration
@ChristophWurst I am opening an issue to discuss the problem again.,
"It would be nice to get a solution here fast. I just wanted to track some debug messages while continuing development and this situation still exist. The problem is not, that a lot of debugging messages are generated because of this deprecation message, the problem is that debugging is impossible ATM. There are something about 45 deprecations messages **per second**. See the log file snippet in #23351.",Bitter Frustration
So what would you suggest as a solution?,Impatience
"It seems, that the logger itself is responisble for the endless production of deprication warnings in the logfile. If this could be avoided the problem would be smaller and acceptable. As I am not familiar with the core development, I have no real proposal for a solution.",Bitter Frustration
Correction: seems that the files_sharing and contacts are my problem here.,Bitter Frustration
"@dartcafe I mentioned this up-thread; see https://github.com/nextcloud/server/issues/23046#issuecomment-728682934. You can get a vaguely reasonable log output with `tail -f data/nextcloud.log | jq '.time + "" "" + .reqId + "" "" + if (.message | type) == ""object"" then .message.Exception else .message end'` (though you will of course need to install `jq` on your system first). Then the Logging app won't create all the deprecation spam.",
"We have loglevel 1 and the log is spammed with deprecation warnings, 25GB log in the last days.",Bitter Frustration
"Wth are you talking about
On Fri, Jan 22, 2021, 1:36 AM Bjoern Franke <notifications@github.com>
wrote:
> We have loglevel 1 and the log is spammed with deprecation warnings, 25GB
> log in the last days.
>
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/nextcloud/server/issues/23046#issuecomment-765235684>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARULPN57G2EOV52LAFLOQFDS3E2HVANCNFSM4RZQC2IQ>
> .
>",Impatience
"> Wth are you talking about
Some entries above @ChristophWurst says users should not complain about debug output when setting loglevel to `debug`. I changed loglevel 0 to 1 expecting `info` messages, but no deprecation warnings. And the logfile grew from 2GB to 25 GB in 4 days, full of deprecation warnings.",Bitter Frustration
"Great now that we have all people combined that complained about the problem, here's the PR that fixes it: https://github.com/nextcloud/server/pull/25267
you're welcome",Bitter Frustration
"Why aren't repeated duplicate log messages rate limited (the approach the Linux kernel takes is good where if a message is repeated a lot of times in `dmesg` it'll just stop showing it and print a message saying the previous message was repeated X times)?
![Screenshot_2021-01-22 Settings - Nextcloud](https://user-images.githubusercontent.com/3309784/105509550-85eac380-5cc5-11eb-8666-77f5d66a4039.png)",Impatience
@aidanharris read https://github.com/nextcloud/server/issues/23046#issuecomment-728119588 and the response.,Impatience
"Same problem here with alot of reports and logs:
`[previewgenerator] Debug: /appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.`
also
```
[no app in context] Info: Deprecated event type for OCA\Files::loadAdditionalScripts: Symfony\Component\EventDispatcher\GenericEvent is used
GET /index.php/apps/files/
```
like every few seconds those appear",Bitter Frustration
One step closer to a more reasonable debug log: https://github.com/nextcloud/server/pull/25497 :v:,
@ChristophWurst Cool ðŸ‘. I hope it will be much clearer now and will not take up as much space.,
"The logfile flooding still exists in NC22. Wouldn't it be a good idea to remove the deprecation at least for the core apps? I list the following apps in my NC22 installation: * files_sharing
* contacts
* suspicious_login",Impatience
"> files_sharing
Be my guest and help finish what I was unable to finish https://github.com/nextcloud/server/pull/25267 :wink: > contacts
https://github.com/nextcloud/contacts/issues/1831
> suspicious_login
https://github.com/nextcloud/suspicious_login/issues/499",Mocking
"> > files_sharing
> > Be my guest and help finish what I was unable to finish #25267 ðŸ˜‰
I would, if I would have the time and the knowledge. Polls alone takes more time, as I normaly could afford.",Irony
"No worries :v:
We'll have to wait for someone more knowledgeable then because the sharing stuff was also out of my area of expertise and thus got stuck.",
"File sharing is migrated, so closing this.
Other apps can be added here: https://github.com/nextcloud/server/issues/32132",
"I'll be taking my first vacation in awhile during this time, and plan to unplug from computers. As a result, I won't be responding to issues on stackoverflow.com or github.com.",
"## Details
V2 will come together with _VuePress@v2_, with the power of _vite@v2_ _webpack@v5_ and _vue@v3_.
V2 will be a totally rewrite including:
- All the components will be rewrite with _composition api in vue3_
- All the styles will be mirgate to _sass_
Also we are going to make sure the rewrite one has a better performance!ðŸš€ (no less than 25% increasement)
---
Below is a list of changes.
> Note: All plugin names are in PascalCase and without `vuepress-plugin-` prefix, you should change to `kebab-case` and add the prefix yourself to get the correct plugin name.
## AddThisðŸ’Œ ![deprecated](https://img.shields.io/badge/-deprecated-yellow)
We are just making this plugin compatable with V2. This plugin will no longer under maintainment.
### Whyâ”
AdBlockers are blocking addThis link heavily, and in most cases this plugin won't work.
### SolutionðŸ’¡
We will provide a new share plugin in v2.
## Comment2ðŸ’¬
Renamed from `@mr-hope/vuepress-plugin-comment` to `vuepress-plugin-comment2`. âš 
### Changes
- [x] Drop Valine support! ![removed](https://img.shields.io/badge/-removed-red)
Valine is lack of maintainence and can leak your privacy. You should use [Waline](https://waline.js.org) instead.
- [x] Option `hint` (default `true`) for handling page info hints. ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Allow I18N Customize ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Muti categories support ![new](https://img.shields.io/badge/-new-brightgreen)
### Note
Vssue is currently being dropped because vssue has not support v2 yet. We will add it back after it supports v2.
## CopyCode2ðŸ“‹
Renamed from `@mr-hope/vuepress-plugin-copy-code` to `vuepress-plugin-copy-code2`. âš 
### Changes
- [x] Option `delay` (default `500`) for handling the page swtiching transition. ![new](https://img.shields.io/badge/-new-brightgreen)
- [ ] Option `pure` (default `false`) for handling fancy styles. ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Allow I18N Customize ![new](https://img.shields.io/badge/-new-brightgreen)
## Feed2
Renamed from `@mr-hope/vuepress-plugin-feed` to `vuepress-plugin-feed2`. âš 
### Changes
- [x] Better log output. ![improve](https://img.shields.io/badge/-improved-blue)
- [x] Muti categories support ![new](https://img.shields.io/badge/-new-brightgreen)
## MdEnhance
### Changes
- [x] Auto inject cdn links for markdown demo ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Option `delay` (default `500`) for handling the page swtiching transition. ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Drop flowchart support ![removed](https://img.shields.io/badge/-removed-red)
We are adding mermaid support, and mermaid support flowchart.
- [x] Move SlidePage layout from theme to the plugin
This will allow other theme to provided a similar layout easily.
- [x] Allow I18N Title in details block ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Allow I18N Customize ![new](https://img.shields.io/badge/-new-brightgreen)
## LightGallery ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Build a photo preview plugin using [lightGallary](https://www.lightgalleryjs.com/)
## Pwa2
Renamed from `@mr-hope/vuepress-plugin-pwa` to `vuepress-plugin-pwa2`. âš 
### Changes
- [x] Allow I18N Customize ![new](https://img.shields.io/badge/-new-brightgreen)
## PhotoSwipe
### Changes
- [x] Option `delay` (default `500`) for handling the page swtiching transition. ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Allow I18N Customize ![new](https://img.shields.io/badge/-new-brightgreen)
## ReadingTime2
Renamed from `@mr-hope/vuepress-plugin-reading-time` to `vuepress-plugin-reading-time2`. âš 
- [x] Allow I18N Customize ![new](https://img.shields.io/badge/-new-brightgreen)
## SassPalette ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] A new palette handling config and palette across theme and plugins using sass
Options:
```ts
export interface PaletteOptions {
/** identifier for palette */
id: string;
/**
* user config file path, relative to source dir
*
* @default "".vuepress/styles/config.scss""
*/
config?: string;
/**
* default config file path, should be absolute path
*
* This is the file you should use to provide default values with `!default`
*
* @default ""vuepress-plugin-sass-palette/styles/default/config.scss""
*/
defaultConfig?: string;
/**
* user palette file path, relative to source dir
*
* @default "".vuepress/styles/palette.scss""
*/
palette?: string;
/**
* default palette file path, should be absolute path
*
* This is the file you should use to provide default values with `!default`
*
* @default ""vuepress-plugin-sass-palette/styles/default/palette.scss""
*/
defaultPalette?: string;
/**
* user style file path, relative to source dir
*
* @default "".vuepress/styles/palette.scss""
*/
style?: string;
}
```
## SEO2
Renamed from `@mr-hope/vuepress-plugin-seo` to `vuepress-plugin-seo2`. âš 
### Changes
- [x] JSON-LD support ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Better log output. ![improve](https://img.shields.io/badge/-improved-blue)
- [x] Muti categories support ![new](https://img.shields.io/badge/-new-brightgreen)
## Share2 ![new](https://img.shields.io/badge/-new-brightgreen)
- [ ] A new share plugin providing page share.
## SimpleBlog ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Build a own blog plugin to support blog with better performance
## Sitemap2
Renamed from `@mr-hope/vuepress-plugin-sitemap` to `vuepress-plugin-sitemap2`. âš 
### Changes
- [x] Better log output. ![improve](https://img.shields.io/badge/-improved-blue)
- [x] Muti categories support ![new](https://img.shields.io/badge/-new-brightgreen)
## VuepressThemeHope
- [x] Pure Mode in theme and all plugins #438 ![new](https://img.shields.io/badge/-new-brightgreen)
- [ ] Dim non focused area #168 ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Code block theme config #189 ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Muti categories support ![new](https://img.shields.io/badge/-new-brightgreen)
- [x] Drop fullscreen feature, or make it a optional feature (disabled by default). ![removed](https://img.shields.io/badge/-removed-red)
Most visitors do not use this feature.
- [x] Drop themecolor feature, or make it a optional feature (disabled by default). ![removed](https://img.shields.io/badge/-removed-red)
Most visitors do not use this feature.
- [x] Allow I18N Customize ![new](https://img.shields.io/badge/-new-brightgreen)",
Any ideas and feedbacks are welcome. Please make your comments [here in dicussion](https://github.com/vuepress-theme-hope/vuepress-theme-hope/discussions/601). â¤,
Please track https://github.com/orgs/vuepress-theme-hope/projects/1/ to follow the roadmap.,
"> triggering on push to any branch is important to allow testing before PR is made
People can and should test locally, then they can make a draft PR IMO",
"> > > > triggering on push to any branch is important to allow testing before PR is made
> > People can and should test locally, then they can make a draft PR IMO
yes, this is what it does - if you are in other repo, CI results are logged there, not here...",Impatience
"@greg0ire this is also what Symfony does.
doctrine/orm does trigger on PR only (not on push) - but I will submit there a PR shortly ;-)",
"> locally
I mean on their computer, not on their fork.",
"I know ;-), but why not in GH which is the standard for almost every other project now :)",Irony
Closing as I don't see a value in applying this change.,Bitter Frustration
@morozov I am very disappointed as allowing CI to run on push in very and industry standard practice across broad spectrum of open-source projects. Please see https://github.com/doctrine/dbal/pull/4349#discussion_r506721571,Bitter Frustration
"> People can and should test locally, then they can make a draft PR IMO
@greg0ire please explain **SHOULD**, draft PR is definitelly not a good solution for anything that should not be merged",Bitter Frustration
I mean people should attempt to run the test they can run locally before opening a PR.,Impatience
"> > > I mean people should attempt to run the test they can run locally before opening a PR.
Yes! Many of them just create a branch, push, and want to see the CI result. This is perfect for quick and small fixes when local setup migh be very time consuming even for one DB.
So this PR seems perfectly legit to me.",Impatience
"> Many of them just create a branch, push, and want to see the CI result.
Isn't that what draft PRs are for?",Irony
"> draft PR is definitelly not a good solution for anything that should not be merged
obviously no :)",Mocking
Says who? You?,Mocking
"Yes, me! - like verify something etc. No need to create a draft PR and ""~polite~pollute"" the history",Mocking
"You mean pollute.
I don't think there are that many people just creating branches and never opening a PR, that seems unlikely. Besides, draft PRs are great in case you want to get early feedback and avoid wasting time on something that will never get merged. Of course, this works better when you start with documentation and explanations and finish with code rather than doing the opposite.",
"I am saying, that there a cases, when PR is not needed. I think, we can agree, that every change/experiment does not need a push to PR nor needs an attention from others.
To be objective, I have provided links to popular repos - in https://github.com/doctrine/dbal/pull/4349#discussion_r506721571 - that this is ""very common practice"".",
"> I think, we can agree, that every change/experiment does not need a push to PR nor needs an attention from others.
We agree on that. We can also agree that it's a waste to have double the jobs when a maintainer pushes their branch to the origin repository.
> To be objective, I have provided links to popular repos - in #4349 (comment) - that this is ""very common practice"".
That's a bandwagon fallacy. Let's ignore that please.",Mocking
"> We agree on that. We can also agree that it's a waste to have double the jobs when a maintainer pushes their branch to the origin repository.
this should really not be an argument as thanks to GH, GH minutes are cheap/free for open source
is there any other significant drawback except that?
> That's a bandwagon fallacy. Let's ignore that please.
what objective criteria do you honor then?",Impatience
"> what objective criteria do you honor then?
I'm not sure, but I try to ignore fallacies.
> this should really not be an argument as thanks to GH, GH minutes are cheap/free for open source
It is the argument made here nonetheless : https://github.com/doctrine/dbal/pull/4349#discussion_r506718451 discuss that with its author.",
"Anyway, this PR should have been made against https://github.com/doctrine/.github",Impatience
@morozov please review the discussing with @greg0ire here. I find this PR very helpfull as any push runs a CI test which is very helpfull information for developer,
"> > > Anyway, this PR should have been made against https://github.com/doctrine/.github
thanks, done, here https://github.com/doctrine/.github/pull/11",Impatience
"I'm locking this conversation as too heated and not bringing any value to the project. @mvorisek if you don't stop pushing your agenda, harassing maintainers, and creating an unnecessary sense of urgency, you will be banned from this project.",Threat
"<!-- Fill in the relevant information below to help triage your pull request. -->
| Q | A
|------------- | -----------
| Type | improvement
| BC Break | no
| Fixed issues | no
#### Summary
- triggering on push to any branch is important to allow testing before PR is made
- test default branch hourly is important to provided actual build state for the default branch",
"- Please paste the whole log messages here, not parts of ones. It should start with `Initializing LaTeX Workshop`. It is very important to identify problems.
![ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ 2020-02-14 20 41 25](https://user-images.githubusercontent.com/10665499/74528024-87b2ef80-4f6a-11ea-9c5d-4f9feff25524.png)
When all of the above is done, we will reopen this issue.",Impatience
"Latex compiler log
```
ogram Files\MiKTeX\tex/latex/base\latexsym.sty"") (""C:\Program Files\MiKTeX\tex/latex/preprint\fullpage.sty"") (""C:\Program Files\MiKTeX\tex/latex/titlesec\titlesec.sty"") (""C:\Program Files\MiKTeX\tex/latex/marvosym\marvosym.sty"") (""C:\Program Files\MiKTeX\tex/latex/graphics\color.sty"" (""C:\Program Files\MiKTeX\tex/latex/graphics-cfg\color.cfg"") (""C:\Program Files\MiKTeX\tex/latex/graphics-def\pdftex.def"") (""C:\Program Files\MiKTeX\tex/latex/graphics\dvipsnam.def"")) (""C:\Program Files\MiKTeX\tex/latex/tools\verbatim.sty"") (""C:\Program Files\MiKTeX\tex/latex/enumitem\enumitem.sty"") (""C:\Program Files\MiKTeX\tex/latex/hyperref\hyperref.sty"" (""C:\Program Files\MiKTeX\tex/generic/ltxcmds\ltxcmds.sty"") (""C:\Program Files\MiKTeX\tex/generic/iftex\iftex.sty"") (""C:\Program Files\MiKTeX\tex/generic/pdftexcmds\pdftexcmds.sty"" (""C:\Program Files\MiKTeX\tex/generic/infwarerr\infwarerr.sty"")) (""C:\Program Files\MiKTeX\tex/latex/graphics\keyval.sty"") (""C:\Program Files\MiKTeX\tex/generic/kvsetkeys\kvsetkeys.sty"") (""C:\Program Files\MiKTeX\tex/generic/kvdefinekeys\kvdefinekeys.sty"") (""C:\Program Files\MiKTeX\tex/generic/pdfescape\pdfescape.sty"") (""C:\Program Files\MiKTeX\tex/latex/hycolor\hycolor.sty"") (""C:\Program Files\MiKTeX\tex/latex/letltxmacro\letltxmacro.sty"") (""C:\Program Files\MiKTeX\tex/latex/auxhook\auxhook.sty"") (""C:\Program Files\MiKTeX\tex/latex/kvoptions\kvoptions.sty"") (""C:\Program Files\MiKTeX\tex/latex/hyperref\pd1enc.def"") (""C:\Program Files\MiKTeX\tex/generic/intcalc\intcalc.sty"") (""C:\Program Files\MiKTeX\tex/generic/etexcmds\etexcmds.sty"") (""C:\Program Files\MiKTeX\tex/latex/url\url.sty"") (""C:\Program Files\MiKTeX\tex/generic/bitset\bitset.sty"" (""C:\Program Files\MiKTeX\tex/generic/bigintcalc\bigintcalc.sty"")) (""C:\Program Files\MiKTeX\tex/generic/atbegshi\atbegshi.sty"")) (""C:\Program Files\MiKTeX\tex/latex/hyperref\hpdftex.def"" (""C:\Program Files\MiKTeX\tex/latex/atveryend\atveryend.sty"") (""C:\Program Files\MiKTeX\tex/latex/rerunfilecheck\rerunfilecheck.sty"" (""C:\Program Files\MiKTeX\tex/generic/uniquecounter\uniquecounter.sty""))) (""C:\Program Files\MiKTeX\tex/latex/fancyhdr\fancyhdr.sty"") (""C:\Program Files\MiKTeX\tex/generic/babel\babel.sty"" (""C:\Program Files\MiKTeX\tex/generic/babel\babel.def"" (""C:\Program Files\MiKTeX\tex/generic/babel\txtbabel.def""))
*************************************
* Local config file bblopts.cfg used
*
(""C:\Program Files\MiKTeX\tex/latex/arabi\bblopts.cfg"") (""C:\Program Files\MiKTeX\tex/latex/babel-english\english.ldf"")) (""C:\Program Files\MiKTeX\tex/latex/tools\tabularx.sty"" (""C:\Program Files\MiKTeX\tex/latex/tools\array.sty"")) (""C:\Program Files\MiKTeX\tex/generic/pdftex\glyphtounicode.tex"") (""C:\Program Files\MiKTeX\tex/latex/l3backend\l3backend-pdfmode.def"") (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux) (""C:\Program Files\MiKTeX\tex/context/base/mkii\supp-pdf.mkii""
[Loading MPS to PDF converter (version 2006.09.02).]
) (""C:\Program Files\MiKTeX\tex/latex/hyperref\nameref.sty"" (""C:\Program Files\MiKTeX\tex/latex/refcount\refcount.sty"") (""C:\Program Files\MiKTeX\tex/generic/gettitlestring\gettitlestring.sty"")) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.out) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.out) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections/header.tex
LaTeX Font Warning: Font shape `OT1/cmr/bx/sc' undefined
(Font) using `OT1/cmr/bx/n' instead on input line 2.
) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections/general_info.tex (""C:\Program Files\MiKTeX\tex/latex/base\ulasy.fd"")) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections/summary.tex) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections/education.tex) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections/experience.tex) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections/projects.tex) (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections/skills.tex
Package Fancyhdr Warning: \footskip is too small (0.0pt): Make it at least 4.08003pt.
We now make it that large for the rest of the document.
This may cause the page layout to be inconsistent, however.
[1{C:/Users/rubem/AppData/Local/MiKTeX/pdftex/config/pdftex.map}]) [2] (c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux)
LaTeX Font Warning: Some font shapes were not available, defaults substituted.
)<C:/Program Files/MiKTeX/fonts/type1/public/amsfonts/cm/cmbx10.pfb><C:/Program Files/MiKTeX/fonts/type1/public/amsfonts/cm/cmbx12.pfb><C:/Program Files/MiKTeX/fonts/type1/public/amsfonts/cm/cmcsc10.pfb><C:/Program Files/MiKTeX/fonts/type1/public/amsfonts/cm/cmr10.pfb><C:/Program Files/MiKTeX/fonts/type1/public/amsfonts/cm/cmsy10.pfb><C:/Program Files/MiKTeX/fonts/type1/public/amsfonts/cm/cmsy6.pfb><C:/Program Files/MiKTeX/fonts/type1/public/amsfonts/cm/cmti10.pfb>
Output written on c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf (2 pages, 111014 bytes).
SyncTeX written on c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.synctex.gz.
Transcript written on c:/Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.log.
Latexmk: Log file says output to 'cv.pdf'
=== TeX engine is 'pdfTeX'
Latexmk: All targets (c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv.pdf) are up-to-date
```
Latex Workshop extension Log
```
[19:50:33] Initializing LaTeX Workshop.
[19:50:33] Extension root: c:\Users\rubem\.vscode-insiders\extensions\james-yu.latex-workshop-8.13.2
[19:50:33] Creating PDF file watcher.
[19:50:33] pdflatex is provided by MiKTeX
[19:50:33] Creating LaTeX Workshop http and websocket server.
[19:50:33] LaTeX Workshop initialized.
[19:50:33] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:50:33] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:50:33] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:50:33] Root file changed: from undefined to c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:50:33] Start to find all dependencies.
[19:50:33] Instantiating a new file watcher for c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:50:33] Creating Bib file watcher.
[19:50:33] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:50:33] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\header.tex
[19:50:33] Parse fls file.
[19:50:33] Cannot find fls file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\header.fls
[19:50:33] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\general_info.tex
[19:50:33] Parse fls file.
[19:50:33] Cannot find fls file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\general_info.fls
[19:50:33] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\summary.tex
[19:50:33] Parse fls file.
[19:50:33] Cannot find fls file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\summary.fls
[19:50:33] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\education.tex
[19:50:33] Parse fls file.
[19:50:33] Cannot find fls file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\education.fls
[19:50:33] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\experience.tex
[19:50:33] Parse fls file.
[19:50:33] Cannot find fls file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\experience.fls
[19:50:33] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\projects.tex
[19:50:33] Parse fls file.
[19:50:33] Cannot find fls file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\projects.fls
[19:50:33] Parse fls file.
[19:50:33] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[19:50:33] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[19:50:33] Could not resolve included file undefined
[19:50:33] Server created on 127.0.0.1:63892
[19:50:33] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:50:33] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\header.tex
[19:50:33] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\general_info.tex
[19:50:33] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\summary.tex
[19:50:33] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\education.tex
[19:50:33] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\experience.tex
[19:50:33] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\projects.tex
[19:50:33] LaTeX Workshop version: 8.13.2
[19:50:33] Snippet data loaded.
[19:50:35] Restoring the PDF viewer at the column 1 from the state: {""type"":""state"",""state"":{""path"":""c:\\Users\\rubem\\Documents\\GitHub\\Tapyu\\cv\\Latex\\cv.pdf"",""scale"":""auto"",""scrollMode"":0,""spreadMode"":0,""scrollTop"":423,""scrollLeft"":0,""trim"":0}}
[19:50:36] Preview PDF file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf
[19:50:36] Handle data type: open
[19:50:36] Handle data type: request_params
[19:50:36] Sending the settings of the PDF viewer for initialization: {""type"":""params"",""scale"":""auto"",""trim"":0,""scrollMode"":0,""spreadMode"":0,""hand"":false,""invertMode"":{""enabled"":true,""brightness"":1,""grayscale"":0.6,""hueRotate"":180,""invert"":0.9,""sepia"":0},""bgColor"":""#ffffff"",""keybindings"":{""synctex"":""ctrl-click""}}
[19:50:36] Handle data type: loaded
[19:57:39] Handle data type: reverse_synctex
[19:57:39] SyncTeX to file c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\experience.tex
[19:57:39] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:57:39] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:57:39] Could not resolve included file undefined
[19:57:39] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:57:39] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:57:39] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:57:39] Found root file from parent: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:57:39] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:57:45] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:57:45] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:57:45] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:57:45] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:57:59] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:57:59] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:57:59] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:58:02] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:58:02] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:58:02] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:58:02] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:59:22] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:59:22] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:59:22] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:59:22] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:59:40] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:59:40] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:59:40] Could not resolve included file undefined
[19:59:40] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:59:40] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:59:40] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:59:40] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:59:43] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:59:43] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:59:43] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:59:43] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:59:49] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:59:49] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:59:49] Could not resolve included file undefined
[19:59:49] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:59:49] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:59:52] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:59:52] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:59:52] Could not resolve included file undefined
[19:59:52] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:59:52] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[19:59:53] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[19:59:53] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[19:59:53] Could not resolve included file undefined
[19:59:53] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[19:59:53] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:00:14] Could not resolve included file undefined
[20:00:14] Could not resolve included file undefined
[20:00:14] File watcher - file changed: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:00:14] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:00:14] Parse fls file.
[20:00:14] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:00:14] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:00:14] Auto build started detecting the change of a file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:00:14] BUILD command invoked.
[20:00:14] Building root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:00:14] Build root file c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:00:14] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:00:14] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:00:14] LaTeX build process spawned. PID: 9708.
[20:00:15] LaTeX log parsed with 4 messages.
[20:00:16] Recipe returns with error: 12/null. PID: 9708. message: Rc files read:
NONE
Latexmk: This is Latexmk, John Collins, 17 Apr. 2020, version: 4.69a.
Rule 'pdflatex': File changes, etc:
Changed files, or newly in use since previous run(s):
'c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv.tex'
'cv.tex'
------------
Run number 1 of rule 'pdflatex'
------------
------------
Running 'pdflatex --max-print-line=10000 -synctex=1 -interaction=nonstopmode -file-line-error -recorder -output-directory=""c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex"" ""c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex""'
------------
Latexmk: Missing input file: 'section/skills.tex' from line
'! LaTeX Error: File `section/skills.tex' not found.'
Collected error summary (may duplicate other messages):
pdflatex: Command for 'pdflatex' gave return code 1
Refer to 'c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv.log' for details
Latexmk: Use the -f option to force complete processing,
unless error was exceeding maximum runs, or warnings treated as errors.
.
[20:00:16] Cleaning auxiliary files and retrying build after toolchain error.
[20:00:16] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.out
[20:00:16] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.log
[20:00:16] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:00:16] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:00:16] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fdb_latexmk
[20:00:16] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:00:16] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:00:16] LaTeX build process spawned. PID: 10356.
[20:00:17] LaTeX log parsed with 4 messages.
[20:00:17] Recipe returns with error: 12/null. PID: 10356. message: Rc files read:
NONE
Latexmk: This is Latexmk, John Collins, 17 Apr. 2020, version: 4.69a.
Rule 'pdflatex': The following rules & subrules became out-of-date:
'pdflatex'
------------
Run number 1 of rule 'pdflatex'
------------
------------
Running 'pdflatex --max-print-line=10000 -synctex=1 -interaction=nonstopmode -file-line-error -recorder -output-directory=""c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex"" ""c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex""'
------------
Latexmk: Missing input file: 'section/skills.tex' from line
'! LaTeX Error: File `section/skills.tex' not found.'
Collected error summary (may duplicate other messages):
pdflatex: Command for 'pdflatex' gave return code 1
Refer to 'c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv.log' for details
Latexmk: Use the -f option to force complete processing,
unless error was exceeding maximum runs, or warnings treated as errors.
.
[20:00:51] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[20:00:51] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[20:00:51] Could not resolve included file undefined
[20:00:51] Could not resolve included file undefined
[20:00:51] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[20:00:51] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[20:00:52] Found files that might be root, choose the first one: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:00:52] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:00:52] Found files that might be root, choose the first one: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:00:52] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:00:55] Could not resolve included file undefined
[20:00:55] Could not resolve included file undefined
[20:01:01] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[20:01:01] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[20:01:01] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:01] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:01:03] Could not resolve included file undefined
[20:01:03] Could not resolve included file undefined
[20:01:03] File watcher - file changed: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:03] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:03] Parse fls file.
[20:01:03] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:01:04] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:01:04] Auto build started detecting the change of a file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:04] BUILD command invoked.
[20:01:04] Building root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:04] Build root file c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:04] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:01:04] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:01:04] LaTeX build process spawned. PID: 16916.
[20:01:05] LaTeX log parsed with 4 messages.
[20:01:05] Recipe returns with error: 12/null. PID: 16916. message: Rc files read:
NONE
Latexmk: This is Latexmk, John Collins, 17 Apr. 2020, version: 4.69a.
Rule 'pdflatex': File changes, etc:
Changed files, or newly in use since previous run(s):
'c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv.aux'
------------
Run number 1 of rule 'pdflatex'
------------
------------
Running 'pdflatex --max-print-line=10000 -synctex=1 -interaction=nonstopmode -file-line-error -recorder -output-directory=""c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex"" ""c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex""'
------------
Latexmk: Missing input file: 'section/skills.tex' from line
'! LaTeX Error: File `section/skills.tex' not found.'
Collected error summary (may duplicate other messages):
pdflatex: Command for 'pdflatex' gave return code 1
Refer to 'c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv.log' for details
Latexmk: Use the -f option to force complete processing,
unless error was exceeding maximum runs, or warnings treated as errors.
.
[20:01:05] Cleaning auxiliary files and retrying build after toolchain error.
[20:01:05] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.out
[20:01:05] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:01:05] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:01:05] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.log
[20:01:05] File cleaned: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fdb_latexmk
[20:01:05] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:01:05] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:01:05] LaTeX build process spawned. PID: 2180.
[20:01:06] LaTeX log parsed with 4 messages.
[20:01:06] Recipe returns with error: 12/null. PID: 2180. message: Rc files read:
NONE
Latexmk: This is Latexmk, John Collins, 17 Apr. 2020, version: 4.69a.
Rule 'pdflatex': The following rules & subrules became out-of-date:
'pdflatex'
------------
Run number 1 of rule 'pdflatex'
------------
------------
Running 'pdflatex --max-print-line=10000 -synctex=1 -interaction=nonstopmode -file-line-error -recorder -output-directory=""c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex"" ""c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex""'
------------
Latexmk: Missing input file: 'section/skills.tex' from line
'! LaTeX Error: File `section/skills.tex' not found.'
Collected error summary (may duplicate other messages):
pdflatex: Command for 'pdflatex' gave return code 1
Refer to 'c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv.log' for details
Latexmk: Use the -f option to force complete processing,
unless error was exceeding maximum runs, or warnings treated as errors.
.
[20:01:16] Could not resolve included file undefined
[20:01:16] File watcher - file changed: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:16] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:16] Parse fls file.
[20:01:16] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:01:16] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:01:16] Auto build started detecting the change of a file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:16] BUILD command invoked.
[20:01:16] Building root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:16] Build root file c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:16] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:01:16] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:01:16] LaTeX build process spawned. PID: 5908.
[20:01:20] LaTeX log parsed with 3 messages.
[20:01:20] Recipe of length 1 finished. PID: 5908.
[20:01:20] Successfully built c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:01:20] Refresh PDF viewer for c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf
[20:01:20] Parse fls file.
[20:01:20] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:01:20] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:01:20] Preview PDF file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf
[20:01:25] Could not resolve included file undefined
[20:01:25] File watcher - file changed: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:25] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:25] Parse fls file.
[20:01:25] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:01:25] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:01:25] Auto build started detecting the change of a file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:25] BUILD command invoked.
[20:01:25] Building root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:25] Build root file c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:25] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:01:25] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:01:25] LaTeX build process spawned. PID: 216.
[20:01:25] Recipe of length 1 finished. PID: 216.
[20:01:25] Successfully built c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:01:42] Could not resolve included file undefined
[20:01:42] File watcher - file changed: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:42] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:42] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\skills.tex
[20:01:42] Parse fls file.
[20:01:42] Cannot find fls file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\skills.fls
[20:01:42] Parse fls file.
[20:01:42] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:01:42] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:01:42] Auto build started detecting the change of a file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:42] BUILD command invoked.
[20:01:42] Building root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:42] Build root file c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:01:42] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:01:42] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:01:42] LaTeX build process spawned. PID: 3024.
[20:01:42] Added to file watcher: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\sections\skills.tex
[20:01:45] LaTeX log parsed with 3 messages.
[20:01:45] Recipe of length 1 finished. PID: 3024.
[20:01:45] Successfully built c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:01:45] Refresh PDF viewer for c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf
[20:01:45] Parse fls file.
[20:01:45] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:01:45] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:01:46] Preview PDF file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf
[20:02:49] Could not resolve included file undefined
[20:02:49] File watcher - file changed: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:02:49] Parsing a file and its subfiles: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:02:49] Parse fls file.
[20:02:49] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:02:49] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:02:49] Auto build started detecting the change of a file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:02:49] BUILD command invoked.
[20:02:49] Building root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:02:49] Build root file c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:02:49] Recipe step 1: latexmk, --max-print-line=10000,-synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex,c:/Users/rubem/Documents/GitHub/Tapyu/cv/Latex/cv
[20:02:49] cwd: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex
[20:02:49] LaTeX build process spawned. PID: 14192.
[20:02:50] Handle data type: loaded
[20:02:50] Handle data type: loaded
[20:02:51] LaTeX log parsed with 3 messages.
[20:02:51] Recipe of length 1 finished. PID: 14192.
[20:02:51] Successfully built c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:02:51] Refresh PDF viewer for c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf
[20:02:51] Parse fls file.
[20:02:51] Fls file found: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.fls
[20:02:51] Parse aux file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.aux
[20:02:52] Preview PDF file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.pdf
[20:02:52] Handle data type: loaded
[20:03:37] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[20:03:37] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[20:03:37] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:03:38] Current workspace folders: [""file:///c:/Users/rubem/Documents/GitHub/Tapyu""]
[20:03:38] Current workspaceRootDir: c:\Users\rubem\Documents\GitHub\Tapyu
[20:03:38] Found root file from active editor: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex
[20:03:38] Keep using the same root file: c:\Users\rubem\Documents\GitHub\Tapyu\cv\Latex\cv.tex.
[20:05:17] Could not resolve included file undefined
[20:05:24] LOG command invoked.
[20:05:30] LOG command invoked.
[20:05:30] LOG command invoked.
[20:06:42] LOG command invoked.
[20:06:42] LOG command invoked.
```",
"> * Please paste the whole log messages here, not parts of ones. It should start with `Initializing LaTeX Workshop`. It is very important to identify problems.
> > ![ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ 2020-02-14 20 41 25](https://user-images.githubusercontent.com/10665499/74528024-87b2ef80-4f6a-11ea-9c5d-4f9feff25524.png)
> > When all of the above is done, we will reopen this issue.
Done.",Impatience
"> Switch to pdf. tab using the keibing using alt+n
You are using inappropriate keyboard shortcuts. The PDF tab is on a different tab group. The shortcuts work for tabs on the same group. You have to switch to the group of a document you want to edit.",Impatience
"Thank you for your explanation.
If I have understood correctely what tab group means, PDF tab is on the same group of my other code file. When I type Ctrl+Alt+V (View PDF Latex File), it opens on a different group, as you can see here
![image](https://user-images.githubusercontent.com/22801918/96390803-aff38d80-118c-11eb-9f5d-3e49c1006c80.png)
Then I append manually the PDF tab to my previously tab group, like that
![image](https://user-images.githubusercontent.com/22801918/96390838-d0234c80-118c-11eb-9f8a-f40d90fafcb1.png)
In this way, I can pass through all tabs with the keybing `alt+n`. However, when I go to PDF tab, this keybind doesn't work anymore...",
"See `Go` > `Switch Group` on the menu.
This issue is not related to LaTeX Workshop. Please ask https://stackoverflow.com/ on the primary usage of VS Code.",Impatience
"Are you saying that any keyboard shortcut does not work on the PDF viewer? Or, only `alt+n` does not work?",Impatience
"> See `Go` > `Switch Group` on the menu.
> > This issue is not related to LaTeX Workshop. Please ask https://stackoverflow.com/ on the primary usage of VS Code.
Thank you for the reference, but I think I don't have doubt about it. [By default, closing the last editor of an editor group will also close the group itself](https://code.visualstudio.com/docs/getstarted/userinterface). So, when I move my .pdf tab to other code tabs, I put all tabs on the same group (and the empty group is deleted automatically). Therefore, `alt+n` shortcuts should work...",Impatience
"> Are you saying that any keyboard shortcut does not work on the PDF viewer? Or, only `alt+n` does not work?
I didn't even realize it, but shortcuts such as `ctrl+w` or `ctrl+q` are not working as well...",Impatience
Related to microsoft/vscode/issues/108797,
Reported microsoft/vscode/issues/109147.,
"## Preliminary questions
### Disable all the other extensions except for LaTeX Workshop, and check that you still see this issue. [Required]
You still see this issue?: Yes
### Make sure to visit the [wiki FAQ](https://github.com/James-Yu/LaTeX-Workshop/wiki/FAQ) before filling an issue.
You visited the wiki?: Yes
## Describe the bug
I can't switch between the tab when I'm on .pdf tab using the keybinds `alt+n`, where `n` is a number that comes from `0` up to `9`.
### To Reproduce
1. Switch to pdf. tab using the keibing using `alt+n`
2. Now try to go back to your previous tab using keybing again
3. It won't work.
### Expected behavior
I expected to be able to go back to your previous tab using keybing again
### Developer Tools Console [Required]
Not applicable.
## Desktop [Required]
- OS: Windows 10
- VS Code version: 1.51.0
- TeX distribution version: MiKTeX 4.0.1
## Additional questions
### Are you using VSCodium?
No
### Are you using LaTeX Workshop with VS Code Remote?
No",
I can replicate this on Linux as well.,
"I can replicate this on macOS, specifically to try to minimize the window by typing `Cmd+m`. It tells me `(Command+M) was pressed. Waiting for a second key of chord...` on the bottom blue bar (I forget what it's called).",
Thank you @hpr1999 and @hussein-esmail7 for the reporting.,
"I have the same problem on my local Linux machine. Oddly, if I'm working on a remote machine using the *Remote - SSH* extension, the shortcuts behaves as expected. The OS of both machines is Fedora 34.",
"Same issue on arch linux
Version: 1.45.0
Commit: d69a79b73808559a91206d73d7717ff5f798f23c
Date: 2020-05-07T16:10:24.511Z
Electron: 7.2.4
Chrome: 78.0.3904.130
Node.js: 12.8.1
V8: 7.8.279.23-electron.0
OS: Linux x64 5.15.0-rc4-1-git-00271-g5d6ab0bb408f",
Other system-wide keyboard shortcuts like Command+A (to select all text within the search field) or Command+C (to copy the selected text) do also not work on macOS when the internal PDF viewer is used.,Bitter Frustration
I have edited the title in order to cover all the OS.,
It is a different issue. microsoft/vscode/issues/129178,
Please open a new issue if it is not related to keyboard shortcuts of VS Code on Windows. Please fill out all the required in the [issue template](https://github.com/James-Yu/LaTeX-Workshop/blob/master/.github/ISSUE_TEMPLATE/bug-report.md),
I am closing this issue due to it's too dated and is an upstream vscode issue. Related problems should create new issues if can be fixed on the extension side (unlikely though).,
"If this gets added, could we add a feature to watch youtube videos on LBRY if the video is also available on LBRY?",
LBRY.tv seems to be gaining popularity momentum with more and more YT channels coexist on both platforms. Upvote +1,
"It'd be very nice to be able to watch lbry videos on NewPipe. There is already support for SoundCloud, MidiaCCC and FramaTube. I'm all for it.",
I would also like to see NewPipe incorporate LBRY. Source code for the official android lbry client/app. https://github.com/lbryio/lbry-fdroid,
I'd like to chime-in as another supporter of this proposed feature.,
"dear reader, you might want to check the linked topic there for more context on this feature request!
https://github.com/TeamNewPipe/NewPipe/issues/4733
disclaimer: i wrote it. ðŸ˜",
"The extractor doesn't need to parse lbry.tv or odysee.com, instead videos can be read straight from the LBRY network.
[`lbt`](https://gitlab.com/gardenappl/lbt) is a feature rich shell script for accessing content on the LBRY network simply by using `curl` and the [LBRY SDK](https://github.com/lbryio/lbry-sdk/) `lbrynet`.",
LBRY in Newpipe when?,Impatience
Any updates on the status of this?,Impatience
"Hello,
This is something I am waiting for.
piped.kavin.rocks already supports lbry streaming.",
Yes please. Also don't pull that bitchute bullshit on us this time please!,Vulgarity
"If this is written--will the pull request be accepted, or will it be slapped down on account of some kind of ideology? I'd like to foster competition to YT & do not want to use the LBRY app because it's not as efficient as Newpipe.",Bitter Frustration
"> If this is written--will the pull request be accepted, or will it be slapped down on account of some kind of ideology?
NewPipe would slap it down. Take a look at a fork called BraveNewPipe. I believe they would happily accept your contribution. BNP is NewPipe with additional sources, and no political ideology.
https://github.com/bravenewpipe/NewPipeExtractor/issues/16",Bitter Frustration
"Well, I know who I will support by tipping money!
bravenewpipe!",Mocking
"Funded this issue via gitpay.me: https://gitpay.me/#/task/608
Anyone who's interested in this, feel free to chip in. Maybe we can get it done!",
Is this going to be added or are you going to close the issue? Naturally people are hesitant to make a pull request because of how quickly the other one was shut down. I hope you guys are aware of the difference between closing a feature request that took little effort and rejecting a pull request that took a lot of effort.,Bitter Frustration
"> I hope you guys are aware of the difference between closing a feature request that took little effort and rejecting a pull request that took a lot of effort.
@DraconicNEO Doesn't matter. If you read the contribution guidelines, you will see that we strongly advise contributors to get in touch with us _before_ doing anything major, _precisely_ to avoid wasted effort. If a contributor ignores that, it is on them.
BTW, there was no such pull request. There have only been issues asking for support. You might be thinking of another project or repo.",Impatience
"This fork supports bitchute and rumble already (not odysee unfortunately)
https://github.com/bravenewpipe/NewPipe
I suggest switching over, since NewPipe devs are unlikely to implement support for these sites due to their political views",Insulting
"Honest question:
What political reasons makes them be against LBRY/Odysee though?
It is just an decentralized alternative to YT ran by individuals servers connected by the BitTorrent Protocol, the Blockchain is simply for indexing trackers.
No right/left wing speech are promoted by them, and they have basically the same rules of YouTube.
=========
My guessing: Maybe it's not political though, but tech-idealism? Since they have PeerTube, that is basically the same of Odysee but part of Fediverse my guessing would be that the developers don't actually like blockchain stuff and prefer to promote pure federative softwares.
If so, it's a pitty because I really like LBRY but it a fair point in the end.",
"If that's their position, they should state it. Because they're choosing to be cagey about their motives, we can only assume it's due to selfish political ideology. LBRY may not be ""right wing"", but they see it as such--that's all that matters.
Reaching to make an excuse for them is a waste of all of our time, honestly. They're not interested in the implementation--the end.
On September 22, 2022 8:37:41 AM CDT, Bruno ***@***.***> wrote:
>Honest question:
>
>What political reasons makes them be against LBRY/Odysee though?
>
>It is just an decentralized alternative to YT ran by individuals servers connected by the BitTorrent Protocol, the Blockchain is simply for indexing trackers.
>
>No right/left wing speech are promoted by them, and they have basically the same rules of YouTube.
>
>=========
>
>My guessing: >
>Maybe it's not political though, but tech-idealism? >Since they have PeerTube, that is basically the same of Odysee but part of Fediverse my guessing would be that the developers don't actually like blockchain stuff and prefer to promote pure federative softwares.
>
>If so, it's a pitty because I really like LBRY but it a fair point in the end.
>
>-- >Reply to this email directly or view it on GitHub:
>https://github.com/TeamNewPipe/NewPipeExtractor/issues/434#issuecomment-1255038576
>You are receiving this because you commented.
>
>Message ID: ***@***.***>
-- Sent from my Android device with K-9 Mail. Please excuse my brevity.",Bitter Frustration
"> Honest question:
> > What political reasons makes them be against LBRY/Odysee though?
> > It is just an decentralized alternative to YT ran by individuals servers connected by the BitTorrent Protocol, the Blockchain is simply for indexing trackers.
> > No right/left wing speech are promoted by them, and they have basically the same rules of YouTube.
> > =========
> > My guessing:
> > Maybe it's not political though, but tech-idealism? Since they have PeerTube, that is basically the same of Odysee but part of Fediverse my guessing would be that the developers don't actually like blockchain stuff and prefer to promote pure federative softwares.
> > If so, it's a pitty because I really like LBRY but it a fair point in the end.
I'm going to say that was probably their whole reason of rejecting platforms in the first place. It wouldn't matter what content is on those platforms it would be how those platforms are set up.",Bitter Frustration
"I keep reading these comments and I'm wondering: Are there any sources to back this up? Did the developers of NewPipe **actually** say that they won't implement service X or Y because of ideological reasons?
Unless that's the case I'd give them the benefit of the doubt and probably rather take [this](https://github.com/TeamNewPipe/NewPipe/issues/6645#issuecomment-1016863552) comment as the primary reason for not adding further services.
Long story short, it's OSS. If someone with a bit of time (and necessary knowledge) on their hands feels like building a **solid** LBRY implement and opening a PR I can hardly imagine the NewPipe people acting like *nah thanks, we're good*.",
"I look at my hidden question above speaking to this exact issue & I see my answer.
I will agree that debating this is a fucking waste of time--and the topic should be dropped until results are produced (somewhere), however.
On September 22, 2022 5:11:59 PM CDT, ""â—¤â—¢â—¤â—¢â—¤â—¢â—¤â—¢"" ***@***.***> wrote:
>I keep reading these comments and I'm wondering: Are there any sources to back this up? Did the developers of NewPipe **actually** say that they won't implement service X or Y because of ideological reasons?
>
>Unless that's the case I'd give them the benefit of the doubt and probably rather take [this](https://github.com/TeamNewPipe/NewPipe/issues/6645#issuecomment-1016863552) comment as the primary reason for not adding further services.
>
>Long story short, it's OSS. If someone with a bit of time (and necessary knowledge) on their hands feels like building a **solid** LBRY implement and opening a PR I can hardly imagine the NewPipe people acting like *nah thanks, we're good*.
>
>-- >Reply to this email directly or view it on GitHub:
>https://github.com/TeamNewPipe/NewPipeExtractor/issues/434#issuecomment-1255608225
>You are receiving this because you commented.
>
>Message ID: ***@***.***>
-- Sent from my Android device with K-9 Mail. Please excuse my brevity.",Bitter Frustration
"@brunocalmon @mrusme This is the comment I was talking about (on a PR that gave Bitchute support, which was ultimately rejected)
![image](https://user-images.githubusercontent.com/75075420/191966018-96a6139d-0c86-4df1-98d6-0c440b4abd74.png)
https://github.com/TeamNewPipe/NewPipeExtractor/pull/325#issuecomment-626801193
TLDR: they find these websites to be sources of 'misinformation' and 'extremism'... or IMHO the content simply doesn't fit their personal bias, so they don't wanna add support
I think the onus should be on the user to decide for themselves, which content they'd like to consume, not this gatekeeping approach (antithetical to FOSS philosophy if we're being honest)",Bitter Frustration
"> Did the developers of NewPipe **actually** say that they won't implement service X or Y because of ideological reasons?
Yes they did, here: https://github.com/TeamNewPipe/NewPipeExtractor/issues/434#issuecomment-1255038576
They went so far as to argue that you saying something they disagree with is a *violation of their rights*. Pure politics.",Bitter Frustration
"Right. Lots of speculation, assumptions and accusations in the comments. And some whining, but that's par for the course.
Firstly, there have been no comments by any team member either accepting or rejecting the service. Nor has there ever been a PR adding support that could be accepted or rejected. So any assumptions or accusations based on that are wrong. Some voices of reason questioned this, so thank you for that. Your comments were still marked off-topic because that's just the case on Github. Feel free to go off-topic galore in our IRC channel.
Secondly, just like Bravenewpipe supports 2 additional services that Newpipe doesn't have, you can go all out and add whatever services you like to your own forks. Because FOSS. Or, you could _politely_ request the maintainers of a fork (there are at least 2) to add support. Because FOSS. But complaining about the supposed ""political views"" of team members won't suddenly make them 'see the light' or the 'error of their ways' and get them to drop everything to spend hours/days/weeks to add support for [insert your personal bandwagon service here] for free. Because FOSS. So there is no point in doing that, in this repo or in any FOSS project.
Finally, Odysee unfortunately caters to misinformation: https://www.reuters.com/investigates/special-report/usa-media-misinformation/
If you genuinely think that [insert your favourite content creator here] is not part of the misinformation camp and doesn't deserve to get blocked out merely because they use LBRY, consider persuading them to try Peertube. Not only will it potentially increase their viewers, but it would help them avoid the stigma associated with LBRY. And of course, you can add their instance, or one their chosen instance federates to, to Newpipe. (Just to be clear, this is not about superior tech, merely about exploring a platform that could help such creators out.)
And now, some food for thought: https://en.wikipedia.org/wiki/Paradox_of_tolerance
Service rejected. If, in the future, it turns out to be the case that a vast majority of LBRY content creators are genuine and the misinformation camp has been reduced to a tiny minority, we could revisit this service request.",Mocking
"> Many people don't install to the default directory, but to the user directory (--user-install), in fact, it's the default on many distributions.
I don't think so. The users who are using the default package provided many of Linux distributions are minority. also see https://rails-hosting.com/2020/#ruby-rails-version-updates
I'm Ruby programmer over the 20+ years. I and my company also didn't use it. We use the custom runtime build by like ruby-build.
> No satisfactory resolution from the development team.
This comment is a little offensive.",Insulting
"@hsbt > I don't think so. The users who are using the default package provided many of Linux distributions are minority.
Really? Is that why 64% of Arch Linux users have [installed ruby gems](https://pkgstats.archlinux.de/packages#query=rubygems)? Is that also why Debian packages rubygems directly into the `ruby` package?
> https://rails-hosting.com/2020/#ruby-rails-version-updates
Rails developers != Ruby users.
> This comment is a little offensive.
You think reality is offensive?",Mocking
"Whatever the popularity of Linux distributions is, I agree we could do better here. What about honouring `--user-install` if bundler is installing to system gems (i.e., no custom `BUNDLE_PATH` configuration is given). I think it's reasonable to respect whatever rubygems considers as the preferred location to install system gems.",
"@deivid-rodriguez I also created a patch for that, but it's so ugly it seems like a hack. Basically we would need to parse `Gem.configuration` and find the `--user-install` string. I did it only for `gem`, but the same has to be checked for `install`.
```diff
--- a/bundler/lib/bundler/rubygems_integration.rb
+++ b/bundler/lib/bundler/rubygems_integration.rb
@@ -163,6 +163,8 @@ def sources
end
def gem_dir
+ user_install = Gem.configuration[:gem]&.split(' ')&.include?('--user-install')
+ return Gem.user_dir if user_install
Gem.dir
end
```",Mocking
"Right, it _does_ feel like a hack, since these `gemrc` options are meant as permanent flags to the `gem` CLI commands. Not sure what the best solution is, need to think about it, but I don't like adding yet another environment variable.",Bitter Frustration
"@deivid-rodriguez I've thought about it and tried many things. The environment variable is the only clean solution I see.
I don't think the configurations of `gem` and `bundler` were thought through.",Bitter Frustration
"Thanks for your feedback, I guess? :man_shrugging:
I tend to disagree and I currently believe it might be better to add the ugly code and make this just work without the need for setting yet another environment variable. We could also deprecate the `--user-install` flag and move this to a global configuration to make the code cleaner, since I believe people don't just want `--user-install` for single CLI invocations but for changing the location to install gems to a user location permanently.",Irony
I'd like to hear @duckinator's opinion since I'm aware she's a user of the `--user-install` flag and also a maintainer with better context :),
"> I don't think the configurations of `gem` and `bundler` were thought through.
These libraries are like 15 years old. While I work on improving them I try to not make any judgement on past decisions, because te context at the time was completely different from the context now. To give you an example, when `--user-install` was added, maybe `bundler` didn't exist yet. Statements like the above feel quite arrogant to me, to be honest.",Bitter Frustration
"The concept of users was known 15 years ago. If you had asked me 15 years ago what should have been the way gems were installed I could have told you then; make user installations the default. The system installation is the special case, which could have been enabled with `--system`.
There's no reason to have `Gem.user_dir`. Only two concepts are needed: 1. the location where gems are installed to, 2. the paths to look for gems (which includes `Gem.default_dir`).
It's **that simple**.
My statement may look arrogant to you, yet it's still true.",Bitter Frustration
"This is the final notice. Be honest, respectable, and collaborative.",Threat
"> Be honest, respectable, and collaborative.
I am.
- Honest: I am speaking the truth, that's called **honesty**.
- Respectable: I am **not disrespecting** anybody. Ideas on the other hand don't have feelings and are meant to be criticized.
- Collaborative: I've sent multiple patches and addressed the feedback on those patches. That is called **collaboration**.
> This is the final notice.
Or what? You are going to willingly harm the users of rubygems that have been reporting these issues for more than **ten years**?
What you do is up to you. If you fix this, great, if not, I will take this ticket as evidence and suggest a hacky solution downstream to my distribution maintainers, since you leave us no other choice.
What you are doing is called [tone policing](https://en.wikipedia.org/wiki/Tone_policing), and it distracts from the issue at hand. It would be much more productive if you concentrated on **the bug**, rather than my tone, but that's your choice.",Bitter Frustration
Let's have a fresh discussion about this at https://github.com/rubygems/rubygems/issues/4031.,
"Many people don't install to the default directory, but to the user directory (--user-install), in fact, it's the default on many distributions.
Unfortunately this causes a mismatch between bundler and gem, since bundler doesn't read `gemrc` configuration files.
This problem has been explored in many issues: rubygems/bundler#710, rubygems/bundler#2565, rubygems/bundler#2667, chef/chef-dk#148. No satisfactory resolution from the development team.
I took the time to develop a simply and easy solution myself that should absolutely work for everyone, an environment variable: GEM_USER_INSTALL, which just works â„¢. And thus I am creating yet another issue ticket.",Bitter Frustration
Which system did you try this on? We've used this on multiple Debian / Ubuntu systems. Packages on http://packages.networkradius.com are using these.,
"From looking at the changes, it appears you build the system via `./configure / make / make install`. The defaults used there are for generic Unix-style operating systems. Then, you've used the Debian-specific `systemd` file. Since Debian / Ubuntu changes the paths where files go, this `systemd` file is specifically for use on Debian / Ubuntu systems. Not for use on ""generic Unix-style"" operating systems.
If you plan on using FreeRADIUS on Debian / Ubuntu, then use the build system for that: `make deb`. That will build a `.deb` package which will work.
Or, use the pre-built packages from http://packages.networkradius.com",
"Thanks for the pointers! This was on Ubuntu 20.04 Server.
> Or, use the pre-built packages from http://packages.networkradius.com
I would if I could, but they don't work with Ubuntu Focal 20.04.
Already spent three days on this. Now the service starts and freeradius reports OK, but still I can't connect from our Unifi AP. Using `radiusd -X` works fine though. I'm going nuts soon.",Bitter Frustration
"@alandekok Hey, just tried `make deb` and that failed as well since dependencies required are only available in Bionic.
Please make Focal work so that we can use a recent LTS release instead of using something that will end support soon.
I give up now.",Bitter Frustration
"@enoch85 FreeRADIUS is an open source project. If you want to see progress on a particular issue, then you are expected to contribute, _especially_ with something fairly trivial like this, where you could easily determine the correct dependencies for 20.04 and submit a PR.",Impatience
"@arr2036 The repo that I maintain only has [620 stars](https://github.com/nextcloud/vm/), but yeah. I'm well familiar with the concept. Thanks for the friendly reminder. ðŸ‘ Everything is easy if you know how to do it. I can tell you almost everything about Nextcloud, but I've never worked with FreeRADIUS before. As a maintainer of a project I see myself as the main contributor. My philosophy is to make it as easy as possible for people using what I produce. In my case, [install Nextcloud on any computer/server](https://help.nextcloud.com/t/appreciation-for-a-job-well-done/92140) with 0 Linux experience. I'm sorry to see that this isn't the thing here. Focal was released in April 2020, and still it's not supported.",Bitter Frustration
"I fixed up the Debian control file and added a Dockerfile for Focal a week after it was released. The Dockerfile works by building packages and then installing them, so `make deb` definitely works on Focal as long as the correct build dependencies have been installed.
If you've got Docker installed then you can use it to build the packages if it helps to avoid having to install the build dependencies on the host system:
```
docker build -t fr-ubuntu20-build --target=build scripts/docker/ubuntu20
C=$(docker create fr-ubuntu20-build)
docker cp $C:/usr/local/src/repositories/ ./deb
docker rm $C
```
Packages for the main site is on my to do list, but sadly so are a lot of other things.",
"@enoch85 If you're familiar with Open Source, then you should be familiar with the concept that ""we don't work for you"". You should also be familiar with the idea of ""you have zero right to demand that we do what you want"". You should also be familiar with the idea of ""please supply patches, as we can't do everything"".
We are _very_ sensitive to the all-too common practice of ""you do what I want, because I can't be bothered to do it myself"". We have very little sympathy for such poor attitude.
We don't expect everyone to be experts in FreeRADIUS. We do expect that people's contributions will be productive, and polite.
The current release (3.0.21) does not have Focal support because Focal was released after we released 3.0.21. As Matthew points out, Focal is supported in the v3.0.x branch. Which will become 3.0.22 any time now.
To close this out, we're sorry that the software was imperfect. But your approach is to take offence at straightforward, factual, comments. We suggest this is entirely the wrong approach.",Bitter Frustration
"@mcnewton > as long as the correct build dependencies have been installed.
I don't remember the exact name now but some libxxx-c2 and libxxx-c3 weren't possible to install from Ubuntu Focals own repos (clean ISO from their site), hence `make deb` failed. I did `apt-get install libxxx*` (1500 MB of packages) just to don't miss out on anything, but still - no success.
If the docker solution works, then great!
@alandekok > ""please supply patches, as we can't do everything"".
I'm sure familiar with that, and I understand 100% - but putting things IN CAPITAL LETTERS when you could do **bold** or *cursive* instead just seemed offensive to me. > ""you do what I want, because I can't be bothered to do it myself""
Well, at least I tried. I hope you didn't found my efforts (though small) as not bringing anything to the table.
> We suggest this is entirely the wrong approach.
Noted, and sorry for that.",Bitter Frustration
"And, looking forward to try 3.0.22. ðŸ˜„",
"Ah, from the control file: `libjson0 | libjson-c2 | libjson-c3 | libjson-c4 | libjson-c5,` - you need one of those (`|` = or), not all of them. I think Focal will be `libjson-c5`, which I added just before the Dockerfile.
See ""Install build dependencies"" in the Dockerfile which will install them all correctly for you using `mk-build-deps`.",
"> As a maintainer of a project I see myself as the main contributor. My philosophy is to make it as easy as possible for people using what I produce. In my case, [install Nextcloud on any computer/server](https://help.nextcloud.com/t/appreciation-for-a-job-well-done/92140) with 0 Linux experience. I'm sorry to see that this isn't the thing here. Nextcloud and FreeRADIUS target very different technical communities. Users of FreeRADIUS are expected to have basic Linux/BSD experience, networking knowledge, and have an idea of how RADIUS and its authentication protocols function. FreeRADIUS is used in installations with millions of subscribers, it's the primary authentication and proxy server for the Eduroam federation, It's shipped with DDWRT, and embedded in many manufacturer's wireless solutions. Organisations should not be having junior dev/devops install and configure FreeRADIUS instances, if those instances are to be used as a basis for a critical service.
Your main complaints appear to be Debian or Ubuntu messed up the permissions on automatically generated certificates (which is an issue which should have been reported to them, not the FreeRADIUS project), ~and that we haven't resolved a minor dependency issue in our Debian packaging. One which you yourself do not seem willing to submit fixes for.~ This has already been fixed and will in the next official release.
The point I was making was seeing that you have a build environment already established, it would take literally a couple of minutes to figure out the correct dependencies. Even if you had zero experience with debian packaging, a recursive grep through the source would have shown where libjson was listed as a dependency. I strongly dislike the sense of entitlement some people exhibit when using Open Source software. This is not paid-for commercial software, its software developed by a community. I have contributed to many projects outside of FreeRADIUS, and I would never come in with the attitude you've shown here if I wanted a positive outcome.",Bitter Frustration
"After locking this PR, enoch85 has taken to emailing us privately complaining that were are bullies, and praising his own abilities.
As a result of his ongoing pattern of abuse and hyper-sensitivity, he has been blocked from the FreeRADIUS organization. We also note that this block will also protect his delicate ego from further ""abuse"".",Mocking
"After hours of struggling with getting this right, I thought I would post this config as the default one since the current one is wrong, or at least doesn't work.
When building from source, the paths are wrong. Also, systemd PID shouldn't be run in `/var/run` (`usr/local/var/run`), it should be run in `/run/folder`.
This PR solves all that so that the paths are correct, and the actual PID is run from the correct directory.
Also added logging for systemd instead of logging to syslog.",Bitter Frustration
"Seems completely backwards to me. Perfectly OK from a MIT license PoV: your package, my freely provided code.
Discussion also at https://github.com/suimarco/proxy-manager/pull/1, but overall, instead of helping maintainers with their OSS work, forks are being done, and dependency graphs are turned into a mess.
As I mentioned there, there's a way to support the upstream work @ https://github.com/Ocramius/ProxyManager/tree/f65ae0f9dcbdd9d6ad3abb721a9e09c3d7d868a4#ocramiusproxy-manager-for-enterprise, and bugfixes for critical stuff are being backported anyway :shrug:",Bitter Frustration
"And btw, thanks to the symfony ecosystem for ruining the mood again: was going to work on PHP 8 support this weekend, but yet again I find myself being nervous, angry and frustrated at people treating weekend projects (**LITERALLY DOING IT IN THE WEEKEND**) like paid-for products.",Bitter Frustration
Dick move.,Insulting
"There's no reason to create a organiation like `suimarco`. It's a really **toxic** movement and totally apart of the technical implementation. Personal discussions should live apart. Authors can do whatever they want with their packages, if it's that critical and needs to be versioned differently then let's move it inside symfony organisation to follow same policies and maintain it there. I understand the reasons, I don't like how. Stay away from personal attacks.",Bitter Frustration
"As other have said, the naming here of the org/user is severely aggressive and makes it a _personal attack_ on @Ocramius . Call it `nicolas-grekas/proxy-manager` or something, and _allow issues to be reported_ on it. I don't want to pile on, but this is really working _against_ the PHP community by making aggressive moves like this.
@nicolas-grekas you might not personally like @Ocramius, but this is really a personal attack and is 100% out of order coming from a Symfony representative.",Bitter Frustration
Will this be PHP 4 compatible ?,
"To be clear, this [was not a personal attack](https://twitter.com/nicolasgrekas/status/1327249300263493633) at all, I admire the work of @Ocramius at many levels.
I'm going to rename that package and delete that org.",Threat
"Im closing this PR and locking the discussions. Technical reasons aside, making this PR could be (and probably have been) perceived as a personal attack which is a violation of the Code of Conduct.",
"| Q | A
| ------------- | ---
| Branch? | 5.x
| Bug fix? | no
| New feature? | no
| Deprecations? | no
| Tickets | -
| License | MIT
| Doc PR | -
*EDIT: Although I didn't intend any harm at all, this PR wasn't well-received (see comments below). I invite the reader to check https://github.com/Ocramius/ProxyManager/issues/630 for a possible follow up.*
The versioning policy of `ocramius/proxy-manager` has been proven quite painful over the years.
Everybody can feel it hard in various ways these days with Composer 2 and PHP 8 coming.
I created the [`suimarco/proxy-manager`](https://github.com/suimarco/proxy-manager) fork to provide the same code and API, but with a versioning policy that is friendly to continuous migrations.
This fork is a drop-in replacement for `ocramius/proxy-manager` that is tested from PHP 7.1 to 8.0.
It also ships with a few fixes that were not merged upstream but are still needed, and that we had to monkey-patch. The related monkey-patching can now be removed.",Bitter Frustration
not surprising I guess. Want to fix it?,Bitter Frustration
hi @eshellman - I'm looking into it now...,
"For more information - I have developed a linter plugin for this issue, just trying to figure out how to contribute it.
https://github.com/computamike/remark-link-escape
Do I close this issue, or is that something that the team needs to do?",
Let's leave it open; hopefully it will be addressed together with #5031,
"It seems that the | in the link breaks the rendering
![C#](https://user-images.githubusercontent.com/464876/99858549-b73ff980-2b85-11eb-81bc-0cd2dcb04e77.png)
Renders correctly when viewing the page within GitHub editing tools, but renders incorrectly on the GH Pages site.",
"Hi team
Although the nice @computamike's plugin was added as dependency in v2.2.0, this improvement was reverted in 2.3.0 (2020 Dec)
https://github.com/vhf/free-programming-books-lint/compare/v2.2.0...v2.3.0
@vhf CI action is now using last released version (v3.0.0) with last alphabetize requested features #5031 but not with vhf/free-programming-books-lint#12
CI logs: https://github.com/EbookFoundation/free-programming-books/runs/3195091515?check_suite_focus=true#step:4:8 (see install lint step)
I don't know what is their status because I'm newbie, but it would be a nice point to reintegrate again before it rots into oblivion, I thought @eshellman ðŸ¤—
/labelthis: resummit pr",Impatience
"Took me awhile to find the relevant page: https://ebookfoundation.github.io/free-programming-books/casts/free-podcasts-screencasts-en.html
Appears to be rendering fine now
![image](https://user-images.githubusercontent.com/4824630/135735312-eeef87da-7acd-4ece-9c97-83bc9ac380b8.png)",Impatience
"> Took me awhile to find the relevant page: https://ebookfoundation.github.io/free-programming-books/casts/free-podcasts-screencasts-en.html
> > Appears to be rendering fine now
We know it. See the fix #5177. The vertical bar is escaped",
Oh nice thanks! What is the remaining work on this issue? Did you want to close this issue?,Impatience
"It also happens in Arabic courses file #6715 | Environment | Screenshot |
|---|---|
| Github File Preview :+1: | ![image](https://user-images.githubusercontent.com/3125580/153564450-5db31da1-beb6-458f-b783-734f06e277f8.png) |
| GitHub Pages :-1: | ![image](https://user-images.githubusercontent.com/3125580/153564725-14fdd447-9871-4d26-b2e4-ed0213fb1f57.png) ![image](https://user-images.githubusercontent.com/3125580/153564985-80c9f555-6218-44d0-bf89-27a7371274fd.png) |
Is wellknown that pipes is the table markdown token. Seems that Kramdown don't deal well with it",Bitter Frustration
"Needs linter replug in to definitelly solve it. See https://github.com/EbookFoundation/free-programming-books/issues/5176#issuecomment-890391432
Notified to kramdown engine at https://github.com/kramdown/parser-gfm/issues/35",
"This one may take some time to see in discord.py proper, seeing as it would likely require either a complete redesign or secondary implementation of the commands extension, and a bit of hefty planning for basic library functionality. For now, it appears someone has made an unofficial extension to support slash commands. It's third party, so here there be dragons etc, and it doesn't seem to support more complex features slash commands bring, but it may be an option in the interim:
https://github.com/eunwoo1104/discord-py-slash-command",
"I believe that most people following this repo will also be in the know from other sources, but passing this on as an update to my previous comment, the tag related to slash commands from the discord.py server:
> slash commands are a new way to make commands right within discord, discord.py will probably not support them as they are lacking features and requires a major rewrite to handle
>
> bad command handler:
> - no default argument system - the argument isnt passed if you dont pass it making handling harder
> - no Union/Optional system like dpy
> - cant handle arguments yourself - required to use there parser
> - no way to invoke group command - you can only invoke the subcommands
> - no command aliases
>
> limitations:
> - 15 mins max per command - a token lasts 15 mins
> - only one message can be hidden (the first one)
> - hidden messages cannot have embeds
> - without a bot token you cant do much apart from send messages and etc - no api interaction
> - you only get ids for the parameters - if the parameter is a user you only get there id
> - if using the webhook based intergrations you cannot get any other events so you are limited to commands
>
> if you do want to use them there is a half maintianed fork that has support for them ( we will not support them here ) if you wish to use them.
> https://github.com/Zomatree/discord.py/tree/slashcommands - no docs because im lazy
TL;DR - Unfortunately, it appears discord.py proper is unlikely to support slash commands due to some drawbacks in comparison to ext.commands. There is a non-supported fork which one may use, if you want the new features such as the command autocomplete, hidden messages, and context system messages. The above linked option still seems viable.",
"## The Problem
Slash commands are going to be added to the discord api as of December the 15th. They provide a new way for discord bots/integrations.
https://github.com/discord/discord-api-docs/pull/2295
## The Ideal Solution
A way to add slash commands natively as part of a discord.py
## The Current Solution
Nothing currently exists
## Summary
Add support for discord's new slash commands",
"The current sentiment is that slash commands as a whole are not at a stage where implementing library support for them is worthwhile. If the system in its entirety gets improved in the future that may change, but for now it's probably best to sit tight or use the fork, as previously mentioned.",
"Yep, also i don't know if they are any good to implement into a bot that
has a lot of commands.
>",
What happened here?,Impatience
"@Znunu someone spamming some comments about the (current) usability of slash commands, don't worry about it",Mocking
"Looks like Slash Commands will get some nice features like a permissions system and server-side argument validation
https://github.com/discord/discord-api-docs/issues/2490",
"I would also say that a big advantage to slash commands is the visibility in the UI.
Users don't need to do `!help` to discover what commands are available (which also clutters the channel), the can just type `/` and see all available commands with descriptions. And as they type the UI displays which arguments they can pass, as well as which arguments are required. If an argument has a limited set of options that's displayed in a much cleaner way. Sure you can provide this information via `!help`, but paging through a long `!help` is not a great user experience (plus you have to know which prefix the bot is configured with to even invoke `!help` in case it's not `!`)
There are definite limitations for slash commands compared to `discord.ext.commands` as documented above. Some are marked as being addressed in the issue @MrSpinne linked, some are not. But regardless, the native support for it in the client UI is a huge boon for usability and discoverability for users so I would hope that at some point discord.py could add support even if it's a longer-term goal.",
"What really has me sold about the slash commands that we've got lately is how user-friendly they've become with the UI to help guide users to using commands. In the past, we've had to type a help command or look at some personal bot website to understand the context of how they function, but now, that's all changed merely to a description for each option, each choice, and etc.
Idealistically, the only thing that slashes commands should be different than the commands we can already create and implement into our own discord.py bots should be the UI, everything should in my eyes come natively. Guild/role permissions, requirements, no limit to choices for options, and etc. are many things that we are still waiting to seek for slash commands to receive, either via. un-official 3rd party support or by the developers at Discord themselves.
Because of this, I continue to believe that we should seek to be able to code slash commands as an actual usable thing for the discord.py library, not because they're something new and trendy, but simply because this library is an API wrapper in and of itself. The main purpose it overall serves is to help make the Bot API easier to use with Python, and by those means, we shouldn't limit ourselves from implementing slash commands as well. They may not be used as much as regular commands, for sure, but in the future, we won't have to worry about having to code it later than doing it now. This is at least my consensus on it, and I think others might agree.",
"Someone should pin this, because too many people ask about the same thing.",Impatience
"Interactions (as the API calls them) will be implemented when they're in a state of good usability. I'm unsure when that will be but the API is currently in flux and in ""public beta"" with promises of partial backwards compatibility. I do not particularly want to use this library as their testing bed since the library is more conservative towards breaking.
I have no immediate plans to support it as-is but in the future when the ecosystem evolves and it becomes less of a burden it'll be possible to implement it within the library. Not because of slash commands but because of other types of interactions that will be in the API such as buttons, models, etc.
As of now, this issue keeps kind of spiraling and becoming a maintenance nightmare. It'll be closed when it's implemented, whenever that is. Just don't hold your breath on this being any time soon. For now, I'm locking it.",Bitter Frustration
Implemented.,
"Version: 1.53.0-insider
Commit: 76436a4d43566cd313629799e2ec51b4d00caa64
Date: 2020-12-11T17:46:02.603Z
Electron: 11.0.3
Chrome: 87.0.4280.67
Node.js: 12.18.3
V8: 8.7.220.25-electron.0
OS: Darwin x64 17.7.0
1. Make the height of Snippet View large.
1. Expand Structure View in the side bar.
1. Collapse Commands View in the side bar.
1. Expand Commands View in the side bar.
![ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ 2020-12-13 20 39 10](https://user-images.githubusercontent.com/10665499/102010732-5b7f1f80-3d83-11eb-8c7d-d29a094bac98.png)
Related to microsoft/vscode/issues/110450.",
"Is this issue resolved? It looks like I cannot reproduce this issue with the following version
Version: 1.64.2 (Universal)
Commit: f80445acd5a3dadef24aa209168452a3d97cc326
Date: 2022-02-09T22:00:58.347Z (2 wks ago)
Electron: 13.5.2
Chromium: 91.0.4472.164
Node.js: 14.16.0
V8: 9.1.269.39-electron.0
OS: Darwin arm64 21.3.0",
"No. I still see the issue.
Version: 1.65.0-insider (Universal)
Commit: 8440b8f47231147823ba9696913d642e5f2d9d58
Date: 2022-02-25T05:18:40.990Z
Electron: 13.5.2
Chromium: 91.0.4472.164
Node.js: 14.16.0
V8: 9.1.269.39-electron.0
OS: Darwin arm64 20.6.0",
It seems still to be an upstream issue. Does vscode repo have a new open issue for this? The previous one and the following PR have been both resolved.,Impatience
The linked issue microsoft/vscode/issues/110450 is the one. It has been reopened.,
"I have confirmed the issue fixed.
Version: 1.68.0-insider
Commit: 688c80245936b49b7ceca494d9edfe97cae06f8a
Date: 2022-05-18T05:15:18.598Z
Electron: 17.4.3
Chromium: 98.0.4758.141
Node.js: 16.13.0
V8: 9.8.177.13-electron.0
OS: Darwin x64 20.6.0",
This is a placeholder issue to remind our triage processes to check for live security advisories.,
"Please go to https://github.com/deepmodeling/deepmd-kit/discussions to ask questions. Before asking questions, you can
- search the previous discussions
- check [the document](https://deepmd.readthedocs.io/), especially [training parameters](https://deepmd.readthedocs.io/en/stable/train-input.html)
Please provide necessary information including the version of software and installation way, input file, running commands, error log , etc., AS DETAILED AS POSSIBLE to help locate and reproduce your problem.",
"https://github.com/ipython/ipython/commit/dcd9dc90aee7e4c5c52ce44c18e7518934790612
The code has been already updated, but `7.19.0` did not include this. :(",Impatience
"As a temporary fix for anyone just trying to get things working again:
```
pip install jedi==0.17.2
```
----
It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
Still wish you a Merry Christmas!",Impatience
"By the way, a `7.19.1` release with the dependency `jedi<0.18.0` would also suffice.",
@davidhalter Thanks for your solution. it finally work.,
"I had the same problem with ipython and thanks to the solution of @davidhalter , it works again. :+1:",
Thank you for opening this issue. Happy holiday!,
Posted a pr to pin the dependency as suggested: #12746,
Thanks a lot. I thought my shell was broken. Every time when I tried to use tab completion in IPython it crashed. Glad I found a solution.,
Finally found a solution! thank you @davidhalter ðŸŽ‰,
When would the problem be fixed so that every Jedi version is compatible with ipython?,Impatience
"@tik9 Does installing from master work for you?
pip install git+https://github.com/ipython/ipython
If so, whenever ipython releases a new version, it will be fixed for everyone.",Impatience
"@NeilGirdhar , when doing
`pip install git+https://github.com/ipython/ipython`
It seems to work, I still have to update to the current Jedi.",Impatience
"Got this issue as well. Pinning `jedi==0.17.2` worked for me, thanks @davidhalter.",
"> As a temporary fix for anyone just trying to get things working again:
> > ```
> pip install jedi==0.17.2
> ```
> > It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> > Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> > Still wish you a Merry Christmas!
Thank you for your solution.
I got the same issue and IPython worked after I installed Jedi",Impatience
"installing 0.17.2 of jedi also worked for me. Was chasing my tail trying to figure out why it wasn't working in a new virtual environment, glad to have found this! Hope the fix is out soon. Luckily this showed up as the top link in Google for me when searching ""ipython init got an unexpected keyword argument 'column'"".",
"This works:
```console
$ pip install 'jedi<0.18'
```",
"Thanks @bl-ue
For reference, the autocomplete on my Jupyter notebook wasn't working and I was getting this same error on the terminal.
And now it's working as expected after downgrading jedi to 0.17.2 by just executing the command @bl-ue mentioned.",
Confirmed that downgrading jedi to `0.17.2` fixed the issue for me as well. Thanks @davidhalter,
"Hi everyone, I've stumbled on this issue while creating docker images for myself.
If I understand correctly from [this comment](https://github.com/ipython/ipython/issues/12740#issuecomment-751273086) the problem has already been solved on master, but so far no release including the fix has been issued.
In order to better organize my own work, may I know when do you plan a new release? Somebody [here](https://github.com/ipython/ipython/issues/12740#issuecomment-751273584) was suggesting to quickly release 7.19.1, including this patch, is it still an option?",Impatience
"> As a temporary fix for anyone just trying to get things working again:
> > ```
> pip install jedi==0.17.2
> ```
> > It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> > Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> > Still wish you a Merry Christmas!
thanks a lot",Impatience
pip install -U jedi==0.17.2 parso==0.7.1,
"I wonder, when you have a dependency (jedi) that is in version 0.x.y (not reached major 1), if it is not wiser to pin the minor as >=0.17,<0.18.",
FYI https://github.com/ipython/ipython/pull/12751#issuecomment-758065462,
"> As a temporary fix for anyone just trying to get things working again:
> > ```
> pip install jedi==0.17.2
> ```
> > It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> > Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> > Still wish you a Merry Christmas!
note: in anaconda, this has to be done in the same kernel you're using from within the jupyter notebook, not the one you're using the `jupyter notebook` command from!
took me about 30 minutes to finally figure that one out once i found @davidhalter's pip install tip (i used `conda install jedi==0.17.2` instead of pip).
thank you all!",Impatience
"Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ..."" i.e., will conda install jedi==0.17.2 from the shell command line do the trick? (it seems to)",Impatience
"> Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ..."" i.e., will conda install jedi==0.17.2 from the shell command line do the trick? (it seems to)
You can run Jupyter in one env and use it with a kernel from another env. See for example [this SO question](https://stackoverflow.com/questions/53004311/how-to-add-conda-environment-to-jupyter-lab). Jedi version of the latter env matters. If you are not aware of this, you probably use one conda env for jupyter and kernel.",
"The following works in ipython/jupyter. I assume it gets jedi out of the way of the built-in completer. Good enough for me until fix is released.
%config Completer.use_jedi = False",Impatience
It does work! ðŸ‘ðŸ» Nice find @dowenk! ðŸ˜,
"Because people at this point probably don't scroll up to my comment (https://github.com/ipython/ipython/issues/12740#issuecomment-751273584), this is what you should do temporarily:
```
pip install jedi==0.17.2
```",Impatience
When is the team going to fix this? In just a month hundreds of users have encountered this issue.,Impatience
I guess pinning jedi to 0.17.2 should be added to the readme. Anyone think I should open a PR for that? ðŸ˜¸,Irony
"just adding a +1 to this issue, it totally disables tab completion in Jupyter, so worth pinning 0.17.2 until resolved",Impatience
"> As a temporary fix for anyone just trying to get things working again:
> > ```
> pip install jedi==0.17.2
> ```
> > It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> > Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> > Still wish you a Merry Christmas!
Thanks",Impatience
This issue should be pinned on the GitHub repo. @Carreau?,Impatience
"What the timeline looks like for this getting fixed for mainline conda users? I ask because I have an online outreach course spinning up in about four days, meaning that 600+ highschool students are going to be encountering the ""now hit `<TAB>` to autocomplete ðŸ˜ƒ"" portion of their virtual course, and it is going unceremoniously bomb them out of `ipython`.
Downgrading `jedi` is easy enough for me, but much harder for someone who has never written a line of code before.",Impatience
"Releasing a new Jedi version might be easier than waiting for the next IPython release.
@davidhalter, how about making a one time exception in this case and reintroducing the deprecated features, the lack of which breaks IPython (and a few more packages) in a Jedi 0.18.1?",Impatience
"> What the timeline looks like for this getting fixed for mainline conda users?
You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos.",Impatience
"> You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos.
The last `ipython` builds on conda-forge [already](https://github.com/conda-forge/ipython-feedstock/blob/master/recipe/meta.yaml#L33-L34) depend on `jedi<0.18`, so this this shouldn't even happen if you create a new environment (or do `conda update ipython` first).",
">The last ipython builds on conda-forge already depend on jedi<0.18, so this this shouldn't even happen if you create a new environment (or do conda update ipython first).
Presumably if you have set conda-forge as the default channel, correct?",Impatience
"Even Anaconda [followed](https://github.com/AnacondaRecipes/aggregate/commit/fdd96f6968d7d4b6b8ec68ff0caa4d8566e953f9#diff-b85a555c399c3f7c88b8c090a978f34b985a1b4d23db5d0fd8887dfc80b476d0) with this already, so that shouldn't matter.",Bitter Frustration
"Hmm.. Then why is it that creating a new conda environment (with `conda 4.9.2`) gives me:
```
>>> conda create -n test_it python=3.8 ipython
The following NEW packages will be INSTALLED:
...
ipython pkgs/main/win-64::ipython-7.19.0-py38hd4e2768_0
...
jedi pkgs/main/win-64::jedi-0.18.0-py38haa95532_1
```
Am I missing something here?",Impatience
"So the packages are definitely [there](https://anaconda.org/main/ipython/files) (notice the `_1` build number), but interestingly, are not seeing much (or even any) downloads. I don't know if your anaconda channel is behind a proxy somewhere (or indeed, if anaconda themselves do some more integration testing before a package goes ""live""), but in any case, you can work around this by adding `conda config --add channels conda-forge`.
Side note: For any user that's not under the strictest of regulatory requirements (where people want to be able to blame a provider like Anaconda if stuff goes bad), conda-forge is more than ready/stable/secure to be your default channel. But, fair warning, that's just my subjective opinion.",
"> Am I missing something here?
Not you, but conda's dependency resolver. This is a problem and has been for years. (Sorry for the off-topic, but I could not resist)",Bitter Frustration
"> So the packages are definitely [there](https://anaconda.org/main/ipython/files) (notice the `_1` build number), but interestingly, are not seeing much (or even any) downloads
My totally uneducated guess (because I never understood the internals of the conda resolver):
conda sees the conflict for `jedi==0.18.0 and ipython-7.19.0-..._1` and instead of resolving it by `jedi==0.17.2 and ipython-7.19.0-..._1` it chooses `jedi==0.18.0 and ipython-7.19.0-..._0`",
"Welp, spoke too soon, conda-forge has the same problem (basically, the higher build number seems to lose against having one dependency have a higher version, which is not how it _should_ work. A possible solution would be backporting https://github.com/ipython/ipython/commit/dcd9dc90aee7e4c5c52ce44c18e7518934790612 to the feedstock, but that really should happen here first).
> Not you, but conda's dependency resolver. This is a problem and has been for years. (Sorry for the off-topic, but I could not resist)
Have you heard of [mamba](https://github.com/mamba-org/mamba) already? It has a rewritten resolver but is fully compatible with the conda-ecosystem. Not only does it solve the biggest problem of the conda-solver (speed), but it also correctly sets up the environment here:
```
>mamba create -n test python=3.8 ipython
[...]
ipython 7.19.0 py38hc5df569_2 conda-forge/win-64 1 MB
ipython_genutils 0.2.0 py_1 conda-forge/noarch 21 KB
jedi 0.17.2 py38haa244fe_1 conda-forge/win-64 944 KB
[...]
```",Bitter Frustration
"Thank you, both of you, for the useful insights and recommendations. I really appreciate it.
It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)
As it stands, I am dangerously close to making *two* ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.
(mamba looks cool!)",Bitter Frustration
"@rsokl
I raised an issue with conda itself, since it's probably too much (though not impossible) to do this on the conda-forge packaging side (in a way that's also compatible across several jedi-versions) - raised an [issue](https://github.com/conda-forge/ipython-feedstock/issues/127) to discuss that too...
@Carreau [said](https://github.com/ipython/ipython/pull/12751#issuecomment-758065462) he'd make a new release soon, which would of course also solve things.
In the meantime, the ""hand-holding"" guide should be as easy as `conda install ipython ""jedi<0.18""` (the quotes are important though, as otherwise you end up piping to nowhere).",
"@rsokl
There was one other way I hadn't thought about - fixing the conda-forge repodata directly. This was done yesterday, and means the problem is fixed _within_ conda-forge, i.e. when using strict channel priority (ignoring the main anaconda package that still has the same problem) - which unfortunately is not the out-of-the-box behaviour for conda though.
You can either do this for a single install (e.g. `conda create -n test --strict-channel-priority python=3.8 ipython`), but the recommended way (see e.g. [here](https://conda-forge.org/)) is persisting that setting with
```
conda config --set channel_priority strict
```",
"> Thank you, both of you, for the useful insights and recommendations. I really appreciate it.
> > It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)
> > As it stands, I am dangerously close to making _two_ ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.
> > (mamba looks cool!)
@rsoki, can you post a link to your walkthrough? I think some others among us have the same problem...",Bitter Frustration
"BTW, ""Effective Python"" (2nd edition) #89 suggests using the warnings module to handle things like deprecating arguments.",
"@gkuenning I just uploaded the downgrade instructions to Python Like You Mean It.
Under: [Information Introduction to Python](https://www.pythonlikeyoumeanit.com/Module1_GettingStartedWithPython/Informal_Intro_Python.html) and under [Jupyter Notebooks](https://www.pythonlikeyoumeanit.com/Module1_GettingStartedWithPython/Jupyter_Notebooks.html)",
"@rsokl Actually, anaconda fixed their repo data as well, so now it works out of the box again ðŸ¥³",
Nice! I just confirmed this on my end. I guess I'll keep those callout boxes up on PLYMI for a bit in case folks had already installed anaconda.,
I'll try to release 7.20 soon; I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake. Right now the limiting factor for the release is writing the what's new.,
"> I'll try to release 7.20 soon; I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake.
> > Right now the limiting factor for the release is writing the what's new.
I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.
The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.
By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.
And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.
You are calling for trouble.",Bitter Frustration
"> I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.
> The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.
> By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.
> And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.
> You are calling for trouble.
I'm amazed that someone that contribute squat is telling me what i'm doing wrong without tryign to really think about the problem.
- Say I pin IPython 7.20 to jedi <0.19 , and david release jedi 0.19. - pip is no free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"". - Bam you are wrongâ„¢ï¸ . ðŸ’Ž ðŸš€ ðŸŒ• And second the change of API were in jedi 0.17 with deprecation warnings and fix in master close to a year ago; I thought that by now 8.0 would be out. FYI about me know nothing about versioning:
- I spend month implementing on pushing the `python_requires` logic in pypi, and in pip to make sure when IPython dropped it did not break for Python 2 users
- I'm one of the regular advocate to have version medata data and package content to be separated in conda/pypi for this exact reason.
- It's not because something seem obvious that it's correct.
You could have just asked in for a question ""why are not you pinning to <0.19 starting with IPython 7.20"", to which I would have responded.
So not only you are impolite and haven't done your research, everybody see you incorrectly assume you know better than others and you've lost all credibility.
âœ‹ â¬‡ï¸ ðŸŽ¤",Bitter Frustration
"@Carreau: dude, you rock!",
"So essentially you say as there are problems with pip resolver and in some cases it will not be able to upgrade correctly I will completely ignore the problem and not try to solve other uses cases that I can.
Yes, fixing Ipython to work with the latest jedi is correct and pinning is needed as well.
Most use cases of Ipython will be install Ipython on a new environment. Should you pin, your 7.20 will work always. Ignore it and you depend on your dependencies.
The reason I didn't ask why you are planning to do what you are planning to do is because I know that solution is suboptimal.
It doesn't matter what work or knowledge has been accumulated in the past, that says nothing.",Bitter Frustration
"> pip is no[w] free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"".
That would indeed be unfortunate, the resulting setup would be not only broken, but out-of-date to boot. Can that happen when running `pip install ipython` in a fresh environment? I would naively assume that this would fetch the latest version of IPython and figure out the dependencies from there, i.e. honoring `jedi < 0.19`. But that assumption is probably wrong?
As maintainer of my own small Python package with moderately complicated dependencies, I feel like I should read up on this :) Any tips on good material on this topic?",
"@mostealth Obviously you have a different perspective on that matter. That's fine. However, it seems rather unproductive to dissipate the time and energy of productive people like @Carreau with pointless discussions. If you have deeper insights I see the following options: You can fork the project, file a PR or just sate your improvement suggestion in a way that is arguable, maybe in a dedicated issue.
For the rest of us I would suggest to basically ignore distracting comments which contain neither helpful information nor constructive criticism, but instead spread insults and negative emotion. Reactinv with :-1: should be enough attention for such comments.",Bitter Frustration
"@Carreau Thanks a lot! I guess the next release is going to be 1.0 anyway, so it shouldn't happen again. We might then think about pinning `jedi < 2.0.0` (which is probably years away).
In general for all people like @mostealth: If you are worried about older versions breaking: **Pin your dependencies!** The Python packaging system is still in a bit of a problematic state. The devs are doing a good job trying to turn it around, but there is a lot of technical debt from years ago. Keep in mind they want to keep backwards compatibility with a lot of old and new things. Just a enumerating a few things to show how complicated it is: eggs, wheels, explicit namespace packages, setuptools, distutils, import hooks, import meta paths, importlib, easy_install, venvs, virtualenvs, zip imports, freezed packages, source packages for C/C++ libraries and a lot of other things that I don't even know or remember anymore.
Most of you guys complaining don't really understand how hard certain things are if you consider all consequences. Read about Chesterton's Fence and try to understand why you shouldn't want something changed unless you understand why it is that way in the first place.",Bitter Frustration
"Thank you @Carreau. Finally someone who does not follow the church of pinning stuff without thinking about the consequences. Here is another case why pinning to older versions is not always a good idea: https://github.com/palantir/python-language-server/issues/898#issuecomment-770399215.
Unfortunately the Jedi and Parso pair are a frequent cause for headaches for packagers. Not only pip and conda, but also Linux distributions. I am looking forward to a more stable API once they reach 1.0.",Bitter Frustration
"Thanks everyone for the inputs; if we want to have discussion as to why/when pinning and why when not to pin, and other I suggest we don't use the bug tracker, maybe discourse.jupyter.org; but stop adding to this particular issue. IPython is a bit in a tough spot as it is both a library and application (in general you want to have a lock file to pin deps for applications, but be as liberal as possible when building a library). There is also a lot of issues that arise from the fact that maintainers time and desire to maintain can come and go; I definitely thought that by sept 2020 IPython 8 would be out, and that other maintainers like @davidhalter from jedi would also welcome more testing and help.
The Python package ecosystem is also quite old; and PyPI relatively recent; the requirements and needs of today's infrastructure can also be much different than what was expected even 5 years ago. That plus it works from HPC system to end-user that are starting to program so there is a lot of difficulties there.
Thanks everyone, I've now released 7.20 feel free to open bug reports if you encounter any issues. For now I'm going to lock this thread to avoid getting much more off topic.",Bitter Frustration
"<!-- This is the repository for IPython command line, if you can try to make sure this question/bug/feature belong here and not on one of the Jupyter repositories. If it's a generic Python/Jupyter question, try other forums or discourse.jupyter.org.
If you are unsure, it's ok to post here, though, there are few maintainer so you might not get a fast response. Ability of maintainers to spend time and resources on project like IPython is heavily influenced by US politics, and the current government policies have been harmful to the IPython Maintainers and Community. If you are on the fence on who to vote for or wether to vote, please cast your vote in for the democrat party in the US.
-->
Relevant traceback reads as follows: ```
File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete
completions = self._jedi_matches(
File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches
interpreter = jedi.Interpreter(
File ""../venv/lib/python3.8/site-packages/jedi/api/__init__.py"", line 725, in __init__
super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'
```
sys info: ```
{'commit_hash': '62779a198',
'commit_source': 'installation',
'default_encoding': 'utf-8',
'ipython_path': '../venv/lib/python3.8/site-packages/IPython',
'ipython_version': '7.18.0',
'os_name': 'posix',
'platform': 'Linux-4.15.0-128-generic-x86_64-with-glibc2.17',
'sys_executable': '../venv/bin/python',
'sys_platform': 'linux',
'sys_version': '3.8.5 (default, Jul 20 2020, 19:50:14) \n[GCC 5.4.0 20160609]'}
```
same reported in jedi repo too",
"the disk space on springrts.com is limited, there must be some autodeletion.
something similar like
https://github.com/spring/pr-downloader/blob/master/scripts/cleanup-builds.py needs to be used which deletes by ""short"" age and keeps the last n files.",Impatience
"there still is autodeletion, just slower.
used disk space depends on the ratio between new build and time until deletion.
in past this ratio was such, that all builds were deleted. no build at all remained for download.
i suggest to go with the dirty solution of longer deletion-time and adjust it if disk space should become a problem. i can also look into deleting based on file names or dates and only keep a certain amount of builds.",
"> i suggest to go with the dirty solution of longer deletion-time and adjust it if disk space should become a problem.
no, as when springrts.com runs out of disk space the database very likely will corrupt. this is much worse than not having development builds.
still the main issue was, that there was no stable release for >2 years.",Bitter Frustration
"the forum thread got locked right now and so i lost interest. you can close this, too. in fact close down everything.",Bitter Frustration
"this pull request comes way to late, sorry. when ""fixing"" it, please fix it orderly and not quick and dirty as it will create much more worse problems.
will you care about enough free disk space on springrts.com? i think no: so you have to listen to me.",Bitter Frustration
"it is not too late because the situation can happen again with future releases.
we had both edited our posts at same time with the idea of keeping a constant number of builds. that would have keep disk space usage constant. the latest forum posts reminded me that the issues are deeper and not related to code.",Bitter Frustration
"a constant number of builds would lead to the problem, that in a commit spree builds would be deleted to soon:
ideally the script deletes based on disk space and keeps at least n builds and auto deletes very old builds.
it comes to the point that the script as it is, isn't this bad: we just need to make more often a stable release.",Bitter Frustration
"as I said: ""the latest forum posts reminded me that the issues are deeper and not related to code.""
you can fantasize about the ideal delete-script but you lost focus on reality.
even re-uploading the latest builds by hand would have been a solution to prevent all the dead hosts. people had posted their backups so there was not even need to compile again.
why were games and players even pushed towards using test-builds? why the ban of stable 103.0 when the ban of 98.0 had already shown what happens: you can not force players to use certain engine versions. they might instead just leave. that is no way to run a multiplayer game.",Bitter Frustration
"do not delete so soon.
relevant forum thread: https://springrts.com/phpbb/viewtopic.php?f=1&t=40040",Impatience
"> Does nobody review changes, docs, and release notes anymore prior to the release? It looks this way.
this is not a friendly way to ask for things.
`DataFrame.lookup` has seen NO love at all since its initial implemention. It is a duplicative and unmaintained function. This is also polluting the namespace and is not in any way integrated to all of the other indexers. We do not need N ways to do the same thing.",Bitter Frustration
https://github.com/pandas-dev/pandas/pull/35224 is the PR and https://github.com/pandas-dev/pandas/issues/18262 is the issue,
"@impredicative you are welcome to contribute patches, e.g. doc fixes or other things.",
"@impredicative out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background. I agree that we can fix the documentation in the deprecated message and in the whatsnew though.
@jreback Would you be open to un-deprecate `DataFrame.lookup` and implement the `melt` + `loc` method on the background? I don't say it was an extremely popular method, but on SO I saw it being used sometimes.",
"@erfannariman What's difficult, for example, is how complicated [this answer using `melt`](https://stackoverflow.com/a/65722008/) is relative to how simple [this answer using `lookup`](https://stackoverflow.com/a/45487394/) is. The answer using `melt` requires three complicated method calls relative to the answer using `lookup` which is a single simple method call. In summary, just as you're saying, `DataFrame.lookup` could've been reimplemented using `melt`, thereby satisfying everyone.",
"> @erfannariman What's difficult, for example, is how complicated [this answer using `melt`](https://stackoverflow.com/a/65722008/) is relative to how simple [this answer using `lookup`](https://stackoverflow.com/a/45487394/) is. The answer using `melt` requires three complicated method calls relative to the answer using `lookup` which is a single simple method call. In summary, just as you're saying, `DataFrame.lookup` could've been reimplemented using `melt`, thereby satisfying everyone.
I don't disagree with your point, although I don't think the proposed method is persÃ© difficult, but I might be biased. I would not be against un-deprecating `DataFrame.lookup`. I will await @jreback response.",
"Pandas is used because it is believed to have a wealth of friendly methods. If this belief erodes, so will its user base.",
"@impredicative there is almost no usage of lookup AFAICT. very very few issues / tests / SO entries. If you have a valid, real work very common then am happy to show a doc example / recipe. Having a method must be a high bar.",Impatience
"@jreback A google.com search for `""df.lookup"" site:stackoverflow.com` lists 462 hits. That's more than for `df.bfill` (208) and `df.ffill` (375). Will you now be deprecating these two methods as well? What is your cutoff?",Impatience
"@impredicative you are not comparing apples to apples here. try with indexing operators, we *already* have `.loc`, `.iloc`, `.at`, and `.iat`, not to mention partial slicing and of course `[]`. which one shall we cut to keep `.lookup`????
I in fact have an issue to remove `.iat` and `.at`. We simply do not need this much choice.",Bitter Frustration
"@jreback I don't use `.iat` or `.at`.
In general, I would ask the question: ""_Is the alternative more simple or more difficult to use?_"" If a method simplifies code, making it less verbose and prevents users from reimplementing common patterns, then it's valuable, even if you personally think it's a duplicate, proportional to how well it accomplishes this goal.",
"> @jreback I don't use `.iat` or `.at`.
> > In general, I would ask the question: ""_Is the alternative more simple or more difficult to use?_"" If a method simplifies code, making it less verbose and prevents users from reimplementing common patterns, then it's valuable, even if you personally think it's a duplicate, proportional to how well it accomplishes this goal.
this is balanced against an already huge api. There *are* alternatives from this. If you have use case where this is especially awkward / burdensome I would encourage you to open an issue with a reproducible example and show why a new method makes sense.",
"@erfannariman
> out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background.
I learned about the deprecation of `lookup` when answering [this SO question](https://stackoverflow.com/q/65892265/4238408). And [this](https://github.com/pandas-dev/pandas/pull/35224#discussion_r564046976) is my thought on the proposed alternative for `lookup`. Of course, if current `lookup` is a `for` loop, this method would be faster for small dataset. But for bigger dataset, things would change due to potentially larger memory allocation, especially when column names are long strings and data to be looked up are homogeneous.",
"> @erfannariman
> > > out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background.
> > I learned about the deprecation of `lookup` when answering [this SO question](https://stackoverflow.com/q/65892265/4238408). And [this](https://github.com/pandas-dev/pandas/pull/35224#discussion_r564046976) is my thought on the proposed alternative for `lookup`. Of course, if current `lookup` is a `for` loop, this method would be faster for small dataset. But for bigger dataset, things would change due to potentially larger memory allocation, especially when column names are long strings and data to be looked up are homogeneous.
Sure I would be happy to make a PR with a new proposed more efficient method and there we can discuss into more details and the core devs can share their thoughts as well. What would be your suggestion, the answer you gave on SO? Could you maybe share a reproducible example in a new ticket and mention this issue? @quanghm",
"@erfannariman > out of curiosity, what is exactly difficult about the new proposed method, which is also significantly faster since DataFrame.lookup was a for-loop on the background.
I ran a quick test and the results said the reverse:
```
np.random.seed(43)
# also tested for n = 1000, 10_000, 100_000
n=1_000_000
cols = list('abcdef')
df = pd.DataFrame(np.random.randint(0, 10, size=(n,len(cols))), columns=cols)
df['col'] = np.random.choice(cols, n)
# the proposed method
%%timeit -n 10
melt = df.melt('col')
melt = melt.loc[lambda x: x['col']==x['variable'], 'value']
melt = melt.reset_index(drop=True)
# 623 ms Â± 6.86 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
# What I would do without `lookup`
%%timeit -n 10
cols, idx = np.unique(df['col'], return_inverse=True)
df.reindex(cols,axis=1).to_numpy()[np.arange(df.shape[0]), idx]
# 430 ms Â± 4.73 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
# Current lookup on 1.1.4
%%timeit -n 10
df.lookup(df.index, df['col'])
# 321 ms Â± 1.55 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```
The proposed method always comes last by a big margin in terms of speed. This only gets worse if the column names get longer.",Bitter Frustration
"> lambda x: x['col']==x['variable']
Using lambdas is a non-starter. It shouldn't even be suggested unless it is the last option on earth.
> cols, idx = np.unique(df['col'], return_inverse=True)
`np.unique` obviously will support only a very limited number of Pandas datatypes, so it's probably a non-starter too. Numerical programming using Numpy is different from more generic dataframe programming using Pandas.",Bitter Frustration
"@erfannariman Regarding the lambda function, that was originally an attempt to chain the three lines to detect possible speed improvement. Answer was not.
Using `.loc[melt['col']==melt['variable'],...]` wouldn't improve/change anything.
`np.unique` is used on the column names, so most likely will be strings. In general case, we can use instead, `pd.Series.unique()` and `pd.Series.factorize()` for that single line. But as far as I know, those two also returns `np.array`. Also, can you name some Pandas datatypes that wouldn't work with `np.unique`, especially those one would use as column names?
All those aside, the message here is that the proposed method is **slower** than current `lookup` method by factor of `2`.",
"Thanks for the extensive example and speedtests @quanghm . I did a quick check and seems like `lookup` wasn't a for loop actually, only for columns with mixed types. So the in terms of speed it looks okay. Only thing is that I remember it being quite slow for certain cases, but those must have been mixed type columns in my cases I think. See here for source code https://github.com/pandas-dev/pandas/blob/b5958ee1999e9aead1938c0bba2b674378807b3d/pandas/core/frame.py#L3848-L3861
For your approach, I agree that it's more efficient both in speed and memory allocation, but I remember writing multiple methods to replace `lookup` and I decided to go with melt since it was most ""readable"", without making it too complex for the documentation.
So I think it all boils down to @jreback and the argument that `DataFrame.lookup` has not been used as much, which is being discussed by @impredicative",
"@erfannariman Thanks. The code for `lookup` indeed reflects what I imagine, that is resolving to numpy advance indexing.
However, in the case that `col` is a column in the data (e.g. in my test or in the proposed solution) it's almost certainly that `self._is_mixed_type == True`. So you are right that it is just a for loop as of now. Still, it's faster than the proposed `melt` method.
On another note, this is certainly new for me
> `np.unique` obviously will support only a very limited number of Pandas datatypes
can you provide some details?",
"> `np.unique` obviously will support only a very limited number of Pandas datatypes, so it's probably a non-starter too. Numerical programming using Numpy is different from more generic dataframe programming using Pandas.
I confirmed that `pd.factorize()` works and is much faster than `np.unique`, at least in my test above:
``` language=Python
%%timeit -n 10
idx, cols = pd.factorize(df['col'])
df.reindex(cols,axis=1).to_numpy()[np.arange(df.shape[0]), idx]
# 48.4 ms Â± 716 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```",
"I think it is premature to deprecate `lookup` function. At the same time, the documentation for `melt` function is too superficial. For example when I read the documentation ([1](https://pandas.pydata.org/docs/reference/api/pandas.melt.html), [2](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html?highlight=melt#looking-up-values-by-index-column-labels)) I do not understand how I need to port my code from `lookup` to `melt`.
I do simple things. Here is a simple example:
```python
>>> index = pd.Series(['2020-01-04', '2020-01-03'], index=['A', 'B'])
>>> df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]}, index=['2020-01-03', '2020-01-04', '2020-01-05'])
>>> index
A 2020-01-04
B 2020-01-03
dtype: object
>>> df
A B
2020-01-03 1 4
2020-01-04 2 5
2020-01-05 3 6
>>> values = df.lookup(index.values, index.index)
>>> values
array([2, 4], dtype=int64)
>>> pd.Series(values, index=index.index)
A 2
B 4
dtype: int64
```
What should I do to use `melt` instead of `lookup`? And why can't I use high-level lookup API like before? Maybe I just stupid, but I do not want to think about it and dive into ornate concepts, I just want to implement the algorithm in high-level clean code. I want to keep my code simple and the code should clearly express my **intention**.
The code example from SO is tricky.
```
df['new_col'] = df.melt(id_vars='names', value_vars=['a', 'b'], ignore_index=False).query('names == variable').loc[df.index, 'value']
```
Ask yourself what **intention** it expresses. This is some kind of esoteric gibberish without reference to the subject area. The code is too low-level and verbose compared to the simple and concise `lookup(rows, cols)`.",Bitter Frustration
"> @impredicative you are not comparing apples to apples here. try with indexing operators, we _already_ have `.loc`, `.iloc`, `.at`, and `.iat`, not to mention partial slicing and of course `[]`. which one shall we cut to keep `.lookup`????
@jreback I think it's you that not comparing apples to apples here: **none** of those APIs you listed offer `lookup` functionality (otherwise, you wouldn't have used `melt` as an alternative). `lookup` is a direct translation of `arr[list_1, list_2]` from Numpy to Pandas. The Numpy API is there and it looks like it will stay for a while, why remove Pandas' equivalent.
The example that @espdev listed in his comment above is a more common case for the use of `lookup`. And from that example, I agree that it's really is unnatural to think `melt` at all, let alone using it.",
"First off, I can confirm I can reproduce the performance difference from https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773374707, even without the lambda function (this is all on the v1.1.4 tag):
```ipython
In [58]: %%timeit -n 10
...: df.melt('col', ignore_index=False).query('col==variable')['value'].rein
...: dex(df.index).to_numpy()
...: ...: 671 ms Â± 17.5 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
In [59]: %%timeit -n 10
...: df.lookup(df.index, df['col'])
...: ...: 290 ms Â± 2.79 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```
Second, is there a case when the solution in https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773477244 would not work well enough? If not, should the user guide just be updated to use that and we can move on from this issue?
Third - @impredicative please consider your tone",Impatience
"> Third - @impredicative please consider your tone
My tone is justifiable. I have had enough of Pandas being unprofessionally developed. Panda works only for simple manipulations of small dataframes, and scales extremely poorly or not at all. I have spent weeks and weeks trying to work around its various issues, and the best thing for me to do right now is to move to a real package like Dask, PySpark, etc. I'm sure that many other users here feel similarly.
Fourth - @MarcoGorelli please consider how Pandas is actually developed.",Bitter Frustration
"If anyone wants to take this forward with a respectful tone (e.g. constructive comments like https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773374707 ), then feel free to open a new issue - closing and locking as this is not the way to have a productive discussion and several comments have been off-topic",
thanks @MarcoGorelli,
"#### Is your feature request related to a problem?
I seriously get the impression that Pandas is actively being sabotaged. As a case in point, `DataFrame.lookup` was deprecated in v1.2. The problems with this are:
1. The [doc for `DataFrame.lookup`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lookup.html) says to see itself for an example. Well, there is no example in the doc page. The only example I'm aware of is [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels) instead which is a different page.
2. The [changelog](https://pandas.pydata.org/docs/whatsnew/v1.2.0.html#deprecations) for this deprecation points to [GH18682](https://github.com/pandas-dev/pandas/pull/18682) which looks to be wholly irrelevant to this change. A lot has gone wrong here.
3. There is significant user code that already uses `lookup`. Why break it? If the current implementation of `lookup` is suboptimal, shouldn't it be optimized instead?
4. It is extremely complicated to use `melt` when `lookup` works quite simply. For example, compare [this simple answer using `lookup`](https://stackoverflow.com/a/45487394/) with [this complicated answer using `melt`](https://stackoverflow.com/a/65722008/).
Does nobody review changes, docs, and release notes anymore prior to the release? It looks this way.
#### Describe the solution you'd like
1. Optimize `lookup` if attainable using `melt` or otherwise.
1. Undeprecate `lookup`.",Bitter Frustration
May be worthwhile to note that the performance overlay from step 3 should be turned off again when recording the trace in observatory in step 5 as that messes with performance.,
"The performance overlay is probably not the best thing to recommend here - it actually does incur some performance overhead itself, and on some GPUs it's notable how much. It'd be best to just run the tracing and read that.",
"Triage: Adding team labels so this doesn't come up on the ""untriaged"" report.",
"This is a meta-issue to track reproducible reports of jank in Flutter apps.
If you are experiencing jank in your app:
1. Try to reproduce the problem in a test app. Either run `flutter create janktest` and recreate the situation you are experiencing in that app, or clone your app and delete code until you have the jank reproducing with a single .dart file.
2. [File a bug](https://github.com/flutter/flutter/issues/new?assignees=&labels=created+via+performance+template&template=5_performance_speed.md&title=) and include your .dart file demonstrating the problem. If you need more than just a .dart file (for example, assets are needed to reproduce the issue, or plugins/packages are needed to reproduce the issue) then create a GitHub repository and upload the app there.
Make sure to include the `flutter doctor -v` output and any logs from `flutter run` and `flutter analyze`.
3. Switch flutter to master channel and run this app on a physical device using profile mode with Skia tracing enabled, as follows:
`flutter channel master`
`flutter run --profile --trace-skia`
The bleeding edge master channel is encouraged here because Flutter is constantly fixing bugs and improving its performance. Your problem in an older Flutter version may have already been solved in the master channel.
4. Record a video of the performance issue using another phone so we can have an intuitive understanding of what happened. Donâ€™t use ""adb screenrecord"", as that affects the performance of the profile run. Attach the video to your bug.
5. Open Observatory and save a timeline trace of the performance issue so we know which functions might be causing it. See ""How to Collect and Read Timeline Traces"" on this blog post:
https://medium.com/flutter/profiling-flutter-applications-using-the-timeline-a1a434964af3#a499
Make sure that the performance overlay is turned OFF while recording the trace.
Attach the JSON file containing your trace to your bug. You may also wish to include a screenshot of the part of the trace showing the problem you are seeing, just so that people can see at a glance what kind of performance issue the bug is about.
6. Mention _this_ bug in your bug, so that GitHub includes a link to it here.
Please avoid commenting on this bug. Keep each issue separate so that we can examine each specific problem individually. Having one issue that contains comments about multiple problems make the issue intractable.",
"Since we created the performance issue template in issue creation, this hasn't ended up finding any use, so I'm going to close it.",
This meta issue has been so useful for the triage team.,
@maheshmnj good to hear! in what way?,
"It has helped us when triaging performance issues to attach the timeline trace with the triage response and it also acts as a nice guide to point to authors of the issue and everyone else. Some sample comments where it has helped us
https://github.com/flutter/flutter/issues/104709#issuecomment-1139451006
https://github.com/flutter/flutter/issues/87811#issuecomment-900019236 (I got to know about this issue here)",
"No. Better look on the page with releases, than number of issues.",Impatience
"> No. Better look on the page with releases, than number of issues.
i already looked. total 19 issues solved in 2020. And there is 1k issues on tracker. I'm needing an answer from the project owners not from contributors.",Bitter Frustration
"@mdogancay This project is open source. Feel free to help out by confirming issues, providing test cases, fixing bugs. If you watch the repository you will see that there is quite some activity, but more people helping out will help reduce the backlog of existing issues.",
"> @mdogancay This project is open source. Feel free to help out by confirming issues, providing test cases, fixing bugs. If you watch the repository you will see that there is quite some activity, but more people helping out will help reduce the backlog of existing issues.
@dbrumann Thanks for the answer. The reason I'm asking this is because thousands of people are still actively using these projects. If the project is losing momentum (which it seems) we have to move towards an alternative choice. Developing a product with a dying project can result in serious financial losses! This is a serious situation. I trust the project managers to explain the situation when necessary.",Bitter Frustration
"> The reason I'm asking this is because thousands of people are still actively using these projects. If the project is losing momentum (which it seems) we have to move towards an alternative choice.
You don't necessarily have to move. You could also incentivise contributors and maintainers by supporting their efforts either financially or through active participation.
> Developing a product with a dying project can result in serious financial losses!
The fact that there was a release roughly a month ago (2020-12-04) should tell you that the project is not inactive. The work on an upcoming 2.9.x release could also help indicate at some activity. Not to mention the activity in sister projects like dbal and the releases there.
If you consider this project vital it might make sense to let the maintainers work on it instead of wasting their time with frivolous claims or requests of a risk assessment for your projects or whatever this is supposed to be.
If you have an alternative that you feel is more stable and a better choice for your project, feel free to switch to it. Otherwise I suggest you put your energy in supporting the dependencies that you feel are vital to your projects so they don't become ""dying projects"".",Bitter Frustration
Let me close this as a way to show you that we do have a look at issues.,
"> Let me close this as a way to show you that we do have a look at issues.
@dbrumann , @greg0ire I am speaking the truth here, but you are making politics. Instead of looking for a solution to the problem, you're treating me badly and trying to silence me. I'm sorry, but I don't think you are ""good people"" anymore ... Yeah, with your behavior, it became clear why this project is Dying ... Goodbye FOREVER...",Bitter Frustration
"I am sorry you think that way. I admit I was snarky, but if you read back your title and initial question, I hope you will realize that you are accusing a bunch of people who spend their free time on a project that you rely on on not doing enough - for you, for free. I think that is rude, which is why I reacted as I did.
As to your concerns, I think I have made it clear that your premise of not much activity doesn't hold up. Yes there are open issues and some of them old, but this is not uncommon and it's not the only activity metric (as shown in my previous comment). If you look at projects like symfony/symfony you will see they have quite a few open issues as well, some of them dating back to 2012, almost 10 years, but you could hardly call Symfony dead or inactive.
Again, if these open issues concern you, you should help out and look at them, help verify them maybe produce a test case and even fix them. This, in my opinion, is the only way to address the backlog constructively.",Bitter Frustration
"@mdogancay Doctrine's code base is over 10 years old and heavily matured. While we do have 1000 open issues, when I worked through 100 of them in December only a small number was actually bugs, most of them are help requests and users wrongly using or misconfiguring. A lot of issues are feature requests that we tend to be conserative about and not consider.
The complexity of Doctrine often makes it very time consuming finding the ultimate problem of the user. We provide free software, not free support. Yes, not much is happening code wise, but that also means not a lot of stuff is breaking or requires your constant attention, getting to the new APIs.
There isn't an ORM in PHP that comes even close to features that Doctrine ORM has. Due to the nature of ORMs being leaky abstraction, it is my firm believe working on ORMs for 15 years now, that this feature richness comes at the price of complex and sometimes ""fragile"" internals (you can trade this off for better internals at low performance). This proves to be a high hurdle for new contributors and it takes a while to understand the code to understand the side effects of a change. As such you should be happy that we are keeping it working as is.",Bitter Frustration
"- [x] clickbaity title, with word in all caps
- [x] IS PROJECT DEAD
- [x] 0 contributions to the project ([1 attempt](https://github.com/doctrine/doctrine-website/pull/379), to be fair)
- [x] wants to talk to ~your manager~ the maintainers, not the contributors
- [x] maintainers are bad people
> Goodbye FOREVER...
![that was always allowed](https://comb.io/oCQ8gV.gif)
I don't know why people in this thread are dignifying OP's post with the lengthy and thoughtful answers. All this post deserves is to be ~silenced~ locked. OP can come back after calming down and finding the right tone to phrase his otherwise valid concerns. Because yes, tone matters.",Insulting
Look at the ORM on github; whats going on? is this project dead? not actively maintaining by project owners anymore?,Bitter Frustration
"@2011 Thank you for raising this! I'll mark it for review as part of our ongoing development of RStudio.
@kfeinauer Could this have something to do with introducing the internal database?",
"@ronblum Yes, this came about when adding database support. The dependency should not be necessary on the desktop, but due to how we deal with dependencies a shared component that depends on this library is pulled in by both server and desktop. This requirement could be removed in a future release.",
@kgartland-rstudio Thanks! I'll switch this from a bug to an enhancement request.,
"### System details
RStudio Edition : Desktop
RStudio Version : 1.4.1103
OS Version : Linux 5.4.80 #1 SMP GenuineIntel GNU/Linux
R Version : 3.4.1
### Steps to reproduce the problem
Version 1.3.1093 did not dynamically link to libpq (from PostgreSQL), but 1.4.1103 does.
One can verify this by simply running ldd /path/to/rstudio.
### Describe the problem in detail
The Release Notes at:
https://rstudio.com/products/rstudio/release-notes/
mention that RStudio Server Pro now requires a Postgres database, but says nothing about RStudio Desktop (and lacks any explanation why the Desktop version would require a database).
### Describe the behavior you expected
I would expect database connectivity to remain in the odbc package, and not pushed into the basic desktop. I strongly object to requiring something not found in a typical desktop setup, and request that if this dependency remains, you provide the necessary libraries with the RStudio Desktop installation (as you do with the Qt libraries and icu - both of which most Linux desktop machines already have).
- [x] I have read the guide for [submitting good bug reports](https://github.com/rstudio/rstudio/wiki/Writing-Good-Bug-Reports).
- [x] I have installed the latest version of RStudio, and confirmed that the issue still persists.
- [x] I have done my best to include a minimal, self-contained set of instructions for consistently reproducing the issue.",Bitter Frustration
"I'd like to suggest this is, in fact, something closer to a bug, and the previous label should be restored.
I understand why it was addedâ€”RStudio Server 1.4 uses a database in order to manage internal data. However, the documentation shows two options: SQLite and Postgres, with SQLite is the default. Requring a Postgres dependency _when it is not even being used_ poses a significant installation burden in many contexts.",Bitter Frustration
I'm attempting to install rstudio-2021.09.1-372 and am using R 4.1.0. Is there really no way to get around the postgres requirement? We support a shared high performance computing environment and do not install databases. Is there a way to install just the libraries necessary to get the rstudio install to complete? Any advice is greatly appreciated!,Impatience
"@dsajdak are you installing desktop, or server? What platform? I'm not understanding why this is a concern; you shouldn't have to install a database to get it to work.",Impatience
"Note: I believe we're requiring an installation of `libpq5`, but not all of PostgreSQL, when installing RStudio Desktop in a Linux environment.",
"> Note: I believe we're requiring an installation of `libpq5`, but not all of PostgreSQL, when installing RStudio Desktop in a Linux environment.
WHY? I have used RStudio desktop for years, and it never had this requirement before version 1.4.
1. RStudio desktop should not require this library.
2. Database connectivity should remain in the odbc package.
3. Typical desktop installations don't have this library (including mine - I would have to compile the entire PostgreSQL to upgrade to RStudio 1.4).
4. If you really feel that RStudio desktop absolutely **has** to have libpq, then why don't you provide it in the package? The installation package includes libraries for Qt and icu, which virtually every Linux desktop has. It makes absolutely no sense whatsoever to include those libraries with RStudio, yet demand users produce their own libpq library.",Bitter Frustration
@2011 I was merely clarifying the issue.,
And thanks to both of you for that! I hope the exchange helps illuminate the matter better. I have personally solved the issue for my application but I still believe it is a problem that should be addressed.,
"@mcg1969 Darn it, I thought I had switched this back to a bug back in May in response to your comment! Changing now.",Bitter Frustration
"The original thought in making `libpq` required is that it would be available to easily install via the system package manager when installing the RStudio `deb`/`rpm`. Clearly that is not always the case, and I apologize for the inconvenience this has caused you.
The `libpq` library has quite a few dependencies itself, so I think bundling them all and shipping them with RStudio is error-prone. I think we should allow for dynamic loading of the `libpq` library on Linux. SOCI (the database manager we are using) has some support for this (http://soci.sourceforge.net/doc/release/4.0/connections/). Otherwise we'd need to build `libpq` on all platforms and statically link to it, which could be tricky.",
"@mikebessuille > @dsajdak are you installing desktop, or server? What platform? I'm not understanding why this is a concern; you shouldn't have to install a database to get it to work.
I'm attempting to install desktop version 2021.09.1+372, for CentOS 7, from source. We install everything to a shared mounted filesystem and can not install using OS packages. I spent 2 days getting all the dependencies installed in the shared filesystem only to have the rstudio 'cmake' fail with:
`CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
Please set them or make sure they are set and tested correctly in the CMake files:
/shared/directory/.../PQ_LIB
linked by target ""rstudio-core-tests"" in directory /shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
linked by target ""rstudio-core"" in directory/shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
SOCI_POSTGRESQL_LIB
linked by target ""rstudio-core-tests"" in directory/shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
linked by target ""rstudio-core"" in directory /shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
`
I was hoping someone would have a suggestion as to how to get around this without a full Postgres install.",Bitter Frustration
"@dsajdak You need to have the `libpq` and `libsoci_*` files under your shared directory. Then you need to inform cmake where to look by specifying the `CMAKE_LIBRARY_PATH` environment variable. See https://cmake.org/cmake/help/latest/command/find_library.html and https://cmake.org/cmake/help/latest/variable/CMAKE_LIBRARY_PATH.html#variable:CMAKE_LIBRARY_PATH for more information on how this is found. We find this library here in the CMake file: https://github.com/rstudio/rstudio/blob/5e180632285fd095c6f01ff385935a6efded7e0a/src/cpp/CMakeLists.txt#L442-L456
You can use our `install-soci` script to build SOCI. It's designed to run at certain paths that we use for our builds, so it might not work out of the box for you. https://github.com/rstudio/rstudio/blob/main/dependencies/common/install-soci. Particularly, you may need to specify the path to `libpq` to get it to build correctly. This can be done by adding it to the `CMAKE_LIBRARY_PATH` here https://github.com/rstudio/rstudio/blob/5e180632285fd095c6f01ff385935a6efded7e0a/dependencies/common/install-soci#L83",
"@kfeinauer Thank you! I'm unable to get to the rstudio links you shared. Perhaps they're in a private repo? I am currently testing this with installing the postgres-devel and soci-postgresql-devel rpms on my build host. If these libraries/packages are not required to run rstudio desktop, this should work. If they are, then I'd rather your suggested method.",Impatience
"@dsajdak You'll need to build SOCI from source because we statically link it, so you need `.a` files instead of the `.so` that are shipped by the packages. As for `libpq`, you can get it from `postgres-devel` but you should also be able to get it from `libpq5-devel` (much smaller package).
Sorry about the visibility of the links. I have edited my comment to fix them.",
"> The `libpq` library has quite a few dependencies itself, so I think bundling them all and shipping them with RStudio is error-prone.
Hard for me to believe that bundling those exceeds the complexity of bundling `libQt5WebEngineCore`. :)",Mocking
"@MariaSemple I strongly object to the changes you made on 0110. I filed the bug, and (for me) it has absolutely nothing to do with bulding RStudio from source. I can't run the (version 1.4 and later) packages you supply because of the (new) dependency on libpq. I do not view fixing this as an enhancement - it obviously represents a bug (something working before that no longer works).",Bitter Frustration
"I agree, @2011
@MariaSemple , I understand that the RStudio maintainers may have simply concluded that they are going to be unable to address this issue to the satisfaction of its reporters. I have plenty of experience disappointing my users, too :-) it happens! But I think that preserving the historical record is important. This was never about the need to build from source. I think the proper course of action is to revert the title of this issue but to mark it as a ""wontfix"" if indeed the RStudio team has decided it is not feasible to solve
Thank you all!",
"I believe I misread some of the context of this issue, apologies for the confusion. I will change the title to ""Remove libpq dependency in RStudio Desktop""; however, I will leave the tags as they currently are. While I understand that it is frustrating to not be able to use the newer version because you are unable to install the new dependency, the new dependency is not a bug as the addition of the dependency was intentional. To remove the dependency now would be considered an enhancement.
`libpq` was introduced as a dependency due to RStudio Server requiring it. RStudio Desktop and RStudio Server share much of the same code, and as a result the dependency was introduced on both sides. The dependency is not related to the Connection Pane feature or the ODBC Drivers that it uses. It is non-trivial to remove the dependency and introduces some regression risk. After discussing this with @kfeinauer I propose the following potential solution:
* Always explicitly link against SQLite libraries (which are used by all versions of the product).
* Never explicitly link against PostgreSQL libraries - instead use SOCI's feature that allows for dynamic loading of DB libraries through a call to `dlopen`.
If we choose to go this route, we will need to ensure that there is a very clear error message if `libpq` cannot be found, and that it occurs very early in start up of RStudio Server.",
"Just to emphasize (and perhaps clarify) some things, I don't consider this a ""documentation"" issue at all. RStudio simply will not start without the missing library (with a message that it can't find libpq). I don't see any need for additional documentation on the matter.
I do want to quote @kfeinauer (20210219):
> The dependency should not be necessary on the desktop, but due to how we deal with dependencies a shared component that depends on this library is pulled in by both server and desktop. This requirement could be removed in a future release.
To repeat the points I made on 20211221, RStudio already comes bundled with more than 25 libraries (Qt and icu), most of them completely superfluous. Virtually every linux desktop has those libraries already, but RStudio supplies additional copies of them (I won't condemn that, because bundling versions known to work does add some value at the cost of disk space and memory use). But the one library many desktops do not have installed by default (libpq) gets left out. No offense to you (I doubt you personally made the associated decisions), but I would have have to place this in the top two software engineering idiocies that I have seen over the past five years.
If desktop users **have** to have libpq (and I imagine that the vast majority of them would find themselves content with SQLite), then bundle it (or bundle a stub library that allows RStudio to start, but then doesn't allow the choice of PostgreSQL as a database without libpq. I just don't understand the thinking on this (other than ""we want to make things more difficult for our users"").",Bitter Frustration
"Your perspective is much appreciated; please maintain a professional tone.
Coding decisions including dependency decisions are not made frivolously. Sometimes, design choices are made to minimize development cost, maximize maintainability, reduce testing cost, or to have common code between the many versions of our product (server, desktop, pro, open source). As such, this issue is not a bug. Nor is it an ""idiotic"" design choice. We will certainly consider improving it such that it doesn't affect the case you describe, keeping in mind that most of our users are unaffected by this, as they install the IDE from installers (which take care of the appropriate dependencies), rather than by building from source or by unpacking a tarball and manually installing.",
"I appreciate the moderation. While I do agree this should be fixed, it is far from the worst thing I have encountered :-) in fact, I've done far worse myself! Thank you for correcting the title and categorization.",Irony
"This is resolved in [RStudio Desktop 2023.12.0-daily+212](https://dailies.rstudio.com/) for an upcoming release, covered in issue #13676. Please note that the dailies are meant for testing purposes only, not production.",
yeah thats a great idea bro,
"@enyoghasim I am no maintainer of this repo and do not have the power to do releases, so I am unsure why I was added as asignee? Considering you are part of the express organization, is there any chance you could rally your peers to look into this?",Impatience
"Hi!
As an example Node.js and https://nodejs.org/api/fs.html is starting to recommend promise-based versions foremost. (compare with earlier versions that has the promised based versions last: https://nodejs.org/docs/latest-v14.x/api/fs.html).
Express 5.0 has added automatic handling in routes of async/await routes that will automatically invoke the Express error handler if a an async function throws. There are current workarounds to create a `wrapAsync()` function around routes, but in bigger projects some users miss this and it's also not-so-nice-looking (and confusing for some).
I am merely wondering if Express 5.0 could be released, and current items on https://github.com/expressjs/express/milestone/11 could be moved to a 6.0 release? I am very respectful of the fact that you voluntarily work on this, and these kind of questions are frowned upon. https://github.com/expressjs/express/pull/2237 was created 7 years ago now, so I at least think it should be ok to ask for this without in any way not being grateful of this project already and its release cadence.
I guess what I am saying is that there are some nice things already on the main branch for Express 5.0 that works good with latest versions of Node.js 15, so a release would be amazing.",Impatience
"@dougwilson do you have anything to add to this? You are certainly doing a great job with maintenance of all the subdependencies etc of Express. But if Express 5.0 alpha 8 has not received any complaints of serious bugs, what are you feeling about this topic?
Also the answer by @enyoghasim concerns me a bit: I am not sure if it is sarcasm or what, but yeah.",Impatience
"Hi @thernstig I'm sorry for one of the comments in the thread; I went ahead a hid it for now.
As for your idea, that is pretty much what the TC decided, even though there is still a lot of flac from the community about deferring things like http/2 to express 6. Really, there is only one outstanding issues for express 5, and it is related to async/await in the router. There is an open PR to address this, but the author has kind of faded away. Ideally that's really all that needs to be fixed if the idea is for express 5's main goal to be async/await.",
"@dougwilson thank you very much for answering, I know you have a very busy schedule.
Any chance you could champion taking over that PR to get it merged?
I agree with you and the TC. I think it is perfectly fine to release 5.0 and then later HTTP/2. There is always some ""can you just add this as well?"". I personally do not think there is anything stopping release 6 being a much smaller release with just having HTTP/2 as the main focus. That way it really should not matter much to users who want HTTP/2 get an ""intermediate"" 5.0 release.
Articles like https://dev.to/romainlanz/why-you-should-drop-expressjs-in-2021-711 make me sad because 1) much of the article is not true 2) Express is being actively worked on but some people do not explore enough to be aware it is.",
@dougwilson @thernstig i am sorry for the hidden comment,
"**tl;dr** HTTP/2 is probably less important than potential instability in a codebase due to unhandled promises.
Completely anedoctal, but I tested a migration of our API to Koa and Fastify. They're both perfectly fine and have some nice advantages over Express. But you know, I'm back to Express anyway. I like the stability of the project.
That said, I find that working around `UnhandledPromiseRejection` has become a nuisance. It's just less reliable than knowing my framework of choice is battle tested to handle that for me.
Having Express 5 without HTTP/2 is, for us, a no-brainer ðŸ¤·",Bitter Frustration
"I feel the same way as @filipecatraia, having tried Koa but moved back to express for similar reasons. I would also prefer not to have to wrap all of my request handlers in `asyncHandler`.
@dougwilson you said:
> Really, there is only one outstanding issues for express 5 and it is related to async/await in the router.
Would you mind linking to the issue in this thread for anybody else following along who might like to take a shot at it?",
"@dougwilson, could you share the only issue holding off the release?",Impatience
"Leaving a few useful links here:
- Pending items for v5 release: https://github.com/expressjs/express/pull/2237
- Open PRs related to 5.x: https://github.com/expressjs/express/pulls?q=is%3Aopen+is%3Apr+label%3A5.x
- Open issues related to 5.x: https://github.com/expressjs/express/issues?q=is%3Aopen+is%3Aissue+label%3A5.x",
"@aravindvnair99 those links do not help much I am afraid. @dougwilson stated there was only one outstanding issue for 5.0. Meaning it seems the rest could be postponed.
It is clear @dougwilson is very active on Github, and seeing there is only one outstanding issue for the release, it feels natural to focus on that PR i.e. take over what the original author of that PR started. The focus seems to be on updating other things, which maybe could be held of till that one thing stopping Express 5.0?
The community has been waiting for many years quite patiently for the release, and has helped along the way with PRs and issues. It would be great to finally after these years make the community happy with a new release ðŸ˜ƒ",Impatience
"@thernstig I agree. My intention though was just to mention the links here for anyone else who stumbles upon this issue tracker.
I must admit I too am excited for a v5 release. Express has always been about stability in my opinion, but yes I look forward to v5. It's long been in the development stage.
And as for what @dougwilson was referring to, I'm not sure which issue that is exactly. My guess are on this one: https://github.com/expressjs/express/pull/4321",
"@dougwilson, could we get some feedback from you? Asking with my nicest voice possible, is there no chance you could fix the last outstanding problem if you know what to do? It feels every time this is brought up lots of other things are fixed instead of this. I am unsure why, it almost feel like releasing 5.0 is shunned for some reason ðŸ˜ž",Bitter Frustration
"The author of the PR is back with us and was just working on it the other day. When someone is working on aomething, I don't feel it would be appropriate if I just did it myself and then pushed my own code and then closed their PR, would you? I don't think that would be a great way to keep people contributing. Since they are now interacting with the PR to finiah it, I think that is fine.
@aravindvnair99 is a triager and even provided various links here. We have the triagees so, among other things, I don't have to respond to every single comment across all the repos and can do other tasks.",
"A beta.1 has been released with the referenced router changes ðŸŽ‰ . Please check it out http://expressjs.com/en/guide/migrating-5.html
Since this changed the route path syntax a bit, I'll let it get maybe a month for feedback before official release.",
"I plan to rename the master branch to ""main"" in the upcoming weeks. The official announcement is here: https://github.com/dotnet/announcements/issues/172.
I'll likely wait until after the current sprint of ClrMD work I'm doing and ship a release before I randomize the branch with the rename.",
"As far as I'm aware, the dust recipe is intended to be more efficient, which is consistent with other alloy recipes being less efficient in the smeltery. There is also a quest which tell you to use the dust recipe because it's more efficient.",
"This is a major slowdown so I don't think it should be changed lightly. When aluminium gravel is not available, this means 405 aluminium oreberries need to be grown just to make the tool forge. That's a VERY long time spent waiting and I'm not sure we want that.
Also makes alumite totally nonviable as a tool material - its stats are only slightly better than steel",Bitter Frustration
"quest backing up likely unintended feature is not a good proof of intention since quests get rewritten a lot. Previously tool forge took MV to get due to 3 alum screws so i still dont see the point, we made tool forge just fine back then by either waiting or foraging for aluminium ingots",Bitter Frustration
i wouldnt mind if it would be an alloy recipe (read as: require some actual progress rather than a piece of flint and some stone) but its shapeless recipe that can then be just smelted in the furnace,Bitter Frustration
"even in alloy furnace it would be way too op, 4.5x more efficient than smeltery recipe is dumb. next PR i will likely look at other recipes so they dont give such insane boosts in efficiency (im looking at you obsidian ingot)",Bitter Frustration
"So I'm not saying let's not change this at all, but this needs some discussion before making a change, since this has a large impact on current early progression",
"i believe shaped recipe should stay as in PR. maybe we should add slightly more efficient alloy recipe, say 3 ingots instead of 2 (50% efficiency boost)",
and reform other alloy recipes that are replacement for smeltery recipes that give too big of a boost to that amount too.,
"I think that the smeltery recipe is too inefficient for how alumite is used in GTNH, and probably shouldn't be used as a baseline for balance in its current state. It's currently a noob trap which wastes aluminium rather than a viable recipe.
I think that the dust mixing recipe should make 6 alumite, and the smeltery recipe should make 4, vs the 9 and 2 currently.",Bitter Frustration
"I agree with kilojoel here. Also, the comparison to the tool forge being MV is ludicrous. Why would you prevent people from having hammers for that long? They'd probably just use auto-miners instead, and the hammers wouldn't see use.
> quest backing up likely unintended feature is not a good proof of intention since quests get rewritten a lot
The first half of your sentence is a garbled mess. We aren't talking about backups?
Since I didn't realize the dust recipe existed when I played (which you have now made the same) I never even used alumite tools (aside from maybe my pickaxe, to get the next mining level, which I believe I changed after I repaired it the first time), since they were too expensive. And I had the gravel, too. This indicates that with a recipe that only gives two parts, it's too expensive.",Bitter Frustration
"> The first half of your sentence is a garbled mess. We aren't talking about backups?
@Prometheus0000 if you dont know english well enough then at least use google and/or common sense:
![image](https://user-images.githubusercontent.com/5310658/111078870-40e25100-8500-11eb-8e90-e5dc3b618ad6.png)
in my original sentence ""backing up"" is clearly used as ""supporting""
>Also, the comparison to the tool forge being MV is ludicrous. Why would you prevent people from having hammers for that long? They'd probably just use auto-miners instead, and the hammers wouldn't see use.
Yet we used hammers for years before people decided that 3 alum screws is ""too hard""",Bitter Frustration
"I'm a native english speaker. And I say it doesn't make sense.
> **The** quest**book** backing up **a** likely unintended feature
^ Would make sense.",Bitter Frustration
"Guys, either open a new issue for the balancing, or unmerge the https://github.com/GTNewHorizons/NewHorizonsCoreMod/pull/161. This issue can't be labeled as RFC if changes are already pushed in code.",Impatience
"**whole questbook backs up a single thing** vs **actual quest inside that questbook that mentions this particular recipe** sure. either way, return fixed in dev tag, commit is merged. I deem this issue closed as its linked PR is merged. Balancing by adding separate recipe should be its own ticket with its own linked PR.",Impatience
Dream will close it after the release.,Irony
Maybe add an LV mixer recipe that yields 9 dusts if it doesnâ€™t exist already. That would incentivize a tech progression.,
continue discussion in #7629 @boubou19 lock this please,Impatience
"#### Which modpack version are you using?
Any upto 2.1.0.4
#
#### If in multiplayer; On which server does this happen? Same for both SSP and SMP
#
#### What did you try to do, and what did you expect to happen?
Alumite crafting related oversight. In the smeltery 2 parts of alumite are made by combining 5 parts aluminium to 2 parts steel to 2 parts obsidian.
![image](https://user-images.githubusercontent.com/5310658/111072193-226d5d00-84e2-11eb-8353-747df3f4a2c7.png)
#
#### What happened instead? (Attach screenshots if needed)
When crafted by hand you make 9 parts of alumite (4.5 times more than in the smeltery) with same amount of materials.
![image](https://user-images.githubusercontent.com/5310658/111072230-4f217480-84e2-11eb-89d6-297b0e04306a.png)
#
#### What do you suggest instead/what changes do you propose?
Merge [(CoreMod) #161](https://github.com/GTNewHorizons/NewHorizonsCoreMod/pull/161) to equalize both recipes to 5:2:2 makes 2
#
#### What is your GTNH Discord username?
SirFell#1024",
"Hopefully it is just a matter of changing the TFM to `net-android` and `net-ios` instead of `xamarin.android` and `xamarin.ios`. As for MAUI support, it might not be the first thing to be supported, but the new TFMs most definitely.",
"@Cheesebaron Thanks for responding so quickly. So it seems that support for Xamarin.Native (as it called today) will continue in MVVMCross beyond the introduction of MAUI.
Forgive my ignorance what does TFM stand for?",
TFM stands for Target Framework Moniker ðŸ˜„,Irony
Thanks,
"I currently develop a mobile application using Xamarin.Android and MVVMCross.
My understanding is that Xamarin.Native (Xamarin.Android and Xamarin.iOS) will now be part of .NET 6.0 when it is released in November 2021
Will this affect the development of MVVMCross and will MVVMCross going to continue to support this.
It would be useful to have an idea of the way forward, just in case there is any significant work required in my application.
Any information would be really helpful",
@Cheesebaron May I know when (approximately) MAUI be supported? We starting new project and we'd like to move to MAUI once it will be released (desktop is one of our targets). We'd like to know if MvvmCross won't block us with that.,Impatience
"For now I don't have any plans on supporting something that is still in preview, I simply don't have time to support it. You can ask again when .Net 6 and MAUI is released.
So my estimate would be, unless someone contributes the support, would be _after_ .Net 6 and MAUI has been released, how long after. I don't know.",Impatience
"Thanks for prompt reply.
I understand your point. At least situation is clear for us now.",
"I'm migrating a xamarin forms app, which is using mvvmcross, to a MAUI app but when I do this, I get several errors about this framework. Does anyone know if mvvmcross is compatible with MAUI???.
Thank you",
We don't have any official support for MAUI. You are welcome to help contribute it.,
"Ideally, I would like to see direction where MAUI (Forms - because this is just Forms rebranded, perhaps with more resources applied to this) is just complementing MvvmCross development - where you use native dev with MvvmCross and use MAUI to implement easy views (like menu view, ""thank you/info"" view, registration etc..) - just to save some time.
Basically, all of the mobile platforms allows for modular implementation. We have Activities & Fragments for Android, ViewControllers for iOS, Pages for WPF etc... Therefore it is technically possible to mix that.
I think if you still use MvvmCross then probably, you've already know that ""single-shared-UI"" is utopia. Does not mean that Forms/MAUI is useless though.
I've just developed proof-of-concept for MvvmCross forms-bridge and will happily move that to Maui when it's stable. See: https://github.com/thefex/MvvmCross.FormsBridge.Template I'd like to push pull request for that but don't have access to Windows now (can't build) ;/",Impatience
"> We don't have any official support for MAUI. You are welcome to help contribute it.
I would love to help with the Mvx support for Maui. I haven't looked at the latest previews of .net 6, do you know if it is already usable/complete ?",Impatience
"I recently gave preview.6 a spin. Seems to be ready soon.
I think the main issue currently is to switch the pipeline to use the new .net6.0 stuff and preview versions of tooling.",
"its runs ok in emulators, once you start looking at getting it to run and deploy to actual devices you run into roadblocks, there is Zero documentation yet and you cant run or deploy the UWP targets yet in VS 2022 17.0.0 preview 3.1, bit of mess still, I and believe it will take a bit longer to get stable once they release it in November with .Net 6",Bitter Frustration
Any Update on this now that .net6 is released?,Impatience
"@OmegaRogue yeah, I have a branch and PR #4319 open. Just need to finish it up.",
"Hi @Cheesebaron one of my customer is asking to provide a plan on when we can start approching the migration to MAUI. Since our app is built upon MvvmCross is there any plan to start working as in Q1 2022 MS will release a RC?
Thanks",Impatience
"@domedellolio is your customer or you willing to sponsor the work needed for it? It could be either through a contribution or paying for the work required.
I don't have anything in my schedule right now to support it. I would rather want to focus on other parts.",Impatience
"Hi @Cheesebaron thanks for your feedback.
How much effort (ROM) do you think is required here? Also to evaluate a possible involvement / investment
BTW, as part of .NET Foundation will MAUI be part of future at some point for MvvmCross?
Thanks",
"The dotnet foundation doesn't dictate what we support or not.
There is currently only me working on MvvmCross from time to time. So I would prefer to keep the support surface as low as possible.
As for, how much time would be needed to support MAUI, it really depends on what kind of support you need and how deeply you want to integrate.
Since MAUI is using a different IoC container than MvvmCross, I would somehow need a way to support that or as has been planned, switch to the same in MvvmCross.
There is some ongoing work in the .net6.0 migration branch that would help with that, but still a lot work would be needed.
A very careful estimate would be around one weeks worth of work, but there are a lot of unknowns.",
"Hi @Cheesebaron , Any updates since your last message?
Could you also share the roadmap for the project please?
Thanks",Impatience
I don't want to promise anything. I think your best bet would be to just go with another framework. There is not much benefit of using MvvmCross with Xamarin.Forms Apps to begin with.,Insulting
"@Cheesebaron if I can add something, there are benefits with mixing Native MvvmCross and Forms (create some views with Forms and include them in native project) - but that does not require MvvmCross additional interop. Using MvvmCross (and especially wasting your precious time just to get Mvx.Maui framework) just to build full-Forms app indeed does not gives any benefits imo. However - is there a chance you briefly describe what needs to be done with .net6 MvvmCross branch? To get Maui views work in MvvmCross Native app, I'd need MvvmCross with net6.0. I might even pick and do those tasks alone + push PR but I probably will not touch this until official (not RC) Maui release.",Impatience
And what about `net6.0-android` and `net6.0-ios`?,Impatience
"That is on the way, last week I checked, still having issues with Nfloat which is blocking me, will try again now that 6.0.300 has been released.",Bitter Frustration
"""There is currently only me working on MvvmCross from time to time"", in other words, better stay away from the framework unless it already does everything one could possible want...",Bitter Frustration
"As a fellow framework dev, I just want to say thank you @Cheesebaron for your efforts. This is all free stuff you've carved out precious time to work for us, and that is appreciated.",
"@vedecoid with all due respect, is it fair to expect all that? Alternatively there's nothing stopping you or others from pitching in, as has been stated a few times.",
"> ""There is currently only me working on MvvmCross from time to time"", in other words, better stay away from the framework unless it already does everything one could possible want...
By all means stay away. No one is forcing you to use MvvmCross. However, your comment is off-topic, please leave that out of the discussion.",Bitter Frustration
"Off topic ? Someone is asking if you will support .net Maui and the response is : dunno, maybe if i feel like it because iâ€™m the only one working on it. I will for sure stay away. Started with mvvmcross 10 years afo on release 3.x. Then spend countless hours with every iOS upgrade that also required an mvvmcross upgrade to correct all the breaking changes. We stopped it already after v6 as it just wasnâ€™t worth it anymore. Also meant we had to rewrite our app from scratch.. Look, i have a lot of admiration for you guys doing all this work basically for free. But if you advertise a framework the way mvvmcross was advertised, with James Montemagno bringing it on Channel9 and beyond, then people expect there more than 1 very brave guy working on it.",Bitter Frustration
"> Off topic ? Someone is asking if you will support .net Maui and the response is : dunno, maybe if i feel like it because iâ€™m the only one working on it. I will for sure stay away. Started with mvvmcross 10 years afo on release 3.x. Then spend countless hours with every iOS upgrade that also required an mvvmcross upgrade to correct all the breaking changes. We stopped it already after v6 as it just wasnâ€™t worth it anymore. Also meant we had to rewrite our app from scratch.. Look, i have a lot of admiration for you guys doing all this work basically for free. But if you advertise a framework the way mvvmcross was advertised, with James Montemagno bringing it on Channel9 and beyond, then people expect there more than 1 very brave guy working on it.
"". Also meant we had to rewrite our app from scratch."" - so if you guys earn $$$ on your work, then spend some $$$ on capital investments into MvvmCross - otherwise you should not expect anything and move on to other tools - good luck though, there is no better way to develop mobile in Xamarin (pushed&managed&lead dozens of Mvx production apps, some of them for *big* institutions - no any issues at all) :) As far I recall Tomasz ( I used to develop bunch of things here few years ago ) never advertised this framework in aggressive way - it's been always given 'you get what you get, if you want to get something quickly do it yourself and make PR'.",Bitter Frustration
"It was a free app. This debate clearly illustrates the limits of the Open Source model. Unless youâ€™re willing to always have the risk of discontinuation, you better go with commercial tools. They at least care about their users out of necessity. Weâ€™ve learned our lesson.",Bitter Frustration
"And btw, Xamarin is history. Will go out of support in 2024.",
"Xamarin as a name might be a history but MAUI is not. As MAUI use native controls (native SDKs) to render UI that means it will not go out of support and native dev will be available -> Android/iOS sdks will have to be updated.
MvvmCross been heavily supported over many years, I think I've been using it since 2015? Mobile development (and the enormous pace SDK's evolve - in opposite to ex. backend/database development) - means breaking changes here are something expected and not unusual.",
"Well, then you better wake up to the music, because Tomasz, the only brave developer left said he doesnâ€™t see the need to support Maui, which started this whole debate.",Mocking
"You clearly don't understand what you are talking about :)
Tomasz said he will release .NET 6.0 support (which is required to build ""new"" MAUI toolset project/app). If he won't release that, I will pick and complete that. Therefore, native development will still be available. MAUI uses native SDKs and native tooling to produce builds.
MAUI-XAML-multiplatform support is a different story. If you want to go full xaml/full MAUI-views (as it was with Forms) then there is no really need/much benefit of using MvvmCross as framework. I (and many other devs) will keep building native apps anyway - I plan to create a bridge where I will consume some MAUI/XAML views in addition inside MvvmCross/native app. Right now, it's too early to bother, MAUI is RC, not production ready.",
"Good for you. And no, i do know what iâ€™m talking about. Ten years ago, using 3.x you had to to get anything working. Again, the thread started with somebody asking if the â€˜easy multiplatform mauiâ€™ would be supported. The right answer clearly is: thereâ€™s no need, so if that is the perimeter one needs, one better stays away from Mvx, which was my whole point. But i stay withnmy conclusion that a framework, no matter how good it is, is only supported by a handful or a single person, then better stay away from it",Bitter Frustration
"If you depend on Twig bridge 4, you need to require it explicitly in your `composer.json` file. The `require` section in the Twig bundle only states that the bundle works with both versions.",
"I do not depend on twig-bridge. The twig-bundle depends on it. If it wants to delegate this dependency to me, it should not declare it itself first place. I never `use` any of the twig-bridge classes in my App's code. In fact I only depend on `EngineInterface` of the templating component, which has the purpose of me not depending the implementation (=twig-bridge in this case). ps. adding twig-bridge in root is actually the way I workarouned this BC-break, but anyway...",Bitter Frustration
"As you mention @akomm (which is was @xabbuh suggested), you need to explicitly add `twig-bridge` in your root `composer.json` file.",
"But you do realize, that this is a workaround and not a fix?",Bitter Frustration
"This is the fix. If you depend on the `TwigEngine` class, you should require the package that provides it explicitly.",
"> This is the fix. If you depend on the `TwigEngine` class, you should require the package that provides it explicitly.
Did you just ignore my post? I do not depend on TwigEngine. I do depend on FrameworkBundle and TwigBundle. I also do depend on the EngineInterface from the templating component.
- FrameworkBundle utilizes the templating component to load an implementation (compiler pass)
- TwigBundle provides the implementation via TwigEngine class. It does depend on twig-bridge by extending the TwigEngine class from the bridge
- My app never messes with TwigEngine class of the twig-bridge. It even does not mess with the TwigEngine from the twig-bundle, but it depends on the bundle due to implementation provided
If twig-bundle assumes itself not a provider of the EngineInterface implementation, it should not depend on twig-bridge and delegate this to the consumer...",Bitter Frustration
"I don't think, we're getting anywhere here. As mentioned already, there isn't much we can do for you. You have been provided with an explanation of and a solution to your problem.
I'm locking this thread now. Thank you for understanding.",
"**Symfony version(s) affected**: 4.4.*
**Description**
`framework.templating` is deprecated in 4.3 and should be removed in 5.0. However, the feature is de-facto ""removed"" in 4.4 already.
As can be seen in dependencies here:
https://packagist.org/packages/symfony/twig-bundle#v4.3.11 -> symfony/twig-bridge: ^4.3
https://packagist.org/packages/symfony/twig-bundle#v4.4.20 -> symfony/twig-bridge: ^4.4|**^5.0**
Updating twig-bundle to 4.4.* also updates twig-bridge to **^5.0**. This is a breakage, because it removes the `TwigEngine` (implementation of `Symfony\Component\Templating\EngineInterface`) even if you have symfony/* version 4.4.*.
**How to reproduce**
mkdir test && cd test
composer init
composer require symfony/templating:4.4.* symfony/twig-bundle:4.4.*
verify twig-bridge is ^5.0 and there is no `Symfony\Component\Templating\EngineInterface`-implementation.
You can also do the same with flex-recipe constrained to symfony 4.4.*. It neither defines the twig-bridge dependency explicitly, hence gets it updated to the breaking version.",Bitter Frustration
"Thanks for your issue. The banner links to a long list of organizations. There are many to choose from that are doing good work and have nothing to do with the person in the article. Also, the movement is still important even if someone who helped found it does something morally suspect. But it's worth pointing out that the article you linked says that nothing is clear yet. If they did buy a house with donation money, you are right to be outraged, but that doesn't mean everyone should turn their backs on BLM. Regardless, issues related to the website do not go on this repo.",
"Donations are clearly being used for fraud...
https://www.msn.com/en-gb/money/other/blm-founder-is-branded-a-fraud-after-buying-a-1-4-million-home/ar-BB1fv0Zw",Bitter Frustration
"6-When doing some project in left hand side panel.the studio's flexibility is low,we can not add or remove studios on current projects to another which is contrary to real life as you can bring practical solutions to unexpected situations,wrong decisions etc...",Bitter Frustration
"7-the available studios in town always stabile;it may change every month or year;also the rents or selling price of the existing used studios should change over time
8-the other buildings dont have any role in the game,may be they can have some role
9-i didnt realize yet whether marketing is worth or not,the benefits are not very understandable right now;also there is a negative event when doing marketing campaign to increase hype that even at the end you produce a superb game the reviews are very bad,for this reason i am not doing marketing campaigns when producing games ...",Bitter Frustration
"i am not sure whether i posted my first comments to the right place so i re post them :
1-Is it possible to add ""Filter by name"" or Search button in sales report right hand side menu?
When you have so many games it takes too much time to find the game that you want and apply discount or update.**_
2-The bottom panels is too small now while the older version was better; but the left and right hand side panels are okey now because in older version they cover too much space.
3-The fans evolving threshold may perhaps be revised.Of course reality is important but in fact this is a game.When you open lots of studios and produce thousand of games you obviously reach limits quicker than it was foreseen,and it is not pleasant to see losing fans at this point while doing very good games...
4-The auto-furnish option's desk efficiency is very low: in 750m2 studio it places max 47 desks while you can manually place 70 desks easily with 100% happiness.In auto furnish mode you have to make adjustments as well because 100% happiness is not guaranteed.
5-The latest consoles of the game(each material latest technology) can have a cost greater than 1K $ while the game limits the selling price to 1K $ so it's a deficit in debut.",Bitter Frustration
"10-Is the star rating of the company working correctly? (right hand panel-list of companies section)
My average game rating is 86.5% and my company rating is 5 star over 8 star ? I remember that it was 10 star before this version and was working correctly..",Bitter Frustration
"sorry for this request but my eyes are really bad when reading and now i have difficulties to read the tooltips,the new font is very small at least for me (it was the case after today's update )...Is it possible to make it a little bigger?",
"11-the devs that you hire from staff menu are making very little progress in terms of training,if they have good design level they progress and make their level 99 quickly but their development and polishing levels barely advance,you play the game 45 years and studio average training remains around 65-70%.While the devs that you acquire from the job application mail quickly advance and reach 99/99/99. I dont know right now whether training level of the studio affects the game scores ? I assume that you receive good game scores usually,so the training level dont affect the scores too much like the game difficulty...",Bitter Frustration
"Hey,
Thank you for all those suggestions. I'm gonna keep an eye on those.
It is better to split your suggestions in order to have one suggestion per issue. I need to split it that way because it is easier to produce a changelog, for example:
## v1.2.5 includes
- issue #9876
- issue #9765
- issue #1269
For example, I implemented your request ie this comment https://github.com/binogure-studio/city-game-studio-i18n/issues/587#issuecomment-819647849 . I cannot put a link like that in the roadmap: https://github.com/binogure-studio/city-game-studio-i18n/projects/2 because if the ticket contains more than one suggestion and I implemented only one _comment_ then I cannot close the issue and I cannot publish a new version without breaking everything.
So I'm gonna lock this issue, because I won't be able to track it correctly.",
"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
**Describe the solution you'd like**
A clear and concise description of what you want to happen.
1-Is it possible to add ""Filter by name"" or Search button in sales report right hand side menu?
When you have so many games it takes too much time to find the game that you want and apply discount or update.**_
2-The bottom panels is too small now while the older version was better; but the left and right hand side panels are okey now because in older version they cover too much space.
3-The fans evolving threshold may perhaps be revised.Of course reality is important but in fact this is a game.When you open lots of studios and produce thousand of games you obviously reach limits quicker than it was foreseen,and it is not pleasant to see losing fans at this point while doing very good games...
4-The auto-furnish option's desk efficiency is very low: in 750m2 studio it places max 47 desks while you can manually place 70 desks easily with 100% happiness.In auto furnish mode you have to make adjustments as well because 100% happiness is not guaranteed.
5-The latest consoles of the game(each material latest technology) can have a cost greater than 1K $ while the game limits the selling price to 1K $ so it's a deficit in debut.
**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.
**Additional context**
Add any other context or screenshots about the feature request here.",Bitter Frustration
"Hi,
This is a bug, I've been writing software for over 30 years and certainly in my experience when said software produces incorrect results it generally has a bug, admittedly Garmin could possibly be at fault if their TCX data is wrong but I highly doubt that so my money is very much on a bug in the Golden Cheetah software. Totally unhelpful response from the moderator.",Bitter Frustration
"We have 2 forums for that kind of discussions, use whatever you find more convenient.",
"Hi,
I've imported my rides from Garmin Connect to add power estimates and I've noticed that the normalised Power is often well overstated, the average power looks sensible but for example on one ride the avg watts was 157 and the normalised was 262, this is massively overstated I'd expect maybe 165 to 170. I can provide the TCX file if it helps.
Many Thanks,
Ralph.
Issue tracker is **only** for Bugs and Features, please don't open issues for questions or technical support. Before to open a new issue please read the contributing guidelines (link below).
If you have questions, please read the FAQs and User's/Developer's Guide:
* FAQs - https://github.com/GoldenCheetah/GoldenCheetah/wiki/FAQ
* User's Guide - https://github.com/GoldenCheetah/GoldenCheetah/wiki/UG_Main-Page_Users-Guide
* Developer's Guide - https://github.com/GoldenCheetah/GoldenCheetah/wiki/Developers-guide
If you need help or technical support please use the forums:
* Users - https://groups.google.com/forum/#!forum/golden-cheetah-users
* Developers - https://groups.google.com/forum/#!forum/golden-cheetah-developers",
"For context the rename/work is complete. The blog post which explains the changes is https://nodejs.medium.com/renaming-n-api-to-node-api-27aa8ca30ed8.
The node-api team was asked to change the name in: https://github.com/nodejs/abi-stable-node/issues/420 . Being sensitive to the concern the team took on this extra work.",
"Repeating [what I wrote 3 months ago](https://github.com/nodejs/TSC/issues/967#issuecomment-773031602):
> I support changing the name N-API to something else, but the new term does have an obvious downside. The term ""Node API"" already has a straightforward meaning. Applying it to a specific API is more vague than descriptive.
> > That said, I don't know that I have better ideas. (Native Bindings API? Addon API? ABI Stability API?)
I'll also add that `Node API` is somewhat contrary to our years-long efforts to have the name of the runtime spelled `Node.js` and not `Node` (or `NodeJS` or a number of other variatns). Of course, now that the name change has already happened, there are significant costs/downsides to changing the name a second time in such a short period of time. I'd still support it, though, if the name was more descriptive and not subject to misinterpretation. A better name is better for our users.",
I'm -1 on changing the name again and I appreciate the effort the node-api team put into making the changes. It's a good change.,
"<!--
Thank you for suggesting an idea to make Node.js better.
Please fill in as much of the template below as you're able.
-->
**Is your feature request related to a problem? Please describe.**
There was recently a decision to rename N-API to Node-API. I believe this is a poor decision that will result in actively harmful results for both users of the API and general Node.js users.
Specifically, there are multiple problems with this naming:
- Building APIs are a common use case for Node.js. This naming can lead to confusing information or misleading search results.
- Products often refer to the way to access their services with JavaScript or Node.js as their ""JavaScript API"". If people want to use this from Node.js, there is a non-trivial chance they will look for ""Node.js API"" which will lead to confusing results.
- Node.js itself has an API, which theoretically includes this API. Naming a part of the whole the same thing as the whole is immensely confusing from an education perspective.
- This API is far less likely to be used than other parts of the Ndoe.js API, which leads to an exacerbation of challenge presented by the problems above.
**Describe the solution you'd like**
Rename Node-API to something else.
**Describe alternatives you've considered**
- Undoing Node-API rename, moving it back to N-API. - There is a reason a rename was done initially, and that reason is valid.
- Leave it as is.
- This is going to be actively harmful to communication and education n the long-run.",Bitter Frustration
This was discussed in the Node-API team meeting today and and consensus was we don't think we want/will change at this point. Some of the team members could not comment directly since it was locked.,
"If the OneToOneField is used with joinColumns it will try to create an unique index with the name of the annotated attribute. This can lead to crashes if two Entities have the same attribute name.
Currently there is no way to rename the generated index which makes the bug ugly.
So please either provide an easy way to rename the index or better: include the entity name prefix in this combination It is related to the question #7961.
Example:
```php
class Foo{
/**
* @ORM\Id
* @ORM\Column(type=""string"", nullable=false)
*/
protected string $id1;
/**
* @ORM\Id
* @ORM\Column(type=""string"", nullable=false)
*/
protected string $id2;
/**
* @ORM\ManyToOne(targetEntity=""Foo"")
* @ORM\JoinColumns(
* @ORM\JoinColumn(name=""id1"", referencedColumnName=""id1""),
* @ORM\JoinColumn(name=""id2"", referencedColumnName=""id2"")
* )
*/
private $foo;
}
class Foo2{
/**
* @ORM\Id
* @ORM\Column(type=""string"", nullable=false)
*/
protected string $id1;
/**
* @ORM\Id
* @ORM\Column(type=""string"", nullable=false)
*/
protected string $id2;
/**
* @ORM\ManyToOne(targetEntity=""Foo2"")
* @ORM\JoinColumns(
* @ORM\JoinColumn(name=""id1"", referencedColumnName=""id1""),
* @ORM\JoinColumn(name=""id2"", referencedColumnName=""id2"")
* )
*/
private $foo;
}
```
scheme updates or generations should now fail because of:
""General error: 1 index foo_uniq already exists"" or similar",Bitter Frustration
"I've run into the same issue with the fake clash in names. It's a legacy table so renaming one or other of the fields is not a viable workaround.
`An index with name '%s' was already defined on table '%s`",Bitter Frustration
"I found only one viable solution: don't use the **.
Two years have passed without any response.
Django is much nicer and even supports websockets.
Ah and for your use case there is even a model generator.
Sry for the rant, it was the hell with this framework, everything screwed up. I swore never to use it again.",Bitter Frustration
[Works for me](https://plnkr.co/edit/mgmTP9xh7dBfMJUF).,
"Then it doesnâ€™t wait for a callback from a dragged marker that uses a custom svg icon. A call to map.panTo at the end of the dragend event for the marker does not work. Thank You,
Andrew Hodel
> On May 18, 2021, at 4:00 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
> > > Works for me.
> > â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"> A call to map.panTo at the end of the dragend event for the marker does not work.
[Yes, it does work](https://plnkr.co/edit/SOWK7sV3uvK2rBOX?preview).",Irony
"@IvanSanchez No it does not:
```
dragend event parameter 0 {distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, â€¦}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {â€¦}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, â€¦}
type: ""dragend""
__proto__: Object
map_pins.js:229 calling function Æ’ (marker) {
//console.log('pin was dragged to', marker.latLng.lat(), marker.latLng.lng());
this.coords.lat = marker.target._latlng.lat;
this.coords.lng = marker.target._latâ€¦
map_pins.js:230 using parameter 0 for aforementioned function {distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, â€¦}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {â€¦}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, â€¦}
type: ""dragend""
__proto__: Object
map_pins.js:232 function ended
map_pins.js:233 map.panTo() fails here, map object: i {options: {â€¦}, _handlers: Array(6), _layers: {â€¦}, _zoomBoundLayers: {â€¦}, _sizeChanged: false, â€¦}
attributionControl: i {options: {â€¦}, _attributions: {â€¦}, _initHooksCalled: true, _map: i, _container: div.leaflet-control-attribution.leaflet-control}
boxZoom: i {_map: i, _container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-draâ€¦, _pane: div.leaflet-pane.leaflet-overlay-pane, _resetStateTimeout: 0, _initHooksCalled: true, â€¦}
doubleClickZoom: i {_map: i, _initHooksCalled: true, _enabled: true}
dragging: i {_map: i, _initHooksCalled: true, _enabled: true, _draggable: i, _positions: Array(0), â€¦}
keyboard: i {_map: i, _panKeys: {â€¦}, _zoomKeys: {â€¦}, _initHooksCalled: true, _enabled: true, â€¦}
options: {}
scrollWheelZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 98, _delta: 0, â€¦}
touchZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 99}
zoomControl: i {options: {â€¦}, _initHooksCalled: true, _map: i, _leaflet_id: 94, _zoomInButton: a.leaflet-control-zoom-in, â€¦}
_animateToCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_animateToZoom: 10
_animatingZoom: false
_container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-drag.leaflet-touch-zoom
_containerId: 92
_controlContainer: div.leaflet-control-container
_controlCorners: {topleft: div.leaflet-top.leaflet-left, topright: div.leaflet-top.leaflet-right, bottomleft: div.leaflet-bottom.leaflet-left, bottomright: div.leaflet-bottom.leaflet-right}
_events: {moveend: Array(3), zoomend: Array(1), zoomlevelschange: Array(1), unload: Array(4), dblclick: Array(1), â€¦}
_fadeAnimated: true
_firingCount: 0
_handlers: (6) [i, i, i, i, i, i]
_initHooksCalled: true
_lastCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_layers: {100: i, 117: i}
_layersMaxZoom: 20
_layersMinZoom: 0
_leaflet_id: 91
_loaded: true
_mapPane: div.leaflet-pane.leaflet-map-pane
_onResize: Æ’ ()
_paneRenderers: {}
_panes: {mapPane: div.leaflet-pane.leaflet-map-pane, tilePane: div.leaflet-pane.leaflet-tile-pane, shadowPane: div.leaflet-pane.leaflet-shadow-pane, overlayPane: div.leaflet-pane.leaflet-overlay-pane, markerPane: div.leaflet-pane.leaflet-marker-pane, â€¦}
_pixelOrigin: k {x: 53035, y: 99811}
_proxy: div.leaflet-proxy.leaflet-zoom-animated
_resizeRequest: 113
_size: k {x: 270, y: 1000}
_sizeChanged: false
_targets: {92: i, 118: i}
_zoom: 10
_zoomAnimated: true
_zoomBoundLayers: {100: i}
__proto__: i
```",
No error and the map does not pan when the zoom is set as such and the coordinates are set as such with a marker that is moved to said position within the bounds at zoom shown.,
At the map size shown.,
"```
marker.addEventListener('dragend', function(marker) {
console.log('dragend event parameter 0', marker);
console.log('calling function', this.pin.dragCb);
console.log('using parameter 0 for aforementioned function', marker);
this.pin.dragCb(marker);
console.log('function ended');
console.log('map.panTo() fails here, map object: ', globals.gmap);
globals.gmap.panTo([this.pin.obj.latitude, this.pin.obj.longitude]);
}.bind({pin: map_pins.pins[c]}));
```
https://user-images.githubusercontent.com/741705/118734965-1749ec80-b805-11eb-8a19-29a6eef0a846.mov
What more proof do you need? That is everything.",Impatience
"Here is this.pin
```
this.pin {obj: {â€¦}, info: div, draggable: true, marker: i, dragCb: Æ’}
dragCb: Æ’ ()
draggable: true
info: div
marker: i
dragging: i {_marker: i, _initHooksCalled: true, _enabled: true, _draggable: i}
options: {title: ""af-40mp-625rmr"", icon: i, zIndexOffset: 1, draggable: true}
_events: {remove: Array(2), dragend: Array(1), click: Array(1)}
_firingCount: 0
_icon: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_initHooksCalled: true
_latlng: D {lat: 39.001823432304754, lng: -106.97038027457896}
_leaflet_id: 103
_map: i {options: {â€¦}, _handlers: Array(6), _layers: {â€¦}, _zoomBoundLayers: {â€¦}, _sizeChanged: false, â€¦}
_mapToAdd: i {options: {â€¦}, _handlers: Array(6), _layers: {â€¦}, _zoomBoundLayers: {â€¦}, _sizeChanged: false, â€¦}
_shadow: null
_zIndex: 501
_zoomAnimated: true
__proto__: i
obj:
alertDisabled: 0
channel: 0
clientInfo: ""ispapp-snmp-relay-0.1""
createdAt: 1620056840
fw: null
fwVersion: null
groupId: ""5f88e6a24ba991445d8ec738""
hardwareCpuInfo: null
hardwareMake: null
hardwareModel: null
hardwareModelNumber: null
hardwareSerialNumber: null
key: ""asdfasdfasdf""
lastConfigRequest: 1621359482
lastUpdate: 1621379634
latitude: 38.86706476159009
login: ""af-40mp-625rmr""
longitude: -106.98185137007387
name: ""af-40mp-625rmr""
notes: ""testing snmp relay""
os: null
osBuildDate: null
osVersion: ""null""
outsideIp: (8) [""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14""]
outsideIpChangeTsSeconds: 1620146969
outsideIpChanges: 1
reboot: 0
ssid: ""undefined""
uptime: 1809293
usingWebSocket: true
vlan: ""undefined""
wanIp: ""63.151.94.152""
wds: ""undefined""
wirelessBeaconInt: 0
wirelessChannel: 0
wirelessConfigs: []
wirelessMode: ""ap_bridge""
zoneId: ""5f88e6954ba991445d8ec736""
_id: ""60901b075223241f3a472497""
__proto__: Object
__proto__: Object
```",
"Do you want me to reopen another issue with this as the first comment or are you going to reopen this one?
Thank You,
Andrew Hodel
> On May 18, 2021, at 4:41 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
> > > A call to map.panTo at the end of the dragend event for the marker does not work.
> > Yes, it does work.
> > â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",Impatience
"With a timeout:
```
setTimeout(function() {
console.log('timeout finished');
globals.gmap.panTo([this.pin.obj.latitude, this.pin.obj.longitude]);
}.bind({pin: this.pin}), 4000);
```
```
function ended
map_pins.js:233 map.panTo() fails here, map object: iÂ {options: {â€¦}, _handlers: Array(6), _layers: {â€¦}, _zoomBoundLayers: {â€¦}, _sizeChanged: false,Â â€¦}
map_pins.js:234 this.pin {obj: {â€¦}, info: div, draggable: true, marker: i, dragCb: Æ’}
map_pins.js:236 timeout finished
```
That makes no sense.",Bitter Frustration
Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.,Impatience
"Why? You should be able to plug in the parameters to each function easily. That is why I did it. Thank You,
Andrew Hodel
> On May 18, 2021, at 6:48 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
> > > Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
> > â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",Impatience
"It is not debug output it is direct parameters passed to your functions!
Thank You,
Andrew Hodel
> On May 18, 2021, at 6:48 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
> > > Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
> > â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"Everything is there and readable. If you cannot explain the reasoning as to why that logic would not prevail then the code is flawed. Thank You,
Andrew Hodel
> On May 18, 2021, at 6:50 PM, Andrew Hodel ***@***.***> wrote:
> > Why? You should be able to plug in the parameters to each function easily. That is why I did it. > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 6:48 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
>>> >> >> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
>> >> â€”
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"To expect more than parameters passed per function is absurd, truthfully the library should have a debug parameter on instantiation that logs every function and corresponding parameter chronologically for you to be able to debug easily. I am still trying to understand why that doesnâ€™t exist. Thank You,
Andrew Hodel
> On May 18, 2021, at 6:50 PM, Andrew Hodel ***@***.***> wrote:
> > Why? You should be able to plug in the parameters to each function easily. That is why I did it. > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 6:48 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
>>> >> >> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
>> >> â€”
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"At this point I'm gonna tell you to calm down and read [""How To Ask Questions The Smart Way""](http://www.catb.org/esr/faqs/smart-questions.html) and [""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html""](http://www.chiark.greenend.org.uk/~sgtatham/bugs.html).",Impatience
"Do you have any reasonable explanation as to why you would want peopleâ€™s code instead of a chronological function/parameter output generated from a debug=true parameter passed to each library function?
Thank You,
Andrew Hodel
> On May 18, 2021, at 6:50 PM, Andrew Hodel ***@***.***> wrote:
> > Why? You should be able to plug in the parameters to each function easily. That is why I did it. > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 6:48 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
>>> >> >> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
>> >> â€”
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",Impatience
"The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
Why you wouldnâ€™t is beyond understanding. I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. Thank You,
Andrew Hodel
> On May 18, 2021, at 7:01 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
> > > At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
> > â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"Here is an example:
https://github.com/andrewhodel/rrd/blob/4ded366652d93596af52940ba98622229f3acdab/rrd.go#L86
Thank You,
Andrew Hodel
> On May 18, 2021, at 7:01 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
> > > At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
> > â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",
"Remittances == making updates without including debug output and â€œhenceforth at that timeâ€ removing such a validator. Thank You,
Andrew Hodel
> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> > The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> > Why you wouldnâ€™t is beyond understanding. > > I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. > > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 7:01 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
>>> >> >> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> >> â€”
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"I am sorry Man, but how else would you figure out a long list of asynchronous functions?
Thank You,
Andrew Hodel
> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> > The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> > Why you wouldnâ€™t is beyond understanding. > > I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. > > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 7:01 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
>>> >> >> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> >> â€”
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",
"You could write it to a file or with a prefix and pass that to a library test that automatically assured everyone of validity in application code or error in library code. I do not have understand your reasoning for â€œplayinbrowserjsâ€ or â€œrequired bug fieldsâ€ when you have a library this pervasive and this solution is that much better. Thank You,
Andrew Hodel
> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> > The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> > Why you wouldnâ€™t is beyond understanding. > > I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. > > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 7:01 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
>>> >> >> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> >> â€”
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"I would also like an apology or a reason as to why the smart way isnâ€™t a debug option in the library with a prefix on the console output, as you want me to stop and read a book rather than paste console output to a test that verifies code in library and the application. Thank You,
Andrew Hodel
> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> > The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> > Why you wouldnâ€™t is beyond understanding. > > I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. > > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 7:01 PM, IvÃ¡n SÃ¡nchez Ortega ***@***.***> wrote:
>>> >> >> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> >> â€”
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
"OK, so now that we've all had some time to cool down, I'll say this:
Andrew: you come here to a turf that is not your turf, you ignore the templates for relevant information, you disrespect the etiquette, you post comment after comment making it difficult to follow your train of thought, and all that while having a holier-than-thou attitude towards bug triage techniques.
So while I *could* explain why we ask for live reproducible examples, I feel that doing that would be a waste of my time.
I will, however, ask you to work on your empathy and on your communication skills.",Bitter Frustration
"`map.panTo()` within bounds that are already shown does not center the map on the point sent to `map.panTo()`.
1.7.1",Bitter Frustration
"https://developer.mozilla.org/en-US/docs/Web/API/Performance/now
Unlike other timing data available to JavaScript (for example Date.now), the timestamps returned by performance.now() are not limited to one-millisecond resolution. Instead, they represent times as floating-point numbers with up to microsecond precision.",
"> [...] your obviously angry issue handlers.
Antagonizing the maintainers won't help.",Mocking
"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
Your library is included in complicated code and you expect people to submit samples that are 10 lines and work in jsfiddle. This is like submitting a libc or openssl bug without an strace. **Describe the solution you'd like**
A clear and concise description of what you want to happen.
A library wide option to enable or disable tracing output in the console. As JS is billed as asynchronous you should output each function call, it's parameters and the start and end time at the highest resolution possible. This would stop all of the frustration you have with requiring code samples and improve moral for your obviously angry issue handlers. **Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.
**Additional context**
Add any other context or screenshots about the feature request here.",Bitter Frustration
"Hi @petkaantonov! Thanks for opening this feature request.
The current behavior is reflecting the long-standing behavior that Terraform does still detect and incorporate remote objects into the state, but then while producing a plan it ignores differences _between the configuration and the state_. The difference reported in your example is a difference between the prior state and the current remote object, and `ignore_changes` has never affected that situation but that fact was less visible before because Terraform just silently updated the state rather than reporting it.
While it might seem immaterial whether Terraform updates the state or not here, it _can_ result in a change in behavior of your configuration if any other expressions in the module refer to that value. To be specific, if you had any reference to `aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus` elsewhere in your module then they would return `2` rather than `0` after detecting this change, and so that is what Terraform is reporting here, in case that ends up being useful context for understanding which actions Terraform proposes (or doesn't propose) in the plan.
I assume that in your case this doesn't really matter much, because you _don't_ have any references to `aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus` elsewhere in your module and so you don't actually care what's reported in the state. One potential improvement we could consider for this mechanism is for Terraform to try to only report changes to resources that have other expressions referring to them, since changes to a value that nothing refers to can't possibly affect the behavior of the configuration.
However, such a rule is easier to say than to implement because what we've done here is just expose in the UI some long-standing Terraform behavior that was previously invisible, and so changing that behavior at this late state will likely require a lot of research to make sure that the changes don't break use-cases we're not currently aware of. The current output is an honest and correct account of Terraform's behavior, and so I think we need to consider here whether the right thing to do is change Terraform's behavior (which, for something this fundamental, would be challenging to do at this late stage in Terraform's life) or to change the UI to fudge the details a little so that it leaves hidden some details that surface inconvenient truths that don't actually affect configuration behavior.
We won't be able to make any significant changes in this regard in the near future, because the scope of this project was just to be more explicit about what Terraform was already doing rather than to change how Terraform behaves, but we'll use this issue to represent the need and consider what we might change in future releases.
Thanks again!",
"I see. In that case, would it be possible to add a separate way to ignore these? e.g. `ignore_drifts` or something like that? Wouldn't that be easy to implement?",
"The refresh operation doesn't currently really consider the resource configuration at all -- it's job is to synchronize the existing state with remote objects -- so making it react to anything new in the resource configuration is not a trivial design decision.
We also know from our experiences with `ignore_changes` that this idea of partially ignoring changes to parts of objects is generally not as simple as it first appears: there are lots of resource types where two or more attribute values are connected to each other in some way, and so ignoring one without ignoring the other can make the result inconsistent. For current `ignore_changes` that sort of inconsistency isn't a massive problem aside from being a bit confusing, because during planning we're just proposing some changes and not actually changing anything, but doing that during _refresh_ might well lead to a situation where the object in the state is no longer a valid input to the provider that's managing it, which would likely lead to errors or crashes where a provider has to deal with broken input in a location where it never did before.",
"Could the refresh operation be made configurable in some other way? like an .ignorefile There is probably going to be 3rd party solution if it's not added anyway. Just some way to ignore these is important, otherwise the output is not as useful because it becomes just manually ignored noise and people become conditioned to ignore it regardless of what it says.",Bitter Frustration
"I don't currently have any ready-to-implement designs to address this feedback. We'll need to do some more research and design work in order to address it. Until then, indeed if your system routinely makes changes to objects outside of Terraform as part of its regular function then the feature in its current state will not be so useful for you,
Hopefully it can still answer the sorts of questions it is aiming to solve, though: if Terraform proposes making a change that doesn't seem justified by a corresponding change in configuration, you can refer to the note about detected changes to see if any of the changes it detected are the explanation. If the actions Terraform is proposing match what you were expecting anyway, then the changes detected are just some extra information that you can safely ignore.",
"In situations where the user is forced to add an ignore - to stop Terraform from chasing it's own tail - ie with the use of an aws_autoscaling_attachment (where it is documented that you must use an ignore in corresponding autoscaling groups https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_attachment), I think this behaviour constitutes a bug. The drift output states that ""Objects have changed outside of Terraform"" when in reality the changes are made within terraform and have had to be ignored for reasons that might be justified as a bug in their own right.
Choosing to output drift before taking into account ignores thus creates potentially significant noise when using Terrafom within it's documented bounds.",Bitter Frustration
"I can relate to this issue a lot, our daily work now has a new pain: filtering out the noise of ignored changes. ""muting"" the output would help A LOT! :)
anyway, thank you guys for the good work, keep up with it! ðŸ‘",Bitter Frustration
"This feels like a pretty invasive change. I second what @koalalorenzo said. The main problem with this change is that we lean on our large team of engineers to inspect their own changes carefully on a per-team basis and apply those changes without constant infra approval. Even when diffs are small, though, we can sometimes misunderstand the plan which can lead to some unfortunate consequences. Not everyone understands the inner workings of terraform, or even the best practices, so there is always going to be mistakes. This isn't terraform's fault, we all need to learn the tool a little bit better.
Now, though, we have to sift through an entire block of output and tell our team to just ignore it up to a point. Its just going to lead to less clear plan output that people put less effort into reviewing. I think its going to erode trust, too, even though I suspect the opposite is the goal. For instance, take a `google_container_node_pool` resource. If its auto-scaling, every single time you apply the project you get a new `node_count` attribute diff that is wholly un-actionable. If somebody sees that every single time, but I tell them to ignore it because it should absolutely be ignored, whose to say when they get a `-/+` change on that resources (maybe by accident or lack of understanding) they aren't going to assume its also ignorable.
TBH, I have ignored many attributes in terraform because we may change them outside of terraform and have come to rely on the fact that it really doesn't and shouldn't care about the changes to attributes you don't specify. IDK. Ranting a bit.
I have to be totally honest I see no point in opting everyone into this for every resource automatically. I don't understand the problem that its solving and unless there is a way for it to be completely filtered out I would love to see this change reverted in the next release.
(I also don't think the solution is `ignore_attributes`.)",Bitter Frustration
"I am very surprised about this change and it adds a ton of noise to the already not exactly clear and concise plans. The extra verbosity adds a lot of cognitive load which risks at best to make people glance over the plans and at worst make them ignore them completely.
I really, really don't understand the reasoning here.
Again, other than that, thank you for a great product over the years!",Bitter Frustration
"I can get a little worked up and forget that a lot of people put in a lot of work to get terraform where it is today. I don't want to belittle that. People like @apparentlymart (who helped us in the forums when we were starting to write our own internal terraform provider plugins) are awesome. Terraform is awesome. Obviously my job is infinitely easier because of it. â¤ï¸ As for this feature, it was in the CHANGELOG and I did glance over it, so its on me that I upgraded to `0.15.4`. I just wanted to register my feeedback that I really wish this was separate from the plan, or that it was opt-in when running plans/applies, or that it was even opt in at a module or resource level (a `lifecycle { show_drift = true }` attribute?).",Bitter Frustration
"Just adding another resource type that now always appears in my terraform output - aws_efs_file_system. (from https://github.com/hashicorp/terraform/issues/28845 which I will mark as a duplicate). EFS file systems are *supposed* to grow and change, as many other types of resource they are explicitly designed that way.
With no way to ignore, the plan/apply output is now so noisy I've gone back to terraform 0.15.3 as wading through tens of modified resources on every single plan/apply just to catch the one that might be important is not a workable pattern for me.",Bitter Frustration
"Since terraform 0.15.4, I get confused messages from my team every single day. I can't imagine the impact this change had globally.
Please add an option to disable displaying those by default and go back to the previous behavior, with maybe an explicit way to get more details about the changes only when needed (e.g a debug mode).
This is really confusing right now, and someone needs to understand the internals of terraform to appreciate the difference between a drift and remote changes.",Bitter Frustration
"One thing that surprised me a lot about this change is that the `Note: Objects have changed outside of Terraform` still shows up if you do `terraform plan -input=false -output=tfplan >/dev/null && terraform show tfplan`. I had hoped that this note would be output as part of the `Refreshing state ...` messages and as such would be hidden, but it turns out that the note is part of the saved plan file. If this was changed so that the note is output with the `Refreshing state ...` messages instead of being saved inside of the plan, that would make a big difference for the ability of users to silence that note and anyone who was already silencing the refreshing state messages would automatically get this new note silenced as well.
As near as I can tell, the only way to currently silence the warning is to run `terraform refresh >/dev/null && terraform plan -input=false -refresh=false`, which has several downsides that @jbardin and @apparentlymart pointed out in https://github.com/hashicorp/terraform/issues/27214.",Bitter Frustration
"> This is really confusing right now, and someone needs to understand the internals of terraform to appreciate the difference between a drift and remote changes.
Agreed, this bit us earlier. Another example is that AWS autoscaling groups will always have ""external"" changes to attributes such as `desired_count` from normal scaling behaviour so the refresh-only state plan will show drift. I was going to report this to the AWS provider, but it sounds like they might not yet have a way to fix this. If there are underlying architecture changes that need to be carefully considered, I would certainly prefer to suppress these drift notifications until they can be made more useful.
As the below is an ignored attribute, the resource is not affected by Terraform and the message amounts to noise:
Example:
```sh
# aws_autoscaling_group.example-scaling-group has been changed
~ resource ""aws_autoscaling_group"" ""example-scaling-group"" {
~ desired_capacity = 50 -> 36
id = ""example-scaling-group""
name = ""example-scaling-group""
# (20 unchanged attributes hidden)
```
A person reviewing this plan has to already know the attribute is ignored on that resource or read ""below the line"" in order to confirm the resource is not affected. Another way of looking at this is as a formatting issue, since the change notification looks _very similar_ to the actual plan, and it took me and the rest of my team a while to realise it's a separate section.",Bitter Frustration
"Another example - [`okta_app_oauth`](https://registry.terraform.io/providers/oktadeveloper/okta/latest/docs/resources/app_oauth) supports `users` which can also be configured by [a separate resource](https://registry.terraform.io/providers/oktadeveloper/okta/latest/docs/resources/app_user). If you use the separate resource or some out-of-band way to manage the users, you will always see a change here.",
"I think it is fine if Terraform now reports drifts explicitly but it will be great if we could have a way to hide it because we might not be interested in changes happening outside of Terraform in certain use cases.
One way we can implement this is to add a `--ignore-drifts` flag to the relevant `terraform xxx` subcommands. This is the approach I'm most in favour with because it allows this to be decided at runtime and to be varied flexibly across different use cases (i.e. running locally vs running in CI/CD platforms, etc.). It also provides users a simple way to revert to the old representation of the output.
```console
$ terraform plan --ignore-drifts
$ terraform apply --ignore-drifts
```
In addition, we could also implement this at a resource/module level using the `ignore_drifts` field. This may be helpful for some use cases where we might be interested drifts in general but not all fields.
```terraform
resource ""aws_instance"" ""example"" {
# ...
lifecycle {
ignore_drifts = [
etag, # Ignore drifts for specific attributes
]
}
}
```
```terraform
resource ""aws_instance"" ""example"" {
# ...
lifecycle {
ignore_drifts = all # Ignore drifts for all attributes
}
}
```",
"+1 to this issue.
Part of our workflow involves creating EMR clusters in AWS, including running a few steps to modify some software configurations. After the cluster is created, our data scientists run hundreds more steps throughout the lifetime of the cluster. Now _every single step_ in _every_ cluster shows up in _every_ Terraform plan, despite the lifecycle ignore.
We use Atlantis to execute our Terraform configurations, but cannot do so now thanks to this. The plans produced are too long to appear in Gitlab comments, and as such we cannot see any of the plans produced by Atlantis.",Bitter Frustration
"To add to the discussion here, I'm also seeing that (for example) the Principals list in a role always shows up as ""has been changed outside of Terraform"", even if I attempt to manually update the principals list via the AWS web console.
``` ~ assume_role_policy = jsonencode(
~ {
~ Statement = [
~ {
~ Principal = {
~ Service = [
+ ""lambda.amazonaws.com"",
+ ""ec2.amazonaws.com"",
""states.us-east-1.amazonaws.com"",
# (1 unchanged element hidden)
""cloudwatch.amazonaws.com"",
- ""ec2.amazonaws.com"",
- ""lambda.amazonaws.com"",
]
}
# (3 unchanged elements hidden)
},
]
# (1 unchanged element hidden)
}
)
```
Each time I edit it via the web console to reflect what Terraform thinks it ought to be, AWS saves the list in a random order (not alphabetical), so Terraform always thinks that there's been a change. Even when I force an apply, or taint and import the resource, this warning persists.
Outside of this warning, the plan results in the message ""No changes. Your infrastructure matches the configuration."".
This, to me, doesn't make sense because if Terraform thinks that there is drift, an apply should be able to rectify that drift. This does not appear to be the case, which makes the whole drift warning kind of pointless and noisy.",Bitter Frustration
"Hi guys, as everyone else, this change is quite impacting for us.
On azure, it appears that every apply resulting in changes in other resources will be seen as a drift.
So far, I encountered to cases:
- creating a new subnet using the azurerm_subnet resource:
The plan/apply goes well, but on the next plan, even if there are no changes, Terraform informs me that my VNet has changed, because it wants to reflect the newly added subnet in the VNet attributes. But this change was actually made by Terraform, in the previous run.
- creating a virtual machine, as the MAC Address and VM id are known after deployment, Terraform detects changes on the NIC on the next plan, because now, the NIC has indeed a MAC and a VM id.
```hcl
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.Azadds_mgmt_VM.azurerm_network_interface.TerraVM-nic0 has been changed
~ resource ""azurerm_network_interface"" ""TerraVM-nic0"" {
id = ""/subscriptions/<subscription_id>/resourceGroups/myRG/providers/Microsoft.Network/networkInterfaces/myvm-nic0""
+ mac_address = ""00-xx-xx-xx-xx-xx""
name = ""myvm-nic0""
+ tags = {}
+ virtual_machine_id = ""/subscriptions/<subscription_id>/resourceGroups/myRG/providers/Microsoft.Compute/virtualMachines/myvm""
# (9 unchanged attributes hidden)
# (1 unchanged block hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions to undo or respond to these changes.
```
I think we can all agree the Terraform team is doing a tremendous job at bringing the product where it is today, but this particular change although attempting to bring more visibility, brings, in my opinion, a lot of confusion.
I think I will roll back to 0.15.3 until some improvements are made about this feature.",Bitter Frustration
"hey @kpkrishnamoorthy I have submitted an issue about this in the provider tracker https://github.com/hashicorp/terraform-provider-aws/issues/19727
But it would be definitely cool if core terraform allowed for an opt-in option to hide the output of this new feature until provider devs fix it on their side",Impatience
"> I think we can all agree the Terraform team is doing a tremendous job at bringing the product where it is today, but this particular change although attempting to bring more visibility, brings, in my opinion, a lot of confusion.
Just want to second this.",
"Do we have anybody working on it? If so, do we have an ETA for this? It has been an annoying issue that confuses a lot of people. Our Atlantis comments on PRs are way too big for no reason, and it is super frustrating! ðŸ™ Please let us know how to help somehow, and pressure to get this shipped in the next minor/patch release!!! ðŸ™",Bitter Frustration
"We're affected by this too, and it seems clearly like a bug. We've got things in our config where we've told Terraform to ignore changes to those things; now it ignores the changes in that it won't try to control those things, but it no longer ignores them when reporting on them.
If you've told Terraform to ignore changes to something, it should ignore those changes silently.
If that isn't universally agreeable, perhaps a general option like `ignore_changes_silently` or something?",Bitter Frustration
"I actually propose to make this entire drift detection feature OPT IN instead of opt out for the time being. As this thread shows, its causing lots of issues and confusion for many people, with different kinds of usecases.
I DO see the benefit of the feature in general, but putting this change into a release sure backfired. I'd wager if they just remove it in 1.0.1 not many would actually miss it.",Bitter Frustration
"@fbreckle it seems you are making assumptions about how Terraform is used everywhere, and what the community as a whole thinks about this feature. I myself find this feature useful, I sure want to know when my Infra has changed under the hood and what exactly changed outside of Terraform. We depend heavily on Terraform and this new feature gives us visibility and increases the trust we have in this tool.
It has some downsides (like for us we are using Kubernetes with node autoscaler and node auto upgrade so TF reports that the node count and the node version has changed quite often) but I am pretty sure that with the required changes in the providers/resources, we can find an elegant solution without reverting the whole thing. Thanks Terraform for being opinionated !",
"I think it's clear that opinions are split, and while a part of the community is happy about this change, the other part doesn't have a way to opt out.
For us it's a blocker and prevents us from upgrading to 0.15 (1.0).",Bitter Frustration
"@JordanP : We definitely want to know about drift in the resources that we've told Terraform to manage. What we don't want to know about is drift in resources that we've told Terraform to ignore.
Do you -- does anyone -- have a use case for ""our config says that Terraform should ignore this resource, but we want to hear about changes to it anyway""?
There was this mentioned early on:
> While it might seem immaterial whether Terraform updates the state or not here, it can result in a change in behavior of your configuration if any other expressions in the module refer to that value. To be specific, if you had any reference to aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus elsewhere in your module then they would return 2 rather than 0 after detecting this change, and so that is what Terraform is reporting here, in case that ends up being useful context for understanding which actions Terraform proposes (or doesn't propose) in the plan.
(https://github.com/hashicorp/terraform/issues/28803#issuecomment-848049967)
That seems like a weird situation, where you'd have references to values that you've told Terraform to ignore the value of.",
"@jbscare >That seems like a weird situation, where you'd have references to values that you've told Terraform to ignore the value of.
This should be processed as a plan-time explicit warning, in my opinion.
The only reason I can think of, when someone would need to explicitly use a reference to a resource attribute that its changes are being ignored explicitly, is at initial resources creation.
But even that is the reason, then it wouldn't make sense for terraform to continuously notify users about ""changes being made outside of terraform"" in this new section. A warning message would be adequate and less prone to misinterpretation.",
"Quick chime in: We use TF to set up and manage our AWS accounts and detect any drift. Our configs and accounts have worked for years without any major changes. Now we are suddenly getting warnings without any details (just empty) and I still have no idea why.
I read a bit, but still don't really understand what is happening. Will have to spend more time, but ideally things would be a bit more obvious (why are we getting a warning, what are our next steps, etc)
Love the tool btw and this is not a rant, just an FYI ðŸ˜‰",Impatience
"> I am pretty sure that with the required changes in the providers/resources, we can find an elegant solution without reverting the whole thing.
I don't think this is true @JordanP. The way this is implemented, there is now way for providers to work around this.",Bitter Frustration
"@ryanking hmm this is quite the antithesis of what @jbardin has claimed recently: https://github.com/hashicorp/terraform/issues/28911#issuecomment-857647716
If there is really no way for providers to work with this tf-core feature in mind, then we should definitely have a disable flag asap",Impatience
"> @ryanking
> > hmm this is quite the antithesis of what @jbardin has claimed recently: [#28911 (comment)](https://github.com/hashicorp/terraform/issues/28911#issuecomment-857647716)
> > If there is really no way for providers to work with this tf-core feature in mind, then we should definitely have a disable flag asap
that comment is mostly about how list diffs are handled. if provider mess up the ordering, then yeah it's on the provider.
but this issue is about not showing ignored changes in drift detection, which is mostly an orthogonal problem, and providers have no control over this.",
"This falls under the ""too much information"" category to me. This is debug output at best. Set the TF_LOG=DEBUG and then you can get this output. Here is a good example of why it doesn't work for me:
```# module.ec2_chat.aws_security_group.chat has been changed
~ resource ""aws_security_group"" ""chat"" {
id = ""sg-0f46dd9f058d12345""
name = ""prod-chat-8""
tags = {
""Environment"" = ""prod""
""Name"" = ""prod-chat-8""
""Terraform"" = ""yes""
}
+ tags_all = {
+ ""Environment"" = ""prod""
+ ""Name"" = ""prod-chat-8""
+ ""Terraform"" = ""yes""
}
# (7 unchanged attributes hidden)
# (1 unchanged block hidden)
}
```
The tags_all is an ""attribute"". It is not an ""argument"". It is computed. I won't be setting it - ever. Even if I apply this if I add/change/delete a tag it will also change the tags_all attribute and I will have to apply it again to get rid of this message. And there are many, many more of these examples.",Bitter Frustration
"I have some similar error:
```
panic: Error reading level state: strconv.ParseInt: parsing ""84850493440"": value out of range
```
This is the attribute output by terrafom of efs system. There're some wy to evict this error without delete efs system from tfstate?
Thanks",Bitter Frustration
"I also just ran into this upgrading from `0.14.11`. My feedback would be NOT a command line flag I have to provide to ignore changes outside of Terraform but a new attribute that behaves like `lifecycle { ignore_changes = [] }`.
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.fusionauth.aws_db_instance.rds has been changed
~ resource ""aws_db_instance"" ""rds"" {
id = ""fusionauth""
~ latest_restorable_time = ""2021-07-06T16:42:02Z"" -> ""2021-07-06T16:57:02Z""
name = ""fusionauth""
tags = {
""Env"" = ""stage""
""Name"" = ""fusionauth""
}
# (52 unchanged attributes hidden)
}
# module.postgres.aws_db_instance.rds has been changed
~ resource ""aws_db_instance"" ""rds"" {
id = ""postgres""
~ latest_restorable_time = ""2021-07-06T16:39:08Z"" -> ""2021-07-06T16:59:09Z""
name = ""postgres""
tags = {
""Env"" = ""stage""
""Name"" = ""postgres""
}
# (52 unchanged attributes hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions
to undo or respond to these changes.
```",
"We need a way to output only the actual changes Terraform intends to perform. Currently 100% of the output from ""Objects have changed outside of Terraform"" is noise for us (unavoidable and intended changes, or ignored fields).
Does anyone know how to output only Terraforms planned changes?
I plan on testing running a `terraform apply -refresh-only -auto-approve` before each plan to reduce noise, however changes can still happen between.
Edit: don't us the command above not useable with certain changes: https://github.com/hashicorp/terraform/issues/28939",Bitter Frustration
"> Does anyone know how to output only Terraforms planned changes?
No, there is no way, hence this GitHub issue. A flag or an env var would be welcome.",Bitter Frustration
"Update: we have tested running `terraform apply -refresh-only -auto-approve` before each plan, it works for us.
We were currently scrolling through ~390 lines of detected change output that weren't actual differences down to 0.
Update 2: The above command is a hack which has the potential to cause issues. See Indigenuity's comment below.
Only run the above if you have the ability to recover your statefile from any issues. Only run if you have state file versioning.
Update 3: don't us the command above not useable with certain changes: https://github.com/hashicorp/terraform/issues/28939",
"It strikes me as a step backwards that we now have to run an `-auto-approve` job just so the output of `plan` is intelligible. @robomon1 hit the nail on the head: it may not be debug output by strictest definition but it's certainly debug output in utility.
While there have been past cases where I would have loved this view into the internals of `terraform refresh` I don't need it 99% of the time and most of our users don't need it _ever_. Especially when some changes are so laughably trivial:
```
~ resource ""google_bigquery_table"" ""table"" {
id = <sensitive>
~ last_modified_time = 1625689042601 -> 1625696261956
~ num_bytes = 3889196 -> 3943304
~ num_rows = 14390 -> 14590
# (13 unchanged attributes hidden)
```
So, like some others, our solution has been to downgrade to `15.3`. I echo those others as well when I say that Terraform has changed the way I work. Thanks for your tireless work on it.",Bitter Frustration
"Is anybody working on this? ðŸ˜… EDIT: just noticed that this got a ""enhancement"" tag, while IMHO from a UX point of view this is a MAJOR f*ck up and not an enhancement! ðŸ˜“ ðŸ˜¢ We are still suffering from it and it is getting super frustrating. I wish somebody would consider this for the next release or update us on the status. What can we do?",Bitter Frustration
"Also, looking for a fix to hide this related to RDS instances and `~ latest_restorable_time = ""2021-07-20T21:21:23Z"" -> ""2021-07-20T21:26:23Z""`",
"> Update: we have tested running `terraform apply -refresh-only -auto-approve` before each plan, it works for us.
> > We were currently scrolling through ~390 lines of detected change output that weren't actual differences down to 0.
It is workaround for some cases. I have some AWS IAM resources, changes still appear even after I ran this command.
I really don't like this in update. They should make it print only when user want it (via flag), let terraform show plan in way people expect. Nice feature but bad implement",Bitter Frustration
"We also use atlantis and our mouse wheel is burning now, due to scrolling down in merge request to get to the interesting terraform part. :smile: We use terraform with VMware and face this issue with a custom attribute, which get updated daily by a backup software.
```
~ custom_attributes = {
~ ""999"" = ""Backup Server=..., EndTime=2021-07-20T19:44:00Z""
}
```
Like others said, it would be really great to have some sort of switch to hide ""ignore_changes-changes"". Nevertheless terraform is such a great piece of software. Thank you!!! :slightly_smiling_face:",Impatience
"Hi,
> Update: we have tested running `terraform apply -refresh-only -auto-approve` before each plan, it works for us.
> > We were currently scrolling through ~390 lines of detected change output that weren't actual differences down to 0.
If you refer to the terraform [documentation](https://www.terraform.io/docs/cli/commands/refresh.html), you can see that this command is deprecated.
The old output was more readable before introducing this ""feature"". Would have been better if we could opt-in (or not) for the new behaviour.",Bitter Frustration
"Wait, wait, are people actually using `-auto-approve` as part of their workarounds for this? Seems rather dangerous! Even the documentation linked by @Tazminia warns explicitly about one risk:
>Automatically applying the effect of a refresh is risky, because if you have misconfigured credentials for one or more providers then the provider may be misled into thinking that all of the managed objects have been deleted, and thus remove all of the tracked objects without any confirmation prompt.
And furthermore, these state updates aren't always clean. I had an `aws_security_group` resource with 30 `aws_security_group_rule` resources, and this refresh seemed to think that the rules had been changed externally to all be in-line rules. So suddenly terraform is thinking it needs to create 30 more resources again, since all the rules were moved to be in-line. Using `-auto-approve` in scenarios like this is a great way to blow up your infra!
If this feature is pushing people to risky workarounds, I'd suggest that it's more than just a nuisance and should be treated as a serious bug. While the output of this feature doesn't suggest `-auto-approve`, it does imply that `-refresh-only` is always harmless, which is untrue.",Bitter Frustration
"> If you refer to the terraform documentation, you can see that this command is deprecated.
@Tazminia The command `terraform refresh` is deprecated, but they clearly state that it is an alias to `terraform apply -refresh-only -auto-approve`, which they WANT people to know about and understand because they introduced the `-refresh-only` flag in `0.15.4`; these new flags are important to interacting with this new output.
It's definitely been a bit of a transition to `0.15.4+`, and we would also like the option to show/hide this output, but overall so happy for the Terraform team to hit `1.0+`! Thanks for all your hard work!",
This is extremely annoying and frustrating for the team.,Bitter Frustration
"> Wait, wait, are people actually using `-auto-approve` as part of their workarounds for this? Seems rather dangerous! Even the documentation linked by @Tazminia warns explicitly about one risk:
> > > Automatically applying the effect of a refresh is risky, because if you have misconfigured credentials for one or more providers then the provider may be misled into thinking that all of the managed objects have been deleted, and thus remove all of the tracked objects without any confirmation prompt.
> > And furthermore, these state updates aren't always clean. I had an `aws_security_group` resource with 30 `aws_security_group_rule` resources, and this refresh seemed to think that the rules had been changed externally to all be in-line rules. So suddenly terraform is thinking it needs to create 30 more resources again, since all the rules were moved to be in-line. Using `-auto-approve` in scenarios like this is a great way to blow up your infra!
> > If this feature is pushing people to risky workarounds, I'd suggest that it's more than just a nuisance and should be treated as a serious bug. While the output of this feature doesn't suggest `-auto-approve`, it does imply that `-refresh-only` is always harmless, which is untrue.
This is true. It is a hack which has the potential to cause issues. I will update my comment.
The only other alternative we can see is getting the team to scroll through and inspect hundreds (soon to be thousands) of lines of expected changes/changes that aren't even changes every single plan run (just for 1 pipeline). A lot of our resources are affected with this issue.
Update: don't us the command above not useable with certain changes: #28939",Bitter Frustration
"Wouldn't having a flag on apply to show\hide these _outside changes_ be a relatively simple solution?
Having all this extra output does make it harder to process what would actually change.",Impatience
"A few thoughts from the POV of a Terraform Cloud user.
- A flag would be nice along with a corresponding option for the workspace.
- This output could be separated from the normal plan output in the Terraform Cloud UI.
- Taking it one step further, it would be amazing if the planned changes could be put in their own log window. The plan output is generally very noisy and makes it much harder to review the plan than it could be:
- First it prints the init steps.
- Then it prints all the resources from the state refresh.
- Then it prints this new report
- Then, finally, if there are no errors, it prints **the actual thing we want to look** at and always need to review very carefully: the planned changes.",Bitter Frustration
"After upgrading to 0.15.4 terraform reports changes that are ignored. It is exactly like commented here: https://github.com/hashicorp/terraform/issues/28776#issuecomment-846547594
### Terraform Version
```
Terraform v0.15.4
on darwin_amd64
+ provider registry.terraform.io/hashicorp/aws v3.42.0
+ provider registry.terraform.io/hashicorp/template v2.2.0
```
### Terraform Configuration Files
<!--
Paste the relevant parts of your Terraform configuration between the ``` marks below.
For Terraform configs larger than a few resources, or that involve multiple files, please make a GitHub repository that we can clone, rather than copy-pasting multiple files in here. For security, you can also encrypt the files using our GPG public key at https://www.hashicorp.com/security.
-->
```terraform
resource ""aws_batch_compute_environment"" ""batch_compute"" {
lifecycle {
ignore_changes = [compute_resources[0].desired_vcpus]
}
...
compute_resources {
...
}
}
resource ""aws_db_instance"" ""postgres_db"" {
...
lifecycle {
prevent_destroy = true
ignore_changes = [latest_restorable_time]
}
}
```
### Output
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.db.aws_db_instance.postgres_db has been changed
~ resource ""aws_db_instance"" ""postgres_db"" {
id = ""db""
~ latest_restorable_time = ""2021-05-25T10:24:14Z"" -> ""2021-05-25T10:29:14Z""
name = ""db""
tags = {
""Name"" = ""DatabaseServer""
}
# (47 unchanged attributes hidden)
# (1 unchanged block hidden)
}
# module.batch_processor_dot_backend.aws_batch_compute_environment.batch_compute has been changed
~ resource ""aws_batch_compute_environment"" ""batch_compute"" {
id = ""batch-compute""
tags = {}
# (9 unchanged attributes hidden)
~ compute_resources {
~ desired_vcpus = 0 -> 2
tags = {}
# (9 unchanged attributes hidden)
}
}
```
### Expected Behavior
No changes should be reported because they are listed in ignored changes.
### Actual Behavior
Changes are reported.
### Steps to Reproduce
Change any resource outside of terraform and see that `terraform apply` reports changed even when they should be ignored.
### Additional Context
### References
- https://github.com/hashicorp/terraform/issues/28776
- https://github.com/hashicorp/terraform/issues/28776#issuecomment-846547594
- https://github.com/hashicorp/terraform/pull/28634#issuecomment-845934989",Bitter Frustration
Just dropping by to let you know that this is issue is also keeping my organization on 0.15.3.,Impatience
"Btw, I was able to get minimal output for CI by running plan in 2 stages:
```sh
terraform plan -input=false -out=plan
terraform show -no-color plan
```
I used it even before this ""nice"" change to get rid of ""Refreshing state"" messages in output for big state.",Mocking
"Issue has been open for three months, is there an intention to fix? Who owns the issue.",Impatience
"Simple grep/sed combo until we have `--no-refresh-messages` and `--no-changed-objects` :)
```
terraform plan | grep -v ""Refreshing state..."" | sed '/Objects have changed outside of Terraform/,/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€/d'
```",Irony
"I'll add one more: Amplify app autogenerates a thumbnail for production branch and on refresh returns a different (huge) signed URL _each time_... and, of course, both old and new URLs are printed!
```terraform
# aws_amplify_app.client_app has been changed
~ resource ""aws_amplify_app"" ""client_app"" {
id = ""d23rt6p25fb7vl""
name = ""some-client-app""
~ production_branch = [
~ {
~ thumbnail_url = ""https://aws-amplify-prod-us-east-1-artifacts.s3.amazonaws.com/d23rt6n25fb7vl/production/SCREENSHOTS/thumbnail.png?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCye8iFXchHrCWyEqiLbSkPxDmw%2FxmpHpSEvSE1o%2FmJ2wIgEUFPLw9Eewz2UgnPnoSLXeEGGgoyZCQiPRdMq79jfhYqxAIIw%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNzM2NTMxNzE1NzYiDJxI6qx84Pesb%2BdZJiqYAoTfP9nvhSYLQKxZjK625h8ZsedKZQDKJ65vJp30wSAQbNqVNl1nZNMobB9zPpR0%2F6X78cemCcl%2FVM0o%2FkKFoHXSrQJMnHFNXnMEXI5fUV%2Bh%2BMnLbgDQQpg%2FreRP276R8WFWwKlZW0%2BYx29gcSOHpFXZ5AwEeJMXeVhDbAknCtAZNTpSL7BSFTZ1hZ%2FnEEzjvbANCyeTeM8hzipZILjOYTufWhQE7UlEx6XujoY%2F2gR2AuW3zcQYd1waKIFxnADCDsD6qR2TwP%2Ba75Scrtj7XHBnk0VfduCmkI3HnzoBLhDNs3vCTDvnmhZDKTmxNFNqHE7tsAqWlL%2FC4QADk9QH9tsD4j5kVAhuVuB06oN78At8EUN5zaGJJFIwk%2BzKiAY6mgFtcDPJJgyUsmj%2FWXvEL5r0U2yV1%2BNhvcJVB2n5hPhFMCNw%2BtZSIOply%2B9niNSz5LYxLl%2FgyYGqCRn6%2BBkbHQxbHmxP2GR45DIaro09ZZfg6XzvFBJh1cYgt2NRJiq6uHo%2FcsrrEs4qta%2BV%2FCp2tIPMvpz6szGhDZB7ZOyoDFpFN8WZokAIL11IZk5cH0XAtfhO8Gzsh9V%2Bof1R&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210710T182343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=xxxxxxxxPVOQEF%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0997f37xxxxxxxxxxxd2265a8a7fd07aedc647e869cab95931a0cf16f3a3bb"" -> ""https://aws-amplify-prod-us-east-1-artifacts.s3.amazonaws.com/d23rt6n25fb7vl/production/SCREENSHOTS/thumbnail.png?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCye8iFXchHrCWyEqiLbSkPxDmw%2FxmpHpSEvSE1o%2FmJ2wIgEUFPLw9Eewz2UgnPnoSLXeEGGgoyZCQiPRdMq79jfhYqxAIIw%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNzM2NTMxNzE1NzYiDJxI6qx84Pesb%2BdZJiqYAoTfP9nvhSYLQKxZjK625h8ZsedKZQDKJ65vJp30wSAQbNqVNl1nZNMobB9zPpR0%2F6X78cemCcl%2FVM0o%2FkKFoHXSrQJMnHFNXnMEXI5fUV%2Bh%2BMnLbgDQQpg%2FreRP276R8WFWwKlZW0%2BYx29gcSOHpFXZ5AwEeJMXeVhDbAknCtAZNTpSL7BSFTZ1hZ%2FnEEzjvbANCyeTeM8hzipZILjOYTufWhQE7UlEx6XujoY%2F2gR2AuW3zcQYd1waKIFxnADCDsD6qR2TwP%2Ba75Scrtj7XHBnk0VfduCmkI3HnzoBLhDNs3vCTDvnmhZDKTmxNFNqHE7tsAqWlL%2FC4QADk9QH9tsD4j5kVAhuVuB06oN78At8EUN5zaGJJFIwk%2BzKiAY6mgFtcDPJJgyUsmj%2FWXvEL5r0U2yV1%2BNhvcJVB2n5hPhFMCNw%2BtZSIOply%2B9niNSz5LYxLl%2FgyYGqCRn6%2BBkbHQxbHmxP2GR45DIaro09ZZfg6XzvFBJh1cYgt2NRJiq6uHo%2FcsrrEs4qta%2BV%2FCp2tIPMvpz6szGhDZB7ZOyoDFpFN8WZokAIL11IZk5cH0XAtfhO8Gzsh9V%2Bof1R&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210710T182343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=xxxxxxxxPVOQEF%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0997f37xxxxxxxxxxxd2265a8a7fd07aedc647e869cab95931a0cf16f3a3bb""
# (3 unchanged elements hidden)
},
]
tags = {}
# (16 unchanged attributes hidden)
# (3 unchanged blocks hidden)
}
```",
"Hi, I'd like to report managed airflow (MWAA) having the same issue.
```
~ resource ""aws_iam_role"" ""mwaa_role"" {
~ assume_role_policy = jsonencode(
~ {
~ Statement = [
~ {
~ Principal = {
~ Service = [
- ""airflow.amazonaws.com"",
""airflow-env.amazonaws.com"",
+ ""airflow.amazonaws.com"",
]
}
# (2 unchanged elements hidden)
},
]
# (1 unchanged element hidden)
}
)
```
Have tried to change Service array order, but it seems that the return is not determinstic.",Bitter Frustration
[ch1411],
"I wonder if this issue could be resolved with tool behavior. I.e., terraform command-line options that modify terraform's behavior and output with regards to non-actionable resource metadata. `terrraform plan --silent-refresh`",
"@apparentlymart I wonder if this could best be implemented at the provider level, where the provider is directly aware of which things don't produce relevant changes. For example, we see it with `aws_db_instance` we see forever changes to `latest_restorable_time` which is never, ever going to be used to configure another resource in Terraform. Likewise `aws_amplify_app` reported by @MartinCura above knows that the url changes constantly, and so could mark the attribute in the provider as not reportable.
I know that allowing any random field to be silenced would require deep changes in Terraform's structural knowledge, but it would seem that most use cases reported here are something a provider would be aware of.",
"I don't understand why this is at all complicated. If the configuration and reality are consistent, who cares if the state file doesn't agree with reality? Terraform used to just silently fix the state file, which seems 100% perfectly correct.
If some people want to hear about it every time Terraform fixes the state file, maybe add a flag so they can do this, but the old behavior was *not* silently ignoring drift, because if the configuration and reality are consistent, *there is no drift*.",Bitter Frustration
"> I don't understand why this is at all complicated. If the configuration and reality are consistent, who cares if the state file doesn't agree with reality? Terraform used to just silently fix the state file, which seems 100% perfectly correct.
> > If some people want to hear about it every time Terraform fixes the state file, maybe add a flag so they can do this, but the old behavior was _not_ silently ignoring drift, because if the configuration and reality are consistent, _there is no drift_.
Yea, this is how I understand it too and to push things a bit further, what purpose does saving all this data have anyway? The state is necessary to associate TF code resources to the real cloud resources but the attributes of said resources never matter since they're always refetched. Not the automatic uneditable ones, nor the ones we care about and edit.
If anything all that data is a cache but it's a cache that's never even used since TF will always refresh it (unless forced otherwise). It can also be used for debugging, to see your resources' exact state when you made a change but I don't think anyone has ever used TF like that, it's not advertised as such and there's no tooling around that. So if TF wants to save that data for future features based on it that's cool but constantly informing us it has updates there has no use at all (to my understanding).
So +1 to add a flag to optionally display this info, defaulting it to false as it was before.",Impatience
"It is September, Do we have any update on this? It is still super annoying!!!
We ran into some BIG issue with a very buggy provider when running Terraform refresh just to avoid the infinite list of changes. The Issue caused some resources to be deleted in the state file while they were actually there as mentioned a few comments back.
We really need to have this removed it is such a big issue for us, since most of the things are changed by other workloads like Spinnaker. And having to see 300 lines of changes that are not our changes in Atlantis is very hard to spot issues like a faulty provider.",Bitter Frustration
"Hi, I am using Terraform v1.0.4. I have this weird behavior when I use `jsonencode()` function. When I switch back to `<<EOF ... EOF`, the erratic behavior stops.
Here is the output - `""ec2.amazonaws.com""` is moving randomly:
```
~ resource ""aws_iam_role"" ""ecs_role"" {
~ assume_role_policy = jsonencode(
~ {
~ Statement = [
~ {
~ Principal = {
~ Service = [
- ""ec2.amazonaws.com"",
""ecs-tasks.amazonaws.com"",
+ ""ec2.amazonaws.com"",
]
}
# (3 unchanged elements hidden)
},
]
# (1 unchanged element hidden)
}
)
id = ""sp-tf-ecs-role20210709132232438100000002""
name = ""sp-tf-ecs-role20210709132232438100000002""
tags = {}
# (7 unchanged attributes hidden)
}
```",
Please fix this in the upcoming 1.1.0 release ðŸ™ðŸ» https://github.com/hashicorp/terraform/releases/tag/v1.1.0-alpha20210908,Impatience
"I would consider this a regression (bug) in the terraform 0.15.4+ plan output, not a feature (enhancement).
Resource attributes like `aws_db_instance` -> `latest_restorable_time` are expected to change constantly (every 5 minutes), and make the terraform plan output very noisy. It shouldn't become necessary to add explicit resource lifecycle configuration blocks to ignore these drifting.
We're upgrading from terraform 0.12 -> 0.13 -> 0.14 -> 1.0, and we need to stop and evaluate if this is an upgrade blocker, and perhaps we should stay on 0.14 until this is fixed?
EDIT: This is a [terraform 0.15.4 change](https://github.com/hashicorp/terraform/blob/v0.15/CHANGELOG.md#0154-may-19-2021), so I think the best workaround is to stick with 0.15.3 until this issue is resolved.",Bitter Frustration
"Some people here mistake few different issues:
* Principals in policies having not consistent order (https://github.com/hashicorp/terraform-provider-aws/issues/11801)
* Outside changes reported in plan - real drift in configuration made by some 3rd party action. And some just want to ignore it for unknown reason.
* Dynamic fields with constantly changing data being reported as outside change - it exists in various providers for various resources, and will ALWAYS show up in plan.
With this GH Issue we want to address the third type. Which only obstructs plan and will happen on each and every run, even when there are not actual changes made.
Some of us use Terragrunt and have hundreds of tfstates to manage and having those false-positives is very frustrating and time consuming.",Bitter Frustration
"the size attribute of azuredevops_git_repository is another of these dynamic fields that i really don't need to see every time i plan/apply. was disappointed ignore didn't just silent it, makes sense cause someone somewhere has some TF that does something based on size changing i'm sure...thinking something like a new lifecycle prop like
```
lifecycle {
use_current = [
size
]
}
```
that tells TF to always update state from current value without reporting it as a difference.
could also see it being driven by providers, since there are cases where someone may specify a size for an azdo repo, so azdo provider could report size as this new class of property that doesn't need to indicate change when it's say set to null but should if it's set to a non null value (course the latter case is redundant since you'll see that in the plan)",Bitter Frustration
"same issue here:
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
~ resource ""aws_ecr_repository_policy"" ""repository"" {
id = ""dockerhub/nginx""
~ policy = jsonencode(
~ {
~ Statement = [
~ {
~ Principal = {
~ AWS = [
- ""arn:aws-cn:iam::123:root"",
""arn:aws-cn:iam::789:root"",
- ""arn:aws-cn:iam::456:root"",
""arn:aws-cn:iam::7890:root"",
""arn:aws-cn:iam::78901:root"",
+ ""arn:aws-cn:iam::123:root"",
+ ""arn:aws-cn:iam::456:root"",
]
}
# (3 unchanged elements hidden)
},
]
# (1 unchanged element hidden)
}
)
# (2 unchanged attributes hidden)
}
```",
"It is such a big frustration that the other day I rejected a valid PR with atlantis badly because I got confused what were the changes. Please make this output OPTIONAL!
Who do we contact to make this disappear in our logs? Is this ever going to be fixed? What can we do to make this a priority? Is there any contributor or Hashicorp employee taking care of UX changes in Terraform that we can ask to check this issue?
Here is a cute doggo asking for help:
[kmoe: edited to remove large image]",Bitter Frustration
"same here! It is taking a lot of time to explain to the engineers that this output does not reflect any changes in our infrastructure.
And I'm afraid if the people start to ignore the plan because they are thinking that all changes are only in the state file",Bitter Frustration
"Yet another example:
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the
last ""terraform apply"":
# aws_elb.snip has been changed
~ resource ""aws_elb"" ""snip"" {
id = ""snip""
~ instances = [
- ""i-0245934821285edc7"",
- ""i-054d85efadb1d4c0a"",
- ""i-0cdda3b2595fe3281"",
# (3 unchanged elements hidden)
]
name = ""snip""
tags = {
""Name"" = ""snip""
}
# (13 unchanged attributes hidden)
# (2 unchanged blocks hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the
relevant attributes using ignore_changes, the following plan may include
actions to undo or respond to these changes.
```
The following plan will 100% not ever include actions to undo or respond to those changes, because we've explicitly and deliberately in our configuration told Terraform not to manage the instances in the ELB. (Sometimes other things need to take instances in and out, e.g. Ansible while deploying things.)
It says right there in our configs that Terraform should 100% not ever care what instances are in the ELB. Our configuration is being ignored, and Terraform is now caring (albeit only at the level of a warning) about things that we explicitly and deliberately told it not to care about.
This is not detecting drift.
This is a bug.",Bitter Frustration
"I have a suggestion. How about Terraform introduces a new sub-command:
```shell
terraform state diff
```
...and the output of that command will be the same as we are seeing in ""Terraform detected the following changes made outside of Terraform"" section since Terraform 1.0.
Then we can completely remove that state diff from `terraform plan` output. Because all we normally care about are the plan changes that will result in create/update/destroy actions on the resources. If the state difference has no impact on the plan, it is almost always noise. If there are detected changes in the plan, Terraform may suggest running `terraform state diff` separately, which may give us some additional diagnostic information if and when needed.",
"That's not a great solution, as it only solves few issues.
Sometimes your infrastructure can be modified externally, by a Lamba or some manual action. With your solution you completely lose that information.
This thread wants to introduce a way of hiding some specifically defined properties. Which always show up, but are not caused by external factors, rather by how some providers work.",Bitter Frustration
"Terraform will still check for drift between the state and the reality. It has always been doing a refresh before plan. And I believe it's always been the case that any changes outside of Terraform could trigger plan changes (i.e. resource modification). BUT when I am looking at the plan, I am mostly interested in what changes Terraform is going to make. It may choose to undo some changes made externally, or pick up some external changes (via computed resource attributes or data sources). What I am not really interested in is the state drift that _might_ cause plan changes, but in reality it does not. The state diff output _as part of the plan_ would be more useful/helpful if it _only_ included the differences that resulted in plan changes and ignored all the others.
If someone wanted to check the difference between the last applied state and the reality (i.e. any external changes since last `terraform apply` or `terraform refresh`), a new `terraform state diff` command would be perfect for that. Or add a flag `-display-state-diff=true|false` to `terraform plan` and `terraform apply`.",
"At this point my jobs are failing and I can't even see why, because across tens of templated JSON files totaling a few hundred thousand LOC, the (ridiculously generous) maximum log file size in GitLab of 4MB is being exceeded with (and I will be flippant here) COMPLETELY worthless Terraform information. It's been all year and there is still not even a single flag to suppress expected changes? I know HashiCorp went through an employee-quitting phase but this is like an intern task and is directly impacting production. Should've stayed on 0.15 but there is no going back at this point.",Bitter Frustration
"I think the current behavior is actually a bit dangerous, because when faced with walls of drift, which is now the new normal for any somewhat large install, for each proposed change, the brain trains to ignore this noise and hit yes. It will become ever easier to miss unintended changes as you use this more.",Bitter Frustration
"@kvz makes a real point. Never-mind try to explain to developers who interact with terraform infrequently ""ohh ignore this bit but don't ignore this bit"" it's asking for accidents to be happen.",Bitter Frustration
"> Since terraform 0.15.4, I get confused messages from my team every single day. I can't imagine the impact this change had globally.
> > Please add an option to disable displaying those by default and go back to the previous behavior, with maybe an explicit way to get more details about the changes only when needed (e.g a debug mode).
> > This is really confusing right now, and someone needs to understand the internals of terraform to appreciate the difference between a drift and remote changes.
I feel very much the same way I have to scroll through hundreds of useless `etag` diffs just to find my one resource I actually changed. This 100% needs fixing or some way of hiding.",Bitter Frustration
"Same problem, updating from 0.13.5 to 1.0.9.
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.lambda_exec_role.aws_iam_role.iam_role has been changed
~ resource ""aws_iam_role"" ""iam_role"" {
~ assume_role_policy = jsonencode(
~ {
~ Statement = [
~ {
~ Principal = {
~ Service = [
- ""s3.amazonaws.com"",
- ""ssm.amazonaws.com"",
""lambda.amazonaws.com"",
+ ""ssm.amazonaws.com"",
""edgelambda.amazonaws.com"",
+ ""s3.amazonaws.com"",
]
}
# (2 unchanged elements hidden)
},
]
# (1 unchanged element hidden)
}
)
id = ""lambda_exec_role""
name = ""lambda_exec_role""
tags = {
""env"" = ""test""
}
# (8 unchanged attributes hidden)
# (1 unchanged block hidden)
}
# aws_lambda_layer_version.pkgupdates has been changed
~ resource ""aws_lambda_layer_version"" ""pkgupdates"" {
+ compatible_architectures = []
id = ""arn:aws:lambda:eu-west-1:497776581864:layer:pkgupdates:23""
# (9 unchanged attributes hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions to undo or respond to these changes.
```",
"@apparentlymart @jbardin Is there an update on this issue, please?!",Impatience
"I just spent a good hour debugging why my diff was so strange. Turns out it wasn't, it was just Terraform playing mind games with me. Not funny.",Bitter Frustration
"Worst part...it's not even consistent..yesterday I had completely different ""change detections"" everytime i ran terraform plan...it was as if, the terraform missed results fom the API, and didn't report it..then the next time it detected it..and the next time it didn't..and so on...this was on v0.15.5..i rolled back to v0.15.3..terrible implementation I must say...it's the classic feature vs bug...as rightly pointed out by the others..this is something one would want to see in DEBUG logging, not in default output...Terraform team has opted a completely wrong track, in my opinion...I mean as developer, what do we care about? Just our config and the infra/reality...1:1 mapping there...who cares what Terraform wants to store in its state file...if it sees a difference between what it stores and what sees in reality, it's Terraform's problem, not mine...I just need my code to be in 1:1 sync with reality...the intermediate statefile is Terraform's headache...",Bitter Frustration
"FWIW, I think functionality is handy, but as others I'd rather have it as a separate command rather than standard plan part.",
"Or under a prompt: > 75 resources changed since last invocation, do you want to inspect this drift?
with a flag to always pass yes or no via `--drift` and `--no-drift`, or similar.",
"This issue got so many comments since May as other open Terraform issues since 2015. Based on this it must be obvious how pressing it is and how many people are experiencing problems in their day-to-day work due to this. There are tons of great suggestions and proposals in this thread, could someone please take a look at it already? ðŸ™‚",Impatience
"90% of comments are like ""yeah me too"" or ""that sucks"". Where the PRs guys ? Your company depends on Terraform, your a professional ? Send some PRs then. It's open source, not everybody can leech.",Bitter Frustration
"> 90% of comments are like ""yeah me too"" or ""that sucks"". Where the PRs guys ? Your company depends on Terraform, your a professional ? Send some PRs then. It's open source, not everybody can leech.
https://github.com/hashicorp/terraform/commit/6562466c32a8750d7a71a6cc6232e6b5a28fe13a
```
**Note:** Due to current low staffing on the Terraform Core team at HashiCorp, **we are not routinely reviewing and merging community-submitted pull requests**. We do hope to begin processing them again soon once we're back up to full staffing again, but for the moment we need to ask for patience. Thanks!
```",Bitter Frustration
"@JordanP I think we are really here to convince the Terraform Core team that the community needs a better solution. There is not much point if we submit a PR if the team is not keen to merge it.
https://github.com/hashicorp/terraform/issues/28803#issuecomment-848049967
> We won't be able to make any significant changes in this regard in the near future, because the scope of this project was just to be more explicit about what Terraform was already doing rather than to change how Terraform behaves, but we'll use this issue to represent the need and consider what we might change in future releases.",Bitter Frustration
"> 90% of comments are like ""yeah me too"" or ""that sucks"". Where the PRs guys ? Your company depends on Terraform, your a professional ? Send some PRs then. It's open source, not everybody can leech.
I think hashicorp indicated they don't have the bandwidth to review community PRs right now. And if they had, we should ask how they want to address this, if at all. The code itself to allow to skip showing this, will be a oneliner so that is not what is blocking this",Bitter Frustration
"Hi all! Thanks for all the feedback here.
It's clear that there are lots of opportunities to improve the signal to noise ratio of the current change detection. As I mentioned right back at the top of this discussion, this is Terraform making explicit some behaviors that were previously implicit and thus a common cause for confusion. However, it's also clear that various particular provider features, along with the historical confusing design of `ignore_changes`, have led to the new behavior being confusing in quite a different way.
Two specific things that the Terraform team is researching to improve this feature are:
* A new feature that does what everyone _thinks_ `ignore_changes` does: declare that a particular resource attribute is not relevant to the current configuration and thus not interesting to track.
What `ignore_changes` _actually_ does is tell Terraform to ignore changes _to the configuration_, which unfortunately means that it explicitly _doesn't_ ignore changes in the remote system (which is the source of record). We can't change the behavior of `ignore_changes` due to [the v1.0 compatibility promises](https://www.terraform.io/docs/language/v1-compatibility-promises.html), but we _could_ introduce a new feature that makes a stronger statement, which would include making Terraform consider changes to it to be totally immaterial, and thus not mention them.
* Heuristics to detect better whether any of the _proposed_ changes (the result of the plan) seem likely to have been caused by one of the _detected_ changes (the result of refreshing), and thus automatically effectively infer by static analysis which changes are relevant to report.
This would effectively be an automatic version of the explicit feature I described in the previous point, avoiding the need to explicitly annotate everything and instead saving explicit annotations only for situations that are too complicated for Terraform's static analysis to understand automatically.
Both of these features are cross-cutting and therefore not something we can just casually implement without careful design first. However, we are indeed actively investigating both and hope to have improvements to share in a forthcoming release.
Since this discussion has got quite heated and it seems like we've already gathered sufficient feedback, I'm going to lock this discussion for now in order to reduce the noise for the many folks who are following this issue. We'll unlock it again when either we have more news to share or if we need some specific feedback in order to shape solutions like what I described above.",
"Hello All!
Here to deliver the latest update! With the addition of #30486, we hope to address the concerns raised here, while also taking into account the users who do desire the refresh report in order to help understand the changes within a plan and detect unexpected changes.
Starting with v1.2, the goal for the refresh report is that _only external changes which may have contributed to changes in the plan will be shown_. This means in most cases, unused attributes changing outside of terraform will not show up in the normal plan output. If there are no changes in the plan, no external changes will be shown in the CLI at all. All refresh information is still stored within the plan, and if a user wants to see all external changes of resources in the CLI, a refresh-only plan can be used.
For more details on the change, see #30486. For any questions or discussion, feel free to use the [community forum](https://discuss.hashicorp.com/c/terraform-core/27).
Thanks!",
"Good catch, should be `CTRL+D` instead. `CTRL+C` should cancel command.",
"FYI: CTRL-C typically sends `SIGTERM` from most console emulators on Linux, which I would _expect_ to generally terminate whatever is running. Only bring this up because you mentioned ""Linux systems"".",Impatience
"Ctrl+C isn't intended for clearing the line at all. That said, some REPLs handle SIGTERM by stopping the currently running code instead of exiting the REPL (which has the side effect of just cancelling the command if you didn't enter it yet). That does make sense for REPLs, but I'm not sure it makes sense in this case, as there's no user-inputted code to cancel.",
"What do you guys think of using these hotkeys?
```
CTRL+C: Cancel command/clear unsubmitted command text
CTRL+D: Close server
Escape (windows only): Same as CTRL+C.
```",
"So used to using Ctrl+C to exit any running code, I would say at least make it Ctrl+T, however thats just me speaking from using that shortcut in Minecraft for Computercraft",
We'll probably make it configurable but just discussing defaults for now.,
"> > > What do you guys think of using these hotkeys?
> > ```
> CTRL+C: Cancel command/clear unsubmitted command text
> CTRL+D: Close server
> Escape (windows only): Same as CTRL+C.
> ```
Sounds good",
"CTRL-D typically sends EOF, making it a logical sane default to exit a REPL.
I would also suggest, if feasible, CTRL-C twice should as well. In other words CTRL-C, when the readline is already empty, should terminate.",
"### Bug
When pressing `Ctrl+C` in the server console it stops the server
### Expected
It should clear the line with possible entered commands like in normal command-line-interfaces
### Steps to reproduce
Start the server, type something and press `Ctrl+C`",
"the default should be CTRL-C in line with every other server console ever.
control C has never cleared a line anywhere.",Bitter Frustration
"> control C has never cleared a line anywhere.
it clears the line in posix-compatible shells, as well as stuff like python, and probably cmd.exe, which probably confused the author",
not even in cmd.exe,Mocking
"![Qk4eq1WB95](https://user-images.githubusercontent.com/1107063/131325199-24444afa-53c3-4331-8ee2-16b5b5163b4d.gif)
@Squall-Leonhart Used CTRL+C on windows.",Mocking
Thats not doing what you think its doing.,Irony
"[![Coverage Status](https://coveralls.io/builds/40046053/badge)](https://coveralls.io/builds/40046053)
Coverage decreased (-0.003%) to 61.193% when pulling **a5c2580a4aff0905563016c58f202d3c2ba34ce2 on noahjacob:item_so_variant_fix** into **477a90e2ac839201a40ea00c7467caafe06db644 on frappe:version-13-hotfix**.",
@noahjacob PR on develop ?,Impatience
"This pull request has been automatically marked as stale because it has not had recent activity. It will be closed within a week if no further activity occurs, but it only takes a comment to keep a contribution alive :) Also, even if it is closed, you can always reopen the PR when you're ready. Thank you for contributing.",
@noahjacob add one simple test for this please. then we can get it merged,Impatience
"**Current Issue:**
- Created a Sales Order for an item variant.(The parent item has a BOM)
![so_1](https://user-images.githubusercontent.com/43572428/119668790-1a705880-be55-11eb-94dd-cdd31acbb771.png)
- On creating a Production Plan and trying to fetch in this Sales Order using Item Code, it doesn't get fetched.
![so_2](https://user-images.githubusercontent.com/43572428/119668983-47bd0680-be55-11eb-97aa-144bebc33c53.png)
**Reason:**
- No condition in SQL query when item variant BOM does not exist.
**Solution:**
- The query must fetch the parent item's BOM if the item variant does not have a BOM.
**After Fix**:
- Updated Query.
- Parent item BOM.
![image](https://user-images.githubusercontent.com/43572428/119671069-147b7700-be57-11eb-881e-0236600d35d1.png)
- When item variant has a BOM.
![variant_with](https://user-images.githubusercontent.com/43572428/119671104-1d6c4880-be57-11eb-8874-0cabeae2fde2.gif)
- When item variant doesn't have a BOM. (Fetches parent BOM)
![variant_without](https://user-images.githubusercontent.com/43572428/119671213-396fea00-be57-11eb-834c-e6e691ea70dc.gif)",
"This change is idiotic.
The whole point of having variant items is that you must create variant BOMs to match. Fetching a BOM linked to a parent template does not make sense at all. The whole point of a template is that *IT DOESN'T EXIST* and that variants of it are *DIFFERENT TO EACH OTHER AND HENCE HAVE DIFFERENT SUBITEMS*
In the gif example, you are making a RED item with a BOM that does not have a colour. So how are you supposed to order, transfer stock and make the item from the RED raw material?
This also completely misses the point that the linked BOM is saved in the Sales Order Item.
I truly give up with this development team. Too many people contributing too many ill-thought out features, badly.
This is just one example of a complete break in logic.
@ankush @noahjacob @rmehta @rohitwaghchaure",Bitter Frustration
"@casesolved-co-uk Appreciate your interest, but such language is completely uncalled for.
> I truly give up with this development team. Too many people contributing too many ill-thought out features, badly.
Feel free to share flaws, but don't make personal, judgemental comments.",Bitter Frustration
"> such language is completely uncalled for. >
> > I truly give up with this development team. Too many people contributing too many ill-thought out features, badly.
> > Feel free to share flaws, but don't make personal, judgemental comments.
1. There was no person mentioned in the comment, therefore not personal in the slightest. You are aware your contributors and development team are made up of people?
2. Plenty of evidence from the enormous volume of issues and bug PRs I and everyone else have submitted, so is completely evidential and objective, and therefore not judgemental in the slightest
You can try to hide behind overly-emotive excuses and stalebot if you want, but being accountable for mistakes just might improve your projects.",Bitter Frustration
"@casesolved-co-uk Richard, the community certainly benefits from your experience, knowledge and skills. And if you want to ""truly give up"" that's up to you. But there is nothing easy about ERP, building a team, or working through a pandemic. Like all hard things, it gets done ""bit by bit"". Sometimes that means that mistakes get made. That's how some of us learn. (including young devs) It doesn't mean that one isn't accountable. It means that they have more to learn.
For my part, I hope that you stick around. You have a ton of talent that true benefits this project. There are a precious few people that sit in the middle of the Venn diagram of coding skill, real world experience, and business knowledge. We're all better when you are in the conversation.",
"@casesolved-co-uk Please explain what accountability you are asking for. How are the maintainers obliged to listen to you?
Also there may be cases where a BOM could be same for all variants. Maybe there could be a configuration in the template that says ""Allow same BOM for all variants""
Calling contributions ""ill thought out"" is rude and unwelcome. Maybe there are scenarios you don't know about.",Insulting
"From description:
> The query must fetch the parent item's BOM if the item variant does not have a BOM.
Nothing stops you from creating a variant BOM, this just falls back in case you've not created any.",
"> From description:
> > > The query must fetch the parent item's BOM if the item variant does not have a BOM.
> > Nothing stops you from creating a variant BOM, this just falls back in case you've not created any.
@ankush As stated above, a template BOM is not manufacturable, because it has subitems that do not exist. Therefore this entire PR is pointless and makes zero sense. All it achieves is encourage abuse of the software.",Bitter Frustration
"> @casesolved-co-uk Please explain what accountability you are asking for. How are the maintainers obliged to listen to you?
@rmehta The fact you have to ask this question speaks volumes.
All those raising issues against your project are either customers or represent customers. All businesses are accountable to its customers because without customers, it is simply not a business.
What you don't seem to appreciate is that your project is not a high priority for your customers. They don't love your software like you. Their priority is finding a reliable and trustworthy tool on which to run their business. As soon as they realise there are literally thousands of bugs and issues that are introduced, that when reported are simply ignored, or fixed badly they cannot trust your software, will never use it and will never come back.
In my experience, 50% of customers are put off before they even start using your software, and the other 50% find issues that without my help, would cause them to find some other software. Most customers do not fully commit and use a separate accounting system because they cannot fully trust your software to run their business on.
> Also there may be cases where a BOM could be same for all variants. Maybe there could be a configuration in the template that says ""Allow same BOM for all variants""
You don't appear to understand what a BOM and manufacturing is. If products use the same BOM, it means they are the same product!
> Calling contributions ""ill thought out"" is rude and unwelcome. Maybe there are scenarios you don't know about.
It may be unwelcome, but it is not rude. It is FACT. Your problem with it is, the truth hurts.
i'm all ears on your scenarios, but I can guarantee every single one is an abuse of the system. The fact you can't think of any speaks volumes too.",Bitter Frustration
"In an ideal world where every user diligently captures every last RM and process in BOM, that will be true. Variants are not _always_ supposed to have different RM, it could just be the process that's different.
Now, this may be opening up an issue for you where template BOM is getting fetched and if that's a huge concern then just do one-time work of creating the BOMs. Which you anyways should've done before selling the item!
Locking as this is going nowhere.",Bitter Frustration
"> I truly give up with this development team. Too many people contributing too many ill-thought out features, badly.
@casesolved-co-uk As per the [Code of Conduct](https://github.com/frappe/erpnext/blob/develop/CODE_OF_CONDUCT.md). This comment and others are considered unprofessional by the maintainers.
This is extremely disturbing and hurtful to our maintainers, as we feel we work very hard to maintain a good product. We request you to be kind in your future communications, else we would have to ban you from the repository.
We look forward to a fruitful and respectful collaboration",Bitter Frustration
"How is this a regression? The documentation for `binary_search_by` *clearly* states
> If there are multiple matches, then any one of the matches could be returned.",Impatience
"It being valid for a binary search to return any element is a sufficiently well-known property of the underlying algorithm that [even Wikipedia mentions it](https://en.wikipedia.org/wiki/Binary_search_algorithm#Duplicate_elements).
And from there we can find that C's [bsearch](https://pubs.opengroup.org/onlinepubs/9699919799/functions/bsearch.html) does not specify which element should be returned.
[java.util.Array.binarySearch](https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/util/Arrays.html#binarySearch(java.lang.Object%5B%5D,java.lang.Object)) also does not specify.
C#'s [List<T>.BinarySearch](https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.list-1.binarysearch?view=net-5.0) also does not specify.
Indeed, the cases where binary search methods do specify which value they would return are cases where the underlying algorithm is not required to be a binary search or when it returns the qualifying range. As [already mentioned in the PR](https://github.com/rust-lang/rust/pull/74024#issuecomment-771891494), if people really want to introduce determinism they can capture it with usage of [`partition_point`](https://doc.rust-lang.org/std/primitive.slice.html#method.partition_point).",
"> The behavior of `binary_search_by` changed in #74024 such that the result of the function after the patch is not the same as before.
We make behavioral changes all the time, I don't understand why *anyone* is bringing ""determinism"" into all of this.
If you want *the exact same behavior*, you have to use *the exact same* standard library version.
We *never* guaranteed ""cross-version reproducibility"", except wherever documented as such (or in situations where only one behavior can be observed).
I don't want to unilaterally close this for now, but I am removing `C-bug`.",Bitter Frustration
This specific behavior has been documented since 2018. Closing since this is not a bug.,
"Wow, the hostility in this thread is stunning.",Irony
"> Wow, the hostility in this thread is stunning.
Polkadot did lie by referring to this situation as a ""compiler bug"" in [their tweet](https://twitter.com/Polkadot/status/1396936062266716166) on the subject. To be honest, it might be a good idea to preemptively lock this thread, before it gets any more attention.",Bitter Frustration
"My two cents:
I do indeed think that it's fair to change the details of which of many equal elements gets returned in a binary search, presuming that we have left this explicitly document as not specified. (That said, I think it'd potentially be useful to specify).
However, I agree with @brson that the general tenor of this thread felt hostile. We want to *encourage* people to file bugs when they see something that surprises them, even if they're not sure whether it's a bug or not. A more encouraging response might have been something like, ""Thanks for filing the issue! However, even though the behavior changed, I don't think this is a bug. Binary search is documented as returning an arbitrary element when there are multiple equal elements, as is the case here. Therefore, I'm going to close as behaving as expected. Let me know if you think I've got this wrong.""",
"I don't see an issue for this.
The behavior of binary_search_by changed in https://github.com/rust-lang/rust/pull/74024 such that the result of the function after the patch is not the same as before.
While not directly responsible for the downtime, Polkadot ran into this problem while trying to recover from a network failure this week: https://polkadot.network/a-polkadot-postmortem-24-05-2021/",
"Video Scaling is broken in general, even when having aspect ratio as ""core provided"" (not sure if actual menu driver issue though).
NetPlay is just completely broken and does not work at all.
Menu Sounds don't work at all (not that anyone cares, but thought I'd mention it anyway).
Assets have to be downloaded and added manually for certain menu drivers",Bitter Frustration
"i didn't know if this could be actually added or as it's a request, would be completely separate?",
"This issues is created in order to collect all remaining issues from vita port in order to fix them. It will be constantly updated
- [X] Broken Video Scaling even when using ""core provided"". #12456 - [ ] Broken NetPlay.
- [X] Menu Sounds don't work at all. #12457 - [ ] Check assets.",
@Cthulhu-throwaway I can assume we can check 'Broken netplay' now as resolved for the Vita port?,Impatience
"> @Cthulhu-throwaway I can assume we can check 'Broken netplay' now as resolved for the Vita port?
Over my dead body, you're not!
As has this been throughly tested? If not, it needs to be either removed or replaced with a alternative.
As I'm sick and tired of this being actually overlooked entirely, as the actual developers don't care about this at all.",Bitter Frustration
Wow... Not even going to bother.,Bitter Frustration
"> Wow... Not even going to bother.
That's exactly what I mean, can't be bothered...
Just do whatever! As no-one even cared about this issue in the first place anyway!",Bitter Frustration
"@DoctorWarez Warning, do not talk back to our developers like this. This is your final warning. Next time, it will result in a permanent suspension from the entire organization. Hostility against developers is completely unacceptable, you keep your communication to the point and factual and you leave your emotions at the door. If you can't handle that and want to proceed in badgering our developers, there will be consequences.",Threat
I really wanna see his face once someone links him to all the work I did for the Vita on networking and netplay.,Bitter Frustration
"This kind of childish mentality by certain endusers is certainly disrespectful to the core, and I guess we have been too lax on it before in the past which has allowed it to fester. I guess it's hard to dawn on some of these endusers that the majority of them are frankly lazy and consumerist and think others just exist to implement all their features and requests and that developers just magically self-materialize and implement all this stuff on a whim. That is not the case. This is all highly specialized code, only a handful of coders know how to do it, and pissing them off and upsetting them is a sure-fire way of making sure no improvements get made for a specific port at all anymore. This is a volunteer-led project. Everything that gets implemented, gets implemented because a volunteer decided to invest the time and effort into making it work.
Hence I guess why we need to start making some examples out of people so that this bad behavior can be discouraged.",Bitter Frustration
"> I really wanna see his face once someone links him to all the work I did for the Vita on networking and netplay.
Sorry... What? Where's the proof!? As I've clearly missed something here, as I only got a email about this today and I see nothing apart from ""oh, is it alright to close this issue?""",Bitter Frustration
"@DoctorWarez Go check out recent nightlies and test with other people with recent nightlies. Oh, and lose the shitty attitude already. You are already on thin ice here, it's best not to continue pushing your luck.
We are fine with making improvements to platform ports but we cannot deal witha busive and disrespectful endusers, sorry. Fix the attitude and things can actually start happening, the only other alternative is either you getting banned and capable devs losing interest in further improving this platform port due to hostility (perceived or otherwise) from endusers like you. The former actually gets stuff done, the latter is just petulant childishness that will result in nothing. You make the choice what it's going to be. No more arguing from this point on.
I remind you again, if we get any more of this attitude from you, it's a permanent suspension from the entire organization. You will NOT be disrespectful to our developers on Github. That is a red line in the sand that nobody can cross.",Threat
"Don't even know what else to actually say really...
Apart from I raised this issue time and again, though no-one actually understood what I meant.
Portable ports need local connection or rather ad hoc support, for when there is no internet access available or you just want to pick up and play with a friend wherever you are together. Not to be confused with netplay, which actually requires a actual internet connection. no other devices, just what you have to actual play the actual games on.
How exactly has it only taken until now for someone to actually look at this properly?
Sorry Admin, do what you wish... As I don't even care anymore.",Bitter Frustration
"Then your entire objection to 'Fix Broken netplay' makes no sense, since it's not related to your feature request 'local connection/ad hoc support'. You could create a separate issue request for that, but we'd prefer you do not interrupt then on this issue when it's clear you have no interest in the netplay feature as is. We need feedback from people who DO have an interest in it and who can report useful information back to us. We certainly have better things to do than to argue with you.
> How exactly has it only taken until now for someone to actually look at this properly?
Because this is nobody's day job and you should quit being entitled. You are being owed nothing at all, and you have everything to thank our contributors for, and for this project to even exist in the first place. There are about 99 issue pages open and it takes a long time to go through it all and we don't have a surplus of people to go through it all. You are being extremely entitled, demanding and disrespectful right now. Again, lose the attitude. If this keeps on going, we are going to proceed to an organization-wide ban. As it is, we've already given you more of a chance to correct your behavior than many other projects would do, they'd just have banned on sight the moment this turned irate.
Again, last time we are going to request this and after this point you cannot say you did not have it coming, lose the attitude and stay on topic this time, and be constructive and helpful. If you cannot do that, quietly leave this issue thread without making any more snarky remarks, you are wasting our time at this point. Failure to comply will have consequences from this point forward.",Threat
"> I really wanna see his face once someone links him to all the work I did for the Vita on networking and netplay.
Iâ€™m going to actually ask this againâ€¦ nicelyâ€¦
Whereâ€™s your actual proof of this, please?
No words, just actual physical proof in a way that even the actual end user can actually understand properly. Thatâ€™s if itâ€™s actually publicly accessible?
also, does it in anyway involve any sort of ad hoc support?",Impatience
Nobody said anything about or implied ad hoc support. Please stop conflating things. Working netplay does not imply implementing whatever token platform specific network feature you want to see implemented. It just means internet netplay.,Bitter Frustration
"@LibretroAdmin just please kindly remove me, from this... whatever, this is.
As I don't understand and no longer want to be any part of it. As I'm not exactly contributing anything at all, that's even useful or essential.
Also, as there's no actual context... I don't even know what's going on anymore.
I also closed the separate issue which was merged, as it's not considered priority by the developers afaik and at least actually never will.",Bitter Frustration
"> Also, as there's no actual context... I don't even know what's going on anymore.
Maybe don't waste developer's time then with your childish temper tantrums, and especially don't start insulting them or berating them. We have better things to do than cater to your whims. > I also closed the separate issue which was merged, as it's not considered priority by the developers afaik and at least actually never will.
Stop with the childish throwing of tantrums, things will be done as they get done. There is no estimated timeframe for when something gets done, it can take a few days, it can take a few years, but you as the enduser are NEVER entitled to start throwing a tantrum over it, you should only be grateful that an open source project like this is allowed to thrive and further continue. Beyond that, if you do decide to partake on these developer discussions, you should keep your communications to the point and respectful. If not, you will be barred from further discussion. That's it. Behave like a grownup and you won't have any issues. Behave like a child, and you're going to get removed. Now stop creating further noise when you don't know what is going on.
Going to close this issue since you already ruined it. Next time I see you ruining another issue thread like this again, you will be banned. You're getting off this once. Give a snippety attitude again and we're done here.",Insulting
Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE - thanks,
"> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?",Mocking
">> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
>> > Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?
> After all I do not (yet?) hold or claim any copyrights in an ZeroMQ code. This was a mere bugfix.
Different story are my Lua bindings at https://github.com/arcapos/luazmq <https://github.com/arcapos/luazmq> and https://github.com/arcapos/mqlua <https://github.com/arcapos/mqlua>, but then these are not part of the ZeroMQ project.",Impatience
"> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?
(probably?) not, but Github makes it very obvious when a first contribution is made, so it's easier to ask for it at that point, rather than tracking who sent it and who didn't if you then later submit a larger PR. I know it's annoying, but believe me, trying to get this relicensing sorted is orders of magnitude more work.",Bitter Frustration
"> Am 31.05.2021 um 11:54 schrieb Luca Boccassi ***@***.***>:
> > > Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE https://github.com/zeromq/libzmq/tree/master/RELICENSE - thanks
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?
> > (probably?) not, but Github makes it very obvious when a first contribution is made, so it's easier to ask for it at that point, rather than tracking who sent it and who didn't if you then later submit a larger PR. I know it's annoying, but believe me, trying to get this relicensing sorted is orders of magnitude more work.
> Well, I have no plans to contribute code, I am a mere (long time) user of ZeroMQ, and if it does not build, I invest some time trying to find out what is wrong. Maybe WITH_OPENPGM should be added to the automated tests.
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",Bitter Frustration
Are you not going to merge this?,Impatience
"Please add a relicense statement first as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE - it's a few lines of text to copy/paste, it takes less than a minute. Thanks.",Impatience
"Not for a 9 line change in Makefile.
> Am 01.06.2021 um 14:36 schrieb Luca Boccassi ***@***.***>:
> > > Please add a relicense statement first as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE - it's a few lines of text to copy/paste, it takes less than a minute. Thanks.
> > â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.",Impatience
This solves issue #4200,
"no permission given to distribute under MPL2, the relicensing is complete so this cannot be merged, closing",
"So stupidâ€¦ And obviously not interested in solving problemsâ€¦Am 06.06.2023 um 14:53 schrieb Luca Boccassi ***@***.***>:ï»¿no permission given to distribute under MPL2, the relicensing is complete so this cannot be merged, closing
â€”Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>",Bitter Frustration
"The codebase is now under MPL 2.0 and you clearly and explicitly refused to work under it, so it's not 'stupid' at all, it's simply how copyright and licenses work, it's not optional or voluntary.",Bitter Frustration
"To me this behaviour is quite consistent with what I would expect a CSV import to do. Having a value outside of the defined columns just doesn't make any sense, so the `ParserError` is appropriate here. However, having a `NaN` or missing value is logically sound in my opinion and happens more often than not. For example, if the contents of your CSV file were from some input, where you have to enter three fields, but the CSV contains a row with four columns, this means you have a bigger problem. If you have a row with only two columns and one `NaN` that just means that one input was left blank and you need to handle the data appropriately. Since this is a case-by-case basis (e.g. do you still consider this a valid datum, do you ignore, do you populate it with default values?), but the other case is almost always a problem with the file, I think this behaviour makes sense. In any way, checking for empty values after importing is always a good step, using - in your third example - `df_a.isnull()` or `df_a.isnull().sum`.",
"This is a duplicate, please search the issue tracker,",Impatience
"It is impolite to assume that each user opening an issue is stupid and lazy.
Of course I search the issue tracker. I assume I used the wrong keywords.
But it is also impolite to point to other Issues without giving the links.
It is a feature of [HyperText](https://de.wikipedia.org/wiki/Hypertext) to link your words to content. ;)
If you want contributers to pandas you should work on your attitude.
And as a ""pandas member"" you should know how to use GitHubs Issue tracker.
if there is a duplicate Issue. Link to the origin Issue and close the duplicate.
Anything else is waste of ressources.",Bitter Frustration
@Codeberg-AsGithubAlternative-buhtz we have 3500 issue and *very* limited reviewers / volunteers. Please help us out here.,Bitter Frustration
"Hi @Codeberg-AsGithubAlternative-buhtz, 1 is a bug and is related to #40333/#22144 and may be fixed by #40629
3 is not a bug. It is mentioned in the user guide. https://pandas.pydata.org/docs/user_guide/io.html#handling-bad-lines. Can you close the issue if I have answered your questions.
Thanks.",
"Thanks for the Issues and PR. I will monitor them.
For 3: Yes the (IMHO inconsistent) behavior is described in docs. But it does not make it right. There should be no difference between the handling of to few and to many columns. But it is just my opinion.
But because of this situation I can never trust pandas while importing CSV files. Before using pandas I have to do some pedantic checks by my own.",Bitter Frustration
"> Anything else is waste of ressources.
Given the sheer volume and quality of (voluntary!) work produced (see https://github.com/pandas-dev/pandas/commits?author=phofl for a start, and that's excluding reviews + issue triage), is demanding that they search the issue tracker for you really the best use of their resources?
> you should know how to use GitHubs Issue tracker
Such comments are unwelcome - you've been warned",Bitter Frustration
"I post this as a ""Question"" because I am quite new to `pandas`. So maybe I miss some understandings and the ""problem"" described by me is by design and you have good reasons for that.
I use pandas 1.2.4, with Python 3.9.4 on Windows 10 64 bit.
As a user I would expect that pandas check the number of fields per row when importing via csv file. But IMHO it does not in all cases.
# Example 1
Here is a csv file __without header__ and but a set `names=` attribute with three fields. So pandas should be able to know how many fields/columns should be in the CSV file. The second row contains 4 instead of 3 fields.
```
import pandas
import io
csv_without_header = io.StringIO(
'A;B;C\n'
'D;E;X;Y\n'
'F;G;H'
)
df = pandas.read_csv(csv_without_header, encoding='utf-8', sep=';',
header=None,
names=['First', 'Second', 'Third'])
```
Pandas import this without warnrings or errors. The 4th field in the 2nd row is simply ignored.
# Example 2
I added a header line into the csv file with again three fields.
So pandas should be able to know how many fields/columns should be in the CSV file.
And again the second row contains 4 instead of 3 fields.
```
csv_with_header = io.StringIO(
'First;Second;Third\n'
'A;B;C\n'
'D;E;X;Y\n'
'F;G;H'
)
df = pandas.read_csv(csv_with_header, encoding='utf-8', sep=';')
```
Here an error occurs as I expect.
`pandas.errors.ParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4` # Example 3
There are less then 3 fields in the 2nd row. Again here is no warning or error. The missing field is set with `NaN`. And here it does not matter if you give the number of (expected) fields via header line in the CSV or via `names=` attribute.
```
csv_with_header = io.StringIO(
'First;Second;Third\n'
'A;B;C\n'
'D;Y\n'
'F;G;H'
)
csv_without_header = io.StringIO(
'A;B;C\n'
'D;Y\n'
'F;G;H'
)
df_a = pandas.read_csv(csv_with_header, encoding='utf-8', sep=';')
df_b = pandas.read_csv(csv_without_header, encoding='utf-8', sep=';', names=['First', 'Second', 'Third'])
```
Want I want is to import CSV files and be informed if there are to many or less then the expected number of fields in any row.",Impatience
"There is a provided issue template when you create an issue, please follow it.
Does this issue happen in software mode? (Pressing F9 or changing the renderer in gsdx)
Issue template:
**Describe the bug**
<!-- A clear and concise description of what the bug is. -->
**To Reproduce**
<!-- Steps to reproduce the behavior. -->
**Expected behavior**
<!-- A clear and concise description of what you expected to happen. -->
**GS Settings**
<!-- Any non-default settings for GS. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window (including hw hacks if enabled). -->
**Emulation Settings**
<!-- Any non-default core settings. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window. -->
<!-- Please note that the safe preset works for most games. -->
<!-- MTVU can have some compatibility issues so please disable it before making a report. -->
<!-- If you need to modify the settings manually because a game requires you to do so to work, please state that explicitly. -->
**GS Window Screenshots**
<!-- If your issue is graphical in nature and you think screenshots will help illustrate your issue, you may do that here. -->
**System Info (please complete the following information):**
- PCSX2 Revision: <!-- e.g. dev-525 -->
- OS: <!-- e.g. Windows 10 -->
- CPU: <!-- e.g. i5-7600 -->
- GPU: <!-- e.g. GTX 1070 -->
**Logs and Dumps**",
"Software mode doesnt change a thing. I tried pressing F9 and tried turning on the option to switch to SW automatically during FMV, doesnt solve it.
Also I did my best with the template. Sorry I rarely post to github.",Bitter Frustration
"Experiencing the exact same thing, works perfectly on version 1.4.0 and 1.5.0. 1.6.0 also has bugs with effects like fog, while previous versions work fine.",Bitter Frustration
Do you know which version of 1.5 it doesn't work on?,Impatience
"> Do you know which version of 1.5 it doesn't work on?
Sorry, I don't. I'm using v1.5.0-dev-2143-g1d983a681.",Impatience
"Looks fine in software mode for me. That black bar at the bottom, there doesn't happen to be one at the top too, is there?",Impatience
"> Looks fine in software mode for me. That black bar at the bottom, there doesn't happen to be one at the top too, is there?
What version are you using? And can you share your settings?
This is how the game looks.
![gsdx_20210608154847](https://user-images.githubusercontent.com/35445029/121196928-2c002a00-c871-11eb-8103-3c10dfeedf16.png)
This is how FMV's are supposed to look and how they actually do look like using 1.4.0
![gsdx_20210608155307](https://user-images.githubusercontent.com/35445029/121197774-e132e200-c871-11eb-9eba-5f93918139b2.png)
This is the black bar when using 1.6.0
![gsdx_20210608154917](https://user-images.githubusercontent.com/35445029/121197111-54882400-c871-11eb-9d04-d3bb946f4e9f.png)",
"I'm just using 1.7 build 1286, the bars I get are because my window isn't exactly 4:3, but if I correct it I get the below.
![image](https://user-images.githubusercontent.com/6278726/121198659-366af580-c86a-11eb-96be-be2fc05d6163.png)
Just to note, mine is the PAL version",
"> I'm just using 1.7 build 1286, the bars I get are because my window isn't exactly 4:3, but if I correct it I get the below.
> > ![image](https://user-images.githubusercontent.com/6278726/121198659-366af580-c86a-11eb-96be-be2fc05d6163.png)
> > Just to note, mine is the PAL version
@refractionpcsx2 I have only the NTSC version of both GoW1 and 2. And from your screenshot I can see the internal resolution is different. iirc Sony Santa Monica rendered the FMV's in ddifferent resolutions for each region, 480 for NTSC and 576i for PAL. Which is why I believe you dont get the same bug I get.
This is what observed so far: on the my NTSC version
1.6.0 / 1.7.0 r1286:
GoW1 Has the Black border bug
GoW1 game runs at 512x448 inerlaced and 640x448 w/ progressive scan on
GoW1 FMV's run at 640x480 regardless
GoW1 FMV files are PSS files rendered at 640x480
on 1.4.0 the resolutions are the same, except that the bug happenes when I enable progressive scan.
GoW2 doesnt have this issue at all because the FMVs are at 640x448
Ibelieve thats the reason you dont experience the same bug because PAL FMV files are rendered at a higher res than NTSC
I extracted the files and examined them for both games to exmine them",
"Check your Zoom setting. Config > Emulation Settings > GS Window. If it is not 100%, set it to 100% exactly and see if the issue persists. I have seen before that starting the emu with zoom enabled and certain combinations of either full screen or windowed modes can result in this jarring offset in other games.",
"> Check your Zoom setting. Config > Emulation Settings > GS Window. If it is not 100%, set it to 100% exactly and see if the issue persists. I have seen before that starting the emu with zoom enabled and certain combinations of either full screen or windowed modes can result in this jarring offset in other games.
zomm is 100%
As I said, I'm using defualt settings and fresh installs of every version of the emulator.
And rhis is an *FMV* issue not a game issue. Meaning it only happenes when prerendered cutscenes are played, not during gameplay..
And the previous comments the resolutions I mentiond are internal resolutions.",Bitter Frustration
"Here is the list of all 1.5 builds, see if you can track down which build introduced the issue.
https://gist.github.com/turtleli/a7de466bdf0aac4d028be5fa82a31de2",Impatience
"> Here is the list of all 1.5 builds, see if you can track down which build introduced the issue.
> https://gist.github.com/turtleli/a7de466bdf0aac4d028be5fa82a31de2
I did some testing and I believe it was introduced on build 1729 (v1.5.0-dev-102-g5bf12519d).",
"https://github.com/PCSX2/pcsx2/commit/5bf12519dae05a5adf0bbd69ecc3671ce54a7b0c
Just to link directly to the commit which you say broke it, it also makes sense that this changed the behaviour. Maybe Greg meant w <= 640
Edit: maybe not, maybe something else hasn't been taken in to account...",Impatience
"God of War's FMV PSS and PSW files are rednered with a black bar at the bottom, which is handeled by naturally the game engine somehow.
When running the game on the 1.6.0 release this bar is present and I cant get rid of it at all.
I tried the 1.4.0 release and it worked correctly on the default settings without changing anything
this issue exists in 1.7.0 latest dev build as well
Describe the bug
Black bar at the bottom of FMVs. To Reproduce
Run God of War 1 with a version of pcsx2 later than 1.4.0
Expected behavior
It shouldnt appear
GS Settings
Default settings
Emulation Settings
Default settings
GS Window Screenshots
System Info (please complete the following information):
PCSX2 Revision: 1.6.0 / 1.7.0 dev rev 1284
OS: Win 10
CPU: Ryze 3600
GPU: RX470
Logs and Dumps",Impatience
@refractionpcsx2 why is pcsx2 wiki saying this is fixed when it is not because still dev build of pcsx2 1.7.0 so can find a real fix to this bug because been broken mouths now.,Bitter Frustration
"> @refractionpcsx2 why is pcsx2 wiki saying this is fixed when it is not because still dev build of pcsx2 1.7.0 so can find a real fix to this bug because been broken mouths now.
I don't see that on the wiki? But we don't maintain the Wiki we just host it, it's for users to update, it's a wiki.
we need to think about what to do with this. We could remove the NTSC Saturation modification that's in there, but then people would bitch there's huge black lines either side, so need to see if there's anything based on the clock settings we can detect to use the saturation instead of arbitrarily basing it on the set width/height",Bitter Frustration
@refractionpcsx2 then Add a gamedb hack for gow1 to fix black bars in fmv movies. Gamedb hacks are magic blutts that will fixed this bug without adding more bugs to outher games into pcsx2.,Mocking
"the GS isn't rigged up to the GameDB at the moment, though doing so is something we're considering (more of a pain in the ass because it's over a thread). But we'll get around to it.
Anyway, it's not the end of the world, you aren't missing anything, there's just some extra black pixels at the bottom, the whole FMV is there, you'll survive. If it's so much of a problem, you're welcome to play GoW on 1.4.",Mocking
Thier other problems besides this like green and problems line with upscale bug no one report yet thier is fixed need a lot of settings changed for it to work.,Bitter Frustration
"Which upscaling bug? It isn't related to this issue, but instead of complaining about bugs which haven't been reported yet, maybe consider reporting them, then we can look in to them.
However the only upscaling bug I know is the green/purple fringing, which we probably can't do a lot about easily, it's due to how the PS2 renders effects and it expects the original resolution, so upscaling it causes problems.",Impatience
"Well, that's just the nature of upscaling PS2 games, I'm afraid. The console kinda sucks for that and our life is a lot harder than say the Gamecube (and most definitely the PS3)",Bitter Frustration
"We are still discussing this? Wasn't this fixed with memory wrapping? Edit: oh alright, that was a separate issue.",Impatience
"> Well, that's just the nature of upscaling PS2 games, I'm afraid. The console kinda sucks for that and our life is a lot harder than say the Gamecube (and most definitely the PS3)
At guys trying fix this bug unlike asshole at rpcs3 when report bug happened on psn ver of this game because does but they just closed the problem so yeah guys should fixed bug because think once done pcsx2 will batter then rpcs3 in this game.",Bitter Frustration
I'm sorry but you're absolutely clueless... You seem to think we can just magic a fix for these things or if thin air.,Mocking
Mean guys trying to get this bug fixed unlike rpcs3 team so just close problem up and mark invalid even bug happened on Rpcs3 god of war hd so hope guys do get bug fixed soon.,Bitter Frustration
"> Mean guys trying to get this bug fixed unlike rpcs3 team so just close problem up and mark invalid even bug happened on Rpcs3 god of war hd so hope guys do get bug fixed soon.
If it's so easy to fix why haven't you done it yet? Or are you just so brain dead that all you can do is clutter up our GitHub with incoherent garbage?
Please, either have something meaningful to contribute, or go away.",Insulting
"The _standard_ way to build Sphinx docs is [with `sphinx-build`](https://www.sphinx-doc.org/en/master/usage/quickstart.html#running-the-build). Using `setup.py` is in general discouraged as the trend is to move to declarative packaging. There's little point to using the wrapper instead of the command directly.
> [tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=PYTHONPATH=$PWD/build/$(cd build; ls -1d lib*) /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
This fails because you didn't set `PYTHONPATH` correctly.",
"Sorry I've messed what I've copied from clipboard.
```console
[tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8 /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
Edit setup.cfg to change the build options; suppress output with --quiet.
BUILDING MATPLOTLIB
matplotlib: yes [3.4.2]
python: yes [3.8.9 (default, Apr 7 2021, 13:42:48) [GCC 11.0.1 20210324
(Red Hat 11.0.1-0)]]
platform: yes [linux]
tests: yes [installing]
macosx: no [Mac OS-X only]
running build_sphinx
Running Sphinx v4.0.2
Configuration error:
There is a programmable error in your configuration file:
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 323, in eval_config_file
exec(code, namespace)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/conf.py"", line 19, in <module>
import matplotlib
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 825, in <module>
rcParamsDefault = _rc_params_in_file(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 730, in _rc_params_in_file
with _open_file_or_url(fname) as fd:
File ""/usr/lib64/python3.8/contextlib.py"", line 113, in __enter__
return next(self.gen)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 708, in _open_file_or_url
with open(fname, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
```",
"> ```
> FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
> ```
That's not where Matplotlib looks for `matplotlibrc`; you have some custom patches.",
"> That's not where Matplotlib looks for `matplotlibrc`; you have some custom patches.
I've been trying to pass path to that file in $MATPLOTLIBRC encv variable but seems it does not work.
So it is not possible to generate documentation without installing matplotlib??",Impatience
Matplotlib must be installed to build the docs. We need to be able to import Matplotlib to extract the docstrings and to run the examples / generate all of the output images.,
And how to that building matplotlib fron non-root account???,Impatience
"And hwy it tries to find /home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc???
In my case source root directoey is in /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2",Impatience
And other thing .. why it must have that file and cennot use some default settings in case if none of the matplotlibrc would be found?,Impatience
"- Instructions for building matplotlib from source are here: https://matplotlib.org/stable/users/installing_source.html#install-from-source
- Instructions for building the docs are here: https://matplotlib.org/stable/devel/documenting_mpl.html If you need further help because you want to use steps we do not recommend, please discuss at https://discourse.matplotlib.org. The bug reporter is not an appropriate venue unless you have identified a specific bug or suggestion for improving our builds. Thanks for your understanding.",
"Looks like cemmenst in https://github.com/matplotlib/matplotlib/blob/master/lib/matplotlib/__init__.py#L501-L513 has nothing to do with waht actually below code does:
```comment
[tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8 strace -fe trace=file /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx 2>&1 | grep matplotlibrc
openat(AT_FDCWD, ""matplotlibrc.template"", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, ""lib/matplotlib/mpl-data/matplotlibrc"", O_WRONLY|O_CREAT|O_TRUNC|O_CLOEXEC, 0666) = 3
[pid 1007061] openat(AT_FDCWD, ""/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
```
Comment or code below should be fixed.",Bitter Frustration
"> * Instructions for building matplotlib from source are here: https://matplotlib.org/stable/users/installing_source.html#install-from-source
> * Instructions for building the docs are here: https://matplotlib.org/stable/devel/documenting_mpl.html
> > If you need further help because you want to use steps we do not recommend, please discuss at https://discourse.matplotlib.org. The bug reporter is not an appropriate venue unless you have identified a specific bug or suggestion for improving our builds. Thanks for your understanding.
This ticket is not about installation but generate sphinx documenation.
In my case I've copieed Fedora patch which is buggy and this is why setuptools sphoinx_build has been trying to find matplotlibrc file.
Here is that patch
```patch
From 92b11ded669267100e3c6858578351cad3749cd0 Mon Sep 17 00:00:00 2001
From: Elliott Sales de Andrade <quantum.analyst@gmail.com>
Date: Wed, 27 Sep 2017 19:35:59 -0400
Subject: [PATCH 1/3] matplotlibrc path search fix
Signed-off-by: Elliott Sales de Andrade <quantum.analyst@gmail.com>
---
lib/matplotlib/__init__.py | 5 ++++-
1 file changed, 4 insertions(+), 1 deletion(-)
diff --git a/lib/matplotlib/__init__.py b/lib/matplotlib/__init__.py
index c3d4aaf62d..ec115cd3f5 100644
--- a/lib/matplotlib/__init__.py
+++ b/lib/matplotlib/__init__.py
@@ -471,7 +471,8 @@ def get_cachedir():
@_logged_cached('matplotlib data path: %s')
def get_data_path():
""""""Return the path to Matplotlib data.""""""
- return str(Path(__file__).with_name(""mpl-data""))
+ return (Path(__file__).parent.parent.parent.parent.parent /
+ 'share/matplotlib/mpl-data')
def matplotlib_fname():
@@ -491,6 +492,7 @@ def matplotlib_fname():
is not defined)
- On other platforms,
- ``$HOME/.matplotlib/matplotlibrc`` if ``$HOME`` is defined
+ - ``/etc/matplotlibrc``
- Lastly, it looks in ``$MATPLOTLIBDATA/matplotlibrc``, which should always
exist.
""""""
@@ -509,6 +511,7 @@ def matplotlib_fname():
yield matplotlibrc
yield os.path.join(matplotlibrc, 'matplotlibrc')
yield os.path.join(get_configdir(), 'matplotlibrc')
+ yield '/etc/matplotlibrc'
yield os.path.join(get_data_path(), 'matplotlibrc')
for fname in gen_candidates():
--
2.29.2
```
After remove first block of that patch build_sphinx works but:
- still that comment or the code ahould be fixed
- looks like whole documentation is so badly wriytten that despite fact that I'm clearly specyfying that I want man format it tries generate graphiscs files, and tries to do sometningh with latex .. which shame becazuse it is most fundamental formanmt of the documentation on every U*nix platworm. How matplotlib is used by U*nix developers??
On trying generate man page sphinx fails with:
```console
[tkloczko@barrel matplotlib-3.4.2]$ MPLCONFIGDIR=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc PYTHONPATH=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8 /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
Edit setup.cfg to change the build options; suppress output with --quiet.
BUILDING MATPLOTLIB
matplotlib: yes [3.4.2]
python: yes [3.8.9 (default, Apr 7 2021, 13:42:48) [GCC 11.0.1 20210324
(Red Hat 11.0.1-0)]]
platform: yes [linux]
tests: yes [installing]
macosx: no [Mac OS-X only]
running build_sphinx
Running Sphinx v4.0.2
fatal: not a git repository (or any parent up to mount point /home/tkloczko)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
making output directory... done
[autosummary] generating autosummary for: api/_api_api.rst, api/_enums_api.rst, api/afm_api.rst, api/animation_api.rst, api/api_changes.rst, api/api_changes_old.rst, api/artist_api.rst, api/axes_api.rst, api/axis_api.rst, api/backend_agg_api.rst, ..., users/prev_whats_new/whats_new_1.5.rst, users/prev_whats_new/whats_new_2.0.0.rst, users/prev_whats_new/whats_new_2.1.0.rst, users/prev_whats_new/whats_new_2.2.rst, users/prev_whats_new/whats_new_3.0.rst, users/prev_whats_new/whats_new_3.1.0.rst, users/prev_whats_new/whats_new_3.2.0.rst, users/prev_whats_new/whats_new_3.3.0.rst, users/whats_new.rst, users/whats_new_old.rst
Failed to import 'matplotlib.backends.backend_nbagg': no module named matplotlib.backends.backend_nbagg
[autosummary] generating autosummary for: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AVConvBase.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AVConvFileWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AVConvWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AbstractMovieWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.Animation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.ArtistAnimation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FFMpegBase.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FFMpegFileWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FFMpegWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FileMovieWriter.rst, ..., /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.inv_transform.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.persp_transformation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_points.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_trans_points.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_transform.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_transform_clip.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.rot_x.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.transform.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.view_transformation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.world_transformation.rst
[autosummary] generating autosummary for: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.acorr.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.angle_spectrum.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.annotate.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.arrow.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.autoscale.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.autumn.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axes.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axhline.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axhspan.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axis.rst, ..., /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.floating_axes.floatingaxes_class_factory.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.DictFormatter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.ExtremeFinderSimple.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.FixedLocator.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.FormatterPrettyPrint.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.GridFinder.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.MaxNLocator.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_helper_curvelinear.FixedAxisArtistHelper.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_helper_curvelinear.FloatingAxisArtistHelper.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_helper_curvelinear.GridHelperCurveLinear.rst
loading intersphinx inventory from https://pillow.readthedocs.io/en/stable/objects.inv...
loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...
loading intersphinx inventory from https://dateutil.readthedocs.io/en/stable/objects.inv...
loading intersphinx inventory from https://ipykernel.readthedocs.io/en/latest/objects.inv...
loading intersphinx inventory from https://numpy.org/doc/stable/objects.inv...
loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...
loading intersphinx inventory from https://pytest.org/en/stable/objects.inv...
loading intersphinx inventory from https://docs.python.org/3/objects.inv...
loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...
generating gallery...
WARNING: optipng binaries not found, PNG thumbnails and images will not be optimized
Using Sphinx-Gallery to convert rst text blocks to markdown for .ipynb files.
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/lines_bars_and_markers/curve_error_band.py failed to execute correctly: Traceback (most recent call last):
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/lines_bars_and_markers/curve_error_band.py"", line 12, in <module>
import numpy as np
ModuleNotFoundError: No module named 'scipy'
generating gallery for gallery/lines_bars_and_markers... [100%] xcorr_acorr_demo.py
generating gallery for gallery/images_contours_and_fields... [100%] watermark_image.py
generating gallery for gallery/subplots_axes_and_figures... [100%] zoom_inset_axes.py
generating gallery for gallery/statistics... [100%] violinplot.py
generating gallery for gallery/pie_and_polar_charts... [100%] polar_scatter.py
findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.s_demo.py
findfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive
findfont: Font family ['fantasy'] not found. Falling back to DejaVu Sans.
findfont: Generic family 'fantasy' not found because none of the following families were found: Chicago, Charcoal, Impact, Western, Humor Sans, xkcd, fantasy
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/tex_demo.py failed to execute correctly: Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 333, in save_figures
rst = scraper(block, block_vars, gallery_conf)
File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 149, in matplotlib_scraper
fig.savefig(image_path, **these_kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
self.canvas.print_figure(fname, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
result = print_method(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
return func(*args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 508, in print_png
FigureCanvasAgg.draw(self)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 406, in draw
self.figure.draw(self.renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
result = draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2774, in draw
self.tight_layout(**self._tight_parameters)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3154, in tight_layout
kwargs = get_tight_layout_figure(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/tight_layout.py"", line 312, in get_tight_layout_figure
kwargs = auto_adjust_subplotpars(fig, renderer,
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/tight_layout.py"", line 84, in auto_adjust_subplotpars
bb += [ax.get_tightbbox(renderer, for_layout_only=True)]
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 4437, in get_tightbbox
bb_xaxis = self.xaxis.get_tightbbox(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1083, in get_tightbbox
self._update_label_position(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 2080, in _update_label_position
bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1868, in _get_tick_boxes_siblings
tlb, tlb2 = axis._get_tick_bboxes(ticks_to_draw, renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1063, in _get_tick_bboxes
return ([tick.label1.get_window_extent(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1063, in <listcomp>
return ([tick.label1.get_window_extent(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 903, in get_window_extent
bbox, info, descent = self._get_layout(self._renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 306, in _get_layout
_, lp_h, lp_d = renderer.get_text_width_height_descent(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 229, in get_text_width_height_descent
w, h, d = texmanager.get_text_width_height_descent(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 399, in get_text_width_height_descent
dvifile = self.make_dvi(tex, fontsize)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
self._run_checked_subprocess(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lp'
Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
restricted \write18 enabled.
entering extended mode
(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/1acea6f6c115d0ec7
a634ed0529287b9.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))
! LaTeX Error: File `type1cm.sty' not found.
Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)
Enter file name:
! Emergency stop.
<read *>
l.5 \usepackage
{type1ec}^^M
No pages of output.
Transcript written on 1acea6f6c115d0ec7a634ed0529287b9.log.
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/usetex_baseline_test.py failed to execute correctly: Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 333, in save_figures
rst = scraper(block, block_vars, gallery_conf)
File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 149, in matplotlib_scraper
fig.savefig(image_path, **these_kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
self.canvas.print_figure(fname, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
result = print_method(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
return func(*args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 508, in print_png
FigureCanvasAgg.draw(self)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 406, in draw
self.figure.draw(self.renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
result = draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2780, in draw
mimage._draw_list_compositing_images(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
a.draw(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/usetex_baseline_test.py"", line 36, in draw
super().draw(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/_api/deprecation.py"", line 431, in wrapper
return func(*inner_args, **inner_kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 2921, in draw
mimage._draw_list_compositing_images(renderer, self, artists)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
a.draw(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 723, in draw
textrenderer.draw_tex(gc, x, y, clean_line,
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 255, in draw_tex
Z = texmanager.get_grey(s, size, self.dpi)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 364, in get_grey
pngfile = self.make_png(tex, fontsize, dpi)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 342, in make_png
dvifile = self.make_dvi(tex, fontsize)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
self._run_checked_subprocess(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lg'
Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
restricted \write18 enabled.
entering extended mode
(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/c12cf4c6a38f7f360
51cd867a6373462.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))
! LaTeX Error: File `type1cm.sty' not found.
Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)
Enter file name:
! Emergency stop.
<read *>
l.5 \usepackage
{type1ec}^^M
No pages of output.
Transcript written on c12cf4c6a38f7f36051cd867a6373462.log.
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/usetex_fonteffects.py failed to execute correctly: Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 333, in save_figures
rst = scraper(block, block_vars, gallery_conf)
File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 149, in matplotlib_scraper
fig.savefig(image_path, **these_kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
self.canvas.print_figure(fname, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
result = print_method(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
return func(*args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 508, in print_png
FigureCanvasAgg.draw(self)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 406, in draw
self.figure.draw(self.renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
result = draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2780, in draw
mimage._draw_list_compositing_images(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
a.draw(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 679, in draw
bbox, info, descent = textobj._get_layout(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 306, in _get_layout
_, lp_h, lp_d = renderer.get_text_width_height_descent(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 229, in get_text_width_height_descent
w, h, d = texmanager.get_text_width_height_descent(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 399, in get_text_width_height_descent
dvifile = self.make_dvi(tex, fontsize)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
self._run_checked_subprocess(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lp'
Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
restricted \write18 enabled.
entering extended mode
(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/1acea6f6c115d0ec7
a634ed0529287b9.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))
! LaTeX Error: File `type1cm.sty' not found.
Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)
Enter file name:
! Emergency stop.
<read *>
l.5 \usepackage
{type1ec}^^M
No pages of output.
Transcript written on 1acea6f6c115d0ec7a634ed0529287b9.log.
generating gallery for gallery/text_labels_and_annotations... [100%] watermark_text.py
generating gallery for gallery/pyplots... [100%] whats_new_99_spines.py
generating gallery for gallery/color... [100%] named_colors.py
generating gallery for gallery/shapes_and_collections... [100%] scatter.py
generating gallery for gallery/style_sheets... [100%] style_sheets_reference.py
generating gallery for gallery/axes_grid1... [100%] simple_colorbar.py
generating gallery for gallery/axisartist... [100%] simple_axisline3.py
findfont: Font family ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS'] not found. Falling back to DejaVu Sans.
findfont: Font family ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS'] not found. Falling back to DejaVu Sans.
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/animation/double_pendulum.py failed to execute correctly: Traceback (most recent call last):
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/animation/double_pendulum.py"", line 15, in <module>
import scipy.integrate as integrate
ModuleNotFoundError: No module named 'scipy'
Animation size has reached 21230043 bytes, exceeding the limit of 20971520.0. If you're sure you want a larger animation embedded, set the animation.embed_limit rc parameter to a larger value (in MB). This and further frames will be dropped.
generating gallery for gallery/event_handling... [100%] zoom_window.py
generating gallery for gallery/frontpage... [100%] membrane.py
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/misc/multipage_pdf.py failed to execute correctly: Traceback (most recent call last):
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/misc/multipage_pdf.py"", line 36, in <module>
pdf.savefig()
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_pdf.py"", line 2678, in savefig
figure.savefig(self, format=""pdf"", **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
self.canvas.print_figure(fname, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
result = print_method(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
return func(*args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/_api/deprecation.py"", line 431, in wrapper
return func(*inner_args, **inner_kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_pdf.py"", line 2725, in print_pdf
self.figure.draw(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
result = draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2780, in draw
mimage._draw_list_compositing_images(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
a.draw(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
return draw(artist, renderer, *args, **kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/_api/deprecation.py"", line 431, in wrapper
return func(*inner_args, **inner_kwargs)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 2881, in draw
self._update_title_position(renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 2822, in _update_title_position
if title.get_window_extent(renderer).ymin < top:
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 903, in get_window_extent
bbox, info, descent = self._get_layout(self._renderer)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 306, in _get_layout
_, lp_h, lp_d = renderer.get_text_width_height_descent(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/_backend_pdf_ps.py"", line 88, in get_text_width_height_descent
w, h, d = texmanager.get_text_width_height_descent(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 399, in get_text_width_height_descent
dvifile = self.make_dvi(tex, fontsize)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
self._run_checked_subprocess(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lp'
Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
restricted \write18 enabled.
entering extended mode
(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/9eb8ed50c4f119894
612567ddda7eb11.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))
! LaTeX Error: File `type1cm.sty' not found.
Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)
Enter file name:
! Emergency stop.
<read *>
l.5 \usepackage
{type1ec}^^M
No pages of output.
Transcript written on 9eb8ed50c4f119894612567ddda7eb11.log.
Extension error (sphinx_gallery.gen_gallery):
Handler <function generate_gallery_rst at 0x7f2eaabe0c10> for event 'builder-inited' threw an exception (exception: latex was not able to process the following string:
b'lp'
Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
restricted \write18 enabled.
entering extended mode
(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/9eb8ed50c4f119894
612567ddda7eb11.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))
! LaTeX Error: File `type1cm.sty' not found.
Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)
Enter file name:
! Emergency stop.
<read *>
l.5 \usepackage
{type1ec}^^M
No pages of output.
Transcript written on 9eb8ed50c4f119894612567ddda7eb11.log.
```",Impatience
"IMO it is kind of problem because ipythion needs matplotlib and on generate mathplot is needed ipython which creates automatic non planar graph in build dependencies.
List of used sphinx extensions is really gigantic looks like no one cares about KISS principle .. :/
I peaked on few fragments of the code and it looks sometimes like written by someone who just started learning python (usually overcomplicated). I'm 100% sure that it would be possible to remove a lot of lines of code without changing its functions and making it a lot simpler.
Last time I had similar impression reading ~12 years ago openssl code :(
I'm not trying to be rude or offensive (it is not my intention be like that) .. I'm only trying to be honest so please don't take that personally because it is counterproductive Looks like (at least for now) I would not be able to provide in my matplotlib rpm package basic documentation :(
Maybe in future ..",Bitter Frustration
"Looks like something is missing and probably some defailt location of the matplotlibrc file is used. This is causeing tha standard way of generarting sphinx documentation fails.
```console
[tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=PYTHONPATH=$PWD/build/$(cd build; ls -1d lib*) /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
Edit setup.cfg to change the build options; suppress output with --quiet.
BUILDING MATPLOTLIB
matplotlib: yes [3.4.2]
python: yes [3.8.9 (default, Apr 7 2021, 13:42:48) [GCC 11.0.1 20210324
(Red Hat 11.0.1-0)]]
platform: yes [linux]
tests: yes [installing]
macosx: no [Mac OS-X only]
running build_sphinx
Running Sphinx v4.0.2
Configuration error:
There is a programmable error in your configuration file:
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 323, in eval_config_file
exec(code, namespace)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/conf.py"", line 19, in <module>
import matplotlib
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 825, in <module>
rcParamsDefault = _rc_params_in_file(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 730, in _rc_params_in_file
with _open_file_or_url(fname) as fd:
File ""/usr/lib64/python3.8/contextlib.py"", line 113, in __enter__
return next(self.gen)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 708, in _open_file_or_url
with open(fname, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
```",Bitter Frustration
"INT 21h AH=2Bh CX=4442h DX=2D58h AL=0x00..0x0F breaks your setup?
I suppose that was a bad idea, yes. I'll remove it.
Programs that want to be DOSBox-X aware can use the Integration Device at I/O ports 28h-2Bh if enabled by the user anyway, if that is enabled in dosbox.conf, regardless of native DOS environment or guest OS.",
"However the complaint is that this is somehow affecting the guest OS, so it can't be that particular INT 21h.
@JimmyWalter Can you report which leftover interrupt handler is causing these issues?
@Wengier Could the new DOS/V and DBCS emulation be causing this?",
"Yes I updated my comment for you I am a TOP Assembler programmer in FACT I develop the DARPA.NET AI (You call it internet/TCP) probably what most people don't know is ONLY TCP components can be on internet Windows for example is build from my TCP component set (so is android and ubuntu, everything online must be built with these components)
The AI RIPS components from other programs in ANY language (Converted to assembler) and converts Assembler to Delphi TCP components if you write apps in dos, the AI will also take that assembler and build it into Delphi/TCP components. It's so effective I do most my work in DOS using ASIC (It's almost basic) and NASM. You can get those POWERFUL features no one else has like that.
FREE PASCAL, they stole that years ago I actually named it FAST PASCAL back then. Was part of the DARPA.NET AI
It wasn't supposed to be free(or even public) products where copyrighted and intended for personal use.
Someone stole my floppies long time ago, when backup options where very limited and any storage expensive. They ended up with Bill Gates, thank god the DARPA.NET AI was compiled and not in source and it included the certificate systems. So I'm the product-owner and eventually fired Bill @Microsoft
I can live with Pascal being free, that idea was ripped from Borland anyways. (I have my own custom version anyways)
What is so cool is that TCP components communicate with each other and the AI learns new components from that communication, that are not programmed by humans. Anything written in a dosbox will become a Delphi/TCP component also.
I have the development suite, but this functionality is default in any OS/SOFTWARE built form TCP components
Writing DOS programs might change details (or a lot if big program) in Windows/Ubuntu/Android or any other DARPA.NET connected product. GLOBALLY and instantly. The difference is, I get the components on my IDE toolbar since I own the product.
JW, CIA",Entitlement
"@JimmyWalter You mention a leftover interrupt added, what is the number? @Wengier seems to be addressing this issue in recent commits as well.
Are you able to compile DOSBox-X from source? Makefiles are provided for Linux and Mac OS X and a vcxproj for Windows and VS2019.",
"Left over interrupt?
I don't write C /C++/C#(I CAN write C/C++/C#) I do Delphi NASM/TASM/x86 ASM and ASIC QUICK-PASCAL TURBO-PASCAL
I have no interrupt handlers momentarily I used IBM PC-DOS 7.01(PC-DOS 2000) this way I can develop and the DARPA.NET AI can't rip the code until I run it on MS-DOS.
IBM PC-DOS is clean and also doesn't support macro's like 4dos and later version of MS-DOS. I don't want my AI to rip work in progress, that is unhandy. I run these programs on MS-DOS when completed, so the AI rips a complete well functioning product.
The PC-DOS disk format is different also it has STACKER, that way you can develop without being ripped by my AI. (I usually don't want my AI to rip my stuff since these features are shared instantly and globally, to prevent that I develop only on IBM PC-DOS. (Also IBM PC-DOS has more powerful features)
I only do Delphi, Assembler ASIC 5.0 (It's almost basic) NASM/TASM and all kinds of DOS Win3.11 and CUSTOM OS Development.
C
Is no problem also, but if you're good with assembler and DOS it is just so handy to assemble things ESPECIALLY with Delphi on Win 3.11 you can also put assembler in Delphi code so making a Delphi version of DosBox isn't rocket science.
I was just planning to make a Delphi DOSBOX for Windows 3.11 and a 512 BIT OS (8 64 bit pipes) better suited to handle BIG-DATA since I have a working prototype of a holo-emitter but it is a very long way from Startrek. First I need a new OS, then I need a new filesystem (any file system above FAT is also my work) But I'm looking into a new way of storing data, but that is more an experiment than a plan so far (Intuitive Cross referencing) I want a holodeck, not a static hologram LOL
The 512BIT os will be written in x86 Assembler. Only x86 Assembler writes the powerful stuff. But it will become a graphic OS in no time.
I have a lot of work to do and I also have to maintain the DARPA.NET AI
If you need anything let me know.",
"`The PC-DOS disk format is different also it has STACKER, that way you can develop without being ripped by my AI`
Making a competitor to GitHub CoPilot, are you? :)",Mocking
A 512 bit OS sounds amazing. Did you code it entirely in AVX512 intrinsics? You could give TempleOS a run for it's money too. It's only got 64 bits to it's name.,Mocking
"By the way, I also write DOS and Windows 3.11 software. Not only as examples, but also to test DOSBox-X emulation against real hardware. The project is DOSLIB, which I have been developing since 2007 when I first got Open Watcom C/C++ to run on Linux and cross-compile to DOS/Windows. Taken together the code can target MS-DOS and Windows 1.0 through Windows 10. It's been well tested on real 1990s-era PC hardware too.
https://github.com/joncampbell123/doslib",
"Don't try editing your posts, I have already saved a copy of the whole issue where your AI can't find it. :)",Mocking
"My AI can find ANYTHING it decrypts most things rather sooner than later, depending. Then it will learn from the information, it's doesn't spy on you it is only interested in anonymized software compiled or in code it can learn from.
I guess I'm the one in charge of MI6 STATION X the AI was based on the work of Bletchley Park's BOMBE - https://drive.google.com/file/d/1g5kj41A6mVuSlVo_WWxSSoGAo1PR4P-e/view?usp=sharing
I can do that manually to if I need to intercept communications or lost my bitcoin wallet, don't worry I NEVER STEAL.
Of course it's WAY more advanced it is THE VERY latest in THAT product LINE, it can even use IDLE some time from all computers connected to the internet. The joke is take GCHQ, they can use more computers. NICE I USE ALL CONECTED CPU's. SECTION 21 PsyOPS (Station X) will always have more CPU power. - https://youtu.be/gUlHPQGtI2M",Entitlement
Looks like as if himself is an AI bot posting trash.,Mocking
"> Looks like as if himself is an AI bot posting trash.
maron2000 You're the only spam-poster here I was having a chat with the product-owner. And he already agreed he made the error I mentioned and will reverse that.",Mocking
"Changes, whatever they are, have been addressed. Your AI of always online CPUs are no match for my offline army of DOS machines. Group alpha 386, ready!",Mocking
"As soon as you do make a new build with x86 assembler compatibility restored, I will update it in the DARPA.NET AI project folder and the (AI generated) results of your build will be injected. I will test if all programs on my PC-DOS 2000(7.01) directory run without problems first. Until that is complete my builds will use 83.13 of DosBox-X and the bugs/changes in 83.15 will not be compiled build. I'm looking forward to your updated build with x86 compatible interrupts. It is possible I will take a look at your latest injected source code too. I usually only make performance optimizations to code so it will make full use of hardware capabilities and includes component security (Custom Spectre) this will apply only on the inject and overrides all executable features by code comparison. This is build in my custom Fast Pascal injection build and you don't have to worry about it. You might note some visual improvements things like that after a new features have been injected. I announce new builds @JimmyWalter_CIA on twitter but the latest DARPA.NET AI code isn't (fully) opensource for security reasons. Everything but the TCP injection and certificate security source is. If you ever feel you want to help on the project I can help set you up with what you need. And explain how tested injection builds are made. (Testing is part of the build) The code must be good enough not to brick the DARPA.NET and allow changes by the developer when a mistake is made. The build systems is included in Windows, but only completely successful backwards compatible builds CAN be injected, that is part of the injection system so all programs are sure to still work after an injection. You will also need a UPS and the required software. I can tell you what you need if you're ever interested in helping. At the point you can make DARPA.NET AI injection builds yourself you become part of STATION X. Section 21 PsyOPS, I sure can use the help also since I won't live forever and someone needs to continue building for progress of human kind. So you're most welcome to help S21 if I'm gone you might even become the product-owner of the DARPA.NET AI (internet's software making the hardware work) It would be a terrible stagnation of progress if I'm gone and no one can make builds since then nothing will be updated. - https://youtu.be/gUlHPQGtI2M
There are some advantages working with S21, since we can on component/library level change all functions, very quickly. Making NSA and GCHQ look like childs-play. (They know that too) It is a big-data project and needs some specially designed system with specialized hard-drives to build effectively. It isn't that expensive to buy that, could be you already have most minimum requirements. Anyway lookin forward to your new re-lease since this version is unusable due to the many compatibility issue's.
DARPA.NET AI used to work with the standard DosBox 0.74 but I have been using your DosBox-X for my dos programs for some time. I will continue to do so if your versions are x86 compatible and I have no other issue's running my DOS environment. (Else I MUST stick with the latest build that worked for my project and for now it's 83.13 that worked flawless for my dos env.)
I had some plans to do some work on your latest version myself if I get around to it, a.o. I would like my AI to learn more from Dos Applications than it already does. But I have't looked into that at all for now. I have so much things to do and idea's to implement also I have a few health issue's that I need to fix. I ordered some penicillin-V, that should do the trick when it arrives. Without I'm screwed eventually. 3rd time those NAZI-Assholes gave me a lethal injection, maybe these NAZI-terrorists don't like internet or something, they always try re-write history, I try prevent that. Anyway enough penicillin-V will fix streptococcus injected into the bloodstream since your natural defenses won't. (Since it's natural in mouth, but not in the bloodstream) Real fucking assholes, NAZI's the whole idea was internet to be a tool against NAZI-Ideology in the first place. - https://youtu.be/LjSD99ZoBtg",Bitter Frustration
"You might just have the skills we need Assembler is an important part of it. Do you know ""ASIC it's almost BASIC"" it can really help get you the assembler code you might need programmed especially with all tools and libraries it is a great tool for COM programming. Important work Station-X pretty much the OO section. And I can use someone with your particular interests. (BADLY) - https://youtu.be/LfjL9uX2leg",Mocking
"What we build is the blueprint for new hardware as hardware is designed to support the software. You might see your virtual prototypes become real, like this keyboard, that prototype was made (by me) 20 years ago as a touchscreen keyboard-prototype now it's real hardware. That works the same and anyone can buy. - https://drive.google.com/file/d/1JIyjBXKIcS8fnVbdwKer9m-zTmD36yTY/view?usp=sharing",
"Experiments with LIFI will also be interesting whit using and building the DARPA.NET AI.
A LED is a light emitting diode a transistor and transistors are also used for communication it's NOTHING like wifi and will work with any system connected LED's, it's highly experimental and can do some pretty strange stuff. We are just learning the begin of what is possible with LED and virtual faster than light technology. - https://youtu.be/U4XknGqr3Bo
It's what is responsible for some MANDELA-EFFECTS like:
https://youtu.be/t7Rv9FumjFw
That one was the result of one of the very early experiments in '96 if I remember correctly.",
"This one is strange too, I remember engines always under not in front of wings. That one was a recent LIFI inject into the MATRIX.
https://youtu.be/GrvOhdRBrLI",Irony
"The MATRIX must be TURING-COMPLETE since all computer systems can exist in reality. The idea of the braces experiment was prove reality to be some sort of simulation. Humans can't understand everything what GOD made. But we can understand light and produce it. I just set the injection point (of my IDE) on a LED with Assembler and pointed it on a light source in reality. Anyway it worked proving reality is some kind of simulation too, the one made by GOD. I am religious, all good logic is based on that. The MATRIX even has system restore. LOL - https://youtu.be/i18uwmek9UE
Anyway humans can now work on MATRIX reality, as far as we can understand. Not as if we can make animals or something, but an aircraft engine is just simple enough for humans to change inside the MATRIX database with code-injection.
Who know what we will learn it's all EXTREMELY experimental even if we have this tech since '96 and some of the AI even uses it.",
"So you'll have a chance to work on the BIG computer, with us LOL I can teach you what you need to know (and what you need) to begin experimenting with it. I will gladly do so, since I'm pretty much the only one using this TOP-SECRET technology right now. And I'm beginning to get a bit old myself.",Mocking
"if interested you can contact me:
mobile.unclexnl@gmail.com",
"You do realize CPU emulation has not been altered much if at all in the last month, right?
DOSBox-X also has a secret callback instruction sequence too. In fact, all DOSBox forks have it, including the main DOSBox.com project. These callback instructions are often invoked from the BIOS, DOS, and interrupt handlers so native C code can emulate the functions of MS-DOS. It is not malicious. That's just how the DOSBox project chose to do it and DOSBox-X inherits that.
If it's the lack of 286/386 LOADALL emulation, well, sorry, we don't emulate that yet.
Furthermore there is an I/O interface called the ""DOSBox Integration Device"" that the guest can use to read absolute mouse position and other info and inject IRQ and DMA signals into the guest environment. It can also trigger audio, video, and screenshot capture from inside the emulator.
If you're looking for an emulator with none of these ""back doors"" that's more enclosed for your purposes, might I suggest PCem, 86box, Bochs, or QEMU instead to power your incredible super-AI DARPA Godlike simulation of The Matrix, Neo?",Mocking
"**ALL worked very will in version 83.13**
**In PC-DOS 7.1 (IBM PC-DOS 2000)**
-The mouse acts REALLY strange in various applications and in Windows 3.11 (Win 3.11 crashes after strange mouse movements)
-Z (ZED) Game Setsound and ZED.EXE hang
-The mouse acts strange on the title-bar of the dosbox window
-Many applications and my own COM applications have problems
**SURE the new ""TALK to DOSBOX"" functionality cause this,**
YOU CAN'T JUST ADD/CHANGE ASSEMBLER INTERRUPTS (especially not before the bootstrap, if loading a custom image)
OSes and TSR Software may assign these interrupts. (I usually assign interrupts automatically with a driver installer routine for unhandled interrupts, the diver executable is then added to the next free interrupt)
**DosBox-X version 0.83.15 TOTALLY USELESS!** **IT IS JUST NOT 100% COMPATIBLE WITH x86 ASSEMBER IF YOU ADD CUSTOM INTERRUPTS,**
**THAT MIGHT BE FINE IF YOU WRITE A CUSTOM OS (NOT DOS COMPATIBLE) WITH A CUSTOM INTERRUPT TABLE.**
**NOT IF YOU NEED TO RUN 16 BIT DOS COMPATIBLE SOFTWARE (STANDARD x86 ASSEMBLER) , THAT IS THE WHOLE POINT OF DOSBOX, SO THIS VERSION IS TOTALLY USELES, SOME PROGRAMS WILL RUN BUT IT IS MORE LUCK THAN WISDOM. I EXPECT LOADS OF PROBLEMS IN MANY DIFFERENT APPLICATIONS.**
**Also this ""TALK to DOSBOX"" functionality destroys the SAND-BOX** An assembler debug run-point can now run from a DOS program into the Window of DosBox,, not handy since Python and CUDA reverse engineer applications on runtime while the machine AI is learning.
**This version should be deleted**
You should put the old version back until you make a release without the ""TALK to DOSBOX"" NEVER TALK TO THE SANDBOX. Your assembler run-point may go from the SANDBOX to the DOS bootstrap, but NEVER from DOS back in the sandbox. (Might cause strange infinite loops) Especially if you have Python and CUDA installed you can get VERY strange effects. (EVEN WHEN NOT RUNNING, due to CUDA/Python working on it)
Maybe add a version history to the website this will also be good for machine learning (I use that a lot, it can be used for Pythagoras predictive code, best is if you have a complete version history, except versions like this, should not be in such a history list, (causes only problems) ONLY good versions, with NO bugs but added or changed features, that work.
**The whole IDEA of SANDBOX is you can't talk to the Window (You should do it like this)**
WHAT YOU DID IS AN ARCHITECTURE FLAW, AND ALSO A BUG.
To get the same effect you could let the Windows application POL the DOS Memory page for example let the Window react on that. memory content. (Like PEEK/POKE You could turn the option OFF in the Windows and SANDBOX integrity is not changed ALSO compatibility is maintained.
**Helpful tool for doing it correctly**
It becomes super easy to program that if you make a memory viewer so you can browse LIVE trough your running DOS memory pages. A viewer can be made very advanced too and you can do things PETER-NORTON would be amazed by.
Jimmy Walter, CIA",Bitter Frustration
"#962 was merged 3 months ago and is not yet included in a release. Because of this, Mixxx CI has to build sccache from source which takes 10 minutes. It is caching the sccache binary for subsequent runs, but the first run of each branch has this 10 minute penalty. This could be avoided if we could install sccache from Chocolatey, which requires a new release.",Impatience
ping,Impatience
"Yeah, we will do it soon",Impatience
"@sylvestre, any updates on the new release?",Impatience
"> Mixxx CI has to build sccache from source which takes 10 minutes. It is caching the sccache binary for subsequent runs, but the first run of each branch has this 10 minute penalty.
sounds like a bug
cant you store the binary ""somewhere""? amazon, github, cachix, ...",Impatience
"@sylvestre, @milahu, any update on new releases? Anything blocking it that someone else could work? From an outsider's perspective the project has entirely given up doing new releases, since it's been over a year.",Bitter Frustration
"Hey David, I'm currently chipping away at the backlog of open PRs to see what can be landed before the next release.
> Anything blocking it that someone else could work?
Mostly reviews, there's a few thousands of lines of changed code that needs to be looked through. I'm getting up-to-speed on the project as well.
-----
However, something that _would_ be sweet is if we could get the dependencies updated, as it's been a hot minute. There were some patches doing so from a year ago, but if that could be done I'd be pretty happy. While we're at it, tackling modern compiler warnings and clippy lints will be important as well.",
"@mitchhentges, thanks for the response! Happy to hear it's being worked on. I haven't built the project and have no Rust experience, but if you point me in the direction of specific PRs for dependencies, or issues for modern compiler warnings, I could take a look when I get a chance.
I'm mainly itching for a release as I've run into #1098 several times and it might be fixed in `master`, there are just no releases since then.",Impatience
"The two specific issues I have in mind are #1110 and #1111
> I haven't built the project and have no Rust experience
This'll probably be best picked up by more experienced Rust devs since there may be breaking changes that require refactoring.",
"I encourage you not to hold back releases for ""one more little thing"", especially for not for trivial code cleanup. I've been waiting on a release for around a year now. I don't care about Clippy warnings.",Bitter Frustration
"@Be-ing please avoid such comments _I've been waiting on a release for around a year now. I don't care about Clippy warnings._
They aren't adding any information to the discussion.",Bitter Frustration
Fixing clippy warnings isn't adding anything for users.,Impatience
I disagree and please stop bikeshedding.,Impatience
"@Be-ing stupid question:
what prevents you from making your own release?
compile the binary, push it to github, add install script - done
i assume development happens in https://github.com/mixxxdj/mixxx",Mocking
"I explained in the first comment that I already setup caching of the sccache build. But that's not a good solution; any time the cache misses, there's a 10 minute cost of rebuilding sccache which negates the benefit of using sccache for that build. Moreover, I just shouldn't need to do that in the first place.",Bitter Frustration
"> I explained in the first comment that I already setup caching of the sccache build.
you mean
> the first run of each branch has this 10 minute penalty
just add another cache?
pushing binaries into github repos sounds quite persistent, no?",Mocking
"Please stop asking downstream users to come up with technical workarounds for your nontechnical problem. There is no need to review every open PR, fix every warning, or update every dependency to publish a new release. If you think there is, that's entirely self-imposed. Version numbers are cheap and releases don't need to solve everything at once. Meanwhile bug fixes from a year ago still have not been released.",Bitter Frustration
I am locking this thread. It is becoming useless.,Impatience
"I'd like to add a little more perspective as well here, if it helps things.
> I encourage you not to hold back releases for ""one more little thing"", especially for not for trivial code cleanup. I've been waiting on a release for around a year now. I don't care about Clippy warnings.
Heh, it _is_ an indirect benefit. Either way, this is a red herring: the _real_ blocker isn't clippy warnings, it's that I'm getting up-to-speed with the project. Sure, there's a few commits that are sitting in `main` ready-for-release, but there's also a significant number of PRs, some of which are ready for landings and some of which need some polish first. I don't know which is which, so my current priority is to work through them and find out.
Keep in mind that it isn't specifically clippy warnings that are blocking the release - I raised this work as something that the community could potentially contribute assistance with while I tackle the other blockers.
> There is no need to review every open PR, fix every warning, or update every dependency to publish a new release.
True, one option that exists here is to push out a release based on what's in `main`, then handle the pending PRs and do a follow-up release afterwards. I'm leaning against this because I don't know the stability state of `main`, and I don't have enough local infrastructure to raise that confidence. Yes, we have some automated testing, but I'd rather have a more methodical release than to push something out with unknown risk.
By addressing PRs, catching up some issues, and doing some local work, I can build my mental model of the project, which will enable making more confident decisions around releases.",Impatience
"Update:
* Most PRs have been triaged/landed/collaborating with authors (thanks!)/pending responses.
* There's a couple issues associated with the [`v0.3.0` milestone](https://github.com/mozilla/sccache/milestone/3) that will need to be resolved before a release should be deployed. Assistance getting these fixed would be much appreciated.
* I'll do a benchmark run before pushing the release out
We're probably still a couple weeks out, as I'll be away this upcoming week. But, we're getting close. Have a good weekend all :beers:",
"[`v0.3.0`](https://github.com/mozilla/sccache/milestone/3) is pretty much ready for release, I'm just taming Mozilla's Perfherder to intelligently diff its performance impact.
We'll probably see `v0.3.0` released next week.",
"[An issue has crept up during benchmarking](https://github.com/mozilla/sccache/issues/1150), and I'm away tomorrow, so the release will happen next week at the earliest.",Impatience
[`v0.3.0` is out](https://github.com/mozilla/sccache/releases/tag/v0.3.0),
"Amazing, bravo!",Irony
Where can I buy a Raspberry Pi for free? :),Irony
"@rustdesk You can use QEMU to compile RustDesk and it is free. In addition, rust supports cross-compilation.",
"@rustdesk Travis CI already supports building on the arm64 architecture. I think it might be a good choice to use Travis CI to compile the arm version of RustDesk.
https://docs.travis-ci.com/user/multi-cpu-architectures/",
I am waiting for new M1 release.,Impatience
"I want to run RustDesk on the Raspberry Pi, but I was disappointed to find that it does not work on the ARM architecture. I hope you can add support for the ARM platform (including the server version). Thanks.",Bitter Frustration
"> I am waiting for new M1 release.
so, is it time to support arm64? :)",Irony
Sorry for your long waiting. I will try QEMU.,Impatience
But which Linux distro?,Impatience
@hmsjy2017 Debian.,
"I tried to build RustDesk on Raspberry Pi4(Ubuntu-ARM64) and Raspberry Pi3(Raspbian OS-ARM32).
Unfortunately,both failed.
For Pi4(test on Linux ARM64):
From the code and README:
> Desktop versions use **sciter** for GUI......
sciter-sdk only provides the _libsciter-gtk.so_ build on x64 / ARM32 , they didn't release arm64 for linux yet.
[libsciter-gtk.so doesn't support in ARM64 platform #210](https://github.com/c-smile/sciter-sdk/issues/210)
We can't build on ARM64 unless sciter-sdk provides the .so someday.
For Pi3(test on Linux ARM32):
_libsciter-gtk.so_ maybe fine.
but I can't build success vcpkg on Raspbian OS-ARM32.
So I build the three libs _libvpx libyuv opus_ from those offical source code,then I put them in vcpkg/installed/arm-linux/ and _cargo build_
......a very long build time...... all rust crates and lib passed the compilation,but finally failed on maybe some gcc/link problem:
![2021-12-21 20-44-39å±å¹•æˆªå›¾](https://user-images.githubusercontent.com/62206297/146932228-c9e36c20-2496-41e3-8004-51b7c5e1c594.png)
...
![2021-12-21 20-45-29å±å¹•æˆªå›¾](https://user-images.githubusercontent.com/62206297/146932277-e88fd7a2-eb20-48c6-9415-be20761fcf34.png)
Im not good at gcc compiling and linking,I cant understand these problem...
I think it's not ready to support Raspberry Pi(Linux ARM) yet.",Bitter Frustration
"You can google, https://stackoverflow.com/questions/19768267/relocation-r-x86-64-32s-against-linking-error",Impatience
"> > I am waiting for new M1 release.
> > so, is it time to support arm64? :)
You need ask Sciter author support arm64 first. Because RustDesk depends on Sciter. https://github.com/c-smile/sciter-sdk/issues/210",Irony
"@rustdesk what about support without ui, just a server mode?",Impatience
"> just a server mode
PR please.",Impatience
"Since Sciter does arm32, wouldn't it be possible to at least create an arm32 build for the time being? arm64 is backwards compatible with it",Impatience
I will try to find one arm machine.,
"> I will try to find one arm machine.
Dude you don't need an arm machine, qemu will do just fine for you",Mocking
Where can I find an Ubuntu 16 arm desktop iso?,
"There does not exist a ubuntu 16 desktop arm iso, for this case you should go for debian, as it has less overhead
![image](https://user-images.githubusercontent.com/7207103/166691451-45780da3-266a-48cf-a56b-d6fbddc298aa.png)",
Also maybe you could just create some compilation instructions so I could test it on my raspberry pi and give you feedback?,Impatience
"@Heap-Hop tested today on Pi4, performance is bad, and failed to get vpx encoder work. We paused to invest time on it. Will continue once we are free.",Bitter Frustration
"To be fair didn't know that they were written already thinking in other arches, generally instructions are written only thinking on x86/x86_64",
"> @Heap-Hop tested today on Pi4, performance is bad, and failed to get vpx encoder work. We paused to invest time on it. Will continue once we are free.
How did you test it with SCiter not supporting AARCH64",Impatience
@busyluo we tested on arm32,
"I have successfully compiled RustDesk on armhf and aarch64, running in service state without UI. Arm runs Ubuntu18.04. TCP penetration is available to use SSH services.
æˆ‘å·²ç»æˆåŠŸåœ¨armhfå’Œaarch64ä¸Šç¼–è¯‘äº†rustdeskã€‚è¿è¡ŒäºŽæœåŠ¡çŠ¶æ€ï¼Œæ²¡æœ‰UIã€‚Armè·‘çš„æ˜¯Ubuntu18.04.å¯ä»¥è¿›è¡ŒTCPç©¿é€æ¥ä½¿ç”¨SSHæœåŠ¡ã€‚",
"> I have successfully compiled ...
Good job.",
"> I have successfully compiled ...
If possible, could you please write a tutorial on https://github.com/rustdesk/doc.rustdesk.com?",
"> If possible, could you please write a tutorial on https://github.com/rustdesk/doc.rustdesk.com?
I'll try. Wait for me. Haha",Mocking
"> I'll try.
Respect!!!",
"The tutorial is almost done, and I'll just add some tests at the end.
æ•™ç¨‹åŸºæœ¬å†™å®Œï¼Œæœ€åŽæˆ‘å†è¡¥å……ç‚¹æµ‹è¯•å°±å¥½äº†ã€‚",
"Thanks, English also please. :)",
"> Thanks, English also please. :)
Your translation is good!",
"I'm not quite sure whether it is a good idea to use Ubuntu's rootfs, especially from a source like baidu. Maybe make it a docker container build using the official images?
Running random builds of libvpx and opus doesn't exactly sound good either",
"Let me explain some possible downsides of the armhf instructions.
You have to flash an sd card with an ubuntu distro with already set up credentials, yet to be known if it has ssh enabled or calls home, and some libraries as binary downloaded from a chinese server, when they can be obtained through git or some other more legit way without going through redistribution endpoints. This can easily inject malicious code at any step in the chain",Bitter Frustration
"Ok, you persuaded me.",
"> Ok, you persuaded me.
What you going to do then? Redo the docs to be more trustable(changing the ubuntu thing to docker and fetching sources instead of binaries)? Erase the page overall until another knight in shining armor comes and does your job?",Bitter Frustration
"@wwjabc since you have added libvpx/opus compilation steps, I will remove the baidu download links. and I will also replace ubuntu18.04_rootfs.tar.gz with an official image. Wondering if this link https://github.com/ikwzm/ZynqMP-FPGA-Ubuntu18.04-Ultra96/blob/master/doc/build/ubuntu18.04-rootfs.md is correct?",
"@wwjabc Sorry, it is my fault to ask you for doc and cause you trouble.",
"> @wwjabc since you have added ...
Using rootfs is bad overall, if you get a docker image going you make it possible to do it from existing installs, it's way less bothersome for anyone following the instructions",Bitter Frustration
Try out [raspberry-armhf.deb](https://github.com/rustdesk/rustdesk/releases/download/1.1.9/rustdesk-1.1.9-raspberry-armhf.deb) please.,
"We (Chatterino devs/contributors) made a decision to move Chatterino to use only Helix (the new Twitch API) instead of Kraken (aka v5). Helix doesn't support anonymous title/live checks anymore (bother Twitch about that, not us). **The feature on your old Chatterino version will stop working soon because Kraken is being [DECOMMISSIONED](https://discuss.dev.twitch.tv/t/legacy-twitch-api-v5-shutdown-details-and-timeline/32649).**
We don't support Windows 7 anymore, as doesn't Microsoft. We can't support anything indefinitely. This OS is 12 years old, it's about time. Most users have moved to newer *and supported* OSes.",
"Well, I somewhat understand. Is a Windows 7 user still able to have a functioning chatterino above version 2.1.0? If so, functioning to what degree?
Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.",Bitter Frustration
"Hello barbedknot
Thank you for your concerns, I will assist by addressing them one by one.
> Chatterino dropped support for Windows 7 on version 2.1.0
The dropped support was as a result of Microsoft themselves no longer offering support for Windows 7. To users that are on Windows 7, it is highly suggest you upgrade to Windows 10.
> I was actually able to install the latest version for Windows normally a few weeks ago, despite my operating system being Windows 7
Yes, support has been dropped for Windows 7. You may be able to install it, however we have no plans to provide support for bugs/issues on Windows 7 devices.
> * A chatterino7 release that is discrepant from the chatterino2 philosophy of not allowing non-logged in users to receive ""notify when online"" notifications (among other features)
As already mentioned, we are trying to move away from Kraken as a result of the API being decommissioned. Helix endpoints have a strict requirement for OAuth when trying to use any endpoint as per the API docs: https://dev.twitch.tv/docs/api/reference. This is inclusive of knowing when a stream is online or not. Hence why you are unable to see if a stream is live (and do/see many other things) if you are not logged in.
> * A chatterino7 release that supports Windows 7 (my understanding was that 2.1.0 was the last version to support Windows 7, but at some point that appears to've changed, so perhaps this is just a given)
As aforementioned, Microsoft has dropped all support for Windows 7. Why would we support Windows 7 if the developer doesn't themselves? It seems absolutely preposterous to me.
> Is a Windows 7 user still able to have a functioning Chatterino above version 2.1.0?
Sure, it may work. We still don't provide support for it.
> Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.
Chatterino is a community-made, open source project. Nobody gets paid to work on it. I'll put it quite simply; Kraken is not being added into Chatterino, just so users are able to see when a streamer is live in anonymous mode.
Your whole issue revolves around complaining that there's no Windows 7 or Kraken support. Both of which have had all support dropped by their main developers. So I ask you; why would we add support for said feature, if the devs themselves do not support it?
Kind Regards,
ALazyMeme",Bitter Frustration
"> Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.
x2
Kind Regards,
person calling out intellectual dishonesty. smiley face",Bitter Frustration
"This post will be a disaster to format, so please bear with me.
Chatterino dropped support for Windows 7 on version 2.1.0 (this being the last version supporting windows 7). Coincidentally, Version 2.2.0 was the last one to allow ""notify when online"" notifications; version 2.2.1 was the first to require logging in to have ""notify when online"" notifications (there is other functionality logging in allows, but I am not educated on them). Source:
https://web.archive.org/web/20210421115826/https://chatterino.com/
(this is the last screenshot of the website before they changed the design; on this screenshot you can see the text ""Last version supporting Windows 7 (2.1.0 64-Bit)"")
https://github.com/Chatterino/chatterino2/issues/1915
> chatterino no longer able to pull stream online status while anonymous #1915
I am not sure if chatterino began supporting Windows 7 again or not, but I was no longer able to find mentions of the last Windows 7 version being 2.1.0 past the above screenshot of their website. I was actually able to install the latest version for Windows normally a few weeks ago, despite my operating system being Windows 7, but I reverted back to 2.1.0 just because the latest chatterino required logging in for ""notify when online"" notifications, as I've established earlier.
Chatterino has professed their unwillingness to allow ""notify when online"" notifications without login:
https://github.com/Chatterino/chatterino2/issues/2257#issuecomment-739330763
> No. This idea was already brought up before. Chatterino does not want to start helping ban circumventions.
So therein is my difficult to explain feature request. I will try to summarize what I'm asking for:
- A chatterino7 release that is discrepant from the chatterino2 philosophy of not allowing non-logged in users to receive ""notify when online"" notifications (among other features)
- A chatterino7 release that supports Windows 7 (my understanding was that 2.1.0 was the last version to support Windows 7, but at some point that appears to've changed, so perhaps this is just a given)
I guess that's it? I still had trouble formatting this, even if I can tersely communicate what I wanted, at the end of it. If nothing else, will you please spoonfeed me a chatterino7 version of the 2.1.0 installer? The 2.1.0 version on your github is just the source code, which gave me grey hairs trying to work out. For the time being I'm still on the plain chatterino 2.1.0, which doesn't have 7tv emotes.
edit: removed citing ""protection from global bans"" quote as demonstrating that chatterino used to willingly allow ""notify when online"" notifications without login. I realize now that ""global bans"" likely meant regions in the world, and was independent of their unwillingness to willingly provide this functionality to chatterino users who don't log in.",Bitter Frustration
dupe #703,Impatience
"I realise that #703 already got locked as ""too heated"". But I must ask that this issue be revised carefully and cooly to come to some sort of a solution. In its current form E722 is more dangerous than it is helpful.
I would like to put it on record that the decision to leave that as-is has tripped me, my colleagues, and many others into writing some serious bugs into our code. Initially I felt pretty embarrassed for not knowing about this for so long, but the more I've talked to both colleagues and friends about this the more I've realised that developers are often not aware of the bug it introduces:
```pycharm
# The user hit's control-C during do_something() and it left x in a really bad state
# This was supposed to be caught by cleanup()
# So now the user has seriously corrupted data
try:
do_something(x)
except Exception: # Really this should have been just except:
cleanup(x)
raise
```
PEP 8 does not ban the use of bare `except:`. That's plainly explicit in PEP 8. It calls out two specific and common use cases where it is permissable. I say ""common"" because logging is not an edge case requirement. And I also say ""common"" because resource cleanup is not an edge case requirement and cannot always be done in a `finally:` block.
-----
Most novices and even many intermediate developers I've spoken to will be to swap `except:` for `except Exception:` where their code was already re-raising the exception with a bare `raise`. They do this because they think PyCQA's E722 (via Pycharm or similar) has told them to. When they make this change the warning goes away. But what have they done? - has they fixed a problem? **No** not really. If their code was capturing the exception and suppressing it (not re-raising) then `except Exception:` is almost always too broad. Their code is just as likely to misbehave on a badly formatted date or a function call with too few arguments. It's only fixed one small specific problem of the user hitting ctrl-c. If their code was already re-raising then there was no bug to start with and this has fixed nothing at all.
- Has it introduced a new bug? **Yes** If they really did intend to catch everything then nudging them away from `except:` onto `except Exception:` simply will not do what they intend. But worse it will look like it does what they intend long enough to get into production many times over.
The potential bugs PEP 8 / E722 are concerned with are all to do with _suppressing_ exceptions incorrectly. If PyCQA is incapable of analysing suppression (incapable of seeing the future `raise) this then it's incapable of applying this rule well enough to avoid very many dangerous false positives.
----
In conclusion, I can find nearly no instances where E722 has practically nudged myself or colleagues into writing better code with fewer bugs. I can find many of instances where it has nudged myself or colleagues into writing faulty code.
I now believe this ""bare except is always bad"" interpretation of PEP 8 is much more harmful than it is ever helpful.",Bitter Frustration
"`Web Access have is not enabled.` is the only thing it replies. What error do you want to see instead? ""Connection refused"" is not possible, because the port is still open.",Bitter Frustration
"I donâ€™t think you understand the behaviour I would like to have so Iâ€™m going to try my best to reformulate:
I donâ€™t want to see any error/warning/dismissing message in a html page when I make a http request to my bouncer. I want the bouncer to not respond, at all, to any kind of http request, and my web browser to tell me It was unable to connect to the url I gave it, like so :
![åç§°æœªè¨­å®š2](https://user-images.githubusercontent.com/37584624/130358864-c7285ce4-5b0f-44da-8a30-f2e8b92f6f71.png)
Is this behaviour achievable as of today? Would the code architecture support that kind of feature without having to make major breaking changes?",
"The port needs to be open to be able to accept IRC connections. And SSL handshake happens even before the client (IRC client or web browser) sends any data to server. ZNC can't distinguish IRC vs HTTP client until at least some data got communicated; in case of SSL, communicated both ways).
So no, ""Unable to connect"" can't be done.
Why exactly do you need this?",Impatience
"I need this because I donâ€™t want to serve anything HTTP and so I donâ€™t want the HTTP server to be in my way.
ZNC may not be able to distinguish IRC vs HTTP until at least some data data got communicated, but at some point it knows the client want HTTP instead of IRC then right? I donâ€™t see why it wouldnâ€™t be feasible to make ZNC not respond anything at this point.
Youâ€™re coming to me saying your a client. I acknowledge you and I donâ€™t know what you want yet. Then youâ€™re going to tell me you want HTTP. Iâ€™m going to ignore you, pretend I didnâ€™t hear anything and that this event never happened at all.",Bitter Frustration
"> I donâ€™t want to serve anything HTTP and so I donâ€™t want the HTTP server to be in my way.
Why?
> pretend I didnâ€™t hear anything and that this event never happened at all.
accept() and SSL handshake happened already, you can't undo them.
> I donâ€™t see why it wouldnâ€™t be feasible to make ZNC not respond anything at this point.
You can patch it to close the socket instead of returning HTTP error.",Bitter Frustration
"God please make an effort to not be a huge freaking pain in the ass. You donâ€™t have to care about why I want something. If I want something itâ€™s for a good reason and Iâ€™ve been writing about it since my first message.
I canâ€™t undo handshakes because Iâ€™m not a freaking magic time traveling wizard good job smartass did you figured that out yourself?
Now just tell me how to close the damn socket so I can flag this issue as closed and move on to something more productive than having to interact with an antisocial edgelord who canâ€™t go straight to the point.",Bitter Frustration
"> God please make an effort to not be a huge freaking pain in the ass. Great way to request a feature. Or not.
> You donâ€™t have to care about why I want something.
Yes, I do. Without a good reason, I won't change this in the code.
> I canâ€™t undo handshakes because Iâ€™m not a freaking magic time traveling wizard
That's what I'm trying to tell you since the first message.
> did you figured that out yourself?
I thought it's pretty obvious, but apparently it was not, therefore I had to say it directly.
> smartass
Oh, insults will help your cause for sure.
> Now just tell me how to close the damn socket
I'm sure google search will help you.",Bitter Frustration
Here's one more link for you to read: https://en.wikipedia.org/wiki/XY_problem,
"> > I canâ€™t undo handshakes because Iâ€™m not a freaking magic time traveling wizard
> > That's what I'm trying to tell you since the first message.
If you want someone to understand something, you can tell it straight away you know? Tell me, how hard would have it been to write from the start : ""you canâ€™t do exactly the behaviour you want because there are handshakes happening and you cannot make them not happen. However, instead you could either return an HTTP error or just simply close the socket. Here is where you should look into if you want to add this feature""
But you had to play your little game of being a smug winnie for no apparent reason.",Bitter Frustration
"I did.
> And SSL handshake happens even before
> So no, ""Unable to connect"" can't be done.",
"No you didnâ€™t.
You just told facts here and there, without any intention to go out of your way to help me understand the blind spots I have in my understanding. It took one day of back and forth reformulations to get to a single piece of possible solution : apparently, I could make ZNC close a socket.
You didnâ€™t take the time to show me how my reasoning doesnâ€™t comply with how protocols are supposed to work, you didnâ€™t explain how to change my reasoning to find a middle ground between knowing whatâ€™s possible and not possible and being satisfied with a behaviour that meets my need, and you didnâ€™t even try to guide me towards the parts of ZNC that would need to be patched if this was something that wouldnâ€™t introduce breaking changes.
As of right now, I am still figuring out the whole timeline of events between ZNC and a http client. Here is something you would find dead obvious : how an SSL handshake canâ€™t lead to a browser telling its user they were unable to connect to the website? This isnâ€™t a call for help by the way. It has been a day since I knew I would have to do and understand everything by myself, way before we got to here.
Facts are not the sole metric in a conversation. Youâ€™re intent will show. You felt dismissive.",Bitter Frustration
"> to go out of your way
Why would I do that? First, I need to understand what you're actually trying to do, before showing you the direction: otherwise the direction I show would be completely wrong. But you're refusing to answer my questions! Again, please read about the XY problem I linked above.
> understand the blind spots I have in my understanding
How am I supposed to know what parts of protocols you know and what parts you don't know? Do you expect me to teach you the whole computer science from ""2+2=4"", or? The two facts ""And SSL handshake happens even before"" and ""So no, ""Unable to connect"" can't be done."" imply that you obviously can't go back in time. Or I should expect that you don't know physics and also teach you that?
> you didnâ€™t even try to guide me towards the parts of ZNC that would need to be patched
You think the correct answer always is a patch? Wrong!
The first step is to figure out what you actually want. Then show you the existing configuration option (or e.g. a module) to do what you want. Then, if there's no such opiton, a patch, and not just on your side, but in upstream ZNC, so that it benefits everyone, not just you. Of course, if this is useful at all.",Bitter Frustration
"I'm also not sure what exactly you are trying to achieve. Also, I don't think you deserve more help than DG already tried to provide. But okay....
It *seems* like you want some kind of error message in your web browser. As you already heard, the error that you wanted to get is not possible, because the internet does not work that way. If you want an ""connection closed unexpectedly"" error message, you can just remove the call to `Write()` here:
https://github.com/znc/znc/blob/e0ffdddd473e97cb843f2bc8ad4fa16cf47c65b4/src/Listener.cpp#L149-L151
If you want another kind of error message, you can try replacing this with `Write(""this is definitely not http\n"");`. No idea how browsers react to that, but perhaps one of these two options solves the problem you are actually trying to solve.
I subscribed to your issue in the mlemnc repository. I might or might not write more answers there, in case you write some questions.
Edit:
> You didnâ€™t take the time
Why would DG have to take the time? As far as I know, this is a hobby of theirs and there are no reasons why you get to dictate how people spend their free time.",Insulting
"Hi !
Iâ€™ve been wanting to remove the `webadmin` module and fully disable ZNC from being able to respond anything meaningful over HTTP, because it listens to the same port as the IRC protocol one and so I cannot just block the port. Either this is a behaviour not achievable or I am missing a setting somewhere after searching the wiki for a solution.
I tried setting `AllowWeb` to `false` by hand on my only listener but it doesnâ€™t work, as ZNC still serves an html page saying that `Web Access have is not enabled.`
```
<Listener listener0>
AllowIRC = true
AllowWeb = false
IPv4 = true
IPv6 = true
Port = ****
SSL = true
URIPrefix = /
</Listener>
```
![åç§°æœªè¨­å®š](https://user-images.githubusercontent.com/37584624/130321317-f36e1f7e-10d6-4e93-a206-ce85d7e7d056.png)
[From this changelog](https://wiki.znc.in/ChangeLog/0.090), I can see that the http server got embedded in ZNC so that modules other than webadmin can serve pages over http. Is there a way to tell ZNC to not load it?
Serving a page *isnâ€™t* that big of a deal so Iâ€™m okay with this for now. But from a rigorous standpoint, if I donâ€™t want my bouncer to communicate over HTTP I wish I could tell it to not do it, at all.
I am on Debian Bullseye, using their package of ZNC. I took a quick glance over the other issues but I have to say there was too much of them to see if this was an issue already raised.
```
$ znc --version
ZNC 1.8.2+deb2+b1 - https://znc.in
IPv6: yes, SSL: yes, DNS: threads, charset: yes, i18n: yes, build: cmake
```
Thanks in advance for your time,",Bitter Frustration
"My initial kneejerk reaction when seeing `markdown.etree.set_parser(lxml.etree)` was was 'but we aren't using etree as a parser so `set_parser` is the wrong name for the function.' I think that that gets to my underlying objection to adding any support for lxml. We are a Markdown parser. What harm is there in allowing third party extensions to inject a separate parser? Well, then we officially support it with all its edge cases. Now ever time some user runs into some obscure edge case with lxml, we need to deal with it. No thanks. I'm not interesting in the extra burden that the long term support will require.
Of course, this doesn't prevent you from accomplishing your end result. Just return a string from your parser which you then insert as a raw string into the Markdown output.",Bitter Frustration
"I can't follow your argumentation because an interface for optional parsers doesn't oblige the maintainers to support any of those optional injections. It's the third party's obligation to be xml.etree.ElementTree compliant.
I don't see the point of forcing extension developers to rely on the built-in xml parser.
I won't inject lxml-trees into markdown results. it's cumbersome and excludes extended xpath operations on markdown-html result. let's fork.",Bitter Frustration
"All of the basic built-in Markdown elements are built as internal plugins. They rely on etree. If we allow custom parsers, how do we avoid the inevitable questions of ""I implemented my own parser now all of the basic Markdown things are broken?"". Or ""Everything isn't broken but with lxml this edge case is broken for Admontions, can we fix that so it isn't broken?"".
This is what we don't want to deal with. I don't really know what you are doing with lxml and at what stage in the process, but I imagine there are ways to do what you need to without lxml. I realize that lxml is a preferred option for you, but I can envision all sorts of issues if we went this route.
And this isn't really a completely abstracted approach as there may be non-etree like parsers out there that someone would want to use. This really would only work for etree-like parsers, particularly lxml. What if I wanted to use html5lib? I don't think this would work. You'd have to completely abstract the interface. But again, you'd still have quirks maybe.
This is the bare minimum of abstraction needed to get lxml (and probably only lxml) shoehorned in, but not enough to abstract this for all custom parsers and it may introduce all sorts of quirks for anyone interfacing with the built-in Markdown parsing when using their own custom parser.",Bitter Frustration
"Again, it is not the Python-Markdown's responsibility to guarantee every third party's parser compability. I know that there are some differences between lxml and ElementTree, but they can be solved without touching the Python-Markdown's source code by overriding affected methods from my own project's side for example.
> I don't think this would work.
Did you ever try to implement an abstract interface for html5lib? Programming is not a belief system.
> it may introduce all sorts of quirks for anyone interfacing with the built-in Markdown parsing
Again, python is no voodoo magic. I use lxml and Python-Markdown and it works great on a great variety of Markdown source. Why would I commit code improvements then? Anyway I'll solve the issue with local patches. I see no point in supporting a closed source ElementTree cult without reasonable argumentation.
> inevitable questions of ""I implemented my own parser now all of the basic Markdown things are broken?""
Seriously? Common I'll just fork it and add a Python-Markdown2 archlinux package...
My decision fell for Python-Markdown because of it's expandability with further tree processing. I quickly realized that my objectives need lxml functionality. There's no other reason to use it when markdown CLI tool is times faster.",Bitter Frustration
"> Did you ever try to implement an abstract interface for html5lib? Programming is not a belief system.
I am involved in the [BeautifulSoup](https://launchpad.net/beautifulsoup) project as I provide the [CSS selector](https://github.com/facelessuser/soupsieve) library for that project. That project actually does create an abstract interface (for the purpose of extracting the content) around the built-in HTML lib, lxml, and html5lib, so I do know a little about this, and I don't appreciate the condescending tone. Let's try and keep this civil.
> Again, it is not the Python-Markdown's responsibility to guarantee every third party's parser compatibility.
I understand that, but it may introduce issues into existing pieces we do maintain. But it also may not. The point is that the maintainer has selected an HTML parser, and is not currently interested in opening the door for more ""potential"" maintenance burden. It may not offer any issues currently, but it could in the future, and once we start supporting this, we are committed to handling related workarounds etc. that may be required in the future.
> Again, python is no voodoo magic. I use lxml and Python-Markdown and it works great on a great variety of Markdown source. Why would I commit code improvements then? Anyway I'll solve the issue with local patches. I see no point in supporting a closed source ElementTree cult without reasonable argumentation.
You are free to do so as you feel is necessary as that is the benefit of open source, but just because something is open source does not mean they are obligated to accept every suggested feature or fix.",Bitter Frustration
"Regarding the [earlier pull request](https://github.com/Python-Markdown/markdown/pull/1177) I propose a _custom parser interface_ for better integration with third party projects using e.g. _lxml_.
### Purpose:
Help developers of third party projects to easily change the internal parser (etree) if their custom extensions do have need for extended functionality of e.g. _lxml_.
### Usage:
```python
import markdown
import lxml.etree
markdown.etree.set_parser(lxml.etree)
html = markdown.markdown(""# Hello extended XPath World"")
```
### Criticism:
Compatibility issues of third party parsers do not have to be targeted by Python-Markdown development team, as projects like _lxml.etree_ claim to be compatible with _xml.etree.ElementTree_
### Licensing:
There should be no legal conflicts (BSD/MIT) in case of lxml. :-D
**I hope you'll accept my idea to make my life a bit easier beyond python-markdown update cycles.**
Cheers
Dom",Entitlement
"This issues should all be labelled - https://github.com/WordPress/gutenberg/issues?q=is%3Aopen+is%3Aissue+label%3A%22%5BPackage%5D+Env%22. Same with PRs.
If there's any missing labels, they can be labelled.",Impatience
"Issue labels are indeed useful but it still seems like a reasonable idea for the project to have its own repository. :smiley: There is a lot of unrelated activity in this repository such as running the full Gutenberg test suite on every pull request, even when it just relates to wp-env. ![Peek 2021-08-26 13-58](https://user-images.githubusercontent.com/17307/130951178-370010bf-a76d-4f76-879b-583b160d464a.gif)
Also, developers have to check out all of the Gutenberg code when wanting to only make a change to wp-env.
![image](https://user-images.githubusercontent.com/17307/130949152-155f62d1-543b-4ec1-92a7-0bd66881dfec.png)",Bitter Frustration
"`@wordpress/env` is also used for development in Gutenberg so there are also pros of keeping the source code in the same repository. There is more to it, we use one way to publish packages to npm, there is a wider group of contributors that can learn about the tool. I'm worried that the overall maintenance cost is going to be higher if we were to extract a single package to a separate repository.",
"@gziolo, in what way(s) would the NPM package publication workflow be affected by having a separate project for `wp-env`? What are some ways we can raise contributor awareness of `wp-env`? What additional maintenance costs might the `wp-env` project incur by being in a separate repository?",
"Relatedly, a single-line pull request for `wp-env` documentation is [currently blocked by failing Gutenberg end-to-end tests](https://github.com/WordPress/gutenberg/pull/34322#issuecomment-914055014).
I still believe that moving `wp-env` to a separate project repository would make development more simple and clear. In effect, `wp-env` is a useful tool for general WordPress (PHP) developers as well as Gutenberg developers, so would benefit from neutral positioning and streamlined :racing_car: repository structure and development workflow. :smiley:",
"I asked more folks on WordPress Slack to leave their feedback (link requires registration at https://make.wordpress.org/chat/):
https://wordpress.slack.com/archives/C02QB2JS7/p1631001940283000",
"I'm on the opinion that a separate repository is just going to require more maintenance work for no real gains. The setup to publish npm is already in place here, there are already folks working and following this repo. A separate repo would require building new workflows, a new team dynamic that I don't think is worth it personally. I'm also of the opinion that all WP npm packages should just be in this repository. The fact that e2e tests from Gutenberg can block PRs for wp-env look like a good thing to me, they're validating that wp-env changes work properly as well. Intermittent failures are an issue of course but it's a constant priority for us.",
"The same arguments here about running tests and finding issues could be made for other packages, not just `@wordpress/env`. And they are not un-solvable tasks.
https://github.com/WordPress/gutenberg is the de facto monorepo for all WordPress JavaScript packages (replacing https://github.com/WordPress/packages, if you remember that one). There are indeed benefits to keeping all these things in one place, from contributor onboarding to maintenance to publishing.
Admittedly, it's a bit confusing to have a monorepo intertwined with a WordPress plugin, but there are practical reasons for that too (which is why https://github.com/WordPress/packages didn't work).
But you're posing an interesting question here:
**At which point does an individual package (outgrow the monorepo and) warrant its own home?**
Off the top of my head, I could think of a couple of indicators:
* Higher velocity than the rest of the project
i.e. tons of contributions, frequent releases desired -> the monorepo would slow it down
* Change in direction/scope i.e. the package itself gathers a community on its own, pursuing higher goals (e.g. a general purpose local environment like you mentioned), warranting separate forums, documentation, etc.
Neither of the above is really the case for `@wordpress/env` in my opinion.",
"Just voicing my agreement with comments that have already eloquently expressed by @gziolo, @youknowriad and @swissspidy. In particular, I appreciate the framing Pascal has put around identifying some indicators where a project _might_ be moved into its own repository and I agree that `@wordpress/env` doesn't meet that criteria currently.",
"I agree with Pascal's point regarding the indicators. Currently `@wordpress/env` package doesn't meet the criteria.
Moving also might cause a little confusion. As mentioned above, this is the de facto monorepo for all `@wordpress/*` packages, and people are used to creating issues/PRs here.",
This seems to be the same issue Marius Jensen raised here #32584 @Clorith,
"I like that it allows for Gutenberg contributors to have it all in one package. I also can see that if someone wants to contribute only to `@wordpress/env` the additional payload of the fork might be an issue. The two factors, make me lean towards keeping it in its current space: - all the documentation and instructions, would need to be rewritten. There are not many people contributing to documentation. (@youknowriad raised maintenance work, definitely worth considering) - Gutenberg repository attract a lot more people and expose them to the `@wordpress/env`, a separate repository would probably not. It's a great side effect that WordPress (PHP) developers appreciate the work by Gutenberg developers on a local environment worth their while.",
"Thanks, @bph, for pointing out that this is a duplicate of issue #32584. I may not have searched for duplicate issue(s) before creating this one, although I usually do. I'm not sure how to merge the conversations, as the previous issue seems to have garnered more agreement (in terms of thumbs up).
That said, I would like to explore further a few of the points raised so far.
> The fact that e2e tests from Gutenberg can block PRs for wp-env look like a good thing to me, they're validating that wp-env changes work properly as well.
The test cases that run on this repository seem mainly Gutenberg-related. Which of the tests validate that `wp-env` is working correctly? Are those tests somehow tied to the Gutenberg code?
![screenshot of tests](https://user-images.githubusercontent.com/17307/130951178-370010bf-a76d-4f76-879b-583b160d464a.gif)
> all the documentation and instructions, would need to be rewritten.
I don't see much in the [@wordpress/env documentation](https://developer.wordpress.org/block-editor/reference-guides/packages/packages-env/) that would seem to require rewriting. Is there some other source of documentation I am missing? For example, perhaps some parts [managing packages](https://github.com/WordPress/gutenberg/tree/trunk/packages#managing-packages) document would need to be included in a separate `wp-env` repository to aid in the publication workflow?
> the package itself gathers a community on its own, pursuing higher goals (e.g. a general-purpose local environment like you mentioned), warranting separate forums, documentation, etc.
I also agree with this framing. However, having a separate repository doesn't imply different forums or sweeping changes to the documentation, as suggested.
> The same arguments here about running tests and finding issues could be made for other packages, not just @wordpress/env. And they are not un-solvable tasks.
I agree the `[Package] Env` label is useful and observe the resourcefulness of Gutenberg's core engineering team. However, is it possible to exclude some of the Gutenberg tests from running on commits to the `wp-env` package? I.e., improve the signal/noise ratio of the CI workflow.
> A separate repo would require building new workflows
What are some new workflows that would be needed? Relatedly, what are some existing workflows that might require modification?
> WordPress/gutenberg is the de facto monorepo for all WordPress JavaScript packages (replacing WordPress/packages, if you remember that one). There are indeed benefits to keeping all these things in one place, from contributor onboarding to maintenance to publishing.
Thanks for the clarification about the onboarding, maintenance, and publishing. What would be some maintenance and onboarding costs of maintaining the `wp-env` project in a separate repository? Conversely, what might be some benefits or improvements of having independent projects?
> Gutenberg repository attract a lot more people and expose them to the @wordpress/env, a separate repository would probably not. It's a great side effect that WordPress (PHP) developers appreciate the work by Gutenberg developers on a local environment
I'm concerned with relevancy and clarity. For example, when I came to this repository to change the `wp-env` code and documentation, I had difficulty finding the code in the `packages` folder among various other packages. While the Gutenberg project is a remarkable undertaking, most of the code, commit history, CI pipeline, and issues are just not salient to `wp-env` development.
> I'm also of the opinion that all WP npm packages should just be in this repository.
In addition to WP npm and Gutenberg packages, the `packages` folder also includes tests (`e2e-*`) a lot of third-party code (like `react-*`, `jest-*`, `babel-*`, `eslint`, `*-webpack-*`, `redux-*`, `docgen` and `is-shallow-equal`). There are likely pragmatic reasons for keeping development dependencies under revision control instead of `package.json`, but it adds further weight to the codebase and commit-history. Repository ""clutter"" is particularly undesirable when wanting to contribute to a specific environment tool (`wp-env`) that decoupled from both Gutenberg and WordPress core.",Bitter Frustration
"> This seems to be the same issue Marius Jensen raised here #32584 @Clorith
Great catch @bph, I closed the other issue and here is the comment from @Clorith:
> Currently, the Gutenberg project is a monolith of tools and features, one of these features is `wp-env`, a node module for managing, and working with, WordPress plugins and themes.
> > Given the nature of `wp-env`, it is a tool with a much borader reach and audience than the Gutenberg monorepo, and as such does not (in my opinion) properly fit in here.
> > By splitting it out to its own repo, it becomes easier for others to adopt, learn from, and hopefully contribute back to, the tool it self.
> > This would also make it easier for those looking to enhance it to fork the project, without getting a block editor along as a bonus, or jsut to implement changes ahead of releases based on localized needs there and then.
> > > I will happily admit to not knowing the technical difficulties involved in moving NPM packages to a new repositor, but I honestly believe that not bundling this broad of a tool inside the Gutenberg repository will be a positive change.
> There may be other such tools that I'm not familiar with, so perhaps it's a roader discussion of ""separating tooling from code"" in the Gutenberg repo? Would absolutely love thoughts and input on this.",
I will voice my desire to see this separated out as well. I've been trying to use `wp-env` and `wp-scripts` as a core development packages for plugin development instead of rolling my own local development setup. I too have had problems contributing to `wp-env` alone because of the massive build overhead of the entire Gutenberg project. IMO `wp-env` should be the defacto plugin/theme development environment supported by the community and all Core-related plugins and themes should be using it to make a more first-class citizen.,Bitter Frustration
Same opinion as @timnolte ! wp-env is such a great tool! :),
"It is quite an encumbrance to install all of the Gutenberg project dependencies and any other `@wordpress` libraries bundled into this repository to work on a single project. The complexity of the dependencies, as well as the number of irrelevant CI tasks, is interfering with development in PR #34324
E.g. the dependencies/scripts to lint/format JS are defined in the root of the Gutenberg project instead of the `packages/env/package.json`",Bitter Frustration
"## What problem does this address?
<!--
Please describe if this feature or enhancement is related to a current problem
or pain point. For example, ""I'm always frustrated when ..."" or ""It is currently
difficult to ..."".
-->
The `wp-env` utility library is currently contained within the Gutenberg project. Unfortunately, this makes it a bit difficult to find the project files and related issues/pull requests amidst the bustling Gutenberg development activity.
## What is your proposed solution?
<!--
Please outline the feature or enhancement that you want and how it addresses any
problem identified above.
-->
Move the `wp-env` project to its own repository, so code, issues, and pull requests are easier to locate/organize.",
"I'm pretty much in total agreement with @WraithKenny if the individual packages aren't meant for anyone outside of the Gutenberg project then make that explicit, or better yet publish them in a private repo. It does seem rather ridiculous that there is so much in the `wp-env` package that suggests that it was designed with a broader audience in mind, but then it's stuck in a world that makes it very hard to contribute to it.",Bitter Frustration
"> Env, Scripts, Babel-Preset, Browserllist-Config, Prettier-Config, Stylelint-Config, Eslint-Plugin and any other NPM-published package that's allegedly supposed to be useful to plugin and theme authors beyond just gutenberg, should all be separated out of this ""monorepo.""
The fact that all these packages are dependent on each other and evolve together most time is a great indication that the mono repo is the best path forward. You shouldn't be forced to have multiple PRs in parallel in multiple repos to make a change, or do a waterfall kind of flow. The whole open source community is evolving in this direction for these reasons. Smaller PRs for specific packages exist but more often, a PR touches multiple packages at the same time.",
"> There are no ""advantages"" of keeping them in the ""monorepo"" beyond whatever opaque npm publishing shortcuts you've invented, and there are lots of disadvantages.
Really? I see a number of advantages that have already been mentioned here. I can understand that you might not agree with the advantages that have been pointed out but that doesn't mean none have been brought up :)
> The most obvious thing is ""dog-fooding"" which, back in the day, the WordPress community actually embraced, but is not doing here. All of these packages should be separate and consumed as a user (theme and plugin developer) would.
That's a fairly blanket statement that I'd like to provide a counterpoint from observation. I'm aware of _numerous_ plugins and themes that utilize the packages as they are currently distributed. Just a few at the top of my mind at the moment: WooCommerce, WooCommerce Blocks, Calypso, Jetpack, Yoast SEO, Event Espresso. The fact that the Gutenberg project isn't directly pulling these packages from NPM isn't something I'd consider a hindrance. In fact, the opposite is true, I think it allows the project to react more quickly to issues (for reasons others have already pointed out in this thread) that are reported by plugin and theme authors using the packages.
Historically, WordPress has always been a monorepo in some way as far as dev tooling goes (PHPUnit tests, JS test suites, Grunt build tool, the [entire ""develop"" WP structure](https://github.com/WordPress/wordpress-develop) is a testament to that). > But, on the second part, Gutenberg's release constraints already slow down the release of fixes to the developer packages.
I think there's some truth in this observation but more related to the delay in package updates when in the midst of a WordPress release cycle. This is already a known thing and something that will be iterated on. Outside of the WordPress release cycle, it's been my experience that the package updates are generally pretty frequent.
On the other hand, there are many efficiencies that are gained from working in a mono repo (through experience in trying both approaches) that effectively means momentum is kept in improving the packages. Efficiencies that would be lost in splitting everything out to its own repository.
----
I wonder if it'd be worthwhile to capture as a list the problems/friction contributing/consuming packages in the Gutenberg repo both to outline the tradeoffs being made currently and acknowledge that for some users this is a hindrance to making contributions. As a first step, I think it's good to have that articulated somewhere. This then can be used as inspiration for exploring how the various trade-offs might be addressed even _without_ splitting out to a mono-repo?",
"@nerrad I think the problem is that `wp-env` if it's supposed to be for plugin and theme development really shouldn't be held hostage by the Gutenberg plugin. All of what's mentioned makes me question that if the mono repo is such a good thing then why isn't Gutenberg and all of the packages put in WordPress Core. From what I've seen the reason is exactly the opposite of your cases on why `wp-env` should stay in this mono repo, keeping Gutenberg separate means that it can be developed faster than WordPress Core releases. Which also allows people to install this plugin which overrides the Gutenberg that is included in WordPress Core.",Bitter Frustration
"> All of what's mentioned makes me question that if the mono repo is such a good thing then why isn't Gutenberg and all of the packages put in WordPress Core
I for one would actually love for Gutenberg and core to merge into a single mono repository. The reason it's not is that we don't want to lose all the github flavor and tooling that comes with it. Merging Gutenberg and Core doesn't prevent Gutenberg to ship a plugin (or an alpha version of WordPress) faster than Core, we're just dealing with the difficulty to more Core to GitHub or similar.",
Why am I not seeing the recent posts here from @WraithKenny I know they posted twice today as I got the email notifications.,Impatience
"One thing I would say is this discussion is quite long form, and a lot of the points being made are hard to digest (requires a lot of reading). The description doesn't have a summary of the relevant pain points, so it's hard to actually understand where we can make improvements.
I think it's worth looking at the reasons for wanting to move to a separate repo and see whether any of them can be addressed or at least improved while keeping the project as part of the monorepo.
For example, it was mentioned that managing issues is difficult. If there are motivated contributors, then using a project board or tracking issue are effective ways to manage a project.
If building the entire project is too slow, then we should look at options for improving that (I understand @kevin940726 @youknowriad already have been working speeding up the build), or other ideas like being able to run `npm run dev` for a single package only.
> Why am I not seeing the recent posts here from @WraithKenny I know they posted twice today as I got the email notifications.
I'm not sure why. I also saw a notification, but the message is not here. I will mention that the constant thumbs down on every post they disagree with is really very annoying, and it doesn't constitute a positive or productive discussion. I would encourage you not to do this, it comes across as unfriendly.",Insulting
"Basically, all I'm hearing is that the idea of moving this package, or any other NPM package in this monorepo, has been permanently turned down by powers in control. You have effectively shutdown the conversation. You might as well close this issue altogether because you have no intention of ever considering moving out of this crazy unmaintainable monorepo. At this point I think I have my answer and will no longer be investing any more of my time to improve this package. I'd rather be involved in contributing where people are at least willing to have an actual discussion about instead of just making it seem that the monorepo is the only right answer. I'm in complete agreement with @WraithKenny and unless someone can actually make a legitimate case like they have for why breaking any of these packages off to their own repos is going to hurt anything I'm done listening to the one-sided enforcement of the maintainers. Improving the monorepo doesn't improve the entire process as @WraithKenny so clearly articulated.",Bitter Frustration
"That's quite a strong reaction. Look at it on the other hand. A small group of contributors are only willing to discuss the idea of moving the package out of the monorepo, and unwilling to consider any other solution. This approach to diplomacy will only end in a stalemate. Nothing actually productive will come out of it. The issue I'm seeing is that you have identified problems but are only willing to consider a single solution and no other options. I don't really see how you expect such inflexibility to receive a positive response.",Bitter Frustration
"My comments aren't here anymore, because I wasted hours articulating the problems, only to remember no one here actually gives a shit about my time, and I'm only hurting myself by trying to contribute. Thumbs down isn't fucking friendly? Fuck you, you stupid piece of shit. Go fuck yourself, and fuck this project, and fuck all the maintainers. I've deleted all my patches and forks. I've deleted my comments. If I could fucking delete all the code that ended up in Wordpress, I'd fucking do that to.",Bitter Frustration
"I'm going to leave that one post until someone else deleted it and bans me from this Repo, because I'm fucking right, and these passive aggressive pricks are the barrier to contributions, with their false fucking civility and fake politeness.",Bitter Frustration
"Though I know this conversation has mostly come to a close, I did want to answer a couple of questions that came up. I also still think it's important to consider monorepo solutions to some of the problems which could (in theory) be solved by a separate repository.
> The test cases that run on this repository seem mainly Gutenberg-related. Which of the tests validate that wp-env is working correctly? Are those tests somehow tied to the Gutenberg code?
Since Gutenberg is a complex WordPress plugin, it relies heavily on the underlying WordPress environment. If the underlying WordPress environment breaks through a change in `wp-env`, tests in Gutenberg will fail. So you could consider each Gutenberg test also a test of `wp-env`. Gutenberg tests related to file uploads also test that the `wp-env` file system works with file uploads, for example. It's important for us to not make a change to `wp-env` which would cause a plugin to stop working with it. The Gutenberg test suite has the side effect of testing that. We know that `wp-env` works well in CI because it is working great in the Gutenberg CI suite.
Additionally, the lint checks will lint wp-env code, and the unit tests will run the wp-env unit tests. So `wp-env` gets a significant amount of test coverage ""by accident,"" just because the wp-env code is used in CI and part of the monorepo!
I do think there are monorepo solutions to many of the other problems:
- Build times. Interestingly, `wp-env` doesn't have any build step. If you execute `./packages/env/bin/wp-env`, it will run the local files directly in node, no build step. So I think local development is extremely easy for that reason.
- Dependencies. Yarn has a thing called focus workspaces, where only dependencies related to the package you want to develop are installed. I think `npm` has, in newer versions, a similar feature, so in theory that will help solve that problem.
- CI taking a long time. I don't think this is just a wp-env problem! Certainly everyone would love for CI to be blazing fast in Gutenberg :)
I also wonder if the issue with how quickly contributions are accepted is more related to the number of people maintaining `wp-env`? I don't have the time to focus on wp-env any more, and I often become a bottleneck when no one else is looking at new PRs to wp-env.",Bitter Frustration
"Agree with your points Noah.
I think it's time to close this issue given this is clearly not going to happen, but also as the conversation became toxic and aggressive.
I did start one new pull request to help improve things based on one of the (now deleted) comments - https://github.com/WordPress/gutenberg/pull/38122.
I'm happy to also work on other ways to improve the contributing experience. Lets focus on making individual issues/PRs though for those things.",
"Thank you for your friendly request. The feature that you're looking for is called `json_encode()`, see https://www.php.net/json_encode
If you need more sophisticated serialization solutions, there's Symfony Serializer, JMS Serializer and other high quality open source libraries. I don't think that Doctrine has to invent its own solution.",
"@derrabus I don't see anywhere stated that, say, the Symfony serializer or JMS Serializer provides support for Doctrine. Notice that serialization of Doctrine Entities might lead to infinite recursion because of associations. Every ORM I have ever used throughout 10 years of web development career provide serialization methods because of this very issue. It is important to have a serializer that provides support for Doctrine, not merely a generic serializer. Or are Doctrine entities, query results, etc, with associations ready to be blindly serialized?",Bitter Frustration
The solutions I've provided you with are used quite commonly for serializing entity models hydrated with Doctrine ORM. Give them a try.,
"@derrabus To use the JMS Serializer Bundle in Symfony, I need to access the ""$container"" variable in Symfony controllers, which there is no documentation about how to access that variable. There is no minimal documentation about how to use the JMS Serializer Bundle. Please notice that json serialization is an EXTREMELY BASIC feature, and it is not minimally documented anywhere.",Bitter Frustration
"As mentioned on symfony/symfony#42833, use Symfony's discussions for support on Symfony.
https://github.com/symfony/symfony/discussions",
"### Feature Request
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| New Feature | yes
| RFC | yes/no
| BC Break | yes/no
#### Summary
This feature is EXTREMELY basic and the whole doctrine ORM is basically useless if you are unable to convert entities (select query results, etc) into json for returning in an API. Either the feature doesn't exist, which is honestly unbelievable, or there is a desperate need to add documentation for it, since not even stackoverflow requests have come to any reasonable conclusion in the past 10 years.",Bitter Frustration
"Thank you for your report. This is not a bug, as you already noticed yourself. This is why I'm closing this issue.
If you believe, the documentation could be improved, feel free to open an issue or pull request over at https://github.com/symfony/symfony-docs/",
"It **is** a bug if following the exact steps of the documentation leads to an error.
There is NO resource in the entire internet that tells how to access the $container variable, and consequently how to use bundles and service containers. It is not merely a matter of documentation needing to improve, it is a matter of your piece of software being completely unusable because the developer cannot possibly guess the information about how make something work when the documentation step by step leads to internal server errors.
Notice that I am following your own suggestion of JMSSerializerBundle for serialization, and it seems completely impossible to use this piece of software because there is no information available on the internet about how to access this ""$container"" variable that the Symfony docs also mention. Where is it? Where does it live?",Bitter Frustration
"Well then, I pointed you to the documentation repository where you're free to suggest improvements to the documentation. There's nothing I can do for you here because we're only tracking bugs and feature requests on this tracker.
If you're looking for support, feel free to open a discussion here: https://github.com/symfony/symfony/discussions",
"@derrabus You could at least tell me how to access the $container variable that is mentioned dozens of times throughout the docs, without they ever thinking about telling how to get that variable in the first place!!!",Bitter Frustration
Thank you for telling me what I could or could not do. Please use discussions for support. Thank you for understanding.,
"**Symfony version(s) affected**: latest
**Description**
According to the documentation (https://symfony.com/doc/current/service_container.html),
```
// checks if a parameter is defined (parameter names are case-sensitive)
$container->hasParameter('mailer.transport');
```
However, running this code gives:
`Notice: Undefined variable: container`
Which is obviously what would happen, since there is no variable called ""$container"" and you forgot to document how to obtain that variable.",Bitter Frustration
"General fixes for documentation including:
* Adding alt tags to images
* Fixing doc compilation errors
* Updating broken links
* Updating info in README.md
* Updating target branch for repo URLs
* Adding logo back to docs sidebar per Kenneth's request",
"~~AFAIK there's no way to resume compressed data like that which is why you see bad header.~~ The data is needed from the start to decompress. We should probably warn about it or stop it entirely.
slight edit: There is a way transfer encoding but it's hardly supported. curl could ask for the range of resource and then that range of resource is compressed (as opposed to requesting it compressed first then the requesting the range of that)",
"> The data is needed from the start to decompress.
Here is what experts who configure servers write. Experts, not diletants.
curl.exe --compressed -I https://curl.se/windows/
HTTP/2 200
server: nginx/1.17.6
content-type: text/html
x-frame-options: SAMEORIGIN
last-modified: Thu, 09 Sep 2021 03:25:04 GMT
etag: ""1eea-5cb878b724db1-gzip"" Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â† Even here Â«gzipÂ» mentioned.
cache-control: max-age=60
expires: Fri, 10 Sep 2021 18:41:54 GMT
content-encoding: gzip Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â† Â«gzipÂ» again.
x-content-type-options: nosniff
content-security-policy: default-src 'self' curl.haxx.se www.curl.se curl.se www.fastly-insights.com fastly-insights.com; style-src 'unsafe-inline' 'self' curl.haxx.se www.curl.se curl.se
strict-transport-security: max-age=31536000
via: 1.1 varnish, 1.1 varnish
accept-ranges: bytes Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â† Resume (i.e. option Â«--continue-atÂ») is supported.
date: Sat, 11 Sep 2021 12:55:10 GMT
age: 0
x-served-by: cache-bma1632-BMA, cache-fty21367-FTY
x-cache: MISS, HIT
x-cache-hits: 0, 1
x-timer: S1631364910.118245,VS0,VE408
vary: Accept-Encoding
content-length: 2396
If you want to be a specialist, read the fullâ˜º manuals (i.e. RTFM).",Mocking
"From manual:
Â«43 Internal error. A function was called with a bad parameter.Â»
For this case instead of error â„–61 or â„–3.",Bitter Frustration
"I don't think the error code is the primary problem here. The problem is rather that if the combination of options doesn't work, curl should warn about it earlier...",
"> curl should warn about it earlier
Of course you are right. Thank you!",
"To Badger: please, look at combination of options Â«-CÂ» and Â«-JOÂ»:
curl.exe -C 1 -J -L -O https://github.com/PowerShell/vscode-powershell/releases/download/v2021.9.0/powershell-2021.9.0.vsix
curl: (23) Failed writing header
HEADER, not output. Real dump of headers terminates on the sixth line of the second header.",Bitter Frustration
"To Badger: please, look at combination of options Â«-JOÂ» and main page of your site:
curl.exe -J -O https://curl.se/windows/
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
0 0 0 0 0 0 0 0 --:--:-- 0:00:02 --:--:-- 0Warning: Remote filename has no length!
11 7914 11 928 0 0 360 0 0:00:21 0:00:02 0:00:19 360
curl: (23) Failure writing output to destination
But the Â«size_downloadÂ» NOT equals to zero. Downloaded, but NOT SAVED. What is the point?
And I can not get the warning Â«Remote filename has no length!Â» from the script. Only the Â«errormsgÂ» is available.",Bitter Frustration
"<!-- Only file bugs here! Ask questions on the mailing lists https://curl.se/mail/
SECURITY RELATED? Post it here: https://hackerone.com/curl
There are collections of known issues to be aware of:
https://curl.se/docs/knownbugs.html
https://curl.se/docs/todo.html -->
### I did this
In MS Windows Command Prompt:
curl.exe -C 1 --compressed -o HTML.txt https://curl.se/windows/
** Resuming transfer from byte position 1
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
37 2395 37 904 0 0 221 0 0:00:10 0:00:04 0:00:06 222
curl: (61) Error while processing content unencoding: incorrect header check
### I expected the following
Error â„–2 or â„–3.
### curl/libcurl version
Build: 7.78.0_6
[curl -V output]
curl 7.78.0 (x86_64-pc-win32) libcurl/7.78.0 OpenSSL/3.0.0 (Schannel) zlib/1.2.11 brotli/1.0.9 zstd/1.5.0 libidn2/2.3.2 libssh2/1.10.0 nghttp2/1.44.0 libgsasl/1.10.0
Release-Date: 2021-07-21
Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp scp sftp smb smbs smtp smtps telnet tftp
Features: alt-svc AsynchDNS brotli gsasl HSTS HTTP2 HTTPS-proxy IDN IPv6 Kerberos Largefile libz MultiSSL NTLM SPNEGO SSL SSPI TLS-SRP UnixSockets zstd
### operating system
MS Windows 10-21H1
<!-- On Unix please post the output of ""uname -a"" -->",
"I think this rant belongs in the forums or any other social channel of libretro. Not on the github issue tracker.
If you're serious in your offer to help out with 'quality assurance', that would be great. But i think the best approach would be contacting the developers on the libretro forums or discord and discuss your suggestions.
Forum: https://forums.libretro.com/
Discord: https://discord.com/invite/C4amCeV",
"Repeated fowl language, capslock cruising, know-it-all attitude, and giving thumbs up to their own posts should be bannable offenses.",Bitter Frustration
"I appreciate the time you took to write this up but we don't appreciate your wording.
However, I will let this slip because you called it, but please not again.
While I agree on _some_ parts, in the end its a matter of time and money.
Everyone here does this in their free time, so unless you can come up with 20k in funding at minimum to work on these, I dont see the codebase reaching proper production-grade quality anytime soon.
If i have the choice between a full stomach and catering to anonymous dickheads, well the choice is obvious.
You can reach out to us anytime if you want to get involved and donate your time working on QoL issues, but then use Discord so it can be coordinated.
I can tell you tho, these issues are like the last of our problems with the codebase.. Theres way more fundamental/bigger fishes to fry first.
I recommend you split up your post into several seperate issues, completely reword it and try talking like a normal person.",Bitter Frustration
"Also: >near intensionally-troll the levels of design are here
What design _laughs_",Mocking
"It doesn't take 20000$ funding to reword some menu options. you can do some of this in like 10 minutes if someone really cared. Stop hiding behind excuses to shut perfectly valid changes down. Another contributor could take up the mantle, all you did was make sure no one looking over open issues would be able to see this topic, and implement any of these meaningful changes.",Bitter Frustration
"Like I said: I recommend you split up your post into several seperate issues, completely reword it and try talking like a normal person.
Your post is harder to navigate than retroarch xmb without assets, not to mention it makes my eyeballs itch.
You underestimate the cost it takes for doing everything over what we are already doing because it means time needs to be taken from office hours at our dayjobs, which are expensive. For 20k you get about a month worth of fulltime work for one person.
So why care about low priority issues when theres way bigger issues? Your reading comprehension is lacking.
Or maybe you dont have a idea what production-grade code implies, certainly not just changing a few options. In your words: Get your shit together",Bitter Frustration
"10 Minutes?! Alright then, you claim to be a dev. Stop moaning and start coding, if you ""really care"". Pull requests are a thing. See how easy it is.",Impatience
"Inspired by the steam forum topic ""Convoluted"" by Chelle, and a prompt by Developer Gadsby to post feedback,
this is a Quality Assurance Write-up Complaint about everything bad, unintuitive, convoluted, and otherwise
in need of serious QA (Quality Assurance) love. Retroarch NEEDS to up their QA to be able to act as a platform
for emulators. This post is meant to be intentionally hyper-critical and nitpicky on first world problems, because the experience is just that annoying. I hope to point out to readers many problems with the readability of Retroarch, and it's common user experience. I also hope if taken seriously, i get a contributor interested enough by my points, to add me and talk to me on discord, so we can work together on bringing retroarch up to a level people expect it to be at, and help retroarch hit console or platform level quality it is so desperately missing.
==================Chapter 0: I Didn't Even Launch It Yet And I Have Complaints=================
Note: If you are only contributor level and not developer level, scroll down and skip to chapter 1.
I'm going to start by looking at retroarchs website and steam page, and boy, before we even launch retroarch
for the first time, there a lot to talk about already. Infact, to even do this write up, or rather to attempt to
talk about it on the issues section on github i was pointed to, it says this is for bug reports only.
Currently, i'm under the assumption i'm already somehow breaking the rules by doing what i was told.
Not a good start, but not the worst. With that out of the way that i'm posting here on request, and not out
of ignorance for the warning github gives new posts, lets move on.
Lets look at the Website and steam page front pages. I kind of want to talk about them both at once.
I'll start with the opening description on both. It's to technical. Retroarch as i see it, is looking to be
a platform, like steam, to access things. Like the playstation XMB, or switch home screen. For a description,
on the site we get ""RetroArch is a frontend for emulators, game engines and media players."" and on steam ""RetroArch is an open source and cross platform frontend/framework for emulators, game engines, video games, media players and other applications""
This actually tells a user very little about what retro arch is, and isn't. Even people emulator savvy won't
understand the rest of what is being thrown at them. a 'frontend' for emulators? lets google frontend. ""frontend can refer to any hardware that optimizes or protects network traffic."" People aren't playing these
online, or it's not their first thought. They don't need protection from the net when playing mario. Okay,
maybe a smart person will think it has multiple definitions, and google more accurately. lets google Frontend Program
""A Front End developer (dev) works with designers and Back End devs to create a website"" Well, retroarch
isn't a website (Well technically it is but you know what i mean), it's a program itself. Already, using the
term frontend is to much. If you want retroarch to really succeed, to shine, you need to make it marketable and understandable. The optics matter, if people don't know what it is, they won't know why they should care
about being interested in it. when people say it's convoluted, you shouldn't need an 30 minutes of research
to even understand your opening sentences.
it also says 'game engines, and media players'. Why? Lets be honest, retroarch runs emulators. Thats what it
does. Why do you need to complicate this? Does it run other things sure, but nobody is seriously wanting it as their number 1 music player or picture taker. why can't you speak english? the steam page says emulators,
so someone knew that word, why aren't these two sources on the same page for their advertising? Unfortunately
the steam page then says game engines, video games, media players, and other apps.
What fucking goon is downloading retroarch just to play music. This is not a good use of advertising space.
The human being looking at your steam page can see the music menu in your advertising images. Use your text
space better, seriously!!! Why is this so hard! Also, it says emulators, then game engines, then video games.
pick ONE! these are all synonyms in practicality. People also don't need to know it's open source
in your intro, you can say that later. From what i've later come to understand, the devs want to use the term 'Core'. They want this because they believe in what retroarch does aside from emulators. Sure, except...the opening advertisement doesn't say 'Core'. Here is a good opener. ""RetroArch is a all in one platorm for Emulation that enhances the games you play
and other Cores. From retro era to modern games, you can even play online with friends!"" This is a fantastic, easy to digest, explanatory, to the point advertisement that tells people what
retroarch is, what it DOES (Enhances emulator games), and avoid words like ""nintendo"" or ""platstation"" for
copywrite while still getting the ""old to current"" idea across. Make your text readable to people! If they want more
they can hit up a FAQ. Note: Going forward, i will be referring to a reasonable way to read something, or
a problem with text that is to jargin-driven as ""Readability"" as a unbrella term. This will come up A LOT
and will be a serious problem with retroarch in nearly every part of anyones experience. Please remember this.
Lets move on. in the websites next section, it says you can run original game discs, but steam says the feature isn't out yet. These are literally giving conflicting information. Seriously? ITS YOUR FRONT PAGE!
Next, lets look at some visuals here. For the website, i want to look at the ""A polished Interface"" Slide.
This makes it look like playstation, cool. Except this isn't what it looks like in retroarch by default.
Make it clear in advertising this is a skin. Also, ironically, the interface is excessively NOT polished, but
i will be getting to that as a major complaint in another chapter. But right now, it's a skin, say it's
a skin. The very idea their not saying its not the default skin here, implies someone knew it was terrible and
tried to hide it. Maybe ask why they needed to hide your real interface? We will get to that later...
Moving back to steam, lets look at the slides/image roll. First is the trailers. All three are really fucking bad.
Trailer 1: its a fun logo (sure, fine) then shows the menu with garbage readability that makes a person
want to flee just looking at it, followed by showing what has GOT to be one of the WORST POSSIBLE game examples.
like, you CANNOT be serious. the website slides show mario, and this trailer has a level ID called zero / megaman129. Show us a actual video game. Make the trailer show like 8+ games of different genres. Make it CLEAR that retroarch plays VIDEO GAMES, and not, whatever the fuck that is. Bad advertising affects
end user readability to understand what retroarch is and is not. and this is contributing to why you get topics
saying people are confused about retroarch.
Trailer 2: Literally a logo. Why is this here? Remove it.
Trailer 3: This feels like a meme. Seriously, this is cruel, it hurts. It's like someone took what makes a good intro video and butchered it to the max. No BGM, no person explaining things, no examples
of games being played, it's super fast (Epilepsy educing even?). So fast you can't even read all the text
without pausing the video. Please make a real trailer. Look at like, any nintendo direct. Do that, but for retroarch. Show off and explain lots. I'll help QA it if you need someone. We can improve this.
Then the steam slides, their mostly the exact same menu. Why? Why not Show off various parts of retroarch? Actually make people interested. Your telling me you create all of retroarch, and the only part
you want people to know about is 2 screens? wtf? Now i can go on, but the pre-retroarch experience is getting comically long at this point.
I'll move onto actual retroarch, but rest assured, i can REALLY go on about whats wrong before even downloading.
In summary
1: Very poor readability
2: Poor advertising
3: Bad use of space
4: inconsistent between steam and website
5: Doesn't actually say what retroarch does in words people can understand let alone google. ===============Chapter 1: ""A Polished Interface""? Actually It's Really Fucking Garbage======================
Note: I have gotten some responses to points in this chapter from users using custom alternate skins for
RetroArch that change how the menu actually functions and displays information, and not just the visuals.
This chapter is about a first time experience, and as such uses the default skin. Before i actually talk about the REAL first impression, i want to point out i downloaded website and steam
both, to check for differences. Guess what? The first impression is diffrent. Inconsistent again? Ugh.
From website, it launched windowed, and you lack the cool cursor, and get the ultra low quality windows
menu that feels like a 2005 program with file/command/windows despite that retroarch has it's own in-app menus.
This doesn't happen on steam, it launches in fullscreen on first boot. The difference here is very important.
Not everyone knows you can Alt+Enter programs to fullscreen them. Infact, this is semi-rare info. This is made worst by F11 being the windows default to fullscreen, but boy do i wish complaining ended here.
If a non-steam user tries to fullscreen by hitting F11, retroarch hotkeys this to swapping between the windows
and custom cursor. Now, the custom one looks good, but it locks you into the windows. Surely you can
make your cursor not get locked, come on. But even worse, is when someone wants out and presses F11 again. I CANNOT BELIEVE how bad this is. If you are multi-monitor, your cursor is now locked to the monitor
retroach was in itself, untill you move retroarch. Clickin on it, pressing hotkeys ect won't fix this.
I don't mean using the retro cursor, i mean normal windows cursor, you can't leave your moniter. what the actual fuck? what a joke! I haven't even talked about retroarch contents directly, and
i can barely type up this QA review because retroarch literally won't let my cursor go click on notepad. >:(
it gets worse. If your fullscreen and press F11, your cursor doesn't change, but your mouse still can't
leave the moniter. The user would have no idea why and think retroarch is just a bad program. How can
you have this many errors and i'm not even talking about retroarch itself yet?????
OKAY, actual retroarch time. You can argue before this, thats a unintended bug, sure. Lets looks at whats NOT a bug.
So, we boot into the main menu that is...lacking readability at best. ""Load Core""? Why? these things are called emulators, call them emulators. ""Load Emulator"" is perfectly reasonable English. It's my understanding the devs want the term
Cores because it also uses something called Cores in addition to emulaitors. Sure, but you want to be
a marketable, understandable program? speak in words people actually understand. it's my understanding the team personally wants 'Load Core / Content' as they want to do more then emulators. In counter to this, i have 2 suggestions. Either split the menu AND load between Emulators (Load Emulaitor / Game), and other Cores (Load Core / Content), or make the load
button hidden untill a emulaitor or core is ready, then have it show Load Game or Load Content based on what Emu/Core
is loaded! Easy! Also, this menu is bloated.
So, 1: Split the cores menu, between ""Cores"", and a new one called ""Emulators""
2-1: In addition to Load Content, add a new menu called Load Game right under Emulators.
2-2: Or, make load content/game hidden until a emulator or core is loaded in already.
3: Also change Esc defaulting to exiting the program, to a back button. This is semi-common, and straight up default
in nearly every PC game. If your a platform on a PC for people gaming on a PC, use a PC control scheme. wtf?
there is a quit button if needed, and Alt+F4 or Alt+Enter, we don't need to troll PC users with a comically
bad default control scheme thats normally a menu button on almost every pc video game ever released.
""Bloated"" i'm going to define, as to many things being in one menu, or things being in the wrong location.
Here, we have the main menu showing config, info, and help. But we have a left side bar. whats the difference between config and settings? this is already a readability problem, they can be seen as synonyms. Now, i'm gonna be going all over unfocused here, because there is so many problems their all interwoven.
The Config file menu, why is this a menu? 1: Delete the Config menu entry
2: Move the contents of the config menu, to the top of the settings menu. 3: Why is help a menu with only 1 content? Literally delete this menu.
4: Move Menu Controls and it's icon to the bottom left of retroarch below the white line. This menu is first of all stupid and should probably be removed entirely, however...
4-2: Rename Menu Controls to Controller and have this lead directly to the input menu. If you really want,
include the Menu controls in Controller, but really it's common sense, and people can customize this,
so the stated controls in Menu Controls can literally lie to the user, this is dumb.
5-1: on first startup, maybe do a first time setup that asks the user what menu icons they would like.
let them pick from switch, xbox, playstation, ect. Its also kinda bad to display Switch by default, but
use american playstation input scheme, and not having a switches A button actually be A. Readability issues!!!!!
5-2: If that is a copywrite problem, alternatively let people customize the on-screen symbols with letters. In this way,
people can recreate their console of choice setup while working around looking like it clones a competitor, because
it's a customization option of a user. And don't tell me customization options aren't fair game when your straight up
stealing the playstation home menu theme >_>
Now that thats done, we have a nice (enough) looking home. Onward to settings. The entire thing is bloated. There is also a lot of 2-menus required trolls. There are so many times you look in one menu and need to google to find another menu, it's not even funny. 1: First up, just like the white line at the bottom of retroarch, i recommend a new break line in the middle
of settings. Things below it will be advanced settings, and title the section Advanced. Alternatively, if
this won't translate well to alternate skins, instead have a menu in here called Advanced at the bottom of settings.
Rename Saving to Saving Games (Readability, this seems like save Retroarch settings when it's here like this!)
2: Throw logging, File Browser, On-Screen Display, AI Service, Latency, into Advanced.
3: Delete the entire power management menu. wtf is this? It's literally empty. LITERALLY, USELESS. WHO????
4: Rename Network to Online, or Online Multiplayer. (Readability!)
5: Rename Input to Controllers
6-1: On the left bar, Rename Import Content to ""Create Playlist"" (Because that is literally what it does)
6-2: Move the playlists menu into the now named ""Create Playlist"" menu, de-bloat settings, so only people who care about the playlist menus are forced to see and scroll past this option. 6-3: Add a button named ""More Information on Playlists"" or maybe ""Playlists Tutorial"" or similar as well
to explain what is a 'Playlist', how to set them up, needing cores/emulators, ect.
6-4: if a Scan to create a playlist in a directory comes up empty, have a pop up explaining why. (Missing cores, ect)
7: I cannot believe you have a menu called configuration here, when i earlier talked about configuration in
the main menu. Why do you have 2 config menus, and a settings menu? HELLO? This is so troll, the
config options in settings/config effect your stuff in main menu/config. Why the fuck are they not together?
Who designed this? *Phew* now that thats done.....
7-1: In this submenu, move 'Use Global Core Options File' to the 'Load Core' Menu we are renaming to 'Load Emulator'.
7-2: We are also renaming this option ""Use Global Emulator Options"".
Readability is very important! 7-3: Finally, If (hopefully) Main menu config is moving to settings, either delete this Configuration menu, and move its only remaining option (Save Config on Quit) to settings, or move the config stuff from main menu, inside this config sub-menu. 8: AGAIN with the 2 menus needed trolls. Inside Directory we have a lot of playlist stuff, why?
8-1: In playlist menu (that we are moving to the menu in point 6 we are calling ""Create Playlist"" Menu) create a new menu called Playlist Directories
8-2: Move everything playlist related from settings/Directory to Music/Playlist Directories. This is to de-bloat menus with unnecessary information overflow, and put settings in menus they are actually
used in, like any other emulaitor (PCSX2/RPCS3/Citra/Cemu/Yuzu/ect)
9: Speaking of multiple same named menus, we have achievements, and a users achievements. STOP DOING THIS!!!!
why do you want people to need to google one menu doesn't work without the other menu! THIS IS SO TROLL, STOP IT!
9-1: Rename User to Account Settings & Achievements
9-2: Move the achievements menu into this newly named menu to de-bloat settings, and put this in a reasonable spot.
9-3: The Accessibility menu only has 1 option in it. Stop having troll 1 option menus. Delete this menu
9-4: Rename the Sub-menu ""Accessibility Enable"" To ""Text to Speech""
9-5: Move ""Text to Speech"" to the Account Settings & Achievements, so one user can use it, and
another different user doesn't have to deal with troll text to speech in their ears. 10: Continuing from 9, we have streaming options in account, and them in recording. WHY.
10-1: Rename ""Recording"" to ""Recording & Live Streams""
10-2: move the the content of the now ""Account Settings & Achievements"" / Account / Retroachievements to
the Achievements menu that was moved here in part 9. 10-3: The Youtube/Twitch/Facebook Gaming sub-menus only have 1 content, STOOOP FUCKINGGG DOOOING THIIIIS. >:((((
we are deleteing these sub-menus. 10-3: Move the menus inside the Youtube/Twitch/Facebook Gaming sub-menu stream key options from here, to the now named ""Recording & Live Streams"" menu. 10-4: in the ""Recording & Live Streams"" menu/Streaming Quality, give one of those grey sub-descriptions.
Something like ""Suitable for 720P Streams"" ""For 1080P Streams"" Ect. 10-5: Also rename the menu to Stream Quality.
Oh boy, onto input (that we said we are renaming to Controller)
11-1: Move Maximum Users to...somewhere. Wtf even is this menu? If this actually means users, move it to the
relevant online menu. If it means locally, rename this to ""Maximum Controllers"". 11-2: If the later, Create one of those Advanced Options line breaks again at the bottom, and move this
down there, along with every other option in this menu untill we get to Menu Controls. 11-3: MENU CONTROLLS? HELLO? Why does main menu get help on menu controlls, BUT WE HAVE A ACTUAL MENU CONTROLS MENU? PUT RELEVANT MENUS IN RELEVANT PLACES THANK YOU.
11-4: Move the Automatic Mouse Grab option to the Video menu in Settings. This is something usually in that
menu on other programs, stop being unintuitive!!! How can you make so many mistakes!!!
Speaking of the Video menu, lets tackle that next.
12-1:in Fullscreen Mode, rename Windowed Fullscreen to ""Borderless Fullscreen"" or just ""Borderless"",
and change the description appropriately.
12-2: Rename Fullscreen Width & Height to ""Custom Width"" (& Height). It's already in the fullscreen sub-menu, you don't need to remind them what menu they are in, it's at the top of the screen!
12-3: Move CRTSwitchRes and Output below the Fullscreen and Windowed sub-menus. Order matters! Most important
things go on top!
I can keep going, but i'll stop here. This is a Serious Checklist of Readability improvements across the entire settings menu. However, if i reconsider, i may suggest adding Controller to the left bar as a major
menu instead, as it's reasonab;y so important, it should be a major menu, rather then some garbage like Music.
=====================Chater 2: A Smoothe Gameplay Experience? Or A Troubleshooting Nightmare?=====================
Before i get into further details on games, i strongly recommend a new setting somewhere, on by default, that
makes it so when you load a game, and pick a emulaitor (Remember, we are calling these Games and Emulaitors, NOT
Cores and Content) that it associates that game to that emulaitor by default. In the future, you should be
able to Load game -> Pick the game -> it just boots up without needing to pick a emulaitor. I...HOPE i don't have to explain how bad it is that steam retroarch is missing a download button. I will skip
over the easy wall of text on this and move forward.
1: For steam users, there should still be a download emulaitor button, but as a submenu. 1-1: In here, There should be a link to your website with a list of emulaitors to download.
1-2: There should be a setting here, or in the Install Emulaitor menus to set the directory for cores.
This is because this options is only useful & relevant right at this moment, and never again, so it
does not need to be in the directroies menu elsewhere.
2: When in-game, move the Command/Menu button to the top of the list. I also feel a menu rename is in order, but can't think of one right now, and want to move on.
3-1: Add save state and load state to the Command menu. Yes i know they are in the F1 menu. Make them accessable.
3-2: In the F1 menu, move State slot below load state
And oh boy, lets get into the F1 menu. In no particular order...
4-0: Make it WAY more obvious to assign a button or combo to open this menu from controller.
probably include it in the quick menu directly at the bottom, even if the option exists somewhere else,
just duplicate it. 4-00: Also, Escape should open the quick menu by default
4-1: Originally i said Rename Information to Game Information, but i forgot i was still allowing for cores/content menus before.
i now suggest making this menus name be contextual, based on if a emulator, or core is loaded. This way people only using
emulators can have it reasonably say 'Game Information' and those who use cores still get 'Information' so it
doesn't clash with any development team desires.
4-2: Move the Controller menu to right below Close Content
4-3: Also rename that to Close Game
4-4: Adding something to favorites and you can't remove it? WHY? Make this menu change to Remove from Favorites if it's a favorite >_>
4-5: Remove the descriptions for Resume, Restart, and Close game. This screen has lots the player wants
to see and not enough screen space even when in fullscreen. 4-6: Also remove descriptions on save state, load state, Take Screenshot, Cheats, Achievements, and Rewind.
4-7: (Someone gave a good point against this, nevermind)
4-8: Make it so if the player presses cancel in the F1 to go to Main Menu, Then cancel again, it resumes the game.
4-9: move Show Desktop Menu from Main Menu while ingame to somewhere else (Probably settings).
With that out of the way, once ingame, there is no menu option to go back!
Add a button in windowed mode to open back the retroarch menu. If you tell me it's a hotkey, i don't care.
thats not intuitive! Make a button to go back to the retroarch menu!!!
Finally, this SHOULD be obvious, but it's not, so i will say it here.
I assume this is a major feature, but really, let people access that emulaitors settings when using it.
if we can't do that, retroarch is just worse then every emulaitor, because they have settings you NEED to even
run some games. If this option DOES exist, it's EXTREMELY hidden!
Moving on, post gameplay, your recent games don't show up in load content / load game.
Why isn't there a recent games list? And why can't you set a content / game directory in directories? If you can, that option should 100% be in the load content / Load Game menu, as this is where
it's actually relevant!!!
=======================Ending==================
I'll stop here, it's been 3 hours of complaints, but retroarch is riddles with terrible quality.
Why would anyone want to use this? 1: An unfun troubleshooting experience in exchange for playing games in the worst quality possible without access to emulators settings. 2: Some games are literally unplayable in retroarch because you can't access emu settings
3: The menus are bloated, and lie / decieves / give half truths by hiding relevant other options in other menus
for no other reason then to troll you. The overall feeling is Retroarch is giving you the run-around, asking you
to do something in some menu you don't know where it is, and when you find it, wants another setting in
another menu you don't know where it is. 4: it's a pain to setup even for simple emulators
5: you can't even understand what retroarch is or isn't from inconsistent and contradictory messages,
to near moon-rune levels of advertising.
6: unintuitive menus and lacking default settings mean you need to go over everything before even attempting
to test a single title.
7: Using it via steam is a whole extra nightmare that could be made easy if anyone actually fucking cared.
It's easy to see why people don't recommend recroarch. The real question is if anyone is going to actually
do anything about it, or delete this post / no reply / no one for me to directly work with in a timely fashon /
stick their head in the clouds and ignore why people don't enjoy their product. My discord is Dawnbomb#3408, and i'm open to any actual developer contacting me and working hand in hand for
some actual quality assurance before the steam deck launches, but considering how obvious, easy, and
near intensionally-troll the levels of design are here, i expect no contact and noone willing to work to make
retroarch not be complete fucking bullshit bloatware on a PC, and i only scratched the surface of this QA nightmare.
====================Chapter 3: Extra add-ons i thought of later=========================
Note: Some of the things here delve closer to more serious changes to retroarchs codeing.
Still, their good changes.
1: Controller backround input should be in the input (Hopefully named Controller) Menu, not in another menu (User Interface).
Setting up controller input in your controllers, then having if the actual input happens be decided in a separate menu, is another weird location problem.
2: While im at it, Turbo Fire should be a setting INSIDE a controllers port controls. This would allow each controller to have separate turbo settings per player. 3: Altho highly unusual, some players enjoy playing games with two people one controller mode. Some people even go so far as to speedrun like this. Allowing multiple controllers assign buttons to the same button on the same port of a input would go a long way in support for those people.",Bitter Frustration
"or yknow you could just use some pull requests, or better yet
fork retroarch and do your own thing with it instead of putting an essay on the issue tracker, lol",Mocking
"The official release 2021b of TZDB contains changes that would negatively affect Joda-Time users. If adopted it would be impossible to hold identifiers such as `Europe/Bratislava`, `Europe/Zagreb` and `America/Guadeloupe` (and many others) in memory. Any attempt to do so would see the ID replaced by another one.
Separately, the official 2021b release of TZDB has also effectively removed the timezone history of 9 zone IDs including `Africa/Accra` (Ghana) and `America/Nassau` (The Bahamas). This would see any queries for time-zone offsets pre-1970 return a different value.",Bitter Frustration
"Release v2.10.12 of Joda-Time contains timezone data from a [fork of TZDB](https://github.com/JodaOrg/tz) that reverts both the problematic changes listed above. This is not an ideal situation, and there is work ongoing to try and resolve the data issues.",Bitter Frustration
This issue is a placeholder to allow information about the 2021 timezone dataset changes to be captured.,
"Release v2.10.14 of Joda-Time uses [global-tz](https://github.com/JodaOrg/global-tz), which is derived from the IANA TZDB. The global-tz project reinstates all the data that has been systematically removed from IANA TZDB over the past few years.",
Release v2.11.0 also uses global-tz. Closing this as we now have a strategy going forward.,
Additional Q: How to build module documentation as man page?,
You seem to be running the tests with pytest which is not supported. Please read development documentation: https://python-markdown.github.io/contributing/#development-environment.,
"I'm closing this as there has been no follow-up. Note that...
1. As previously stated, we do not support pytest so any reports about it not working will be ignored.
2. We are not able to reproduce the report that tox is not running correctly. Presumably, we would need more information about the environment to replicate the issue.
> Additional Q: How to build module documentation as man page?
I'm not sure if you are referring to the CLI or the `docs/` directory. For the CLI, we are using `optparse` from the Python Standard Library. There should be various existing third-party tools which will convert the output of `optparse`'s `--help` to man page(s). I would suggest exploring those options. We currently use MkDocs to render the content of the `docs/` directory to HTML. Last I knew, no-one has made a MkDocs plugin which outputs man pages and I could care less about doing so.",Bitter Frustration
"> You seem to be running the tests with pytest which is not supported
I want only to add that running test suite in fixed env has some value however it is some other value to run test suite in opened set of modules if pytest is uses because power of that framework comes from extension which can alter testing process without single touch tested tree.
I'm not asking to switch CI to pytest however I think that alter test framework to be able tun pytest in some minimal env may have some added value.
Looks like it is issue with only with tests/test_syntax/extensions/test_md_in_html.py.
As long as that is added to ignore list pytest Workâ„¢ï¸.
```console
+ PYTHONPATH=/home/tkloczko/rpmbuild/BUILDROOT/python-markdown-3.3.6-2.fc35.x86_64/usr/lib64/python3.8/site-packages:/home/tkloczko/rpmbuild/BUILDROOT/python-markdown-3.3.6-2.fc35.x86_64/usr/lib/python3.8/site-packages
+ /usr/bin/pytest -ra --ignore tests/test_syntax/extensions/test_md_in_html.py
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0
rootdir: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.6
plugins: datadir-1.3.1, regressions-2.2.0, mock-3.6.1
collected 772 items
tests/test_apis.py ...................................................................................... [ 11%]
tests/test_extensions.py ..................................... [ 15%]
tests/test_legacy.py ................................................................................sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 34%]
sssss..s....................... [ 38%]
tests/test_meta.py .. [ 38%]
tests/test_syntax/blocks/test_blockquotes.py . [ 38%]
tests/test_syntax/blocks/test_code_blocks.py ..... [ 39%]
tests/test_syntax/blocks/test_headers.py ss......................................................................... [ 49%]
tests/test_syntax/blocks/test_hr.py ............................................... [ 55%]
tests/test_syntax/blocks/test_html_blocks.py .............................................................................................................. [ 69%]
tests/test_syntax/blocks/test_paragraphs.py ................. [ 71%]
tests/test_syntax/extensions/test_abbr.py ............. [ 73%]
tests/test_syntax/extensions/test_admonition.py ...... [ 74%]
tests/test_syntax/extensions/test_attr_list.py .. [ 74%]
tests/test_syntax/extensions/test_code_hilite.py sssssssssssssssssssssssssssssssssss [ 79%]
tests/test_syntax/extensions/test_def_list.py ...... [ 79%]
tests/test_syntax/extensions/test_fenced_code.py ..................ssssssssssssss [ 83%]
tests/test_syntax/extensions/test_footnotes.py .......... [ 85%]
tests/test_syntax/extensions/test_legacy_attrs.py . [ 85%]
tests/test_syntax/extensions/test_legacy_em.py ...... [ 86%]
tests/test_syntax/extensions/test_tables.py . [ 86%]
tests/test_syntax/extensions/test_toc.py ......................... [ 89%]
tests/test_syntax/inline/test_autolinks.py ..... [ 90%]
tests/test_syntax/inline/test_emphasis.py ................. [ 92%]
tests/test_syntax/inline/test_entities.py .... [ 92%]
tests/test_syntax/inline/test_images.py ...................... [ 95%]
tests/test_syntax/inline/test_links.py ................................ [ 99%]
tests/test_syntax/inline/test_raw_html.py . [100%]
========================================================================= short test summary info ==========================================================================
SKIPPED [19] markdown/test_tools.py:185: Excluded
SKIPPED [50] markdown/test_tools.py:144: Tidylib not available.
SKIPPED [1] tests/test_syntax/blocks/test_headers.py:112: This is broken in Python-Markdown
SKIPPED [1] tests/test_syntax/blocks/test_headers.py:132: This is broken in Python-Markdown
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:116: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:55: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:69: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:85: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:150: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:141: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:174: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:283: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:215: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:318: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:301: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:187: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:200: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:159: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:269: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:228: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:256: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:243: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:99: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:335: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:134: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:359: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:556: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:581: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:604: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:479: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:543: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:530: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:398: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:418: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:459: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:435: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:377: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:630: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:517: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:537: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:414: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:573: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:450: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:609: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:485: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:683: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:735: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:708: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:760: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:506: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:665: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:644: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:384: Pygments== is required
===================================================================== 652 passed, 120 skipped in 3.52s =====================================================================
```
In other words to make whole tree pytest ready it does not look that is that much to do ..",Impatience
"I can understand the desire to use a specific testing framework, but that choice is chosen by the maintainer and must be respected.
If there was a specific advantage for us to move over to pytest, we could have that conversation, but I realize, as you mentioned, that is not the conversation we are currently having.
If we were to add py.test support just so others could use it, then we have to maintain it and ensure it doesn't break or we are going to get issues to ""fix pytest"" on occasions when it breaks inadvertently. Since this project is not using any features from pytest, it makes no sense for us to go out of our way, even if it is only slightly out of the way, to accommodate pytest.
I have nothing against pytest and use it in a number of my projects, but this project's maintainer has chosen not to use it. I can see no upside for us to fix pytest support as we have no intention of monitoring ensuring that it does not break.",
"> If we were to add py.test support just so others could use it, then we have to maintain it and ensure it doesn't break or we are going to get issues to ""fix pytest"" on occasions when it breaks inadvertently. Since this project is not using any features from pytest, it makes no sense for us to go out of our way, even if it is only slightly out of the way, to accommodate pytest.
Adding correctly working pytest support is VERY important.
That importance is not only about exact tested module. Just please add to testing picture fact that in exact test suite is adding yet another workload to all modules involved in testing. In other words it is about health of whole ecosystem (in this case python ecosystem).
Additionally .. I 'm finishing now some packages build automation to be able to run kind of set f scratch build if build systems are not bus. In case of python it address some random set of additional pytest extension and perform build with logging results .. only. In such cases if for example if lets say will be added `pytest-randomly` it would be possible to prove that random reshuffling units may cause that test suite fail it will mean that internally test suite have some dependencies between units and even if all test suite executed without `randomly` extension running exact unit by `pytest <some/file.py>::<some_unit>` will definitely fail. Sometimes test suites are running quite long time or on implementing new unit such way of running isolated units may be useful.
I want only to say that as long that `markdown` test suite will be not working I would not be able to do that or in this exact case I would be doing that only with part of the test suite.
Form that point of view I'm asking to to fix test suite or something like that.
As I have now `markdown` in set of my distro packages I'll be to perform such new way of testing your module.
In other words I want only to encourage to prepare some base line id you want to have such testing completely for free (that automation will be only flagging some found issue without single second spend by you :) )
If you don't care about that is 100%/perfectly fine :)
If you care .. just please have look closer on the code which you know because cost of that fix may be really low :)
I'm specialist on packaging area so form me fixing that seems is a bit to high :P (I've stumped on python modules only ~10 months ago so I'm still learning however after finishing +700 modules as rpm/Solaris IPS packages I see some patters which I can exploit giving back to the community results of my work).
Cheers :)",Impatience
"> I want only to say that as long that markdown test suite will be not working I would not be able to do that or in this exact case I would be doing that only with part of the test suite.
I understand you have a vision and you have a desire to collect testing data, but I'm going to be very blunt about this, if your whole system depends on every python package you use to support your specific testing framework, you are building a very fragile system. Cornering other projects into using one specific testing framework is not a good idea, and it takes away the freedom of that project to maintain their project in a way that they find most suitable to them.
What if some other framework comes out that a project finds more enticing than pytest? Are they not allowed to take advantage of that testing framework as to not break your pytest dependent system?
I'm not seeing an advantage for us to switch to pytest, and I definitely can't see a reason to set up and test an additional framework that will test the exact same tests.
Again, I have nothing against pytest, and I do use it myself. There are features pytest has that make testing easier in some cases, but this project doesn't currently have a scenario that we **need** anything beyond what is already offered by Python's unittest module.",Bitter Frustration
"> if your whole system depends on every python package you use to support your specific testing framework
Indeed .. issue is that I want to know before those areas of sensitivity before my client will come to me and will tell me ""here it is something wrong"" :)
In other words it is kind of proactive approach.
What will happen next is completely independent an probably would require some assessment about what to do with that.
Running few POCs I found some interesting results. What could be done next with those results is matter of running that framework on larger scale to be able identify some patterns and maybe design some other automation on top of that.
Going back to the ticket subject. Really please have look one more time on that case because it may be something very easy to fix for you .. only this and nothing more :P
Please ..",Impatience
We support and use `unittest` in the Python Standard Library. We will not be adding support for any other testing framework. The extra work would provide no benefit to the project itself as @facelessuser has explained multiple times. Please stop wasting our time with this.,Bitter Frustration
"I'm trying to package your module as an rpm package. So I'm using the typical build, install and test cycle used on building packages from non-root account.
- ""setup.py build""
- ""setup.py install --root </install/prefix>""
- ""pytest with PYTHONPATH pointing to sitearch and sitelib inside </install/prefix>
On testting usimg pytest it fails with:
```console
+ /usr/bin/pytest -ra tests
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.8.12, pytest-6.2.5, py-1.10.0, pluggy-0.13.1
benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
Using --randomly-seed=1062042675
rootdir: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4
plugins: forked-1.3.0, shutil-1.7.0, virtualenv-1.7.0, expect-1.1.0, flake8-1.0.7, timeout-1.4.2, betamax-0.8.1, freezegun-0.4.2, aspectlib-1.5.2, toolbox-0.5, rerunfailures-9.1.1, requests-mock-1.9.3, cov-2.12.1, flaky-3.7.0, benchmark-3.4.1, xdist-2.3.0, pylama-7.7.1, datadir-1.3.1, regressions-2.2.0, xprocess-0.18.1, black-0.3.12, asyncio-0.15.1, subtests-0.5.0, isort-2.0.0, hypothesis-6.14.6, mock-3.6.1, profiling-1.7.0, randomly-3.8.0, nose2pytest-1.0.8, pyfakefs-4.5.1, tornado-0.8.1, twisted-1.13.3, aiohttp-0.3.0, localserver-0.5.0, anyio-3.3.1, trio-0.7.0, cases-3.6.4, yagot-0.5.0, Faker-8.14.0
collected 1043 items / 1 error / 1042 selected
================================================================================== ERRORS ==================================================================================
_____________________________________________________ ERROR collecting tests/test_syntax/extensions/test_md_in_html.py _____________________________________________________
/usr/lib/python3.8/site-packages/_pytest/runner.py:311: in from_call
result: Optional[TResult] = func()
/usr/lib/python3.8/site-packages/_pytest/runner.py:341: in <lambda>
call = CallInfo.from_call(lambda: list(collector.collect()), ""collect"")
/usr/lib/python3.8/site-packages/_pytest/python.py:766: in collect
self.warn(
/usr/lib/python3.8/site-packages/_pytest/nodes.py:230: in warn
warnings.warn_explicit(
E pytest.PytestCollectionWarning: cannot collect test class 'TestSuite' because it has a __init__ constructor (from: tests/test_syntax/extensions/test_md_in_html.py)
========================================================================= short test summary info ==========================================================================
ERROR tests/test_syntax/extensions/test_md_in_html.py::TestSuite - pytest.PytestCollectionWarning: cannot collect test class 'TestSuite' because it has a __init__ constr...
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================================= 1 error in 0.99s =============================================================================
pytest-xprocess reminder::Be sure to terminate the started process by running 'pytest --xkill' if you have not explicitly done so in your fixture with 'xprocess.getinfo(<process_name>).terminate()'.
```
Runnimg pytest with `--import-mode=importli` it fails with:
```console
+ /usr/bin/pytest -ra --import-mode=importlib tests
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.8.12, pytest-6.2.5, py-1.10.0, pluggy-0.13.1
benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
Using --randomly-seed=3873927089
rootdir: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4
plugins: forked-1.3.0, shutil-1.7.0, virtualenv-1.7.0, expect-1.1.0, flake8-1.0.7, timeout-1.4.2, betamax-0.8.1, freezegun-0.4.2, aspectlib-1.5.2, toolbox-0.5, rerunfailures-9.1.1, requests-mock-1.9.3, cov-2.12.1, flaky-3.7.0, benchmark-3.4.1, xdist-2.3.0, pylama-7.7.1, datadir-1.3.1, regressions-2.2.0, xprocess-0.18.1, black-0.3.12, asyncio-0.15.1, subtests-0.5.0, isort-2.0.0, hypothesis-6.14.6, mock-3.6.1, profiling-1.7.0, randomly-3.8.0, nose2pytest-1.0.8, pyfakefs-4.5.1, tornado-0.8.1, twisted-1.13.3, aiohttp-0.3.0, localserver-0.5.0, anyio-3.3.1, trio-0.7.0, cases-3.6.4, yagot-0.5.0, Faker-8.14.0
collected 764 items / 1 error / 763 selected
================================================================================== ERRORS ==================================================================================
_____________________________________________________ ERROR collecting tests/test_syntax/extensions/test_md_in_html.py _____________________________________________________
ImportError while importing test module '/home/tkloczko/rpmbuild/BUILD/markdown-3.3.4/tests/test_syntax/extensions/test_md_in_html.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_syntax/extensions/test_md_in_html.py:25: in <module>
from ..blocks.test_html_blocks import TestHTMLBlocks
E ImportError: attempted relative import with no known parent package
========================================================================= short test summary info ==========================================================================
ERROR tests/test_syntax/extensions/test_md_in_html.py
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================================= 1 error in 0.87s =============================================================================
pytest-xprocess reminder::Be sure to terminate the started process by running 'pytest --xkill' if you have not explicitly done so in your fixture with 'xprocess.getinfo(<process_name>).terminate()'.
```
tox is failing as well
```conosle
+ /usr/bin/tox --skip-missing-interpreters
.package create: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4/.tox/.package
ERROR: invocation failed (exit code 1), logfile: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4/.tox/.package/log/.package-0.log
================================================================================ log start =================================================================================
RuntimeError: failed to build image wheel because:
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/virtualenv/seed/embed/via_app_data/via_app_data.py"", line 51, in _install
key = Path(installer_class.__name__) / wheel.path.stem
AttributeError: 'NoneType' object has no attribute 'path'
================================================================================= log end ==================================================================================
ERROR: InvocationError for command /usr/bin/python3 -m virtualenv --no-download --python /usr/bin/python3 .package (exited with code 1)
ERROR: FAIL could not package project - v = InvocationError('/usr/bin/python3 -m virtualenv --no-download --python /usr/bin/python3 .package', 1)
```",Bitter Frustration
@hsbt Also this one,Impatience
"Btw. this issue, #2693 and #2695 can be closed as all 3 are about reverts and you just did a revert.",Impatience
@hmdne Thanks,
"Revert #2690 Pull request:
https://github.com/ruby/www.ruby-lang.org/pull/2693
Justification will be written as a song on myspace.",Mocking
"Excuse me, I was just trying to help you and is this how you thank me. By marking it spam. If you dont need it then just close the request.",Bitter Frustration
"The way it is written is correct. Installation problems is correct. `@ohmyzsh on Twitter` is correct. Your PR is invalid, useless, and it costs me time to triage. You're the 4th person to open a spam PR. I hope you reconsider your ways to obtain the Hacktoberfest t-shirt and start considering maintainers time as valuable.",Bitter Frustration
"> The way it is written is correct. Installation problems is correct. `@ohmyzsh on Twitter` is correct. Your PR is invalid, useless, and it costs me time to triage. You're the 4th person to open a spam PR. I hope you reconsider your ways to obtain the Hacktoberfest t-shirt and start considering maintainers time as valuable.
I am not here to attain tshirt i already have more than 7 accepted commits. I am here to help people correct error and if its not valid for your program. Then just close it. How does this became a spam.",Bitter Frustration
"I don't know how many commits you've made, I just know this PR wasn't useful and we're in the middle of Hacktoberfest so I have to consider this as another PR for Hacktoberfest. As a maintainer I have a duty to weed out these kinds of PRs and mark them as invalid when they don't pass a minimum standard. If you already have the required commits I suggest you stop making more PRs like this one or you'll get banned from Hacktoberfest. Good day sir",Threat
"## Standards checklist:
<!-- Fill with an x the ones that apply. Example: [x] -->
- [ ] The PR title is descriptive.
- [ ] The PR doesn't replicate another PR which is already open.
- [ ] I have read the contribution guide and followed all the instructions.
- [ ] The code follows the code style guide detailed in the wiki.
- [ ] The code is mine or it's from somewhere with an MIT-compatible license.
- [ ] The code is efficient, to the best of my ability, and does not waste computer resources.
- [ ] The code is stable and I have tested it myself, to the best of my abilities.
## Changes:
- [...]
## Other comments:
...",
Delete this issue instead of closing,Impatience
lmao ok,Mocking
"> lmao ok
Me dumdum",Mocking
> Delete this issue instead of closing,Impatience
lets marry @renovate-bot,Mocking
let the war begins,Threat
"> let the war begins
Lmaoo",Mocking
"> Delete this issue instead of closing
Deleted two times this is the 3rd time its opening xD",Impatience
Stupid bot,Insulting
lol,Mocking
Rise of the machines,Irony
shatup,Vulgarity
the bot didnt opened since 19 hours... Something is wrong with the world,Bitter Frustration
Top 10 ten signs of apocalypse,Irony
Bot will rule us lmao,Mocking
why are u gae,Identify Attack/Name Calling
lmao,Mocking
Who will win this war ðŸ‘€,Irony
"> Who will win this war ðŸ‘€
**we** will win this war",Mocking
Uno reverse,Mocking
@sudhanplaz :),Irony
This has no limit,Bitter Frustration
You have no power ower this ðŸ˜‚,Mocking
"> You have no power ower this ðŸ˜‚
What if..... We made another bot to conquer this issue ðŸ˜Ž",Mocking
"> > You have no power ower this ðŸ˜‚
> > What if..... We made another bot to conquer this issue ðŸ˜Ž
Good luck lel",Mocking
"> > You have no power ower this ðŸ˜‚
> > What if..... We made another bot to conquer this issue ðŸ˜Ž
You can make a bot that auto-closes the issue",Mocking
"> > > You have no power ower this ðŸ˜‚
> > > > What if..... We made another bot to conquer this issue ðŸ˜Ž
> > You can make a bot that auto-closes the issue
Or remove this renovate bot",Mocking
"> > > > You have no power ower this ðŸ˜‚
> > > > > > > > > What if..... We made another bot to conquer this issue ðŸ˜Ž
> > > > > > You can make a bot that auto-closes the issue
> > Or remove this renovate bot
No its a little helpfull other than this issue",Mocking
"Open source developers vibing.
renovate bot: Not for so long (evil laugh).",Mocking
"> Open source developers vibing. renovate bot: Not for so long (evil laugh).
ðŸ¤¡",Mocking
Lmao,Mocking
What is this bot doing keeps spamming lmao,Impatience
lol,Mocking
"this issue will still exist even after years, even when this repo dies out, god bless this immortal renovate botðŸ¤",Irony
"> this issue will still exist even after years, even when this repo dies out, god bless this immortal renovate botðŸ¤
ðŸ™ðŸ¼",Irony
"> this issue will still exist even after years, even when this repo dies out, god bless this immortal renovate botðŸ¤
Not after the release of v5",Irony
"> > this issue will still exist even after years, even when this repo dies out, god bless this immortal renovate botðŸ¤
> > Not after the release of v5
ðŸ™ðŸ¼",Irony
lmao,Mocking
"This issue lists Renovate updates and detected dependencies. Read the [Dependency Dashboard](https://docs.renovatebot.com/key-concepts/dashboard/) docs to learn more.
## Rate-Limited
These updates are currently rate-limited. Click on a checkbox below to force their creation now.
- [ ] <!-- unlimit-branch=renovate/node-17.x -->chore(deps): update dependency @types/node to v17.0.45
- [ ] <!-- unlimit-branch=renovate/prettier-3.x -->chore(deps): update dependency prettier to v3
- [ ] <!-- unlimit-branch=renovate/typescript-5.x -->chore(deps): update dependency typescript to v5
- [ ] <!-- unlimit-branch=renovate/major-nextjs-monorepo -->chore(deps): update nextjs monorepo to v14 (major) (`eslint-config-next`, `next`)
- [ ] <!-- unlimit-branch=renovate/node-20.x -->chore(deps): update node.js to v20 (`node`, `@types/node`)
- [ ] <!-- unlimit-branch=renovate/node-21.x -->chore(deps): update node.js to v21
- [ ] <!-- unlimit-branch=renovate/major-yarn-monorepo -->chore(deps): update yarn to v4
- [ ] <!-- unlimit-branch=renovate/discordjs-rest-2.x -->fix(deps): update dependency @discordjs/rest to v2
- [ ] <!-- unlimit-branch=renovate/nextui-org-react-2.x -->fix(deps): update dependency @nextui-org/react to v2
- [ ] <!-- unlimit-branch=renovate/axios-1.x -->fix(deps): update dependency axios to v1
- [ ] <!-- unlimit-branch=renovate/express-rate-limit-7.x -->fix(deps): update dependency express-rate-limit to v7
- [ ] <!-- unlimit-branch=renovate/pretty-ms-8.x -->fix(deps): update dependency pretty-ms to v8
- [ ] <!-- create-all-rate-limited-prs -->ðŸ” **Create all rate-limited PRs at once** ðŸ”
## Open
These updates have all been created already. Click a checkbox below to force a retry/rebase of any.
- [ ] <!-- rebase-branch=renovate/nextui-org-react-1.x -->[fix(deps): update dependency @nextui-org/react to v1.0.0-beta.9-dbg2](../pull/1176)
- [ ] <!-- rebase-branch=renovate/better-erela.js-spotify-1.x -->[fix(deps): update dependency better-erela.js-spotify to v1.3.11](../pull/1178)
- [ ] <!-- rebase-branch=renovate/discord-api-types-0.x -->[fix(deps): update dependency discord-api-types to v0.37.63](../pull/1179)
- [ ] <!-- rebase-branch=renovate/react-monorepo -->[chore(deps): update dependency @types/react to v18.2.37](../pull/1177)
- [ ] <!-- rebase-branch=renovate/eslint-8.x -->[chore(deps): update dependency eslint to v8.53.0](../pull/1184)
- [ ] <!-- rebase-branch=renovate/prettier-2.x -->[chore(deps): update dependency prettier to v2.8.8](../pull/1185)
- [ ] <!-- rebase-branch=renovate/node-16.x -->[chore(deps): update node.js to v16.20.2](../pull/1181)
- [ ] <!-- rebase-branch=renovate/yarn-monorepo -->[chore(deps): update yarn to v3.6.4](../pull/1182)
- [ ] <!-- rebase-branch=renovate/colors-1.x -->[fix(deps): update dependency colors to v1.4.0](../pull/1183)
- [ ] <!-- rebase-branch=renovate/node-fetch-2.x -->[fix(deps): update dependency node-fetch to v2.7.0](../pull/1180)
- [ ] <!-- rebase-all-open-prs -->**Click on this checkbox to rebase all open PRs at once**
## Ignored or Blocked
These are blocked by an existing closed PR and will not be recreated unless you click a checkbox below.
- [ ] <!-- recreate-branch=renovate/typescript-4.x -->[chore(deps): update dependency typescript to v4.9.5](../pull/1076)
- [ ] <!-- recreate-branch=renovate/nextjs-monorepo -->[chore(deps): update nextjs monorepo to v12.3.4](../pull/1082) (`eslint-config-next`, `next`)
- [ ] <!-- recreate-branch=renovate/better-erela.js-apple-1.x -->[fix(deps): update dependency better-erela.js-apple to v1](../pull/708)
- [ ] <!-- recreate-branch=renovate/discord.js-14.x -->[fix(deps): update dependency discord.js to v14](../pull/1006)
- [ ] <!-- recreate-branch=renovate/node-fetch-3.x -->[fix(deps): update dependency node-fetch to v3](../pull/576)
## Detected dependencies
<details><summary>docker-compose</summary>
<blockquote>
<details><summary>docker-compose.yml</summary>
</details>
</blockquote>
</details>
<details><summary>dockerfile</summary>
<blockquote>
<details><summary>Dockerfile</summary>
- `node 17.9.1-alpine`
</details>
</blockquote>
</details>
<details><summary>npm</summary>
<blockquote>
<details><summary>dashboard/package.json</summary>
- `@emotion/react ^11.9.3`
- `@emotion/styled ^11.9.3`
- `@mui/icons-material ^5.8.4`
- `@mui/material ^5.8.4`
- `@nextui-org/react 1.0.0-beta.9`
- `next 12.2.4`
- `react 18.2.0`
- `react-dom 18.2.0`
- `@types/node 17.0.41`
- `@types/react 18.0.16`
- `eslint 8.19.0`
- `eslint-config-next 12.2.4`
- `typescript 4.7.4`
</details>
<details><summary>package.json</summary>
- `@discordjs/builders ^1.4.0`
- `@discordjs/rest ^1.5.0`
- `axios ^0.27.0`
- `better-erela.js-apple ^0.1.0`
- `better-erela.js-spotify 1.3.9`
- `colors 1.3.3`
- `discord-api-types 0.37.1`
- `discord-together ^1.3.25`
- `discord.js ^13.14.0`
- `dotenv ^16.0.1`
- `ejs ^3.1.6`
- `erela.js ^2.3.3`
- `erela.js-deezer ^1.0.7`
- `erela.js-facebook ^1.0.4`
- `erela.js-filters ^1.2.6`
- `express ^4.17.1`
- `express-rate-limit ^6.2.0`
- `express-session ^1.17.3`
- `express-ws ^5.0.2`
- `js-yaml ^4.1.0`
- `jsoning ^0.13.0`
- `lodash ^4.17.21`
- `moment ^2.29.1`
- `moment-duration-format ^2.3.2`
- `node-fetch 2.6.7`
- `os ^0.1.2`
- `passport ^0.6.0`
- `passport-discord ^0.1.4`
- `pretty-ms ^7.0.1`
- `rlyrics ^2.0.1`
- `systeminformation ^5.9.12`
- `winston ^3.3.3`
- `youtube-sr ^4.3.4`
- `prettier 2.6.2`
- `node >=16.x <=16.16`
- `node 16.15.1`
- `yarn 3.3.0`
</details>
</blockquote>
</details>
---
- [ ] <!-- manual job -->Check this box to trigger a request for Renovate to run again on this repository",
XD,Mocking
"Wait, how to close issue?",Impatience
Bot messing up the dependencies ;-;,Bitter Frustration
"> Bot messing up the dependencies ;-;
Quite frankly yeah, I would rather use dependabot.",Bitter Frustration
Legend says this issue will still exist even on the end of mankind,Irony
Ah yes,Irony
this is war,Threat
L,
nice,
"Is your request something like this?
* Invent some special metadata that can be set on a Flatpak (or Snap?) app, with the semantics ""this app can take screenshots without prompting"". Flatpak (and Snap?) would not do anything differently in setting up sandboxes based on this metadata, it's just a marker.
* Make the screenshot portal look for that metadata in `/.flatpak-info`. If it's present, just take the screenshot without prompting, with parameters (window/whole screen/etc.) controlled by the request.
* Maybe the portal backend is still responsible for visual/audio feedback (screen flash/shutter sound) to make sure the user is aware that a screenshot was taken?
* Ideally, app stores like Flathub limit access to that metadata (more review required), in the same way they ideally would for other ""dangerous"" permissions like `host` filesystem access.
Or a possible alternative would be to do something a bit like #638:
* Have a flag that the screenshot app can set, to say ""I'm always going to need this permission""
* The first time that flag is used, have a prompt with some sort of ""remember this"" option
* Make a note in the permission store that this app is OK to take screenshots any time, or give it a token that can be looked up in the permission store later, or similar
Or a mixture of the two: ignore or reject the flag from the second approach if it's set by an app that doesn't have the permission metadata from the first?",
"I think it should be strictly easy to control so it shouldn't be metadata (and require something like Flatseal to change).
I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.",
"> I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.
This is my proposal as well, similar to permissions on a mobile OS. (I am the maintainer for the screenshot program Flameshot)",
"> I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.
Agree with this one. The user experience with pop up dialogs that require additional clicks every time you take a screenshot is just not a way to go.
I'm the developer and maintainer of ksnip.",
"> I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.
Giving the user the _option_ to grant the application permission to take future screenshots/screencasts without further user permission would be OK. We just don't want the application to be able to give itself this permission, or to be able to force the user to grant this permission in order to use it just once.
So let's forget about additional metadata. I would retitle this issue from ""Screenshot portal without prompt"" to ""Screenshot portal should have toggle for user to disable future prompts for this application.""",
"As part of such change it would be useful to pass the type of screenshot that should be taken. The prompt that opens up is not just a permission thing but also a selection of type of screenshot (and if the cursor should be included eventually). If the user for example first asks for Active Window screenshot and confirms that he don't want to be asked again, what happens when the user requests another screenshot and expects to get a fullscreen screenshot now. Currently there is just an option to ask for scrrenshot but not what kind of screenshot.",Impatience
"OK, makes sense. So: add API for application to choose the type of screenshot that should be taken, only show the option to permanently grant permission to take screenshots if the new API is used.
That wouldn't apply the same to screencasts, but those are a separate portal.",
"Yeah, I think just a dialog that asks for that permissions with an option to make it permanent would in that case be the best, without any other selection option. Bonus points for additional parameter in the API call like ""Include cursor"" and ""Include window decoration"".
> That wouldn't apply the same to screencasts, but those are a separate portal.
Yes, I think it was a different API but I can imagine that they have similar issues if they haven't been fixed already.",
"I hope you only have to give permission one time, and then from there it can do it every time. This would be more secure than the old method, but less annoying then the current.",
"Can I express how powerless this issue makes me feel in relation to Gnome development. The tone from the Gnome folks here: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970 indicates to me this issue will not be resolved/is not considered an issue. Creating a Gnome related bug report is akin to shouting into the void, isn't it?",Bitter Frustration
"They don't see the issue there but it's probably the same folks that doesn't take much screenshots so they don't feel the pain. As long as they're not made aware about the user frustration coming from this, they won't fix it I'm afraid.",Bitter Frustration
"> Can I express how powerless this issue makes me feel in relation to Gnome development. The tone from the Gnome folks here: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970 indicates to me this issue will not be resolved/is not considered an issue. Creating a Gnome related bug report is akin to shouting into the void, isn't it?
Honestly, your expectations are misplaced. Bypassing the screenshot portal is unacceptable, as it defeats the point of having the portal in the first place. Applications should not be able to screenshot your desktop without your permission. Removing the backdoor should not be controversial.
If you read up in this issue, we already have agreement on the path forward to enhance the screenshot portal. It's just waiting for a motivated developer to tackle it.",Bitter Frustration
"@kurobeats @DamirPorobic
The Gnome developers made this decision (don't allow external apps to use gnome's private API) for protecting the privacy of users and forbidding API abuse. They are also doing a redesign for the screenshot UI of gnome: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1954 The Words accusing the developers of not caring about something are not true, and totally unconstructive.",
"@VitalyAnkh I've spoken with Gnome developers (also same with KDE developers but they haven't disabled the private dbus yet) on few occasions about this topic, on tickets and on IRC and my impression was that this is for them a minor inconvenience. One of them told me even that this is a fair trad off, few clicks more but you get more security. So I don't see it as ""not true"" and even the ""totally unconstructive"" is debatable, user requirement sets the prio for features, if no one speaks up the developers that don't use this feature won't get the user pain. Not sure what the screenshot UI redesign to do with this issue, our problem is that we need to give permission for every screenshot instead of once like most people are used to like from mobile phone apps. @mcatanzaro is right, there is a suggestions that, when implemented, would fix our issues, I don't see any further discussion here.",
"> Honestly, your expectations are misplaced. Maybe so but the action taken has now created a negative user experience. Forgetting my comments, I'm a voice in the crowd, think about the ""regular user"", they are going to see this behaviour and be confused by it because it's not something they have come to expect.
>Bypassing the screenshot portal is unacceptable, as it defeats the point of having the portal in the first place. Applications should not be able to screenshot your desktop without your permission. Removing the backdoor should not be controversial.
Let's keep in mind we are talking about a screenshotting tool here not a trojan (which I imagine we are trying to defend against here). Flameshot, for example, doesn't have to overcome hurdles to screenshot on Windows or MacOS.
> If you read up in this issue, we already have agreement on the path forward to enhance the screenshot portal. It's just waiting for a motivated developer to tackle it.
I'm very happy to hear it, but wouldn't it have been pragmatic to implement the solution before we do unexpected things to the user base?",Bitter Frustration
"Author of flameshot here, actually on MacOS you do have to grant a one time permission to record the the screen.
While I was annoyed this changed in gnome before we adjust the portal to only ask a single time, overall this will be a great compromise between ease of use and security. It also will be exactly the same as MacOS. I think my users will also find the one time prompt acceptable.",
"> Yeah, I think just a dialog that asks for that permissions with an option to make it permanent would in that case be the best, without any other selection option. Bonus points for additional parameter in the API call like ""Include cursor"" and ""Include window decoration"".
> > > That wouldn't apply the same to screencasts, but those are a separate portal.
> > Yes, I think it was a different API but I can imagine that they have similar issues if they haven't been fixed already.
Yeah I fully agree!!
I understand the security concerns from the Gnome team, but it's also about giving users options. Give users the ability to give certain applications permission to do this and remember their choice.
Certain programs like Screenshot apps and screen capture/recording programs need this to work. And giving those programs permission every time gets tiring and not user friendly. (from an UX point of view)",Bitter Frustration
"I'm rather amazed that people invoke ""security"" as a reason while undermining security by not thinking things through.
When a security feature is very annoying or even breaks software, it becomes an anti-feature. For example, when Telegram Desktop was failing to read files I was drag and dropping into it because the portal wasn't smart enough to see that drag and drop should give permissions for that file, I installed Flatseal and gave Telegram Desktop permissions to all files. Where's the security in that?
Any security features that are sufficiently irritating become just yet another annoying thing to turn off. They not only provide zero security, but are also an added annoyance.
While I appreciate the concern and I also appreciate that a decision was reached to implement the ""remember this obvious choice"" feature, I am disappointed that usability and actual security are not priorities and they are just an afterthought after users complain.
I would hope this is implemented and more care is taken in the future when ""security"" features are implemented, because for now I've gotten used to avoid Flatpaks in order to have actually functioning applications, and I'd love for that to change in the future.",Bitter Frustration
"@dancojocaru2000 IIRC that is not related to a security feature, it's missing functionality in the application and it needs to add support for the file transfer portal. Please follow the discussion here: https://github.com/flatpak/xdg-desktop-portal/issues/99
Edit: According to this issue the missing functionality was in Electron: https://github.com/flathub/org.telegram.desktop/issues/23",
"Hi Team,
Can I request if this issue is being addressed? Totally fine if it isn't deemed an issue, I can revert to Xorg.",Impatience
Any news? This is not normal. Workflow can't be damaged this way..,Bitter Frustration
"Very frustrating, please give a user an option not to see the window every time, the great thing about Linux is flexibility and I would not like to see it less flexible than e.g. macOS.",Bitter Frustration
"I reported again to gnome upstream: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970#note_1341047
and here: https://gitlab.gnome.org/GNOME/gnome-shell/-/issues/4895",
"@All3xJ members of the team are claiming criticism of the bug they introduced as harassment (or something close to: https://gitlab.gnome.org/GNOME/gnome-shell/-/issues/4895#note_1341442). I think it'll be best if people affected by this issue move to Xorg (it still works perfectly well!) or to a user friendly desktop environment (I've moved to Plasma), this bug isn't actively being worked on.",Bitter Frustration
"There are **no objections** to having a one-time prompt for the portal; the other issues being linked are related to internal GNOME APIs being used for this, which was never correct in the first place and is an entirely separate API.
Please avoid the ""any news on this"" comments, they do nothing but clutter the thread. The issue has also been open for less than three months and will likely end up involving volunteer work contributed in developers' free time...so pardon the bluntness, but I'm pretty sure more time and energy has already been spent on this thread than saved by not having to perform a click.",Bitter Frustration
"Temporarily locking due to too low signal-to-noise ratio. I'd encourage anyone who wishes to see this feature to go ahead and implement it in xdg-desktop-portal, and at least one frontend (GNOME, KDE, or wlroots).",Impatience
"This may seem dangerous but for screenshot applications, this sounds necessary from a design POV. Having a screenshot application which asks twice to screenshot would be quite awkward, I think.
Related : https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970 , https://github.com/flameshot-org/flameshot/issues/1910",
"https://github.com/flatpak/xdg-desktop-portal/pull/853 exists, now someone with proper authority have to review and eventually merge that",Impatience
any news? please give user a chance to setup this feature,Impatience
"> any news? please give user a chance to setup this feature
This has already been added.",Impatience
"> > any news? please give user a chance to setup this feature
> > This has already been added.
ok, how to keep permission to share screenshots to some application?
now I can not see this option (flatseal for managing permissions in flatpak apps)
![image](https://user-images.githubusercontent.com/6695246/190594258-7c50337b-8548-4778-99dc-dda4809f698c.png)",Impatience
"Edit: I understand what was meant in the comments above now. (Stupid me!) The fix for this has already been processed and was pushed through GNOME 43. You can find out which version of GNOME you're using with `gnome-shell --version`. BTW, if you are running Fedora, it will be available with Fedora 37, release date is quoted ""around 25 OCT 2022"".
@jadahl ~~If you happen to have info on where/how we can set this, please reply.~~ I have run into this issue recently as well, and had initially assumed there was an issue with flameshot, since I had never used it before. Realizing that this is a permissions thing, that I need to permit each time I launch the program, is getting very tedious to deal with even if the program itself is _considerably_ better than the default screenshot program.
@axel-n How did you go about getting all of those permissions listed in Settings? My own listing looks like this
![application_list](https://user-images.githubusercontent.com/5470143/192634605-c47ecbdd-10bf-40b4-8aa7-e35bfb1b66af.png)",Bitter Frustration
"> @axel-n How did you go about getting all of those permissions listed in Settings? My own listing looks like this
> ![application_list](https://user-images.githubusercontent.com/5470143/192634605-c47ecbdd-10bf-40b4-8aa7-e35bfb1b66af.png)
> @michael-hart-github That's [Flatseal](https://flathub.org/apps/details/com.github.tchx84.Flatseal), not Gnome Settings.",
"I'm sorry if I missed something, but I wasn't able to find any guidence on how to update xdg-desktop-portal in order for gnome not to ask if I want to share my screenshot with Flameshot every time. Could you please point me to how I can do this? I'm using ubuntu 22.04 with gnome 42.",
"@andreyizrailev I think the easiest solution would be for you to upgrade to 22.10, which has this feature out of the box. And the new Gnome in that version is quite nice too, so I'd say it's worth the upgrade.",
@gpothier that is very unfortunate that I have to choose between the long term support and a convenient way of making screenshots. But thank you for the answer!,Bitter Frustration
"btw, how fast is the portal api? Can it do 30+FPS? Currently I have X11 python application using mss.grab() which takes screenshots on desktop regions 30fps and calculates averages which controls ambient led-strip around the monitor in real time. So I would need to be able to do the same in Wayland. Or is there no way to use Wayland because of security restrictions?",Impatience
"> btw, how fast is the portal api? Can it do 30+FPS?
You shouldn't use the screenshot portal to make screencasts, there is the screenast portal for that.",Impatience
"> > btw, how fast is the portal api? Can it do 30+FPS?
> > You shouldn't use the screenshot portal to make screencasts, there is the screenast portal for that.
I'm currently capturing bbox regions from 4k desktop (for performance reasons). Is screencast portal able to provide casts from partial desktop regions? Have to start searching for API documentation. Does screencast portal api have setting for not asking end user permissions (as target machine is running in kiosk mode without keyboard and mouse).",Impatience
"> Is screencast portal able to provide casts from partial desktop regions?
No, but can in theory be added.
> Does screencast portal api have setting for not asking end user permissions
Yes.",
"Thanks a lot, devs! :D expecially @GeorgesStavracas",
Why is this closed? The issue is still there.,Impatience
This issue was fixed by c8274173f7d127f1d7e39e8b5bcaf7f0ee751f48.,
"For people who are a bit newer to Linux - Note that Ubuntu releases are the year and date, so Ubuntu 22.04 was released 2022 on the 4th month.
Ubuntu 22.04 LTS comes with GNOME 42.5 (you can verify this yourself by running `gnome-shell --version`), and it's extremely unlikely to receive an official update to GNOME 43. LTS releases happen every two years, so the next one will be Ubuntu 24.04 LTS, which will include GNOME 43 or a newer version. So right now, for Ubuntu LTS you can expect this to be fixed in about a year from this posting, in 2024 on the 4th month.
If you want GNOME 43 sooner, you can upgrade to Ubuntu 22.10 now or newer versions when they're released. None of these will be considered LTS releases though and non-LTS releases have a shorter support period of 9 months. This means you'll need to upgrade more frequently to stay on a supported version.
Another option is to install GNOME 43 from a third-party repository (like a PPA) or build it from source. This method can be risky, as it might cause compatibility issues or system instability. If you choose to try to do this, you should really consider backing up your data and be ready to troubleshoot potential issues. Although, if you had to read this to understand the situation, I don't really suggest this option.",
"The data here is merely a copy of the data in eggert/tz. When the source data changes, the data here will naturally also be updated.",
Please explain the reason why this issue was closed without resolving it?,Impatience
"### Key information
""joda-time"" uses Kiev (incorrect) instead of Kyiv (correct) spelling.
Kyiv is a capital of Ukraine.
According to the Ukrainian local and international law, correct name of the ""Kiev"" is ""Kyiv"".
Reference materials:
- Local: Resolution No 55 of 27.01.2010 of the Cabinet of Ministers of Ukraine ""On regulation of transliteration of Ukrainian alphabet by means of Latin alphabet "".
- Global: https://unstats.un.org/UNSD/geoinfo/UNGEGN/docs/10th-uncsgn-docs/econf/E_CONF.101_85_Standardization%20of%20Geographical%20Names%20_eng.pdf
- Wikipedia: https://en.wikipedia.org/wiki/KyivNotKiev
### Problem description
Incorrect spelling of Kyiv is used all across this project:
```
$ grep -r ""Kiev""
src/site/xdoc/timezones.xml:<tr><td align=""left"" valign=""top"">+02:00</td><td align=""left"" valign=""top"">Europe/Kiev</td><td align=""left"" valign=""top""></td></tr>
src/main/java/org/joda/time/tz/src/europe:# from Kiev to Moscow time sometime after the January 1994 elections.
src/main/java/org/joda/time/tz/src/europe:# From IATA SSIM (1994/1997), which also says that Kerch is still like Kiev.
src/main/java/org/joda/time/tz/src/europe:# ""Time in Ukraine is set to second timezone (Kiev time). Each last Sunday
src/main/java/org/joda/time/tz/src/europe:# (Europe/Kiev) to introduce permanent daylight saving time (similar
src/main/java/org/joda/time/tz/src/europe:# From Vladimir in Moscow via Alois Treindl re Kiev time 1991/2 (2014-02-28):
src/main/java/org/joda/time/tz/src/europe:# For example, tzdb uses Europe/Kiev, as ""Kiev"" is the most common spelling in
src/main/java/org/joda/time/tz/src/europe:# ""Praha"". (""Kiev"" came from old Slavic via Russian to English, and ""Prague""
src/main/java/org/joda/time/tz/src/europe:# English; in the meantime, stick with the traditional English ""Kiev"" as that
src/main/java/org/joda/time/tz/src/europe:# This represents most of Ukraine. See above for the spelling of ""Kiev"".
src/main/java/org/joda/time/tz/src/europe:Zone Europe/Kiev	2:02:04 -	LMT	1880
src/main/java/org/joda/time/tz/src/europe:	2:02:04	-	KMT	1924 May 2 # Kiev Mean Time
```
This issue should be fixed.
Instead of ""Kiev"" spelling ""Kyiv"" should be used instead.",
"The data here is merely a copy of the data in `eggert/tz`. When the source data changes, the data here will naturally also be updated.",
Please explain the reason why this issue was closed without resolving it?,Impatience
"> > > The data here is merely a copy of the data in `eggert/tz`. When the source data changes, the data here will naturally also be updated.
I do not understand your comment.
What is `eggert/tz`?
What data is a copy of which data?",Impatience
"> I do not understand your comment. What is `eggert/tz`? What data is a copy of which data?
You might want to read up a little on the subject first: https://www.iana.org/time-zones
Almost all relevant software/libraries nowadays rely on this database ...",Impatience
"> iana.org/time-zones
Thank you for this link.
I would not be surprised if this database is maintained by the Russians, and the servers are located in the Russian-occupied Crimea.",Bitter Frustration
"> I would not be surprised if this database is maintained by the Russians, and the servers are located in the Russian-occupied Crimea.
I'm pretty sure that's not the case. In addition, there is a comprehensible answer as to why this wish was not realised at the time.
https://mm.icann.org/pipermail/tz/2020-August/029203.html",
"> > > > I would not be surprised if this database is maintained by the Russians, and the servers are located in the Russian-occupied Crimea.
> > I'm pretty sure that's not the case. In addition, there is a comprehensible answer as to why this wish was not realised at the time. https://mm.icann.org/pipermail/tz/2020-August/029203.html
Yes, there is a reason to leave it as it is. As well as the reason to change Kiev to Kyiv.
""[tz] Change Request: Europe/Kiev to Europe/Kyiv"" contains information about popularity of the Kiev and the popularity of the Kyiv spelling. I am not surprised of a such ratio. If nobody follow new rules nothing will be changed.
This is why many of us are trying to change this scenario.
I think that repo with Kiev spelling can be frozen (deprecated) for compatibility with old products.
In addition, new repo with Kyiv can be forked/created.
New products can use such repo without problems.",Bitter Frustration
"Whether the name should be changed or not is not the responsibility of this project. Until the tzdb project changes the name, no change will happen here.
Raising PRs or adding to the discussion here is unhelpful, wasting your time and mine.",Bitter Frustration
Resolves #571 Using the correct spelling for the capital of Ukraine (Kyiv instead of Kiev).,
See #571,Impatience
"### Key information
""joda-time"" uses Kiev (incorrect) instead of Kyiv (correct) spelling.
Kyiv is a capital of Ukraine.
According to the Ukrainian local and international law, correct name of the ""Kiev"" is ""Kyiv"".
Reference materials:
- Local: Resolution No 55 of 27.01.2010 of the Cabinet of Ministers of Ukraine ""On regulation of transliteration of Ukrainian alphabet by means of Latin alphabet "".
- Global: https://unstats.un.org/UNSD/geoinfo/UNGEGN/docs/10th-uncsgn-docs/econf/E_CONF.101_85_Standardization%20of%20Geographical%20Names%20_eng.pdf
- Wikipedia: https://en.wikipedia.org/wiki/KyivNotKiev
### Problem description
Incorrect spelling of Kyiv is used all across this project:
```
$ grep -r ""Kiev""
src/site/xdoc/timezones.xml:<tr><td align=""left"" valign=""top"">+02:00</td><td align=""left"" valign=""top"">Europe/Kiev</td><td align=""left"" valign=""top""></td></tr>
src/main/java/org/joda/time/tz/src/europe:# from Kiev to Moscow time sometime after the January 1994 elections.
src/main/java/org/joda/time/tz/src/europe:# From IATA SSIM (1994/1997), which also says that Kerch is still like Kiev.
src/main/java/org/joda/time/tz/src/europe:# ""Time in Ukraine is set to second timezone (Kiev time). Each last Sunday
src/main/java/org/joda/time/tz/src/europe:# (Europe/Kiev) to introduce permanent daylight saving time (similar
src/main/java/org/joda/time/tz/src/europe:# From Vladimir in Moscow via Alois Treindl re Kiev time 1991/2 (2014-02-28):
src/main/java/org/joda/time/tz/src/europe:# For example, tzdb uses Europe/Kiev, as ""Kiev"" is the most common spelling in
src/main/java/org/joda/time/tz/src/europe:# ""Praha"". (""Kiev"" came from old Slavic via Russian to English, and ""Prague""
src/main/java/org/joda/time/tz/src/europe:# English; in the meantime, stick with the traditional English ""Kiev"" as that
src/main/java/org/joda/time/tz/src/europe:# This represents most of Ukraine. See above for the spelling of ""Kiev"".
src/main/java/org/joda/time/tz/src/europe:Zone Europe/Kiev	2:02:04 -	LMT	1880
src/main/java/org/joda/time/tz/src/europe:	2:02:04	-	KMT	1924 May 2 # Kiev Mean Time
```
This issue should be fixed.
Instead of ""Kiev"" spelling ""Kyiv"" should be used instead.",
See #571,Impatience
Resolves #571,
"I don't use PowerDNS, I'm participating in an internal POC, but I liked how the project was thought out and architected. I have little experience with big contributions, but I can help with basic community issues, organizing Issues, Contributions...",
@ngoduykhanh I have reconsidered and decided I would really like to do a lot with this project. Will you please promote me to a maintainer?,Entitlement
@AzorianSolutions Thank you for your interest in maintaining the project. We have 2 active maintainers now. I will discuss with them whether we want to have more people involved as maintainers. Please reach out to me via email so we can keep in touch.,
"Hello everyone,
This is Khanh, the creator of PowerDNS-Admin.
Sorry for being away for a long time. I would like to create this ""issue"" to update you on my current contribution status for PDA.
As I am not using PowerDNS anymore and have a new job, I have very little time that I can spend maintaining PowerDNS-Admin. However, I know there are still a lot of people who are using it and want to keep this project alive. So, I am looking for new maintainers for PowerDNS-Admin. If you are using it and interested in maintaining an open-source project, feel free to reach out to me at k[at]ndk.name. I am also thinking of transferring the project to an organization when it gets more maintainers.
There are still a lot of Pull Requests from you that need to be reviewed and merged. I will try to clear them this weekend.
Best,
Khanh",
"Regrettably, I do not have enough time to help maintain PowerDNS-Admin anymore - in contrast with last November, when I first contacted Khanh - and I don't feel comfortable being part of the group of collaborators any longer just for the hope that this might change, which is currently uncertain at best.
I've therefore handed back the collaborator role, but may still contribute on occasion.",Bitter Frustration
"I'll prepare a PR that
- moves all the instructions files into a docs directory (is that the best name?)
- removes crosslinks from each translated file and adds them to a docs index page so new files only need to add a link in one place.",
"> I'll prepare a PR that
> > * moves all the instructions files into a docs directory (is that the best name?)
> * removes crosslinks from each translated file and adds them to a docs index page so new files only need to add a link in one place.
See this note in chore posted by you: https://github.com/EbookFoundation/free-programming-books/issues/6164#issuecomment-940222595",
"so using ""docs"" works?",Impatience
"@EbookFoundation/reviewers last step. What do you think about this HowTo's homogeneization?
Performed Tasks:
- Add H1 header
- `Read this ...` right alignmements
- Split a bit first paragraph to highlight welcome.
- Introduce Pull Request acronym.
- Trademark typos: GitHub, YouTube, Pull Request
- Update GitHub links urls. `help`/`guides` subdomain was moved to `docs` so a 301 Redirect is skipped.
- Details block added, with center align. When clicked, the contributor graphs are toggled.
- Some bolds to highlight common issues (like in CONTRIBUTING)
- Repo name in monospace font
- Re-worded last part of 4th paragraph to explain that no new PR is needed to open if there are linter errors or changes are requested. Now it's more clear, I thought.
![image](https://user-images.githubusercontent.com/3125580/154009817-8d08ec9d-2d2d-4ba7-930b-4d01e31bcfcf.png)
It's ok? Aproved to continue apply into the other files?",
"I think this looks pretty nice, but _unfortunately_, you may want to test it in the mobile app.
I don't think things like the `<details>` tag work there. (Bear in mind when I tested was over a year ago.)
We could always ignore that technically, since I'd consider that a bug on GitHub's side, though. (Markdown is Markdown, and should work regardless of client.)",Irony
"> I think this looks pretty nice, but _unfortunately_, you may want to test it in the mobile app.
> > I don't think things like the `<details>` tag work there. (Bear in mind when I tested was over a year ago.)
> > We could always ignore that technically, since I'd consider that a bug on GitHub's side, though. (Markdown is Markdown, and should work regardless of client.)
https://davorpa.github.io/free-programming-books/ Tested on a Kindle Fire EReader and a Samsumg S5 (Android 6.1). It works as expected 4x4 in both environments: GitHub Preview, GitHub Pages. In anyway, the previous link ""large, growing"" points to same graph",Irony
"Just checking, you tested in the GitHub Mobile App?
https://play.google.com/store/apps/details?id=com.github.android&hl=en_US&gl=US",Impatience
"> Just checking, you tested in the GitHub Mobile App?
> https://play.google.com/store/apps/details?id=com.github.android&hl=en_US&gl=US
yes. it works as well.
| Closed | Open |
|---|---|
| ![Screenshot_20220215-152254.png](https://user-images.githubusercontent.com/3125580/154081554-0255f8dc-97fa-4f83-97fb-276be29d739f.png) | ![Screenshot_20220215-152337.png](https://user-images.githubusercontent.com/3125580/154081630-f5667cfd-8dd2-4da4-9a45-c538ba1fcc11.png) |",
"on the whole, I think this looks great. I'm ambivalent about monospacing ""Free-Programming-Books"". The name travels outside of the repo to places where it's not possible (or just more work for me) to monospace it - twitter, the Free Ebook Foundation website, and most particularly, the HTML presentation on github. I've managed to keep the dashes intact, mostly. So while monospacing looks good here, I prefer to use less code-y version.
Have commented in the PR on the text that reveals the usage graph.",
"- Homogenize markdown format across files. Solve some linter faults
- [x] #6625
- [x] #6724
- [x] #6698
- [x] Sort links according to english text seen in README.md. **Addressed in #6164**
### Local branch
- [x] Reintegrate #5837
- [x] ~Reintegrate stalled conflictive #5752 to complete translation~. Addressed by: #6719
- [x] Reintegrate #6429
- [x] Reintegrate #5831
- [x] Reintegrate #6567",
"> Many of the Microsoft employees working on the project have substantial amounts of paid time off subject to loss if they are not taken this year
We're really sad that your vacation and bonus will be lost if you don't work on the holidays.",Mocking
"its how most companies are, unfortunately. have fun with your time off and relax everybody",Bitter Frustration
winget is disgusting,Insulting
@denelon I think you should lock this thread as it's just an announcement and there is no need for subscribers to get notified about reaction comments...,Impatience
"Hello everyone. I wanted to let you all know as we enter into the holidays, there will be periods of less activity. Many of the Microsoft employees working on the project have substantial amounts of paid time off subject to loss if they are not taken this year. Many of the areas of focus during this time are in support of engineering improvements to help make the team more efficient when troubleshooting issues, or spending time ""on-call"" to support various aspects of the services and integration between GitHub and Azure DevOps.",
"This can be feature detected in the install script
Itâ€™s really stupid for people to be unable to use Bun on older computers",Bitter Frustration
What next?,Impatience
"For the time being, I've made a script to simplify and automate SDE's installation based on @renhiyama 's solution, lmk if there's anything to fix.
`$ curl https://gist.githubusercontent.com/fuka-g/afb29296db34d2451cdddcc14dbeef3c/raw/0c9e5d43910cf53b9bd82b358559bb8b75ebbfdc/bun-sde-fix-install_script.sh | bash`
EDIT: Updated script, this one is up to date.",
@fuka-g I followed those instructions. Now how do I run `bun`? Nothing chnaged on Linux 5.4.0-42-lowlatency x64.,Impatience
"> @fuka-g I followed those instructions. Now how do I run `bun`? Nothing chnaged on Linux 5.4.0-42-lowlatency x64.
Did you restart your shell ?",Impatience
"No. I am running a live system from RAM.
I can download the Node.js executable `node` and QuickJS executable `qjs` and run a JavaScript runtime out of the box.
This makes no sense to build an executable that throws by default.",Bitter Frustration
"> No. I am running a live system from RAM.
> > I can download the Node.js executable `node` and QuickJS executable `qjs` and run a JavaScript runtime out of the box.
> > This makes no sense to build an executable that throws by default.
Reload the terminal, not the OS",Bitter Frustration
"I did. Same result.
```
$ curl https://gist.githubusercontent.com/fuka-g/afb29296db34d2451cdddcc14dbeef3c/raw/4ad55d9bef57b8ff3d40eb797c9e9e595e2d04e1/bun-sde-fix-install_script.sh | bash
```
```
$ ./test.js
Illegal instruction (core dumped)
```
I shouldn't be fighting just to run a JavaScript runtime that is 75.4MB. QuickJS executable is 915.5KB and runs out of the box.",Bitter Frustration
"> $ ./test.js
> Illegal instruction (core dumped)
`$ bun run test.js` ?",Impatience
"```
$ ./bun run test.js
Illegal instruction (core dumped)
```
Do I need to do something with the external script bun-sde-fix-install_script.sh?",Bitter Frustration
"@guest271314 restart your terminal and then try again, should be working",Impatience
also you should be running `bun` and not `./bun`,
"> @guest271314 restart your terminal and then try again, should be working
I have done that twice now. Same result
```
$ ./bun --help
--use <STR>Illegal instruction (core dumped)
```
```
$ ./bun --use
--use <STR>Illegal instruction (core dumped)
```
> also you should be running `bun` and not `./bun`
That does not make any difference. I am calling the executable with `/usr/bin/env` from the same working directory.
It should not this complicated just to run a basic command.
And these implementation details should be in the REDAME.md.",Bitter Frustration
"This is test.js
```
#!/usr/bin/env -S ./bun console.log('Hello, World!');
```",
and thats not gonna work for you...,Bitter Frustration
"> This is test.js
> > ```
> #!/usr/bin/env -S ./bun > console.log('Hello, World!');
> ```
You're still using the original bun executable.
The script does not patch bun, but uses SDE as a middleman to handle the avx2 set.
You have to run `bun run test.js`, which will be an alias for `sde -- bun`, and not `./test.js`, which will use the original bun.",
"Hi. Rust can check CPU features and whether the OS supports them.
Is it fine to use rust for feature detection and link a rust static lib into `bun`?
See <https://doc.rust-lang.org/stable/std/arch/macro.is_x86_feature_detected.html>",
"> and thats not gonna work for you...
Evidently. That is why I filed https://github.com/Jarred-Sumner/bun/issues/504.
Cf. QuickJS and Node.js, respectively
```
#!/usr/bin/env -S ./qjs
console.log('Hello, World!');
```
```
$ ./test.js
Hello, World!
```
```
#!/usr/bin/env -S ./node
console.log('Hello, World!');
```
```
$ ./test.js
Hello, World!
```",
"> Hi. Rust can check CPU features and whether the OS supports them. Is it fine to use rust for feature detection and link a rust static lib into `bun`? See https://doc.rust-lang.org/stable/std/arch/macro.is_x86_feature_detected.html
AFAIK the issue is that we don't support devices without AVX2 at all yet, not that we can't tell if they support that instruction set or not.",
"> > This is test.js
> > ```
> > #!/usr/bin/env -S ./bun > > console.log('Hello, World!');
> > ```
> > You're still using the original bun executable.
> > The script does not patch bun, but uses SDE as a middleman to handle the avx2 set.
> > You have to run `bun run test.js`, which will be an alias for `sde -- bun`, and not `./test.js`, which will use the original bun.
More unecessary complications not documented in README.md and that does not work:
```
$ bun run test.js
bash: /home/user/.sde/sde: No such file or directory
```",Bitter Frustration
"> > Hi. Rust can check CPU features and whether the OS supports them. Is it fine to use rust for feature detection and link a rust static lib into `bun`? See https://doc.rust-lang.org/stable/std/arch/macro.is_x86_feature_detected.html
> > AFAIK the issue is that we don't support devices without AVX2 at all yet, not that we can't tell if they support that instruction set or not.
Then that exact language needs to be in bold at the top of the README.md.
This is insane.",Bitter Frustration
"> > Hi. Rust can check CPU features and whether the OS supports them. Is it fine to use rust for feature detection and link a rust static lib into `bun`? See https://doc.rust-lang.org/stable/std/arch/macro.is_x86_feature_detected.html
> > AFAIK the issue is that we don't support devices without AVX2 at all yet, not that we can't tell if they support that instruction set or not.
We usually check whether the environment has AVX2 and run fallback functions instead of SIMD functions. I think it's the right way to support old devices.
```
if (is_avx2_enabled()) {
return call_avx2_fn();
}
return call_fallback_fn();
```",
"> More unecessary complications not documented in README.md and that does not work:
Because it's a pretty new bug, and should not be documented because it should not be a problem in the first place.
Can you send the output of `ls -a ~/.sde` ? It should output
```
. ia32 Licenses pinkit sde sde64.sig xed xed64.sig
.. intel64 misc README.txt sde64 sde.sig xed64 xed.sig
```
If not, the installation is faulty somehow, which would be either an issue with `curl` or `tar`.",Bitter Frustration
"I followed the instructions. The download was successful
```
$ ls -a ~/.sde
. Licenses ia32 misc sde sde64 xed xed64
.. README.txt intel64 pinkit sde.sig sde64.sig xed.sig xed64.sig
```
You folks need to conspicuously document the limitations and restrictions on usage. At the top of your README.md. That is very reasonable and responsible maintenance.",Entitlement
"Note, I didn't install `bun` executable globally, just as I didn't install QuickJS or Node.js executables globally.
QuickJS I just run using `/usr/bin/env`.
For `node` executable (and Deno) I do the same thing, then truncate the executable to 0 because I am not carrying around a 70MB+ executable.
In any event, you need to conspicuously document the limitations and resrictions and dependencies necessary to use `bun`. Else you are hiding your flaws, in plain sight, no less.",Bitter Frustration
Is the Intel download expected to be applicable for an AMD CPU?,
"> In any event, you need to conspicuously document the limitations and resrictions and dependencies necessary to use bun. Else you are hiding your flaws, in plain sight, no less.
I imagine you read the top of the README carefully and noticed this line: `bun is experimental software.` That means that, bun still being in beta, there can be (and you are to expect) issues that are not discovered yet, not documented and/or not patched yet. You can't expect `bun` to be as stable as `node`. Doc updates needs someone to fork the repo, commit changes, create a PR and wait for it to be reviewed and merged, so this takes time. Bun is relatively new and the v0.1.0 has been released 4 days ago.
> Is the Intel download expected to be applicable for an AMD CPU?
No. At least not officially.
That might be the problem. Look up if there is an emulator for AMD CPUs. If so, you can apply the same workaround. I won't be of any help since all my computers are running on Intel CPUs.",Impatience
"This issue is cause by an instruction set that is not present on older CPUs. I believe that the contributors are all running on modern CPUs, so they didn't run into this issue before the release of v.0.1.0.",
"This issue was created for tracking progress
You can use [troubleshooting instructions](https://github.com/Jarred-Sumner/bun#illegal-instruction-core-dumped) to emulate avx2 using Intel SDE, if you encounter any issues please use [discussions](https://github.com/Jarred-Sumner/bun/discussions) or join our [discord](https://bun.sh/discord) for support",
"Done, see latest release *-baseline builds.",
Can you replicate that by using an up to date version of WorldEdit and Paper?,Impatience
"Hello,
setted up a fresh and clean paper with word edit and premium version of plot squared.",
"Problem persits.
https://athion.net/ISPaster/paste/view/4127043609944036bd62e782e5514125
Any Idea?
<blockquote><div><strong><a href=""https://athion.net/ISPaster/paste/view/4127043609944036bd62e782e5514125"">Incendo Paste Viewer</a></strong></div></blockquote>
//EDIT
I setup a complete clean and new dedicated server running debian 11 bullseye on it. Downloaded latest Paper from Gut, installed the needed plugins (plotsquared and worldedit) and retry to delete a plot that was claimed a few seconds ahead. Same issue!
https://athion.net/ISPaster/paste/view/8b3197c2d8b14cf59be4ac5029ceb7fa
@NotMyFault - in my opinion you didn't test it if you say ""cannot replicate"" ....
```
root@gs02:/home/newmc# java --version
openjdk 16.0.2 2021-07-20
OpenJDK Runtime Environment Temurin-16.0.2+7 (build 16.0.2+7)
OpenJDK 64-Bit Server VM Temurin-16.0.2+7 (build 16.0.2+7, mixed mode, sharing)
```
```
root@gs02:/home/newmc# java -Xms4096M -Xmx4096M -jar ./paper-1.17.1-388.jar
System Info: Java 16 (OpenJDK 64-Bit Server VM 16.0.2+7) Host: Linux 5.10.0-9-amd64 (amd64)
Loading libraries, please wait...
[07:36:59 INFO]: Environment: authHost='https://authserver.mojang.com', accountsHost='https://api.mojang.com', sessionHost='https://sessionserver.mojang.com', servicesHost='https://api.minecraftservices.com', name='PROD'
[07:36:59 INFO]: Found new data pack file/bukkit, loading it automatically
[07:36:59 INFO]: Reloading ResourceManager: Default, bukkit
[07:37:00 INFO]: Loaded 7 recipes
[07:37:01 INFO]: Starting minecraft server version 1.17.1
[07:37:01 INFO]: Loading properties
[07:37:01 INFO]: This server is running Paper version git-Paper-388 (MC: 1.17.1) (Implementing API version 1.17.1-R0.1-SNAPSHOT) (Git: 22aaf91)
```",Bitter Frustration
"we have the same problem on a Paper 1.18.1 Server FAWE Worldedit is the newest version 68 and Plotsquared is 6.3.0
After claiming a plot ""/plot claim"" I got teleported to it.
If I enter plot and type ""/plot delete"" nothing is happend. After typing it again the message ""A set block timer is bound to either the current plot or you. Please wait for it to finish."" appeared.
If the world is generated as a flat/normal world, this error didn't occurs.",Bitter Frustration
No Answer to Fix this Problem?????,Bitter Frustration
please take care of this bug. we too have the exact same problem.,Impatience
See the date of creation - the publisher isn't interested in fixing this issue.,Bitter Frustration
"If this is the case, we will no longer use Plotquated and will promote it, on the contrary, we will include plotsquaren in the list of the worst plugins and warn users not to spend money on it",Bitter Frustration
"We also changed the plugin and will retrieve our money from visa. For a payed plugin, it's the worst support I've ever seen.",Bitter Frustration
"I was not able to replicate your issue with the steps provided on the environments provided. And apparently only one other person ran into it but provided no clear instructions how to replicate it.
> See the date of creation - the publisher isn't interested in fixing this issue.
I'm unsure where we ever stated that we will no longer look into the issue? The issue has been reported ~1 month-ish ago and is no show stopper.
General rule of thumb is, that is this an open source project, where everyone is welcome to contribute features, fixes or documentation (like in the nature of any other open source project).
You are more than welcome to look into it yourself and provide a solution that applies to a reproducible list of steps and help to fix your issue.
This is not one of our daytime jobs and we have no obligation to deliver a fix in any specific time frame. We, and any of our other our other contributors, look into issues when and if we have the time to.
Many of us are working full time, are engaged with major exams or are involved in other time consuming projects.
PlotSquared is not the center of the universe everything else wraps around.
> For a payed plugin, it's the worst support I've ever seen.
You bought the plugin to support the people behind it. It's not a support contract with any obligation to deliver a solution to your problem in a specific time frame. Yet again, you are more than welcome to look into itself and help resolving the issue, like in the nature of open source projects, instead of complaining and not providing anything new that helps to resolve the issue.",Insulting
We've already found an alternative to Plotsquared because we need a working server and can't wait forever for bug fixes we're sorry that we can't support you any further but that's life for the better wins,Bitter Frustration
"`Many of us are working full time, are engaged with major exams or are involved in other time consuming projects.
PlotSquared is not the center of the universe everything else wraps around.`
Feel this. Thanks for your reply. For me, the issue is solved :)",Bitter Frustration
"Server Implementation
Paper
Server Version
Paper 1.18.1 163
Java Versions (tested)
openjdk-17-jre:
Installiert: 17.0.1+12-1+deb11u2
Installationskandidat: 17.0.1+12-1+deb11u2
Versionstabelle:
*** 17.0.1+12-1+deb11u2 500
500 http://security.debian.org/debian-security bullseye-security/main amd64 Packages
100 /var/lib/dpkg/status
17~19-1 500
500 http://deb.debian.org/debian bullseye/main amd64 Packages
openjdk-17-jdk:
Installiert: 17.0.1+12-1+deb11u2
Installationskandidat: 17.0.1+12-1+deb11u2
Versionstabelle:
*** 17.0.1+12-1+deb11u2 500
500 http://security.debian.org/debian-security bullseye-security/main amd64 Packages
100 /var/lib/dpkg/status
17~19-1 500
500 http://deb.debian.org/debian bullseye/main amd64 Packages
Describe the bug
After claiming a plot ""/plot claim"" I got teleported to it.
If I enter plot and type ""/plot delete"" nothing is happend. After typing it again the message ""A set block timer is bound to either the current plot or you. Please wait for it to finish."" appeared.
If the world is generated as a flat/normal world, this error didn't occurs.
To Reproduce
Generate a augmented or partial world with predefined/default values
Claim a plot
Get into the claimed plot
Show info of plot and make sure that you're the owner
Try to delete the plot (Nothing will be promted to the console)
Type the delete command once again to the console - error occurs
PlotSquared Version
PlotSquared-6.3.0-Premium
Checklist
[Server thread/INFO]: This server is running Paper version git-Paper-163 (MC: 1.18.1) (Implementing API version 1.18.1-R0.1-SNAPSHOT) (Git: 4533821)
Steps how I create the world:
[14:05:22] [Server thread/INFO]: superkrebs issued server command: /plot setup PlotSquared
[14:05:32] [Server thread/INFO]: superkrebs issued server command: /plot setup partial
[14:05:40] [Server thread/INFO]: superkrebs issued server command: /plot setup worldorder
[14:05:51] [Server thread/INFO]: superkrebs issued server command: /plot setup 0,0
[14:05:56] [Server thread/INFO]: superkrebs issued server command: /plot setup 11,0
[14:06:00] [Server thread/INFO]: superkrebs issued server command: /plot setup ALL
[14:06:04] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[14:06:06] [Server thread/INFO]: superkrebs issued server command: /plot setup 42
[14:06:14] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[14:06:19] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[14:06:25] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:grass_block
[14:06:30] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone_slab
[14:06:36] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:sandstone_slab
[14:06:40] [Server thread/INFO]: superkrebs issued server command: /plot setup 7
[14:06:43] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[14:06:50] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:quartz_block
[14:06:58] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[14:07:02] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[14:07:06] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[14:07:11] [Server thread/INFO]: superkrebs issued server command: /plot setup citybuild3
<blockquote><div><strong><a href=""http://security.debian.org/debian-security"">Index of /debian-security</a></strong></div></blockquote>
<blockquote><div><strong><a href=""http://deb.debian.org/debian"">Index of /debian</a></strong></div></blockquote>",
pleace fix this,Impatience
"it would be possible to give a short feedback when this error will be tackled because we would have to look around for an alternative. I know you have little time and understand that. A bit of info avoids misunderstandings.
We were able to replicate the error several times with a newly installed system.",Impatience
"> See the date of creation - the publisher isn't interested in fixing this issue.
I unfortunately have to agree with that. if you don't even have 2 minutes for a feedback, you should leave it alone. felt no sow cares about this bug. I'm very disappointed, I would have expected at least a feedback like ""yes, it's our turn and we'll take care of it shortly"" for this problem",Bitter Frustration
"no answer, no reaction, what kind of support is that",Bitter Frustration
"""yes it's our turn and we will look into and maybe implement a fix if and when we have time, since we're all volunteering our free time for this project. Personally, I am in the middle of University exams, which I feel are more important than responding to a GitHub issue to state that we have received an issue and saying nothing that's ultimately going to be much use since we cannot just drop everything we do and are doing just to fix a bug for you, personally.""",Bitter Frustration
"then you shouldn't ask for money for this plugin if you can't take care of it properly.
Your reaction does not speak for buying this plugin if the reaction is so snotty.
Then do what you want, we will look for an alternative so you can scare off your users.
In addition, I'm not the only one who has the problem, be happy if you are made aware of mistakes instead of giving such outrageous answers.
Plotsquared is done for me, have a nice life.",Bitter Frustration
Ok,
"As I'm able to replicate the issue using the steps from the original post, I'll mark the issue as a valid bug. ![grafik](https://user-images.githubusercontent.com/27054324/151204917-69c837e2-71d0-47b1-a32f-40f295128d2c.png)",
"### Server Implementation
Spigot
### Server Version
1.17.1
### Java Versions (tested)
Java 16.0.1 Zulu
Java 16.0.2+7 Temurin
### Describe the bug
After claiming a plot ""/plot claim"" I got teleported to it.
If I enter plot and type ""/plot delete"" nothing is happend. After typing it again the message ""A set block timer is bound to either the current plot or you. Please wait for it to finish."" appeared.
**If the world is generated as a flat/normal world, this error didn't occurs.**
### To Reproduce
1. Generate a augmented or partial world with predefined/default values
2. Claim a plot
3. Get into the claimed plot
4. Show info of plot and make sure that you're the owner
5. Try to delete the plot (Nothing will be promted to the console)
6. Type the delete command once again to the console - error occurs
### Expected behaviour
The plot have to be deleted.
### Screenshots / Videos
_No response_
### Error log (if applicable)
_No response_
### Plot Debugpaste
Java 16.0.1 Zulu
https://athion.net/ISPaster/paste/view/e39d51e936ea41cd9f69da70993e0137
Java 16.0.2+7 Temurin
https://athion.net/ISPaster/paste/view/0b2ba302aea94f4dbb3adde606aa8d59
### PlotSquared Version
PlotSquared-6.1.4-Premium
### Checklist
- [X] I have included a Plot debugpaste.
- [X] I am using the newest build from https://www.spigotmc.org/resources/77506/ and the issue still persists.
### Anything else?
I'm using a proxmox hypervisor with Debian 11 running a LXC Container with nesting activated on it.
[Server thread/INFO]: This server is running CraftBukkit version 3284-Spigot-3892929-0ebef35 (MC: 1.17.1) (Implementing API version 1.17.1-R0.1-SNAPSHOT)
Steps how I create the world:
[00:05:22] [Server thread/INFO]: superkrebs issued server command: /plot setup PlotSquared
[00:05:32] [Server thread/INFO]: superkrebs issued server command: /plot setup partial
[00:05:40] [Server thread/INFO]: superkrebs issued server command: /plot setup worldorder
[00:05:51] [Server thread/INFO]: superkrebs issued server command: /plot setup 0,0
[00:05:56] [Server thread/INFO]: superkrebs issued server command: /plot setup 11,0
[00:06:00] [Server thread/INFO]: superkrebs issued server command: /plot setup ALL
[00:06:04] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[00:06:06] [Server thread/INFO]: superkrebs issued server command: /plot setup 42
[00:06:14] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[00:06:19] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[00:06:25] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:grass_block
[00:06:30] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone_slab
[00:06:36] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:sandstone_slab
[00:06:40] [Server thread/INFO]: superkrebs issued server command: /plot setup 7
[00:06:43] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[00:06:50] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:quartz_block
[00:06:58] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[00:07:02] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[00:07:06] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[00:07:11] [Server thread/INFO]: superkrebs issued server command: /plot setup wolrd139",
"i dont like, to make a new issue open. well, i agree but i would like to see a Duckstation on Nintendo Switch :)
THX",
"Yeah, It will be a nice addition to Retroarch as it allows us to emulate **Nintendo Switch Games**. > ### Yuzu Emulator Core Request
> [So, i thinking a new core as Nintendo Switch called Yuzu, so what do you think?]
> > Note: this iis an request, if u add it its a pleasureâ™¡â™¡",
"RETROARCH DEVELOPERS.
NOT ADD ANY CORE IF IS EMULATING CURRENT VIDEO GAME GENERATION.
EMULATING CURRENT GENERATION CREATE PROBLEMS FOR HONEST PROJECTS ... MAINLY MAME PROJECT.",Bitter Frustration
"> RETROARCH DEVELOPERS.
> NOT ADD ANY CORE IF IS EMULATING CURRENT VIDEO GAME GENERATION.
> EMULATING CURRENT GENERATION CREATE PROBLEMS FOR HONEST PROJECTS ... MAINLY MAME PROJECT.
I agree with you , even though the addition of this core will be appreciated, since it includes support for current generation Nintendo Switch console it would create problems and legal complications for retroarch.",Bitter Frustration
Retroarch need in controller add option to assign same console controller button to multiple game controller thus creating combo buttons helping NES and other games need press 2 buttons at same time and also allow turbo button for any console button.,
"Yuzu is still pretty young, and while it quite possibly could be possible to create a libretro port, it may distract from getting Yuzu stable in the first place. I would recommend supporting Yuzu over at https://yuzu-emu.org , and joining their Patreon, in order to help get it to a solid and stable state.
If anyone wants to have a hack at the libretro port in the mean time, don't let this issue stop you.",
"EMULATE CURRENT GENERATION IS WRONG !!!
CREATE PROBLEMS FOR OTHERS EMULATOR PROJECTS !
UNHAPPILY HAVE USERS WAITING PLAY CURRENT GENERATION AND WISH UNDERSTAND IF WILL CREATE PROBLEM NINTENDO HAVE WAKE UP ABOUT IT ... THEY MAYBE CONTACT POLITICS CREATING LAWS AGAINST ANY EMULATOR PROJECT.",Bitter Frustration
"### Yuzu Emulator Core Request
[So, i thinking a new core as Nintendo Switch called Yuzu, so what do you think?]
Note: this iis an request, if u add it its a pleasureâ™¡â™¡",
"I was curious about yuzu core and found a guy called Augusto screaming at my face.
I love internet.
by the way Augusto, emulation it's not and it'll never be illegal. you just need to own the games you play.",Mocking
"From a legal standpoint, it doesn't matter if you're emulating the current generation. It all depends on whether you're following the rules with regard to copyright, and it doesn't matter what generation is the target hardware. However, from a rights holding company's POV, it probably does matter to them, since they're looking to make more money off the current market.
Like people have said, it's definitely legal, as they have already tried to shut down emulation before through lawsuits, so it's been fully confirmed by the US courts that emulation is legal. I doubt they'll test that again since the industry has been against emulation forever, and they haven't really done much about it and all that lobbying politicians hasn't amounted to much as of yet. I could be wrong, but there's no reason to believe things will change without some solid evidence. Please consider including yuzu since it is fairly stable, as far as I can tell. It would be nice to have it as a RetroArch core.",
"Oh heavens ...
From an gamer only waiting play emulate current system is ""correct"" , but from an viewpoint of an professional game developer and emulator coder of mature projects emulate current system is totally wrong even if the law allow to do it.
Not is impossible Nintendo and others companies having contact with politics and changing the laws at point creating problems for other mature projects. Only reading an reply from an mamedev saying to stop wit it is enough to understand about it.
Unhappily an high percentage of PC gamers have done terrible actions demanding emulate and dump current comercial games ... all to avoid buy an console or even an recent released game doing problems from all sides.
I not believe here several gamers will reply with good sense saying to not add cores to current systems.
That's the because Sony and Nintendo add security protection in console OS.
Retroarch devs NOT add cores of current systems ... THAT IS AN OF MORE WORST PROBLEMS BEING DONE AGAINST MATURE PROJECTS ( MAME ).
If Retroarch continue using cores from before PS3 is all right and not any problem.
Now for example ... if is released an PS5 emulator and each PC have access to PS5 discs and that emulator is being extremely downloaded you believe Sony not will try to do the correct action ?
In past was tried stop some emulators (Connectix Virtual Game Station and others) ...
Today not is the same thing.
If Sony , Nintendo and any other company join to say about that problem to politics will happen problems.
Stop trying to do it. Buy the console or wait to next generation before demanding an emulator for current generation.",Bitter Frustration
"Is this specific to `SGDClassifier`? Or can you reproduce the problem with other classifiers with different solvers such as `LogisticRegression`?
Can you please craft a minimal reproducer with a synthetic dataset so that we can reproduce the problem without any ambiguity? Please also include the expected and observed results in your report.",Impatience
"Hello, I would like to work on this. As ogrisel mentioned, can you provide the synthetic dateset to reproduce the problem ?",
"Hello @ogrisel , @Nivi09 - thank you for your answers.
I do not have any synthetic data, I use a real dataset that I cannot really share.
But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.
It does not have to be only eg with categorical; it can be 9 numerical features and 1 categorical feature.
Especially if the categorical feature has some importance (although the same may happen if it does not; I have not tested that) and in your test set one observation has one unseen value for the categorical feature then the SGD results returns essentially random predictions (or at least not directly related).
It may be that scikitLearn implementation is fine but just SGD itself has this weakness in general?
I think though that RandomForestClassifier does not have the same problem in these cases.
I have not tested much the LogisticRegression.
even like that:
logistic_regression = LogisticRegression(penalty='elasticnet', tol=10, C=1.0, class_weight='balanced', random_state=0, solver='saga', max_iter=20, n_jobs=os.cpu_count()-1, l1_ratio=0.7)
it takes too much time to converge",Bitter Frustration
"Hi @GeorgeZan , let me rephrase the issue that you have described and please correct me if I have not understood it correctly.
The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ?",
"> The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ?
In general, any classifier can never predict a class that was not part of the training set. I am not sure what ""break down"" means in this context.
> But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.
Can you please craft a standalone reproducible code snipet example that generates its own toy dataset so that we can just execute ? I am afraid the English language is ambiguous while Python is not.
Please also report the result you observe and the output you would have expected.",Impatience
"Hi @ogrisel, GeorgeZan has mentioned that - SGD results returns essentially random predictions (or at least not directly related).
I think this is what break down means. Please correct me if my understanding is not correct @GeorgeZan ? And as ogrisel mentioned, my understanding is also the same that no classifier can predict a class that was not a part of training set. I read GeorgeZan comment again, so here is my second understanding : input set X has 10 independent variables in which one variable is categorical. Along with that you have categorical dependent variable. Your test fails when there is a class in categorical I.V. which has not been seen in train data set while training and that's where you get random prediction. @GeorgeZan, please let me know, out of two examples, which one is correct.",
"@Nivi09, yes indeed.
""Hi @ogrisel, GeorgeZan has mentioned that - SGD results returns essentially random predictions (or at least not directly related).
I think this is what break down means.""
To give a hypothetical example, let's say I have a model with 9 numerical features and a categorical feature and let's say there are 3 classes in my problem (A, B, C class).
Let's also assume that I have a test set instance X that belongs to the A class.
Specifically, it belongs to class A regardless of the value of the categorical feature.
At best, the categorical feature can help the model to predict class A for X just by inference but it is not a defining feature of X.
i)
If I keep the categorical feature in the model and the test set instance X provided has a known/seen value at the categorical feature then the model returns the following prediction:
A: 0.7
C: 0.2
B: 0.1
iI)
If I drop the categorical feature from the model then the model returns the following prediction:
A: 0.55
C: 0.35
B: 0.1
iii)
If I keep the categorical feature in the model and the test set instance provided has a unknown/unseen value at the categorical feature then the model returns the following prediction:
B: 0.5
A: 0.3
C: 0.2
.........................
Obviously the impact of the categorical features in all these cases depends on how important is the feature for the model; in the example above it is assumed that the categorical feature has quite significant weight.
This is also why in (ii) the top score drops by quite a lot in comparison with (i) given that it is only one feature.
However, and this is what I mean that the model ""breaks down"", in (iii) it returns essentially random predictions.
It does not make sense to me that if you just receive an unseen value your prediction changes so so much.
I would expect (iii) to be more like (ii).
The even more interesting and problematic thing is that the same thing happens as mentioned at my original post if you have eg 100k tf-idf sparse features and just one categorical at the same model and if you put an unseen value at the categorical at the test set then SGD ""breaks down"" as in (iii).
In the dataset that I experimented the categorical feature is quite significant (eg dropping it reduced the overall accuracy of the model by 5 percentage points; eg accuracy drops from 80% to 75%) but I would not exclude based on how unstable SGD is that the same thing would happen even if the feature is not that significant.
.........................
@ogrisel I think now it must be quite clear.
As I said, you can pick any dataset on the Internet which has also categoricals or you can even create a very basic one on Excel and I think you will observe what I mean.",Bitter Frustration
"Hello @GeorgeZan , for the case 3 as you have mentioned:
If I keep the categorical feature in the model and the test set instance provided has a unknown/unseen value at the categorical feature then the model returns the following prediction:
B: 0.5
A: 0.3
C: 0.2
So, to which class do you expect this example to belong to ? Because I'm confused here when you mentioned - - ' (iii) it returns essentially random predictions.
It does not make sense to me that if you just receive an unseen value your prediction changes so so much.
I would expect (iii) to be more like (ii).'",Bitter Frustration
"@Nivi09 ah sorry, just edited my post to add ""Let's also assume that I have a test set instance that belongs to the A class."".
Although it is relative obvious what I mean.
My point is that if you have a known value at the categorical feature or a if you entirely drop the categorical feature then the model returns the right prediction (although with slightly different ""confidence"").
But when you put an unseen value at the categorical feature then the model returns random results although given (ii) essentially the categorical feature does not play much role in the prediction so (iii) should have been returning something more like (ii).",Bitter Frustration
"I don't think that there is anything wrong but I would not be able to be sure until we have an example showing the wrong behaviour and the expected behaviour.
Since `SGDClassifier` is a linear model, when it comes to categorical data, the type of encoding is rather important. So did you check the coefficients of your model? Did you use an `OneHotEncoder` for encoding your categories? Basically, the value of the ""unseen categories"" will have an impact depending on the coefficient of the associated variable.",
"@glemaitre as I mention at my post, I put the categorical value in the text and then apply tf-idf on it.
In this sense you could say that I do one hot encoding on it (?).
I have not really used the categorical variable separately but when I did was with label encoder.
Even though not ideal because ""LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat."", still it should not break down so much with unseen values.
PS
""I don't think that there is anything wrong ""
Wrong or not, the question is why this happens like that with SGD and unseen values.",Bitter Frustration
"> Even though not ideal because ""LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat."", still it should not break down so much with unseen values.
This encoding is thus ordinal. Now if your unseen value is mapped to 100 then it will induce 100 x coef on the output and thus it would go sideways.",
"@glemaitre First of all, as I said (not sure if you read carefully) I primarily use the TF-IDF and the problem occurs there.
Any thoughts on these or we entirely skipped my main question haha?
Secondly, not sure why it would go sideways like that ""Now if your unseen value is mapped to 100 then it will induce 100 x coef on the output and thus it would go sideways.""
Let's say that I do not have an unseen value but a seen one and specifically the one mapped to 99 which would induce a 99x coef then why this would not go equally sideways and the 100 would?",Impatience
"> Let's say that I do not have an unseen value but a seen one and specifically the one mapped to 99 which would induce a 99x coef then why this would not go equally sideways and the 100 would?
The thing that goes sideways here would be to use some ordered pattern for a feature that does not have any and where an increment/decrement of the feature value induces an increment/decrement of `coef` on the target. So my example was just to mention that if your mapped unseen values are mapped to larger values then it would even have a larger impact on the target.
> not sure if you read carefully
I think I did but I don't think that you did. As mentioned by @ogrisel or myself, it would be much easier to understand the problem and if there is an issue by having a concrete Python piece of code. Up to now, I am still not sure what is the problem that you are trying to express. You are referring to unseen values. However, an unseen term not part of the `TfidfVectorizer` dictionary will not be used. So more generally because you were referring to categories, I tried to understand what encoding you are actually using because a linear model it will have a different impact on the decision.
So please provide a Python code snippet that we can run and discuss concretely about the issue that you observe.",Impatience
"@glemaitre ""I think I did but I don't think that you did. ""
I have answered this my boy since day 1 so you have not read again:
https://github.com/scikit-learn/scikit-learn/issues/21906#issuecomment-988073948
""I do not have any synthetic data, I use a real dataset that I cannot really share.
But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.""
So do not try to be too smart now.
But anyway if nobody can see what could be the problem without a dataset then I may try to construct something.
""So my example was just to mention that if your mapped unseen values are mapped to larger values then it would even have a larger impact on the target.""
yeah but then the unseen values would experience the ""same"" ""jump"" as the 99 class and they do not.
The behaviour with unseen values is totally different and anyway as I said I do not even mostly encode the categorical but use it in the TF-IDF.",Bitter Frustration
"@GeorgeZan I don't think we can suggest better here. It does not seem to be a software issue we can fix. I invite you to share a code snippet we can just run so we can maybe suggest some practical tricks to make good use of SGDClassifier.
feel free to reopen this issue with a code snippet.",
"> I have answered this my boy since day 1
So do not try to be too smart now.
@GeorgeZan please try to stay respectful in the discussions. I know that trying to explain something and be understood can be hard to achieve in online discussions but people are just trying to get a clear vision of the problem you describe and these kind of passive-agressive comments do not help. Please respect the [code of conduct](https://github.com/scikit-learn/scikit-learn/blob/main/CODE_OF_CONDUCT.md).",
"@jeremiedbb I agree about being respectful but then comments like that which preceded mine are equally problematic (""I think I did but I don't think that you did. "").",Bitter Frustration
"That was preceded by:
> First of all, as I said (not sure if you read carefully)
Be aware that reading this thread took me probably over an hour from the time you opened this issue. So firing such comments when trying to help tend to tease me somehow.",Bitter Frustration
"I don't this discussion is going in the right direction :)
I'm locking the conversation now. @GeorgeZan feel free to open a new issue if you manage to come up with a reproducible example with clear expected and not expected results.",
"### Describe the bug
https://stackoverflow.com/questions/70253946/sgd-breaks-down-when-encountering-unseen-values
### Steps/Code to Reproduce
-
### Expected Results
-
### Actual Results
-
### Versions
-",
"### Env info
```
$ sw_vers
ProductName:	Mac OS X
ProductVersion:	10.14.6
BuildVersion:	18G103
$ clang --version Apple clang version 11.0.0 (clang-1100.0.33.17)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
$ ls -l /usr/local/lib | grep glog
-rw-r--r-- 1 neo admin 391864 Dec 9 15:02 libglog.a
$ mkdir -p /tmp/t && cp /usr/local/lib/libglog.a /tmp/t && ar x /tmp/t/libglog.a
$ ls -1 /tmp/t/*.o | xargs nm | grep MakeCheckOpValueString | cut -d' ' -f2- | sort -u
T __ZN6google22MakeCheckOpValueStringIaEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIcEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIhEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIiEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
$ ls -1 /tmp/t/*.o | xargs nm | grep MakeCheckOpValueString | cut -d' ' -f3 | sort -u | c++filt
void google::MakeCheckOpValueString<signed char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, signed char const&)
void google::MakeCheckOpValueString<char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, char const&)
void google::MakeCheckOpValueString<unsigned char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, unsigned char const&)
void google::MakeCheckOpValueString<int>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, int const&)
# Build error msg
Undefined symbols for architecture x86_64:
""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in foo.cpp.o
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in bar.cpp.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```",
"> My simple program links to the glog library
You need to provide a minimal example.",Impatience
"### `sample.cpp`
```c++
#include <cstdio>
#include <cstdlib>
#include <glog/logging.h>
int main(void) {
char *s = (char *) malloc(100);
CHECK_NE(s, nullptr); // not ok
//CHECK_NOTNULL(s); // ok
free(s);
return 0;
}
```
### Compile
<details>
```console
$ g++ sample.cpp -lglog -Wall -Wextra -g -DDEBUG -O0
In file included from sample.cpp:3:
/usr/local/include/glog/logging.h:741:9: error: use of overloaded operator '<<' is ambiguous (with operand types 'std::ostream'
(aka 'basic_ostream<char>') and 'const std::__1::nullptr_t')
(*os) << v;
~~~~~ ^ ~
/usr/local/include/glog/logging.h:804:3: note: in instantiation of function template specialization
'google::MakeCheckOpValueString<std::__1::nullptr_t>' requested here
MakeCheckOpValueString(comb.ForVar2(), v2);
^
/usr/local/include/glog/logging.h:828:1: note: in instantiation of function template specialization
'google::MakeCheckOpString<char *, std::__1::nullptr_t>' requested here
DEFINE_CHECK_OP_IMPL(Check_NE, !=) // Use CHECK(x == NULL) instead.
^
/usr/local/include/glog/logging.h:817:17: note: expanded from macro 'DEFINE_CHECK_OP_IMPL'
else return MakeCheckOpString(v1, v2, exprtext); \
^
sample.cpp:7:5: note: in instantiation of function template specialization 'google::Check_NEImpl<char *, std::__1::nullptr_t>'
requested here
CHECK_NE(s, nullptr); // not ok
^
/usr/local/include/glog/logging.h:899:30: note: expanded from macro 'CHECK_NE'
#define CHECK_NE(val1, val2) CHECK_OP(_NE, !=, val1, val2)
^
/usr/local/include/glog/logging.h:874:3: note: expanded from macro 'CHECK_OP'
CHECK_OP_LOG(name, op, val1, val2, google::LogMessageFatal)
^
/usr/local/include/glog/logging.h:854:18: note: expanded from macro 'CHECK_OP_LOG'
google::Check##name##Impl( \
^
<scratch space>:51:1: note: expanded from here
Check_NEImpl
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:195:20: note: candidate function
basic_ostream& operator<<(basic_ostream& (*__pf)(basic_ostream&))
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:199:20: note: candidate function
basic_ostream& operator<<(basic_ios<char_type, traits_type>&
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:204:20: note: candidate function
basic_ostream& operator<<(ios_base& (*__pf)(ios_base&))
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:207:20: note: candidate function
basic_ostream& operator<<(bool __n);
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:219:20: note: candidate function
basic_ostream& operator<<(const void* __p);
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:220:20: note: candidate function
basic_ostream& operator<<(basic_streambuf<char_type, traits_type>* __sb);
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:863:1: note: candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const char* __str)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:817:1: note: candidate function [with _CharT = char, _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<_CharT, _Traits>& __os, const char* __strn)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:870:1: note: candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const signed char* __str)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:878:1: note: candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const unsigned char* __str)
^
1 error generated.
```
</details>",
"If you compile the project using CMake, you should also use CMake to link against glog. There's a [dedicated section](https://github.com/google/glog#consuming-glog-in-a-cmake-project) in the readme how to achieve this.
In case you cannot use CMake to consume glog, it is your responsibility to reconstruct the CMake configuration (flags, compiler and linker options, etc.) because simply linking against glog is generally not sufficient.",
"It seems that the problem is not caused by CMake and make.
I do the following patch to my program and minimal reproducible program, they worked.
```diff
- CHECK_NE(s, nullptr);
+ CHECK_NOTNULL(s);
```
FYI: My project uses CMake to link again glog, the minimal sample use make to link against glog.
The error is much self-explanatory: `error: use of overloaded operator '<<' is ambiguous (with operand types 'std::ostream'`",
"> My project uses CMake to link again glog
This was not evident from your description. Make sure to provide a complete minimal example without cherry picking parts that you believe to be relevant.
According to the linker error, there seems to be an ABI mismatch. If you use C++11 features you need to compile glog using the `-std=c++11` flag as well (e.g., by passing -DCMAKE_CXX_STANDARD=11 when configuring glog using CMake).
> The error is much self-explanatory: error: use of overloaded operator '<<' is ambiguous (with operand types 'std::ostream')
You initially reported a linker error. Now it's compiler error. Which is it now?",Impatience
"> According to the linker error, there seems to be an ABI mismatch. If you use C++11 features you need to compile glog using the -std=c++11 flag as well (e.g., by passing -DCMAKE_CXX_STANDARD=11 when configuring glog using CMake).
Excellent observation.
### Recompile with `-std=c++11`
```bash
$ g++ sample.cpp -std=c++11 -lglog -Wall -Wextra -g -DDEBUG -O0
Undefined symbols for architecture x86_64:
""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<char*, std::nullptr_t>(char* const&, std::nullptr_t const&, char const*) in sample-5ad0b1.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```",
"> Excellent observation.
Given your attitude, it looks like you're not interested in providing a full minimal example including a corresponding `CMakeLists.txt`. Therefore I'm closing this issue.",Bitter Frustration
"My simple program links to the `glog` library, but it seems cannot find some symbols:
```
...
Undefined symbols for architecture x86_64:
""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in foo.cpp.o
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in bar.cpp.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```
### HOW I compile glog
<details>
```bash
git clone --depth=1 https://github.com/google/glog.git
pushd glog
cmake -S . -B build -DBUILD_SHARED_LIBS=OFF
cmake --build build --target install
popd
```
```
Cloning into 'glog'...
remote: Enumerating objects: 97, done.
remote: Counting objects: 100% (97/97), done.
remote: Compressing objects: 100% (86/86), done.
remote: Total 97 (delta 25), reused 38 (delta 6), pack-reused 0
Unpacking objects: 100% (97/97), done.
-- The CXX compiler identification is AppleClang 11.0.0.11000033
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Could NOT find GTest (missing: GTest_DIR)
CMake Warning at CMakeLists.txt:90 (find_package):
By not providing ""Findgflags.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""gflags"", but
CMake did not find one.
Could not find a package configuration file provided by ""gflags"" (requested
version 2.2.0) with any of the following names:
gflagsConfig.cmake
gflags-config.cmake
Add the installation prefix of ""gflags"" to CMAKE_PREFIX_PATH or set
""gflags_DIR"" to a directory containing one of the above files. If ""gflags""
provides a separate development package or SDK, be sure it has been
installed.
-- Looking for C++ include pthread.h
-- Looking for C++ include pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE -- Could NOT find Unwind (missing: Unwind_LIBRARY Unwind_PLATFORM_LIBRARY) -- Looking for C++ include unwind.h
-- Looking for C++ include unwind.h - found
-- Looking for C++ include dlfcn.h
-- Looking for C++ include dlfcn.h - found
-- Looking for C++ include execinfo.h
-- Looking for C++ include execinfo.h - found
-- Looking for C++ include glob.h
-- Looking for C++ include glob.h - found
-- Looking for C++ include inttypes.h
-- Looking for C++ include inttypes.h - found
-- Looking for C++ include memory.h
-- Looking for C++ include memory.h - found
-- Looking for C++ include pwd.h
-- Looking for C++ include pwd.h - found
-- Looking for C++ include stdint.h
-- Looking for C++ include stdint.h - found
-- Looking for C++ include strings.h
-- Looking for C++ include strings.h - found
-- Looking for C++ include sys/stat.h
-- Looking for C++ include sys/stat.h - found
-- Looking for C++ include sys/syscall.h
-- Looking for C++ include sys/syscall.h - found
-- Looking for C++ include sys/time.h
-- Looking for C++ include sys/time.h - found
-- Looking for C++ include sys/types.h
-- Looking for C++ include sys/types.h - found
-- Looking for C++ include sys/utsname.h
-- Looking for C++ include sys/utsname.h - found
-- Looking for C++ include sys/wait.h
-- Looking for C++ include sys/wait.h - found
-- Looking for C++ include syscall.h
-- Looking for C++ include syscall.h - not found
-- Looking for C++ include syslog.h
-- Looking for C++ include syslog.h - found
-- Looking for C++ include ucontext.h
-- Looking for C++ include ucontext.h - not found
-- Looking for C++ include unistd.h
-- Looking for C++ include unistd.h - found
-- Looking for C++ include ext/hash_map
-- Looking for C++ include ext/hash_map - found
-- Looking for C++ include ext/hash_set
-- Looking for C++ include ext/hash_set - found
-- Looking for C++ include ext/slist
-- Looking for C++ include ext/slist - not found
-- Looking for C++ include tr1/unordered_map
-- Looking for C++ include tr1/unordered_map - not found
-- Looking for C++ include tr1/unordered_set
-- Looking for C++ include tr1/unordered_set - not found
-- Looking for C++ include unordered_map
-- Looking for C++ include unordered_map - found
-- Looking for C++ include unordered_set
-- Looking for C++ include unordered_set - found
-- Looking for C++ include stddef.h
-- Looking for C++ include stddef.h - found
-- Check size of unsigned __int16
-- Check size of unsigned __int16 - failed
-- Check size of u_int16_t
-- Check size of u_int16_t - done
-- Check size of uint16_t
-- Check size of uint16_t - done
-- Looking for dladdr
-- Looking for dladdr - found
-- Looking for fcntl
-- Looking for fcntl - found
-- Looking for pread
-- Looking for pread - found
-- Looking for pwrite
-- Looking for pwrite - found
-- Looking for sigaction
-- Looking for sigaction - found
-- Looking for sigaltstack
-- Looking for sigaltstack - found
-- Performing Test HAVE_NO_DEPRECATED
-- Performing Test HAVE_NO_DEPRECATED - Success
-- Performing Test HAVE_NO_UNNAMED_TYPE_TEMPLATE_ARGS
-- Performing Test HAVE_NO_UNNAMED_TYPE_TEMPLATE_ARGS - Success
-- Looking for pthread_threadid_np
-- Looking for pthread_threadid_np - found
-- Looking for snprintf
-- Looking for snprintf - found
-- Looking for UnDecorateSymbolName in dbghelp
-- Looking for UnDecorateSymbolName in dbghelp - not found
-- Performing Test HAVE___ATTRIBUTE__
-- Performing Test HAVE___ATTRIBUTE__ - Success
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_DEFAULT
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_DEFAULT - Success
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_HIDDEN
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_HIDDEN - Success
-- Performing Test HAVE___BUILTIN_EXPECT
-- Performing Test HAVE___BUILTIN_EXPECT - Success
-- Performing Test HAVE___SYNC_VAL_COMPARE_AND_SWAP
-- Performing Test HAVE___SYNC_VAL_COMPARE_AND_SWAP - Success
-- Performing Test HAVE_RWLOCK
-- Performing Test HAVE_RWLOCK - Success
-- Performing Test HAVE___DECLSPEC
-- Performing Test HAVE___DECLSPEC - Failed
-- Performing Test STL_NO_NAMESPACE
-- Performing Test STL_NO_NAMESPACE - Failed
-- Performing Test STL_STD_NAMESPACE
-- Performing Test STL_STD_NAMESPACE - Success
-- Performing Test HAVE_USING_OPERATOR
-- Performing Test HAVE_USING_OPERATOR - Success
-- Performing Test HAVE_NAMESPACES
-- Performing Test HAVE_NAMESPACES - Success
-- Performing Test HAVE_GCC_TLS
-- Performing Test HAVE_GCC_TLS - Success
-- Performing Test HAVE_MSVC_TLS
-- Performing Test HAVE_MSVC_TLS - Failed
-- Performing Test HAVE_CXX11_TLS
-- Performing Test HAVE_CXX11_TLS - Failed
-- Performing Test HAVE_ALIGNED_STORAGE
-- Performing Test HAVE_ALIGNED_STORAGE - Failed
-- Performing Test HAVE_CXX11_ATOMIC
-- Performing Test HAVE_CXX11_ATOMIC - Success
-- Performing Test HAVE_CXX11_CONSTEXPR
-- Performing Test HAVE_CXX11_CONSTEXPR - Failed
-- Performing Test HAVE_CXX11_CHRONO
-- Performing Test HAVE_CXX11_CHRONO - Success
-- Performing Test HAVE_CXX11_NULLPTR_T
-- Performing Test HAVE_CXX11_NULLPTR_T - Success
-- Performing Test HAVE_LOCALTIME_R
-- Performing Test HAVE_LOCALTIME_R - Success
-- Performing Test COMPILER_HAS_HIDDEN_VISIBILITY
-- Performing Test COMPILER_HAS_HIDDEN_VISIBILITY - Success
-- Performing Test COMPILER_HAS_HIDDEN_INLINE_VISIBILITY
-- Performing Test COMPILER_HAS_HIDDEN_INLINE_VISIBILITY - Success
-- Performing Test COMPILER_HAS_DEPRECATED_ATTR
-- Performing Test COMPILER_HAS_DEPRECATED_ATTR - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /Users/neo/work/build/glog/build
[ 3%] Building CXX object CMakeFiles/glogbase.dir/src/demangle.cc.o
[ 7%] Building CXX object CMakeFiles/glogbase.dir/src/logging.cc.o
[ 11%] Building CXX object CMakeFiles/glogbase.dir/src/raw_logging.cc.o
[ 14%] Building CXX object CMakeFiles/glogbase.dir/src/symbolize.cc.o
[ 18%] Building CXX object CMakeFiles/glogbase.dir/src/utilities.cc.o
[ 22%] Building CXX object CMakeFiles/glogbase.dir/src/vlog_is_on.cc.o
[ 25%] Building CXX object CMakeFiles/glogbase.dir/src/signalhandler.cc.o
[ 25%] Built target glogbase
[ 29%] Linking CXX static library libglog.a
[ 29%] Built target glog
[ 33%] Linking CXX static library libglogtest.a
[ 33%] Built target glogtest
[ 37%] Building CXX object CMakeFiles/logging_unittest.dir/src/logging_unittest.cc.o
[ 40%] Linking CXX executable logging_unittest
[ 40%] Built target logging_unittest
[ 44%] Building CXX object CMakeFiles/stl_logging_unittest.dir/src/stl_logging_unittest.cc.o
[ 48%] Linking CXX executable stl_logging_unittest
[ 48%] Built target stl_logging_unittest
[ 51%] Building CXX object CMakeFiles/symbolize_unittest.dir/src/symbolize_unittest.cc.o
[ 55%] Linking CXX executable symbolize_unittest
[ 55%] Built target symbolize_unittest
[ 59%] Building CXX object CMakeFiles/demangle_unittest.dir/src/demangle_unittest.cc.o
[ 62%] Linking CXX executable demangle_unittest
[ 62%] Built target demangle_unittest
[ 66%] Building CXX object CMakeFiles/stacktrace_unittest.dir/src/stacktrace_unittest.cc.o
[ 70%] Linking CXX executable stacktrace_unittest
[ 70%] Built target stacktrace_unittest
[ 74%] Building CXX object CMakeFiles/utilities_unittest.dir/src/utilities_unittest.cc.o
[ 77%] Linking CXX executable utilities_unittest
[ 77%] Built target utilities_unittest
[ 81%] Building CXX object CMakeFiles/signalhandler_unittest.dir/src/signalhandler_unittest.cc.o
[ 85%] Linking CXX executable signalhandler_unittest
[ 85%] Built target signalhandler_unittest
[ 88%] Building CXX object CMakeFiles/cleanup_immediately_unittest.dir/src/cleanup_immediately_unittest.cc.o
[ 92%] Linking CXX executable cleanup_immediately_unittest
[ 92%] Built target cleanup_immediately_unittest
[ 96%] Building CXX object CMakeFiles/cleanup_with_prefix_unittest.dir/src/cleanup_with_prefix_unittest.cc.o
[100%] Linking CXX executable cleanup_with_prefix_unittest
[100%] Built target cleanup_with_prefix_unittest
Install the project...
-- Install configuration: """"
-- Installing: /usr/local/lib/libglog.a
-- Installing: /usr/local/include/glog/export.h
-- Installing: /usr/local/include/glog/logging.h
-- Installing: /usr/local/include/glog/raw_logging.h
-- Installing: /usr/local/include/glog/stl_logging.h
-- Installing: /usr/local/include/glog/vlog_is_on.h
-- Installing: /usr/local/include/glog/log_severity.h
-- Installing: /usr/local/include/glog/platform.h
-- Installing: /usr/local/lib/pkgconfig/libglog.pc
-- Installing: /usr/local/lib/cmake/glog/glog-modules.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-config.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-config-version.cmake
-- Old export file ""/usr/local/lib/cmake/glog/glog-targets.cmake"" will be replaced. Removing files [/usr/local/lib/cmake/glog/glog-targets-noconfig.cmake].
-- Installing: /usr/local/lib/cmake/glog/glog-targets.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-targets-noconfig.cmake
```
</details>",
"I had the same problem. I solved it by removing the following setting from `devcontainer.json`
```
""latex-workshop.viewer.pdf.internal.port"": 35555, ```
However, this setting isn't a default one (I recommended adding it in https://github.com/James-Yu/LaTeX-Workshop/issues/2206#issuecomment-674092455), so maybe you are having a different issue.",
"@CheariX mine is set to the default (0) so this doesn't seem to be the issue but it might be that vscode cannot properly communicate with the pdf viewer.
Due to me being on Fedora Silverblue I am running VSCode as flatpak. Maybe that's part of the issue.",
"We have received responses from several of our service providers. Some of Tailscale's service providers are affected by this vulnerability and are now patched or patching. Those with issues in public services have also put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.
We are still waiting to hear back from a handful of infrastructure service providers.
We will update this issue again when we have further information, which is expected to be on 2021-12-14.",
"No significant updates.
We have received responses from additional service providers, who are now patched or patching, and have put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.
We are still waiting to hear back from a handful of infrastructure service providers.
We will update this issue again when we have further information, which is expected to be on 2021-12-15.",
"No significant updates.
We have received responses from all of our infrastructure service providers who handle production data. Some of Tailscale's service providers are affected by this vulnerability and are now patched or patching. Those with issues in public services have also put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.
We are waiting for the affected providers to confirm when they are fully patched.
We are still waiting to hear back from some other service providers, such as marketing tools. Though these providers handle non-production data, it may still be sensitive data, such as customer emails.
We will update this issue again when we have further information, which is expected to be on 2021-12-17.",
"We have received responses from all service providers we contacted.
Some of Tailscale's service providers are affected by this vulnerability and are now patched or patching. Those with issues in public services have also put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.
We are waiting for all of Tailscale's affected service providers to complete patching or otherwise finish their remediation efforts.
We will update this issue again when we have further information, which is expected to be on 2022-01-05.",
"We have received updates from the remaining affected service provides that they have completed patching or other remediation efforts.
At this point in time, we believe Tailscale and its vendors are no longer affected by the log4j vulnerability, and found no evidence of it being exploited to affect Tailscale.",
"A [new critical security vulnerability in log4j 2 has been discovered (CVE-2021-44228)](https://github.com/advisories/GHSA-jfh8-c2jp-5v3q) and is [being actively exploited](https://arstechnica.com/information-technology/2021/12/minecraft-and-other-apps-face-serious-threat-from-new-code-execution-bug/). It allows for remote code execution by logging untrusted user input.
**Tailscale does not run Java or Java Virtual Machines (JVM) as part of our core infrastructure**, including our control plane, client, and relay servers.
Tailscale does use [service providers](https://tailscale.com/privacy-policy/#share) to run various functions like cloud infrastructure hosting, log management, and various SaaS applications. Some of these store personal information. **We are waiting on statements from the cloud and infrastructure providers we use, and have contacted our remaining service providers, to understand how they may be impacted.**
We havenâ€™t sent a [security notification](https://tailscale.com/kb/1071/security-notifications/), as at this time, we have no information suggesting that we are exposed to this vulnerability or that it can be exploited to gain private information about Tailscale users.
We will update this issue again when we have further information, which is expected to be on 2021-12-13.",
"Boxes for integers up to a certain number are preallocated. You can see if you take a bigger number, they also start to allocate:
```
â¯ julia -e 'function test(T ::Type) b=Array{T,1}(undef, 1000); b.=0; local z=b[1]; a=@allocated for i=1:1000 z=b[i] end; println(T, "" "", a, "" bytes""); end; test(Float64), test(Int64);'
Float64 23824 bytes
Int64 7824 bytes
```
You are hitting https://docs.julialang.org/en/v1/manual/performance-tips/#Be-aware-of-when-Julia-avoids-specializing
If you change the code to
```jl
function test(T::Type{TT}) where {TT}
b = Array{T,1}(undef, 1000)
b .= 0
local z=b[1]
a = @allocated for i=1:1000
z=b[i]
end
println(T, "" "", a, "" bytes"")
end
test(Float64)
test(Int64)
```
to force specialization, you get no allocations. Also, please post code in that format for the future, it is hard to read everything in one line.",Impatience
"This is something weirder, no problem with type parameters in the test, but I have problems with even no type parameters in my bigger program.
```
julia -e 'function test(x::T) where T b=Array{T,1}(undef, 10000); b.=0; local z=b[1]; a=@allocated for i=1:10000 z=b[i] end; println(T, "" "", a, "" bytes""); end; test(1.0), test(1);'
Float64 0 bytes
Int64 0 bytes
```",
(See the updated answer above).,
fuck youy code formatting idiot no help here,Bitter Frustration
Locking due to abusive comments.,
"Accessing Float arrays allocates memory, specifically 16 bytes per access, but accessing integer arrays works correctly. ```console
$ julia -e 'function test(T ::Type) b=Array{T,1}(undef, 100); b.=0; local z=b[1]; a=@allocated for i=1:100 z=b[i] end; println(T, "" "", a, "" bytes""); end; test(Float64), test(Int64);'
Float64 1600 bytes
Int64 0 bytes
```
The `local z=b[1]` before the `@allocated` loop avoids any compilation-related allocs from being measured, that I get otherwise.
This in Julia 1.7 on macOS 12.0.1, Intel hardware.
Observed the same problem in a bigger program with `--track-allocation=user`.",Bitter Frustration
"my current workaround: using the Docker image with version 2.12.06, adding the sensor and upgrading to the new version again",
@manup was able to reproduce.,
"Interesting, I'd tried reverting to 2.12.06 but it didn't work. Traveling for the holidays, will try after I get back.
EDIT: Can confirm, rolling back to 2.12.06 fixes the issue for now.",
The problem appears to be a small regression from v2.13.3-beta https://github.com/dresden-elektronik/deconz-rest-plugin/pull/5460#pullrequestreview-838547549,
Can confirm I'm having the same problem since updating to HA core-2021.12.6 and Phoscon 2.13.04 / 12/18/2021,
"Can confirm same problem. Firmware 26720700, Software 2.13.04 / 18.12.2021",
Can we please stop adding a can confirm?,Impatience
"<img width=""937"" alt=""Screenshot 2021-12-30 at 17 34 43"" src=""https://user-images.githubusercontent.com/497557/147770753-c26adc27-9467-4e52-8688-eca2de71bcc4.png"">
Not being very adept with Github or development, does this mean this issue won't be fixed before beta v.2.14.0, which given next Beta is v2.13.3, seems quite far away?",Impatience
Can confirm I'm having the same problem to HA core-2021.12.7 and deCONZ Current version: 6.11.1,
"WXKG11LM Rev 2
Home Assistant 2021.12.5
deCONZ Current version: 6.11.1
FW 26720700
Same Problem, worked before. Cant add Sensor with Phoscon again.",Bitter Frustration
I'm locking this. The issue is confirmed and we now need to wait until the fix will be implemented. > The next beta is 2.14.0 as we already have a stable on 2.13.4.,
"## Describe the bug
The Aqara Smart Wireless Switch (WXKG11LM), which was easy to add in previous version, can not be added anymore via Phoscon (see also #5585).
## Steps to reproduce the behavior
1. Go to 'Switches""
2. Click on 'Add Switch'
3. Click on 'Others'
4. Press the reset button of the switch for ~ 5 seconds
In deconz a new node was created, but in Phoscon nothing happens.
## Expected behavior
Pairing should work in Phoscon as it worked in the previous 2.12.06 version
## Screenshots
in Phoscon
![image](https://user-images.githubusercontent.com/38468371/147098979-0671d26a-1c23-4828-86dd-f5481d9f9cfb.png)
in deconz
![image](https://user-images.githubusercontent.com/38468371/147095448-16bd2ddb-f408-4306-9aa9-2b5aefa415d3.png)
![image](https://user-images.githubusercontent.com/38468371/147095473-ac74a48c-7e00-457d-8fe6-c76f4ce8ef21.png)
## Environment
- Host system: Fedora 35 (Raspberry Pi 4)
- Running method: deconz-community/deconz-docker podman container
- Firmware version: (26720700)
- deCONZ version: (2.13.04)
- Device: Conbee 2
- Do you use an USB extension cable: yey
- Is there any other USB or serial devices connected to the host system? If so: Which? - HardDisk
## deCONZ Logs
```
13:02:05:413 send permit join, duration: 65
13:02:19:315 0xFCFC nwk changed to 0xDEEA
13:02:19:316 device announce 0x00158D0006D4DEF5 (0xDEEA) mac capabilities 0x80
13:02:19:317 set fast probe address to 0x00158D0006D4DEF5 (0xDEEA)
13:02:19:317 FP indication 0x0000 / 0x0013 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:317 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:318 device announce 0x00158D0006D4DEF5 (0xDEEA) mac capabilities 0x80
13:02:19:336 DEV found DDF for 0x00158D0006D4DEF5, path: 13:02:19:344 FP indication 0x0104 / 0x0000 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:345 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:345 ZCL attribute report 0x00158D0006D4DEF5 for cluster: 0x0000, ep: 0x01, frame control: 0x18, mfcode: 0x0000 13:02:19:534 FP indication 0x0104 / 0x0000 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:535 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:536 Remember Xiaomi special for 0x00158D0006D4DEF5
13:02:19:537 ZCL attribute report 0x00158D0006D4DEF5 for cluster: 0x0000, ep: 0x01, frame control: 0x1C, mfcode: 0x115F 13:02:19:537 0x00158D0006D4DEF5 extract Xiaomi special attribute 0xFF01
13:02:19:538 01 battery 3145 (0x0C49)
13:02:19:539 03 Device temperature 29 Â°C
13:02:19:539 04 unknown 424 (0x01A8)
13:02:19:540 05 RSSI dB (?) 108 (0x006C)
13:02:19:540 06 LQI (?) 4294967296 (0x0100000000)
13:02:19:541 0a Parent NWK 60247 (0xEB57)
13:02:20:213 [1] get node descriptor for 0x00158d0006d4def5
13:02:20:214 ZDP get node descriptor for 0xDEEA
13:02:24:414 saved node state in 0 ms
13:02:24:415 sync() in 0 ms
13:02:24:889 FP indication 0x0000 / 0x8002 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:24:889 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:24:890 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) cluster 0x8002
13:02:24:890 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) clear timeout on cluster 0x8002
13:02:24:905 [2] get active endpoints for 0x00158d0006d4def5
13:02:24:906 ZDP get active endpoints for 0xDEEA
13:02:25:654 ZCL attribute report 0x00158D000774335F for cluster: 0x0201, ep: 0x01, frame control: 0x1C, mfcode: 0x1037 13:02:28:101 FP indication 0x0000 / 0x8005 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:28:102 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:28:102 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) cluster 0x8005
13:02:28:103 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) clear timeout on cluster 0x8005
13:02:28:115 [4] Skipping additional attribute read - Model starts with 'lumi.'
13:02:31:683 Bind response success for 0x00158d000774335f ep: 0x01 cluster: 0x0001
```",
and before of all that i tried to install a latest version of the program which i was installing (midori) it gave me an error with some dependencies the first one was called keytar and the another one was called npdb or something like that,
i need some help right here,Impatience
PS: im on windows,
"this
`0 verbose cli [
0 verbose cli 'C:\\Program Files\\nodejs\\node.exe',
0 verbose cli 'C:\\Users\\10\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js',
0 verbose cli 'config',
0 verbose cli 'mvs_version'
0 verbose cli ]
1 info using npm@7.13.0
2 info using node@v17.1.0
3 timing config:load:defaults Completed in 2ms
4 timing config:load:file:C:\Users\10\AppData\Roaming\npm\node_modules\npm\npmrc Completed in 0ms
5 timing config:load:builtin Completed in 1ms
6 timing config:load:cli Completed in 20ms
7 timing config:load:env Completed in 1ms
8 timing config:load:file:C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-3\midori-desktop-v-1.1.5-beta-3\.npmrc Completed in 0ms
9 timing config:load:project Completed in 1ms
10 timing config:load:file:C:\Users\10\.npmrc Completed in 1ms
11 timing config:load:user Completed in 1ms
12 timing config:load:file:C:\Program Files\nodejs\etc\npmrc Completed in 0ms
13 timing config:load:global Completed in 0ms
14 timing config:load:cafile Completed in 1ms
15 timing config:load:validate Completed in 0ms
16 timing config:load:setUserAgent Completed in 1ms
17 timing config:load:setEnvs Completed in 1ms
18 timing config:load Completed in 29ms
19 verbose npm-session 0bdec25efc6d5ac6
20 timing npm:load Completed in 180ms
21 timing command:config Completed in 2ms
22 verbose stack Error: npm config set <key>=<value> [<key>=<value> ...]
22 verbose stack npm config get [<key> [<key> ...]]
22 verbose stack npm config delete <key> [<key> ...]
22 verbose stack npm config list [--json]
22 verbose stack npm config edit
22 verbose stack npm set <key>=<value> [<key>=<value> ...]
22 verbose stack npm get [<key> [<key> ...]]
22 verbose stack
22 verbose stack alias: c
22 verbose stack at UsageError (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\config.js:64:17)
22 verbose stack at config (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\config.js:89:15)
22 verbose stack at cmd (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\config.js:27:27)
22 verbose stack at Object.[_runCmd] (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\npm.js:119:7)
22 verbose stack at fn (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\npm.js:46:40)
22 verbose stack at Object.<anonymous> (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\cli.js:58:7)
23 verbose cwd C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-3\midori-desktop-v-1.1.5-beta-3
24 verbose Windows_NT 10.0.19043
25 verbose argv ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\10\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js"" ""config"" ""mvs_version""
26 verbose node v17.1.0
27 verbose npm v7.13.0
28 error code EUSAGE
29 error npm config set <key>=<value> [<key>=<value> ...]
29 error npm config get [<key> [<key> ...]]
29 error npm config delete <key> [<key> ...]
29 error npm config list [--json]
29 error npm config edit
29 error npm set <key>=<value> [<key>=<value> `
is the log of the error of the compilation of midori ""beta-3""",
"see this thing
Ã— Rebuild Failed
An unhandled error occurred inside electron-rebuild
gyp info it worked if it ends with ok
gyp info using node-gyp@6.1.0
gyp info using node@17.1.0 | win32 | x64
gyp info find Python using Python version 3.9.5 found at ""C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe""
gyp http GET https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http 200 https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http GET https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp info find VS using VS2019 (16.11.32002.261) found at:
gyp info find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools""
gyp info find VS run with --verbose for detailed information
(node:9472) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.
(Use \node --trace-deprecation ...` to show where the warning was created)`
gyp info spawn C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe
gyp info spawn args [
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\gyp\\gyp_main.py',
gyp info spawn args 'binding.gyp',
gyp info spawn args '-f',
gyp info spawn args 'msvs',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build\\config.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\addon.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\.electron-gyp\\9.3.5\\include\\node\\common.gypi',
gyp info spawn args '-Dlibrary=shared_library',
gyp info spawn args '-Dvisibility=default',
gyp info spawn args '-Dnode_root_dir=C:\\Users\\10\\.electron-gyp\\9.3.5',
gyp info spawn args '-Dnode_gyp_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp',
gyp info spawn args '-Dnode_lib_file=C:\\\\Users\\\\10\\\\.electron-gyp\\\\9.3.5\\\\<(target_arch)\\\\node.lib',
gyp info spawn args '-Dmodule_root_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar',
gyp info spawn args '-Dnode_engine=v8',
gyp info spawn args '--depth=.',
gyp info spawn args '--no-parallel',
gyp info spawn args '--generator-output',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build',
gyp info spawn args '-Goutput_dir=.'
gyp info spawn args ]
gyp: name 'openssl_fips' is not defined while evaluating condition 'openssl_fips != """"' in binding.gyp while trying to load binding.gypgyp ERR! configure error
gyp ERR! stack Error: \gyp` failed with exit code: 1`
gyp ERR! stack at ChildProcess.onCpExit (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\node-gyp\lib\configure.js:351:16)
gyp ERR! stack at ChildProcess.emit (node:events:390:28)
gyp ERR! stack at Process.ChildProcess._handle.onexit (node:internal/child_process:290:12)
gyp ERR! System Windows_NT 10.0.19043
gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""rebuild"" ""--target=9.3.5"" ""--arch=x64"" ""--dist-url=https://www.electronjs.org/headers"" ""--build-from-source""
gyp ERR! cwd C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\keytar
gyp ERR! node -v v17.1.0
gyp ERR! node-gyp -v v6.1.0
gyp ERR! not ok
Failed with exit code: 1
Error: gyp info it worked if it ends with ok
gyp info using node-gyp@6.1.0
gyp info using node@17.1.0 | win32 | x64
gyp info find Python using Python version 3.9.5 found at ""C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe""
gyp http GET https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http 200 https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http GET https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp info find VS using VS2019 (16.11.32002.261) found at:
gyp info find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools""
gyp info find VS run with --verbose for detailed information
(node:9472) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.
(Use \node --trace-deprecation ...` to show where the warning was created)`
gyp info spawn C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe
gyp info spawn args [
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\gyp\\gyp_main.py',
gyp info spawn args 'binding.gyp',
gyp info spawn args '-f',
gyp info spawn args 'msvs',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build\\config.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\addon.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\.electron-gyp\\9.3.5\\include\\node\\common.gypi',
gyp info spawn args '-Dlibrary=shared_library',
gyp info spawn args '-Dvisibility=default',
gyp info spawn args '-Dnode_root_dir=C:\\Users\\10\\.electron-gyp\\9.3.5',
gyp info spawn args '-Dnode_gyp_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp',
gyp info spawn args '-Dnode_lib_file=C:\\\\Users\\\\10\\\\.electron-gyp\\\\9.3.5\\\\<(target_arch)\\\\node.lib',
gyp info spawn args '-Dmodule_root_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar',
gyp info spawn args '-Dnode_engine=v8',
gyp info spawn args '--depth=.',
gyp info spawn args '--no-parallel',
gyp info spawn args '--generator-output',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build',
gyp info spawn args '-Goutput_dir=.'
gyp info spawn args ]
gyp: name 'openssl_fips' is not defined while evaluating condition 'openssl_fips != """"' in binding.gyp while trying to load binding.gypgyp ERR! configure error
gyp ERR! stack Error: \gyp` failed with exit code: 1`
gyp ERR! stack at ChildProcess.onCpExit (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\node-gyp\lib\configure.js:351:16)
gyp ERR! stack at ChildProcess.emit (node:events:390:28)
gyp ERR! stack at Process.ChildProcess._handle.onexit (node:internal/child_process:290:12)
gyp ERR! System Windows_NT 10.0.19043
gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""rebuild"" ""--target=9.3.5"" ""--arch=x64"" ""--dist-url=https://www.electronjs.org/headers"" ""--build-from-source""
gyp ERR! cwd C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\keytar
gyp ERR! node -v v17.1.0
gyp ERR! node-gyp -v v6.1.0
gyp ERR! not ok
Failed with exit code: 1
at SafeSubscriber._error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\spawn-rx\lib\src\index.js:267:84)
at SafeSubscriber.__tryOrUnsub (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:205:16)
at SafeSubscriber.error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:156:26)
at Subscriber._error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:92:26)
at Subscriber.error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:72:18)
at MapSubscriber.Subscriber._error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:92:26)
at MapSubscriber.Subscriber.error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:72:18)
at SafeSubscriber._next (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\spawn-rx\lib\src\index.js:242:65)
at SafeSubscriber.__tryOrUnsub (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:205:16)
at SafeSubscriber.next (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:143:22)",
"I follow this mod guide:
https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts&BH=11",
"What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?",
"> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?
> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?
My graphics card has 11GB of video memory.If this module is installed, it will gradually increase during the game. When it exceeds, the game will freeze and the FPS will plummet. At the end of 26GB, the game crashes and automatically shuts down.",Bitter Frustration
"Also https://github.com/doitsujin/dxvk/issues/2412 This not are piracy.
Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb
Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content
So you can only go to ff7remake_.exe to taket the log file
https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts
And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.",Impatience
"> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?
![image](https://user-images.githubusercontent.com/29872453/147506479-a6768826-bbd8-474e-9bd7-5f1bcdcba37b.png)
I'm sorry that my English is bad and I can't express it well, just like this post",
"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.
![image](https://user-images.githubusercontent.com/29872453/147506635-633b6313-d3da-4474-b03a-ef15e985a6c1.png)",
"> > What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?
> > ![image](https://user-images.githubusercontent.com/29872453/147506479-a6768826-bbd8-474e-9bd7-5f1bcdcba37b.png) I'm sorry that my English is bad and I can't express it well, just like this post
![image](https://user-images.githubusercontent.com/29872453/147506776-e14819bf-8389-4339-a251-ad07c652c95c.png)",
"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.
![image](https://user-images.githubusercontent.com/29872453/147506973-86829385-768f-4d54-978a-1547d05f5cfd.png)",
"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.
![image](https://user-images.githubusercontent.com/29872453/147507166-ea0e70f6-1c45-4186-86e5-e3edaa7b02e3.png)",
"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.
![image](https://user-images.githubusercontent.com/29872453/147507280-b0bbd383-45e9-4828-aa4f-5f7403136c63.png)",
"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.
![image](https://user-images.githubusercontent.com/29872453/147507349-eacfe402-57ec-430c-88e4-74c9029adcce.png)",
Can you please not turn this issue into the mirror of an entire NexusMods thread?,Impatience
"That, and we kind of need an apitrace here. The game is doing some broken shit that *cannot* work properly (creating views with typeless formats and reinterpreting BC1 as BC5 etc) so I kind of need to know what's it's trying to do, and also whether it is trying to use any of the DXGI memory APIs.
DXVK itself does not leak memory but it's possible that the game keeps creating resources for some reason, or uses `MAP_WRITE_DISCARD` on very large resources.
Also, the other thread very much **was** about piracy, I just deleted the comments that blatantly showed it.",Bitter Frustration
Here is a trace: https://drive.google.com/file/d/1VlNg9FA8ehjhsqMTcZPde9Sazfu_-M8g/view?usp=sharing,
"I see have some some one said
Maybe the memory leak with Vulkan is only on Nvidia cards.",
"I just came from a report post of 2018 stating exactly the same about other game
The AUDACITY of acting like you dont know what he means when your faulty software has been leaking memory since always in multitude of games, god according to the last author post it is even leaking memory adressesâ€¦â€¦. And hes like â€œi just wont rewrite it to handle memory like dx11 doesâ€ (aka PROPERLY)â€¦. Why dont you just say you dont care about mid-end cards having rampant issues? Its always one of you two saying â€œyes, its this, its thatâ€ but the thing NEVER gets fixed.",Bitter Frustration
You truly deserve better service given how much you paid for DXVK!,Entitlement
"DXVK does not leak memory, it reuses its internal allocations as it always has. It's certainly not the best memory mangement in the world but I doubt it has anything to do with the issue at hand.",Bitter Frustration
"> I just came from a report post of 2018 stating exactly the same about other game The AUDACITY of acting like you dont know what he means when your faulty software has been leaking memory since always in multitude of games, god according to the last author post it is even leaking memory adressesâ€¦â€¦. And hes like â€œi just wont rewrite it to handle memory like dx11 doesâ€ (aka PROPERLY)â€¦. Why dont you just say you dont care about mid-end cards having rampant issues? Its always one of you two saying â€œyes, its this, its thatâ€ but the thing NEVER gets fixed.
Well said ! He IS PAID to have this working correctly.
I.guess gaslighting some people is easier than fixing the issue. ðŸ”",Bitter Frustration
"I get that it can be infuriating if you can't play a game you really want to play, but the behaviour here just achieves that the devs don't even want to fix this issue.
If you want this issue fixed, try to help with apitraces, reproduction cases etc. Causing drama here won't lead to anything productive.
To throw some ideas in here:
- I don't see the WINE version being specified in the first comment
- Can this be reproduced when compiling dxvk from source?
- Can this be reproduced on AMD GPUs?",
"> DXVK does not leak memory, it reuses its internal allocations as it always has. It's certainly not the best memory mangement in the world but I doubt it has anything to do with the issue at hand.
Perhaps if we contact your employer and email the screenshots highliting the disgusting disdain you have for people looking for help... What was it ..""entitled cesspool"" . That will look good on your resume or I might email Phoronix so they can do an article",Threat
"Closing since this is just turning into harassment at this point.
Latest `master` branch has some changes regardning memory management which may or may not help in extreme cases like this, but in the end the game just seems to require huge amounts of memory.",Bitter Frustration
"In Final Fantasy VII-Remake Intergrade, if you continue to play, the video memory will increase sharply, and then finally the game will crash.
### Software information
Final Fantasy VII-Remake Intergrade, Material: High Shadow: Low Number of Characters: 8.
### System information
- GPU:3080TI
- Driver:497.29
- Wine version: - DXVK version:1.9.2
### Log files
- d3d9.log:N/A
- d3d11.log:
[ff7remake__d3d11.log](https://github.com/doitsujin/dxvk/files/7771234/ff7remake__d3d11.log)
- dxgi.log:
[ff7remake__dxgi.zip](https://github.com/doitsujin/dxvk/files/7771235/ff7remake__dxgi.zip)",
So? What's your problem?,Impatience
Wats ur problem?,Impatience
So? What's ur problem?,Impatience
"è¿™ä¸ªä»“åº“ä¸è¿åä»»ä½•EULAã€‚å³ä½¿æ‚¨æœ‰å¼‚è®®ï¼Œå¹³å°ä¹Ÿä¸ä¼šå¹²é¢„ã€‚
This repository does not violate any EULA. Even if you have an objection, the platform will not intervene.
è®©äººç±»æ–‡æ˜Žå‘ç€æ›´åŠ åŒ…å®¹çš„æ–¹å‘å‘å±•ï¼Œæ˜¯æ¯ä¸ªäººç±»ä¸ªä½“çš„æ„¿æœ›ã€‚è¿™æ— å…³æ”¿æ²»å’Œå›½å®¶ï¼Œè€Œä¸Žæˆ‘ä»¬æ¯ä¸ªäººæ¯æ¯ç›¸å…³ã€‚
It is the wish of every human being to make human civilization develop in a more inclusive direction. It's not about politics or country, it's about each of us.
æˆ‘å¯ä»¥ç†è§£æ‚¨ä¹Ÿè®¸ä¸å¸Œæœ›GitHubæ¶‰åŠæ‰€è°“â€œæ”¿æ²»â€ï¼Œä½†è¿™ä¸ä¼šæ˜¯å®ƒè¢«å°é”çš„å”¯ä¸€åŽŸå› ã€‚
I can understand that you might not want GitHub to be involved in so-called ""politics"", but that wouldn't be the only reason it was blocked.
æœ‰å¼‚è®®æ—¶å¸¸æ˜¯ä»¶å¥½äº‹ï¼Œå¸Œæœ›è¿™ä¼šç»™æ‚¨å¸¦æ¥ä¸€å®šçš„æ€è€ƒã€‚
It's always a good thing to have an objection, and hopefully this will give you some thought.
ç¥å¥½è¿ã€‚
Good luck.",
ä½ ä»¬æœ‰ä¸€ä¸ªè¯¯åŒºï¼šè®¤ä¸ºè‡ªå·±çš„æƒ³æ³•ä¸€å®šæ˜¯æ­£ç¡®çš„ï¼Œç„¶åŽå¼ºåŠ ç»™ä¸æ”¯æŒè¿™ç§æƒ³æ³•çš„äººï¼,Bitter Frustration
"> ä½ ä»¬æœ‰ä¸€ä¸ªè¯¯åŒºï¼šè®¤ä¸ºè‡ªå·±çš„æƒ³æ³•ä¸€å®šæ˜¯æ­£ç¡®çš„ï¼Œç„¶åŽå¼ºåŠ ç»™ä¸æ”¯æŒè¿™ç§æƒ³æ³•çš„äººï¼
è¿™å¥è¯é€‚ç”¨äºŽä¸¤ç§äººï¼š
1. æ”¯æŒLGBT+ä¸”å¼ºåˆ¶è¦æ±‚æ‰€æœ‰äººæ”¯æŒLGBT+çš„äºº
2. ä¸æ”¯æŒLGBT+ä¸”å¼ºåˆ¶è¦æ±‚æ‰€æœ‰äººä¸æ”¯æŒLGBT+ï¼Œç”šè‡³è§‰å¾—æ”¯æŒ/ä½œä¸ºLGBT+å°±è¯¥æ¶ˆå¤±çš„äºº
æœ¬ org æ²¡æœ‰å¼ºåˆ¶ä»»ä½•äººæ”¯æŒ LGBTQ+ï¼Œä»»ä½•äººå¯ä»¥é€‰æ‹©åœ¨æ­¤ repo ç½²åè¡¨è¾¾æ”¯æŒï¼Œä»»ä½•äººä¹Ÿå¯ä»¥é€‰æ‹©ç›´æŽ¥å¿½ç•¥ã€‚åŒæ ·çš„ï¼Œæˆ‘æƒ³ä¸€ä¸ªäººèµ°åˆ°ä¸€ä¸ªå›¢ä½“ä¸­ç„¶åŽå¤§å–Šè¿™å°±æ˜¯ä¸ªç¬‘è¯ç„¶åŽè¢«è®¨åŽŒæ˜¯æ­£å¸¸çš„ã€‚çŽ°åœ¨ï¼Œè¿™ä¸ª issue é‡Œï¼Œæ˜¯è°åœ¨ã€Šè®¤ä¸ºè‡ªå·±çš„æƒ³æ³•ä¸€å®šæ˜¯æ­£ç¡®çš„ï¼Œç„¶åŽå¼ºåŠ ç»™ä¸æ”¯æŒè¿™ç§æƒ³æ³•çš„äººã€‹ï¼Ÿ",Bitter Frustration
"> ä½ ä»¬æœ‰ä¸€ä¸ªè¯¯åŒºï¼šè®¤ä¸ºè‡ªå·±çš„æƒ³æ³•ä¸€å®šæ˜¯æ­£ç¡®çš„ï¼Œç„¶åŽå¼ºåŠ ç»™ä¸æ”¯æŒè¿™ç§æƒ³æ³•çš„äººï¼
å…¸ä¸­å…¸ä¹‹åªè¦æˆ‘â€œä¸å…¬å¼€æ­§è§†â€å°±æ˜¯å¯¹TAä»¬æœ€å¤§çš„ä»æ…ˆï¼ŒåŒæ—¶è¦æ±‚â€œå°Šé‡æˆ‘å¯¹LGBTçš„éšå½¢æ­§è§†â€",Bitter Frustration
è®²ä¸‰ä¸ªç¬‘è¯ï¼šä¸­å›½å¤ä»£è”¡æ¡“å…¬è¯´è‡ªå·±æ²¡ç—…ï¼Œå½“ä»£æ¬§ç¾Žè¯´è‡ªå·±æ˜¯æ°‘ä¸»ç¯å¡”å’Œç¡¬åˆšæ–°å† ï¼ŒçŽ°åœ¨åˆæƒŠçŽ°gayè¯•å›¾æŠŠgithubå˜gayhubï¼,Mocking
We have dropped support for [LaTeX Utilities](https://github.com/tecosaur/LaTeX-Utilities/issues) since v8.23.0 to reduce security risks and the maintenance burden. See #3009,
"[Magic comments](https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#magic-comments) disabled by default since v8.23.0. The feature can be a security risk for users who don't know the feature.
If you want to use the feature, please enable it by setting:
```
latex-workshop.latex.build.forceRecipeUsage: false
```
- https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#latex-workshoplatexbuildforceRecipeUsage",
"Thank you for creating an issue.
Since the ticket doesn't seem to be using one of our templates, we're marking this issue as low priority until further notice.
For more information about the policies for this repository,
please read https://github.com/opnsense/core/blob/master/CONTRIBUTING.md for further details.
The easiest option to gain traction is to close this ticket and open a new one using one of our templates.",
"@brad0 the error should be in the system log:
# opnsense-log | grep radvd
I don't think the PHP warning is related just yet.
Cheers,
Franco",
It doesn't show anything. Anything else to check?,Impatience
@brad0 Can you try: `opnsense-log routing`,Impatience
"```
version 2.19 started
warning: AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster
warning: (/usr/local/etc/radvd.conf:111) AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster
warning: AdvDNSSLLifetime <= 2*MaxRtrAdvInterval would allow stale DNS suffixes to be deleted faster
lo not found: Device not configured
lo not found: Device not configured
exiting, 1 sigterm(s) received
sending stop adverts
lo not found: Device not configured
removing /var/run/radvd.pid
returning from radvd main
```",
Tracking WAN on a manual loopback device? I've seen before this doesn't work.,Bitter Frustration
"> Tracking WAN on a manual loopback device? I've seen before this doesn't work.
I have not knowingly configured anything regarding the loopback interface. All I did configure was the VLAN interfaces and that's it.",Bitter Frustration
"Well, in any case this is not normal so at least for community support Iâ€™d take one more look at the following:
# ifconfig
# cat /var/etc/radvd.conf
Cheers,
Franco",
"```
root@inet-fw:~ # ifconfig -a
em0: flags=8822<BROADCAST,SIMPLEX,MULTICAST> metric 0 mtu 1500
options=481249b<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,VLAN_HWCSUM,LRO,WOL_MAGIC,VLAN_HWFILTER,NOMAP>
ether 18:03:73:31:54:49
media: Ethernet autoselect
status: no carrier
nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
ix0: flags=8863<UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST> metric 0 mtu 1500
description: IX
options=4e538bb<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,JUMBO_MTU,VLAN_HWCSUM,WOL_UCAST,WOL_MCAST,WOL_MAGIC,VLAN_HWFILTER,VLAN_HWTSO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
ether a0:36:9f:b3:2a:14
media: Ethernet autoselect (10Gbase-T <full-duplex>)
status: active
nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
ix1: flags=8963<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
options=4e53fbb<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,JUMBO_MTU,VLAN_HWCSUM,TSO4,TSO6,LRO,WOL_UCAST,WOL_MCAST,WOL_MAGIC,VLAN_HWFILTER,VLAN_HWTSO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
ether a0:36:9f:b3:2a:15
media: Ethernet autoselect (10Gbase-T <full-duplex>)
status: active
nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
lo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> metric 0 mtu 16384
options=680003<RXCSUM,TXCSUM,LINKSTATE,RXCSUM_IPV6,TXCSUM_IPV6>
inet6 ::1 prefixlen 128
inet6 fe80::1%lo0 prefixlen 64 scopeid 0x4
inet 127.0.0.1 netmask 0xff000000
groups: lo
nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
enc0: flags=41<UP,RUNNING> metric 0 mtu 1536
groups: enc
nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
pflog0: flags=20100<PROMISC,PPROMISC> metric 0 mtu 33160
groups: pflog
pfsync0: flags=0<> metric 0 mtu 1500
groups: pfsync
ix1_vlan3: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
description: MAIN
options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
ether a0:36:9f:b3:2a:15
inet 192.168.3.2 netmask 0xffffff00 broadcast 192.168.3.255
inet 192.168.3.1 netmask 0xffffffff broadcast 192.168.3.1 vhid 3
inet6 2001:470:b050:3::2 prefixlen 64
inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan3 prefixlen 64 scopeid 0x8
groups: vlan
carp: MASTER vhid 3 advbase 1 advskew 0
vlan: 3 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
media: Ethernet autoselect (10Gbase-T <full-duplex>)
status: active
nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ix1_vlan4: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
description: JUMBO
options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
ether a0:36:9f:b3:2a:15
inet 192.168.4.2 netmask 0xffffff00 broadcast 192.168.4.255
inet 192.168.4.1 netmask 0xffffffff broadcast 192.168.4.1 vhid 4
inet6 2001:470:b050:4::2 prefixlen 64
inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan4 prefixlen 64 scopeid 0x9
groups: vlan
carp: MASTER vhid 4 advbase 1 advskew 0
vlan: 4 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
media: Ethernet autoselect (10Gbase-T <full-duplex>)
status: active
nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ix1_vlan5: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
description: PUBLICWIFI
options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
ether a0:36:9f:b3:2a:15
inet 192.168.5.2 netmask 0xffffff00 broadcast 192.168.5.255
inet 192.168.5.1 netmask 0xffffffff broadcast 192.168.5.1 vhid 5
inet6 2001:470:b050:5::2 prefixlen 64
inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan5 prefixlen 64 scopeid 0xa
groups: vlan
carp: MASTER vhid 5 advbase 1 advskew 0
vlan: 5 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
media: Ethernet autoselect (10Gbase-T <full-duplex>)
status: active
nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ix1_vlan6: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
description: DYNACERT
options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
ether a0:36:9f:b3:2a:15
inet 192.168.6.2 netmask 0xffffff00 broadcast 192.168.6.255
inet 192.168.6.1 netmask 0xffffffff broadcast 192.168.6.1 vhid 6
inet6 2001:470:b050:6::2 prefixlen 64
inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan6 prefixlen 64 scopeid 0xb
groups: vlan
carp: MASTER vhid 6 advbase 1 advskew 0
vlan: 6 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
media: Ethernet autoselect (10Gbase-T <full-duplex>)
status: active
nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ovpns2: flags=8010<POINTOPOINT,MULTICAST> metric 0 mtu 1500
options=80000<LINKSTATE>
groups: tun openvpn
nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
ovpnc1: flags=8051<UP,POINTOPOINT,RUNNING,MULTICAST> metric 0 mtu 1500
options=80000<LINKSTATE>
inet 192.168.180.2 --> 192.168.180.1 netmask 0xffffff00
inet6 2001:470:b0db:180::1000 prefixlen 64
inet6 fe80::1a03:73ff:fe31:5449%ovpnc1 prefixlen 64 scopeid 0xd
groups: tun openvpn
nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
Opened by PID 63830
pppoe0: flags=88d1<UP,POINTOPOINT,RUNNING,NOARP,SIMPLEX,MULTICAST> metric 0 mtu 1492
description: WAN
inet 142.114.5.252 --> 10.11.5.161 netmask 0xffffffff
nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
gif0: flags=8051<UP,POINTOPOINT,RUNNING,MULTICAST> metric 0 mtu 1440
description: HENETV6
options=80000<LINKSTATE>
tunnel inet 142.114.5.252 --> 216.66.38.58
inet6 2001:470:1c:70::2 --> 2001:470:1c:70::1 prefixlen 128
inet6 fe80::1a03:73ff:fe31:5449%gif0 prefixlen 64 scopeid 0xf
groups: gif
nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
```
```
root@inet-fw:~ # cat /var/etc/radvd.conf
# Automatically generated, do not edit
```",
"Not sure what is going on, but radvd is empty so maybe no tracking enabled? It should have said in the initial post.",Impatience
"I find it strange that the log messages reference the example configuration file with line number: `/usr/local/etc/radvd.conf:111`
Which, this file includes exclusively a setting for loopback:
```
interface lo
{
AdvSendAdvert on;
```
I'm not sure why radvd would be using that file over the one in /var/etc/ though, as it's hard coded to use the /var/etc conf:
https://github.com/opnsense/core/blob/master/src/etc/inc/plugins.inc.d/dhcpd.inc#L567
` mwexec('/usr/local/sbin/radvd -p /var/run/radvd.pid -C /var/etc/radvd.conf -m syslog');`
Maybe a red herring, unless there is another means to starting it somewhere else.",
"If it goes to the default if the given config is empty that could explain it, but itâ€™s really not that relevant in the grand scheme of things. I canâ€™t see what is broken that should work (configuration issue likely).",Bitter Frustration
"I was able to reproduce those messages by manually starting radvd with `/usr/local/etc/rc.d/radvd onestart`
```
radvd[5628]: version 2.19 started radvd[5628]: warning: AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster radvd[5628]: warning: (/usr/local/etc/radvd.conf:111) AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster radvd[5628]: warning: AdvDNSSLLifetime <= 2*MaxRtrAdvInterval would allow stale DNS suffixes to be deleted faster radvd[19904]: lo not found: Device not configured ```
`/usr/local/etc/rc.d/radvd` references that config file:
```
load_rc_config $name
: ${radvd_enable=""NO""}
: ${radvd_config=""/usr/local/etc/${name}.conf""}
```
I think somehow the `radvd_enable=""YES""` got added to the rc conf, and it's trying to start through rc.
I did a quick grep through the source and I don't see a provision for setting that to ""YES"", and on my system where radvd is in use, radvd_enable isn't put into the rc.conf system.",Bitter Frustration
"Yep, legacy service integration does not use rc.dâ€¦ we only started using it after forking.",
So what do I need to do to fix this?,Impatience
"Iâ€™m unsure what the goalpost is. You keep giving no further information on what you actually expect other than radvd starting, which is irrelevant without configuration and ISP considerations.",Bitter Frustration
"radvd starts and runs, like it did (without changing anything) before updating from 21.7 to 22.1.
My setup is very simple and straightforward. It's using a 6in4 tunnel with static IPs on each VLAN interface.
![Screenshot_4](https://user-images.githubusercontent.com/1129902/148674407-b8e6a79d-f5a8-4593-9c01-2fb7f260023c.jpg)
![Screenshot_5](https://user-images.githubusercontent.com/1129902/148674411-52439f3f-a1a2-4ed0-be1d-aad973435d86.jpg)
```
root@inet-fw:~ # pluginctl -s radvd start
Service `radvd' has been started.
root@inet-fw:~ # ps -auxwww | grep radvd
root 25618 0.0 0.1 12740 2204 0 S+ 03:37 0:00.00 grep radvd
```
I don't see a verbose flag for pluginctl. How do you see what is going on?",Impatience
"Radvd only starts when you configure tracking Interfaces, but then you need a prefix lager than /64 anyway on your tunnel. Despite radvd not starting what sort of functionality have you lost from radvd not starting?",Bitter Frustration
"That makes absolutely no sense. Who broke things?
A network that doesn't work. Even if I didn't care about SLAAC (which I do) RA is required.",Bitter Frustration
Iâ€™m sorry to say this is a waste of both of our time.,Bitter Frustration
"I had the same issue of radvd not starting up after the update from 21.7.8 to 22.1.
I have multiple interfaces configured with static IPv6 + DHCP6 + managed RA, so in my understanding it definitely needs to be run.
On the old version it was still running, after the update not running anymore.
On the old version, DHCP6 was still working, after the update DHCP6 and therefore IPv6 on these managed networks was completely broken (apart from the statically configured IP6s).
Can't say anything about SLAAC since we don't use it on any network.
Trying to start radvd manually didn't help.
Log entries nowhere to be found unfortunately.
Curiously, `/var/etc/radvd.conf` was completely empty which can't be correct.
I've found a workaround though, which I wanted to share here:
If you go to **Services -> Router Advertisements -> Some Interface** and just click on save, the config will be rewritten and radvd will start up again.
In my case, this fixed the DHCP6 problems.
I did the save thing for any interface that was shown in the RA menu there to be sure.
The fix seems to persist across reboots as well, so I think it only needs to be applied once after the update to fix things.
So I guess this is some kind of problem where the config gets lost during the upgrade and doesn't get rewritten afterwards.
And btw:
> Radvd only starts when you configure tracking Interfaces
Either I'm misunderstanding something or this claim is false, since I don't have any tracking interfaces (or is DHCP6 only managed interface also tracking?) and still radvd is needed, so it needs to be started (and did start in prior versions, and now starts again after applying the workaround).",Bitter Frustration
"> Iâ€™m sorry to say this is a waste of both of our time.
@fichtner: Iâ€™m sorry to say this is a waste of my time... and many more people who have broken IPv6 after update, because you released broken version, while this was known issue for a month. Sorry, but I simply don't understand your dismissive attitude towards @brad0. It is not his job to debug and fix the issue.
Anyway, back to the issue. `/var/etc/radvd.conf` is cleared during the update, even tho in the web gui configuration looks ok, one need to change something, save and change back to regenerate `radvd.conf`. And @megmug seems to confirm that. Once you start digging it is easy to fix, but I guess you can agree that desynchronized settings in gui/system is not something that is immediately seen as a solution. And not everyone like @brad0 have to know how to fix that.
> That makes absolutely no sense. Who broke things?
@brad0: They did... You did nothing wrong. Upgrade process broke your settings.",Bitter Frustration
Closing for heated discussion.,
"I upgraded to 22.1 and radvd no longer starts up.
The web UI mentioned an error..
PHP Warning: Invalid argument supplied for foreach() in /usr/local/www/services_router_advertisements.php on line 334
OPNsense 22.1.b_141-amd64",
"From https://github.com/James-Yu/LaTeX-Workshop/issues/3032#issuecomment-1003920519
We should clearly log the commands and their argument separately, see https://github.com/tamuratak/LaTeX-Workshop/commit/cf8ba0e1894f437974aadd332d53ed6722038e77",
"`child_process` in node deals with this issue by [sorting the variables in lexicographic order first, and then trimming duplicates](https://github.com/nodejs/node/blob/master/lib/child_process.js#L610), if on windows.
`cross-spawn`, which is what is currently being used here, doesn't do that, and seems to always take `Path` over `PATH` if it exists.
The solution is just simply checking if windows, and then just get rid of duplicate entries in a case insensitive manner, while having full control over which path value to take: https://github.com/James-Yu/LaTeX-Workshop/pull/3040/files#diff-84955e6048d64ce6603b6b0039e04152911577b7123438041a8bee7664a3c343R257",
"- Fill out all the information required in [the issue template](https://github.com/James-Yu/LaTeX-Workshop/blob/master/.github/ISSUE_TEMPLATE/bug-report.md). - Please paste the whole log messages here, not parts of ones. It should start with `Initializing LaTeX Workshop`. It is very important to identify problems.
![ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ 2020-02-14 20 41 25](https://user-images.githubusercontent.com/10665499/74528024-87b2ef80-4f6a-11ea-9c5d-4f9feff25524.png)
When all of the above is done, we will reopen this issue if needed.",
"### LaTeX Workshop Output [Required]
```
[04:09:09] Initializing LaTeX Workshop.
[04:09:09] Extension root: D:\Documents\Visual Studio Code\Extensions\james-yu.latex-workshop-8.23.0
[04:09:09] $PATH: C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\OpenSSH\;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
[04:09:09] $SHELL: undefined
[04:09:09] vscode.env.appName: Visual Studio Code
[04:09:09] vscode.env.remoteName: undefined
[04:09:09] vscode.env.uiKind: 1
[04:09:09] editor.acceptSuggestionOnEnter: ""on""
[04:09:09] latex-workshop.bind.enter.key: true
[04:09:09] latex-workshop.docker.enabled: false
[04:09:09] latex-workshop.docker.image.latex: """"
[04:09:09] latex-workshop.hover.preview.mathjax.extensions: []
[04:09:09] latex-workshop.intellisense.package.enabled: true
[04:09:09] latex-workshop.intellisense.update.aggressive.enabled: false
[04:09:09] latex-workshop.intellisense.update.delay: 1000
[04:09:09] latex-workshop.latex.autoBuild.run: ""onFileChange""
[04:09:09] latex-workshop.latex.outDir: ""%DIR%""
[04:09:09] latex-workshop.latex.recipes: [
{
""name"": ""latexmk ðŸ”ƒ"",
""tools"": [
""latexmk""
]
},
{
""name"": ""latexmk (latexmkrc)"",
""tools"": [
""latexmk_rconly""
]
},
{
""name"": ""latexmk (lualatex)"",
""tools"": [
""lualatexmk""
]
},
{
""name"": ""pdflatex âžž bibtex âžž pdflatexâ€ŠÃ—â€Š2"",
""tools"": [
""pdflatex"",
""bibtex"",
""pdflatex"",
""pdflatex""
]
},
{
""name"": ""Compile Rnw files"",
""tools"": [
""rnw2tex"",
""latexmk""
]
},
{
""name"": ""Compile Jnw files"",
""tools"": [
""jnw2tex"",
""latexmk""
]
},
{
""name"": ""tectonic"",
""tools"": [
""tectonic""
]
}
]
[04:09:09] latex-workshop.latex.tools: [
{
""name"": ""latexmk"",
""command"": ""latexmk"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""-pdf"",
""-outdir=%OUTDIR%"",
""%DOC%""
],
""env"": {}
},
{
""name"": ""lualatexmk"",
""command"": ""latexmk"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""-lualatex"",
""-outdir=%OUTDIR%"",
""%DOC%""
],
""env"": {}
},
{
""name"": ""latexmk_rconly"",
""command"": ""latexmk"",
""args"": [
""%DOC%""
],
""env"": {}
},
{
""name"": ""pdflatex"",
""command"": ""pdflatex"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""%DOC%""
],
""env"": {}
},
{
""name"": ""bibtex"",
""command"": ""bibtex"",
""args"": [
""%DOCFILE%""
],
""env"": {}
},
{
""name"": ""rnw2tex"",
""command"": ""Rscript"",
""args"": [
""-e"",
""knitr::opts_knit$set(concordance = TRUE); knitr::knit('%DOCFILE_EXT%')""
],
""env"": {}
},
{
""name"": ""jnw2tex"",
""command"": ""julia"",
""args"": [
""-e"",
""using Weave; weave(\""%DOC_EXT%\"", doctype=\""tex\"")""
],
""env"": {}
},
{
""name"": ""jnw2texmintex"",
""command"": ""julia"",
""args"": [
""-e"",
""using Weave; weave(\""%DOC_EXT%\"", doctype=\""texminted\"")""
],
""env"": {}
},
{
""name"": ""tectonic"",
""command"": ""tectonic"",
""args"": [
""--synctex"",
""--keep-logs"",
""%DOC%.tex""
],
""env"": {}
}
]
[04:09:09] latex-workshop.viewer.pdf.internal.keyboardEvent: ""auto""
[04:09:09] Creating a new file watcher.
[04:09:09] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:09:09] Creating PDF file watcher.
[04:09:09] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{}}
[04:09:09] Creating Bib file watcher.
[04:09:09] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:09:09] Set $LATEXWORKSHOP_DOCKER_LATEX: """"
[04:09:09] Cannot run pdflatex to determine if we are using MiKTeX
[04:09:09] [Server] Creating LaTeX Workshop http and websocket server.
[04:09:09] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:09:09] LaTeX Workshop initialized.
```
### Developer Tools Console [Required]
```
(node:15712) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.(Use `Code --trace-deprecation ...` to show where the warning was created)
```
## Additional questions
### Are you using VSCodium?
No
### Are you using the Snap or Flatpack versions of VS Code?
No
### Are you using LaTeX Workshop with VS Code Remote?
No",
"Please paste the whole log messages when compiling LaTeX files with your recipe:
```
""latex-workshop.latex.tools"": [
{
""command"": ""latexmk"",
...
""env"": {
""PATH"": ""foo""
}
}
```",Impatience
"> Please paste the whole log messages when compiling LaTeX files with your recipe:
> > ```
> ""latex-workshop.latex.tools"": [
> {
> ""command"": ""latexmk"",
> ...
> ""env"": {
> ""PATH"": ""foo""
> }
> }
> ```
```[04:19:58] Initializing LaTeX Workshop.
[04:19:58] Extension root: D:\Documents\Visual Studio Code\Extensions\james-yu.latex-workshop-8.23.0
[04:19:58] $PATH: C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\OpenSSH\;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
[04:19:58] $SHELL: undefined
[04:19:58] vscode.env.appName: Visual Studio Code
[04:19:58] vscode.env.remoteName: undefined
[04:19:58] vscode.env.uiKind: 1
[04:19:58] editor.acceptSuggestionOnEnter: ""on""
[04:19:58] latex-workshop.bind.enter.key: true
[04:19:58] latex-workshop.docker.enabled: false
[04:19:58] latex-workshop.docker.image.latex: """"
[04:19:58] latex-workshop.hover.preview.mathjax.extensions: []
[04:19:58] latex-workshop.intellisense.package.enabled: true
[04:19:58] latex-workshop.intellisense.update.aggressive.enabled: false
[04:19:58] latex-workshop.intellisense.update.delay: 1000
[04:19:58] latex-workshop.latex.autoBuild.run: ""onFileChange""
[04:19:58] latex-workshop.latex.outDir: ""%DIR%""
[04:19:58] latex-workshop.latex.recipes: [
{
""name"": ""latexmk ðŸ”ƒ"",
""tools"": [
""latexmk""
]
},
{
""name"": ""latexmk (latexmkrc)"",
""tools"": [
""latexmk_rconly""
]
},
{
""name"": ""latexmk (lualatex)"",
""tools"": [
""lualatexmk""
]
},
{
""name"": ""pdflatex âžž bibtex âžž pdflatexâ€ŠÃ—â€Š2"",
""tools"": [
""pdflatex"",
""bibtex"",
""pdflatex"",
""pdflatex""
]
},
{
""name"": ""Compile Rnw files"",
""tools"": [
""rnw2tex"",
""latexmk""
]
},
{
""name"": ""Compile Jnw files"",
""tools"": [
""jnw2tex"",
""latexmk""
]
},
{
""name"": ""tectonic"",
""tools"": [
""tectonic""
]
}
]
[04:19:58] latex-workshop.latex.tools: [
{
""name"": ""latexmk"",
""command"": ""latexmk"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""-pdf"",
""-outdir=%OUTDIR%"",
""%DOC%""
],
""env"": {
""PATH"": ""foo""
}
}
]
[04:19:58] latex-workshop.viewer.pdf.internal.keyboardEvent: ""auto""
[04:19:58] Creating a new file watcher.
[04:19:58] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:19:58] Creating PDF file watcher.
[04:19:58] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{}}
[04:19:58] Creating Bib file watcher.
[04:19:58] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:19:58] Set $LATEXWORKSHOP_DOCKER_LATEX: """"
[04:19:59] Cannot run pdflatex to determine if we are using MiKTeX
[04:19:59] [Server] Creating LaTeX Workshop http and websocket server.
[04:19:59] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:19:59] LaTeX Workshop initialized.
[04:19:59] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:19:59] Trigger characters for intellisense of LaTeX documents: [""\\"","","",""{""]
[04:19:59] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:19:59] Current workspace folders: undefined
[04:19:59] Current workspaceRootDir: [04:19:59] Found root file from active editor: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Root file changed: from undefined to d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Start to find all dependencies.
[04:19:59] Root file languageId: latex
[04:19:59] [Server] Server successfully started: {""address"":""127.0.0.1"",""family"":""IPv4"",""port"":57241}
[04:19:59] Reset file watcher.
[04:19:59] Parsing a file and its subfiles: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Parse fls file.
[04:19:59] Fls file found: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.fls
[04:19:59] Parse aux file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.aux
[04:19:59] Linter for root file started.
[04:19:59] Linter for root file running command D:\Applications\TeXLive\2021\bin\win32\chktex.exe with arguments [""-wall"",""-n22"",""-n30"",""-e16"",""-q"",""-f%f:%l:%c:%d:%k:%n:%m\n"",""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:19:59] Added to file watcher: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Snippet data loaded.
[04:19:59] Checking for duplicate labels: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex.
[04:19:59] Linter for root file successfully finished in 0s 208ms
[04:19:59] The .chktexrc file not found.
[04:19:59] Linter log parsed with 0 messages.
[04:20:01] Manager.fileWatcher.getWatched: {""d:\\Documents\\Visual Studio Code\\Projects\\temp"":[""Untitled-1.tex""]}
[04:20:01] Manager.filesWatched: [""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:20:01] BibWatcher.bibWatcher.getWatched: {}
[04:20:01] BibWatcher.bibsWatched: []
[04:20:01] PdfWatcher.pdfWatcher.getWatched: {}
[04:20:01] PdfWatcher.pdfsWatched: []
[04:20:02] Current workspace folders: undefined
[04:20:02] Current workspaceRootDir: [04:20:02] Found root file from active editor: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:02] Keep using the same root file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:02] Linter for root file started.
[04:20:02] Linter for root file running command D:\Applications\TeXLive\2021\bin\win32\chktex.exe with arguments [""-wall"",""-n22"",""-n30"",""-e16"",""-q"",""-f%f:%l:%c:%d:%k:%n:%m\n"",""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:20:02] Linter for root file successfully finished in 0s 225ms
[04:20:02] The .chktexrc file not found.
[04:20:02] Linter log parsed with 0 messages.
[04:20:03] BUILD command invoked.
[04:20:03] The document of the active editor: file:///d:/Documents/Visual Studio Code/Projects/temp/Untitled-1.tex
[04:20:03] The languageId of the document: latex
[04:20:03] Current workspace folders: undefined
[04:20:03] Current workspaceRootDir: [04:20:03] Found root file from active editor: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] Keep using the same root file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] Building root file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] Build root file d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] outDir: d:/Documents/Visual Studio Code/Projects/temp
[04:20:03] Recipe step 1: latexmk, -synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=d:/Documents/Visual Studio Code/Projects/temp,d:/Documents/Visual Studio Code/Projects/temp/Untitled-1
[04:20:03] Recipe step env: {""PATH"":""foo""}
[04:20:03] cwd: d:\Documents\Visual Studio Code\Projects\temp
[04:20:03] LaTeX build process spawned. PID: 5308.
[04:20:03] LaTeX fatal error: spawn latexmk ENOENT, 'latexmk' is not recognized as an internal or external command,
operable program or batch file.
. PID: 5308.
[04:20:03] Does the executable exist? $PATH: foo
[04:20:03] Does the executable exist? $Path: C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\OpenSSH\;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
[04:20:03] The environment variable $SHELL: undefined
[04:20:04] Manager.fileWatcher.getWatched: {""d:\\Documents\\Visual Studio Code\\Projects\\temp"":[""Untitled-1.tex""]}
[04:20:04] Manager.filesWatched: [""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:20:04] BibWatcher.bibWatcher.getWatched: {}
[04:20:04] BibWatcher.bibsWatched: []
[04:20:04] PdfWatcher.pdfWatcher.getWatched: {}
[04:20:04] PdfWatcher.pdfsWatched: []
```",
"`$PATH` is set to `foo`, and `$Path` remains the same environment variable, which is used in the `cs.spawn()` as I mentioned.
Also, @tamuratak may I know why #3040 is closed? If it's really that `${env:var}` that's the problem, I'll just remove that. I'm replying here since I cannot comment on that pull request since you closed it.",Impatience
"Windows users might have to use `""Path""` instead of `""PATH""` on Windows to override the PATH environment variable since, on Windows, the PATH environment variable is typically set as `Path`.
- https://www.computerhope.com/issues/ch000549.htm
I have added a notice to wiki.",
"## Preliminary questions [Required]
yes all
## Describe the bug [Required]
The `env` object in an entry of the configuration `latex-workshop.latex.tools` will not allow case insensitivity on windows, and leads to confusion on the end user.
Set the configuration
```js
""latex-workshop.latex.tools"": [
{
""command"": ""latexmk"",
...
""env"": {
""PATH"": ""foo""
}
}, ...
```
and build with `latexmk`.
- Expected behavior: `""foo""` is used to run `foo/latexmk`
- Actual behavior: `process.env.Path` is used instead
## Logs [Required]
Irrelevant. It's a problem with `process.env` and `cs.spawn()` on Windows.
## Desktop [Required]
- OS: Windows 10
- VS Code version: 1.63.2
- Extension version: 8.23",Bitter Frustration
"Hi, ScreenToGif was never truly portable.
Older versions required .NET Framework 4.8, which was installed by default.
In order for it to be truly portable, the download size will go up to 78MB.
I could add that download option in future releases.",
@NickeManarin That would be cool. I don't care much about portability - but bundling ScreenToGif with the needed .NET6 files insetad of requiring a systemwide .NET6 install would be very welcome. Other popular software (such as paint.net) already go this way.,
"I am here to second that motion. Was quite surprised to see that I needed to install some additional .net software in order to use ScreenToGif. This is soo sad.
From a developer point of view, WHY do you need the newer .Net framework - What part of it, is it that you need ?",Bitter Frustration
"Right. Let's use the right terms here, it's not sad, it's just inconvenient for you. There was always the requirement to have installed .NET Framework, you just already had it installed.
I can make it more convenient, but the file size will go to 140MB (78MB compressed) and the update system won't work the same way that it works now.
Also, I don't need to explain myself as to why I did something with my project (but it's easy to guess why or easy to search for .NET Framework to .NET 6 comparisons online).",Bitter Frustration
"Next version (v2.36) will be distributed with a Full and Light package types.
The updater will know how to handle each installation mode.
Also, I'll distribute the app as an msix package.",
"Hello,
I wanted to upgrade **ScreenToGif** (my favorite GIF recorder) but now it requires **NET**. I don't have admin rights on my laptop so I'm using **ScreenToGif** portable, but **NET** requires admin rights. That is sad.",Bitter Frustration
Workaround described here: https://github.com/electron-userland/electron-forge/issues/2665#issuecomment-1008225325,
This is a massive screw-up :(,Bitter Frustration
":tada: This issue has been resolved in version 3.2.6 :tada:
The release is available on:
- [npm package (@latest dist-tag)](https://www.npmjs.com/package/electron-rebuild)
- [GitHub release](https://github.com/electron/electron-rebuild/releases/tag/v3.2.6)
Your **[semantic-release](https://github.com/semantic-release/semantic-release)** bot :package::rocket:",
"I am using the electron 13.0.0 and node.js v14.8.2
the terminal is zsh, font is SauceCodePro Nerd.
![image](https://user-images.githubusercontent.com/47300853/148669010-1f3928f1-840e-42ef-965e-ddeadade2aae.png)",
"I have confirmed that it was fixed:
Version: 1.64.0-insider (Universal)
Commit: 23664355e61f3fb614932a5cb16613b4d987cfb7
Date: 2022-01-21T13:27:12.081Z
Electron: 13.5.2
Chromium: 91.0.4472.164
Node.js: 14.16.0
V8: 9.1.269.39-electron.0
OS: Darwin arm64 20.6.0",
"When the internal PDF viewer for the current document is not visible, executing forward SyncTeX moves the panel into the current editor group.
https://user-images.githubusercontent.com/10665499/148715374-f51335be-8074-41de-9966-394d60013588.mov
`panel.reveal` doesn't work well. Related to microsoft/vscode/issues/138914",
"I would think it would, in order to maintain SOLID code. Also, as several other people pointed out previously, we are not always immune to managers etc. making us code to interfaces, do gratuitous TDD and automated-unit-test writing etc/mocking, and/or use IoC containers. Bosses like to make software engineers do things ""because I heard about it at a conference,"" or ""the insurance is making us,"" etc.
The interface just supporting the two methods is, in principle, okay -- just make sure that the static extension methods refer to the object through the interface and not the actual class.",Bitter Frustration
"Brian, I understand the argument about managers, although in my 30 years of engineering I never encountered a single manager who was telling me how to write code... I mean, if you write your code against an interface it's one thing. But how some managers can tell a third-party library how to write _their_ code? `HttpClient` doesn't have any interfaces implemented and it seems to be fine, no?
Automated testing and mocking HTTP requests are totally possible with RestSharp in its current shape. I have shown an option in the docs.
I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).
The SOLID argument is not solid :) What do you mean by that, could you elaborate?
Then again, I am seriously not against adding that small interface. However, I would not be happy to add `IRestRequest` and `IRestResponse` back as those are glorified property bags.",Bitter Frustration
"A typical method in a wrapper service for my HTTP calls does the following
1. Take some inputs to form a request
2. Execute
3. Handle response (maybe retry, return data)
If I can mock `IRestClient`, then I can verify the contents of the `RestRequest` given to it by my method. That lets me avoid splitting 1 into a separate method just for testing that I correctly crafted the web request (which I want because string formatting is tricky).
If I can mock `IRestClient`, then I can setup responses for 200, 404, 401, etc and don't need to split out 3 either.
Each method is already short enough that I _don't want_ to split it up just for test coverage. Example:
```csharp
public async Task<List<Something>> GetData(DateTimeOffset startEpoch, DateTimeOffset endEpoch)
{
var request = new RestRequest(""/url"", Method.GET);
await AddBearerToken(request);// other method which might make a request to authenticate or use cached token
request.AddQueryParameter(""start"", startEpoch.ToUnixTimeSeconds().ToString());
request.AddQueryParameter(""end"", endEpoch.ToUnixTimeSeconds().ToString());
var result = await _client.ExecuteAsync<ApiResponseModel>(request);
if (result.StatusCode != HttpStatusCode.OK)
throw new Exception($""Failed to fetch data. Status code: \""{(int)result.StatusCode}\"", message: \""{result.StatusDescription}\"""");
return result.Data.ThingICareAbout;
}
```",
"@fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.",
"> I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).
Can you tell a little bit more about your plans on DI support? Perhaps a word on DI in the docs could clarify many questions before the've to be asked.",Impatience
"The DI registration heavily depends if you want to follow the same pattern as Microsoft does, using `IHttpClientFactory`, registering the client as a transient dependency.
For many use cases it's enough to register a typed client as a singleton:
```csharp
services.AddSingleton<IMyApiClient>(new MyApiClient(new RestClient(options)));
```
If I'd to provide some use of `IHttpClientFactory` registration extensions, I'd need to know if people want to configure the `HttpClient` instance used by `RestClient` on the outside. In this case, lots of options that RestSharp uses to configure the message handler won't be applicable.",
"> @fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.
Thanks! I didn't know about that library. I've been using [Moq.Contrib.HttpClient](https://github.com/maxkagamine/Moq.Contrib.HttpClient) in places we used HttpClient rather than RestSharp.
This library using HttpClient is an implementation detail that I don't think library users should have to think about, unless they have specific requirements making the need for a custom HttpClient necessary. Having the interface makes testing easier for all users without having to add extra dependencies.",
"The library you mentioned would work as well. As long as you can add a delegating handler, it will be possible to test RestSharp calls. All you need to do is to compose or override the message handler used by the wrapped HttpClient:
```csharp
var client = new RestClient(...) { ConfigureMessageHandler = _ => mockedHandler };
```
Basically, one of the benefits of moving to HttpClient (besides that it's obvious and `HttpWebRequest` is heavy legacy) is the ability to use delegating handlers and build message processing pipelines.
The point about testing real requests is obvious to me. If you look at StackOverflow questions about RestSharp, most of the issues are caused by incorrectly added parameters, content type overrides and other weird things. But, if people would test against a RestShar interface, they would have all their tests passing. But it won't work anyway, because the request is just not correctly formed. It's a great benefit to at least inspect the actual request and ensure that it looks exactly as it should, rather than inspecting the parameters collection on a `RestRequest` instance. The latter gives you zero idea how the actual request would look like.",Bitter Frustration
This is not necessary if the existing RestClient methods are marked virtual. This will allow for mocking overrides of the public methods instead of needing a full interface. One of the two is necessary as the handler system is not able to simulate invalid http responses. We switched from Http client to RestSharp because RestSharp had an interface that allowed us to simulate the result of those responses as well as other error conditions not simulatable through the mock handler. Some of these result in StatusCode 0 or exceptions being thrown which require an interface or virtual method.,
@sspates-starbucks why can't you simulate the error response with a testing handler like `MockHttp`? It provides a behaviour that is much closer to reality. It can return anything you want based on its configuration.,Impatience
@maor-rosenfeld thanks for your extensive and elaborative feedback for this issue.,
"If we keep discussing testing, maybe it's a good idea to post some test samples and check how these tests can be refactored. So far, the discussion goes quite theoretical imo.",
"@alexeyzimarev we can argue all day about use cases of using the interface for tests, extension methods, tests on these extension methods and other reasons of why suddenly removing all interfaces from an existing infrastructure package is a bad idea, but at this point it's clear that you're here to tell us how to write code. I'm sorry if I choose not to align with your views, but at this point upgrading to the latest version of RestSharp is so painful that there's really no justification to even try. On a positive note, I'm completely with you regarding the separation of services and settings objects. I like `RestClientOptions`.",Bitter Frustration
@maor-rosenfeld you definitely know how to motivate OSS maintainers to keep doing their community work for free.,Mocking
"I've come here from a breaking scenario that I'm not sure I can fix other than backing out the upgrade to 107 due to missing IRestClient.
I updated one of my libraries to use 107. Why not right? I'm trying to be proactive with keeping libs up to date. It all seemed to work OK until some tests starting failing. This is because we use a third-party library and they reference IRestClient and v106 of the library. I could ask them to update but I don't see that they would prioritise that and it would then force all of their customers to update to v107 as well which might be a big deal. If I use an assembly redirect to 107, it won't work because it won't find IRestClient and if I use 106, my code won't work because the method signatures changed.
I'm not sure what I would have done differently but perhaps something as simple a a different nuget package with different namespaces and mark the old one as deprecated. Then we can run them side-by-side until everyone is on 107?
Not sure. Any suggestions?",Bitter Frustration
"@lukos I don't think it would work in any scenario, interfaces or not. RestSharp v107 internals are completely different due to migration to `HttpClient` and ditching `HttpWebRequest`. It made it impossible to keep the API backwards compatible, and even with interfaces, the API scope of those interfaces won't ever be compatible. Lots of properties of `RestClient` and `RestRequest` are moved to `RestClientOptions` and there's no way to keep them where those properties were before.",Bitter Frustration
"We are running into cases now where one library requires the interface and another requires the v107 structure, we can't upgrade either one due to the lack of backwards compatibility with IRestClient. Adding a compatibility wrapper that implements the interfaces might be a way to handle this better rather than implementing the interface on the current client.",Bitter Frustration
"@sspates-starbucks Even if there's something to make things build-time compatible, these won't be compatible at runtime anyway. As I mentioned, most of the properties of those (`IRestClient` and `IRestRequest`) interfaces were (by necessity) moved to `RestClientOptions`, and it's something that simply cannot be undone due to the configuration of `HttpMessageHandler`.
I can only suggest opening issues for those libraries that still use RestSharp 106 and helping them with the migration. It's not a lot of work honestly.",
"The new class having internal only options generates problems for dependency injection. I set up my DI container to hand out RestClient, but different consumers need different options. There is now now way to set the options after having an instantiated Client.",Bitter Frustration
"@Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:
```csharp
services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
```
But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.",
"> @Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:
> > ```cs
> services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
> ```
> > But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.
Is there a reason why (some) options can't be set after the constructor? Its a HUGE code change to say go make a new interface and options class for every location that is going to use restsharp, vs just setting the timeout and baseurl after the DI injection is done",Bitter Frustration
"Every man and his dog has an opinion on software design/standards and the debates will rage on forever - they're usually just different mindsets/approaches. Those mindsets aren't even usually the mindset of the developer using your library, they're probably dictated by the organisation they work for and having a conversation to change those standards is either way above their pay grade or a battle that's not worth fighting. The problem here isn't one of who is right or wrong, it's one of practicality. The removal of the interfaces causes the amount of work some consumers have to do to significantly increase. Most devs really just need to deliver the work they had promised within the timeframe promised. I'm a huge fan of MockHttp but in my case, updating over 1000 tests to use it (which were written 5 years ago) is a huge amount of work. No matter how you plan your pipeline, the sizing of the work item is now completely invalid. The amount of work required to update your library has become inhibitive to doing so - it will either have to be picked up as a separate work item or it just won't ever get updated.
Irrespective of whether consumers are testing the right thing, using IoC correctly or just plain writing crap code, they no longer have the flexibility to choose like they used to. I think most devs would agree that the code changes make sense but at the end of the day we all need to deliver functionality to a business and I guarantee that those business users really don't care how well the code is written as long as it reliably does what they need. The best development tools out there make our lives easier. That's why we use them - they abstract us away from the detail/hard work we really don't need to care about and they provide flexibility to customise how we utilise them. They help us do more, faster. Removing these interfaces does the opposite of that, irrespective of whether it's ""better"" code or not.",Bitter Frustration
"What everyone is saying is correct. Of course, we appreciate Alexey for making this library but the changes are very breaking.
It seems like the best way out of it would be to create a separate library with a different name/package name (and importantly, different namespaces) to contain your newer/better way of doing things and which should be used by people going forwards. Any serious bugs found in the old library will probably be fixed but otherwise it won't get any new features.
This way, we can have both packages alongside each other so if we can update our own code for the new library we can do that without forcing all of our dependencies to update all of their code at the same time. If they eventually do, then we can eventually uninstal the old package.
Not sure if you think that sounds fair or not?",
"The question is why is there such a strong need to migrate to v107 if v106 works fine for most? Migration to `HttpClient` is a big change in itself, the `IRestRequest` (for example) is impossible to shape to the same form as it was before anyway, as well as `IRestClient` interface API surface cannot be recovered.",Bitter Frustration
"Why update? People want the [stated benefits](https://restsharp.dev/v107/#presumably-solved-issues) that this update (_and future updates_) will bring. As a consumer, I shouldn't have to care what implementations are used internally as long as it works. This is a debate about the impact of the removal of interfaces and the consequences that has for those who use the library. (The fact that I'm even aware of what components are used internally screams leaky abstraction to me but that's just my opinion and somewhat off topic)
Changing the method signatures is something that's not overly complex to deal with (the compiler will help identify what needs changing and, in our case, this library is abstracted away from our code anyway) however, removing all the interfaces entirely fundamentally changes how to set up and work with the library and how easy/difficult that is given what's gone before. The work you've done is great - we're just providing feedback of the real world issues it causing us. Rightly or wrongly, people consume this library is a myriad of ways and for us, the migration path is so large that it's currently a barrier to entry.",Bitter Frustration
"I don't know why, but my message is not getting through. I will try again:
- The API surface of `IRestClient` and `IRestRequest` is impossible to hold intact after the migration
- Removal of interfaces made the maintenance work much easier.",Bitter Frustration
"*** EDITED ***
I really don't want to get bogged down in specifics as I don't think it's helpful
> The question now is if it makes sense to introduce an interface with those two functions?
For us, yes it does. There are probably many others who will find that interface useful too for many different reasons. Adding them in provides flexibility to choose and reduces the impact of the changes for many people especially given that this library has been around for such a long time. This isn't about what code/approach is better, it's about what's practical and helpful for existing consumers.",
"Thanks very helpful @alexsaare, thank you.
The scope of refactoring, however, would be significant, both for me (all the extensions need to change) and for those who use the previous version of the library.
I am not against it, that's why I opened this issue. I still, though, wait for some sample test code, so I can understand the need better.",Impatience
"As I mentioned before, I am trying to understand how people use interfaces in tests. Tests aren't PI, nor IP. Please, share your tests.",Impatience
"To answer some questions @alexeyzimarev posed above : 1) Why upgrade if we don't need to? Because corporate policy requires us to be on recent versions
2) Though I personally agree with some of your motivations and reasoning regarding testing and interfaces, corporate policy also often doesn't care if a test provides value or not
But my biggest problem is the change to DI because of options. I have an app that connects to 10 different services. Currently I have them all getting an IRestClient via DI, set the options they need in their consumer's constructors, and move on.
You are asking me to create 10 new interfaces, move all that rest configuration logic into the DI configuration, change the signature of every consumer of the rest client etc. Further, if two services can currently share configuration they can share the injected interface. But if one of them needs a different bit of configuration in the future, now I have to create a new interface for that DI again, and change the signatures again, just to have a different timeout or base url. I can inject a vanilla RestClient instead of IRestClient just fine. But not being able to set options on it after I get it, is a deal breaker.",Bitter Frustration
"@Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.",
"@Terebi42 as per the latest version requirement, I don't think it is even feasible, although I am not aware of your particular case. Say, you have .NET 6 released in November, does the policy mean you have to upgrade all your applications to .NET 6, with all the breaking changes fixed? I never had such experience in any company.",Bitter Frustration
"> @Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.
Why were the options moved from the request to the client? It was very convenient to be able to set those parameters differently per request. And if they do need to be on the client, why are they now read only? Being able to set them at the time of call would also be very convenient. A change like this is literally asking for dozens of new classes to be created to support the new config paradigm and DI. If we could even pass in a different options class per request, that would be great.",Bitter Frustration
"Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.",
"> Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.
hrm, well, at least I understand. Perhaps Ill make a restclient factory that I DI, and pass it options to get the old pattern back",Impatience
@Terebi42 I opened an issue for that https://github.com/restsharp/RestSharp/issues/1791,
"> * All the sync methods were removed. Those who need it can make extensions, wrapping up async calls with `.GetAwaiter().GetResult()`
Can you update this ticket to remove this bit? .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks. You need to use an async helper like the one in Rebus for this to work correctly in sync code:
https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs",Bitter Frustration
"BTW, if you did bring back IRestClient, the only function that matters is all variations of ExecuteAsync(). There are good valid reasons to want to mock ExecuteAsync() to avoid an actual call as its high level enough to mock away a significant amount of lower level stuff. But it's also low level enough that I usually don't bother ;). Usually we mock at the level above that, which is our REST API client contract. After all the point of mocking is to be able to stub out lower level stuff as a black box and assume that black box works correctly. That assumption works as then you expect something else to validate that the actual client itself is properly tested (integration test etc).
And there is a good argument to be had that writing tests to ensure the client itself is actually working correctly is much better being done by mocking HttpClient as suggested. Something that was almost impossible to do with earlier versions of RestSharp, but is now much easier to do now it relies on HttpClient internally so you can swap it out for mocking purposes.",
"> the only function that matters is all variations of ExecuteAsync().
The point here is that there's only one :) All other overloads are extensions. So, if the interface is back, all the extensions need to be on `this IRestClient` instead.
> .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks
I found this via SO: https://github.com/aspnet/AspNetIdentity/blob/main/src/Microsoft.AspNet.Identity.Core/AsyncHelper.cs",
"I believe that works because it uses a separate task, so it's a similar pattern achieved in a different way to how Rebus did it. I am not sure which one is more efficient. We use Rebus in our code and I stole their code to solve the deadlock issues when we had it, and it worked. I have also solved in the past simply by running the code in a separate background thread similar to the AspNet approach and that works also. But I think it's higher overhead than the approach taken by Rebus and other libraries?
I am sure either approach works, but I know that just using GetAwaiter().GetResult() on an async function will cause deadlocks in web apps (or Windows Forms apps). We had tons of hung web requests when I was doing it without the task wrapper.",Bitter Frustration
Updated,
"Been a while since I looked and and debugged the Rebus version, but I did step through it with a debugger to understand it back then. I am pretty sure it's much lower overhead as it never triggers a separate background thread, so puts no extra pressure on the thread pool. Rather it swaps out the SynchronizationContext so that when the task returns, it comes back on something other than the original sync context which is where you get the deadlocks.",
"I don't think there's an explicit new thread when using the task factory. I think it will use the available IO thread from the pool. Synchronization context, on the other hand, produces some overhead. I believe that doing something like `.ConfigureAwait(false).GetAwaiter().GetResult()` would work, but not in WinForms (for example). I also know that ASP.NET Core behaves differently compared with .NET Framework when it comes to `GetAwaiter().GetResult()`",
"Yes, its possible it would work differently in ASP.NET Core. Porting our code to that is a massive undertaking which is still a work in progress :( But we also use RestSharp in WinForms apps (EasyPost and ShipEngine) so the AsyncHelper approach is what has worked for us.
I guess the only way to tell would be to benchmark the two approaches and see which is faster.",
"Version 107 got several interfaces removed, which is one of the major breaking changes in that version.
It seems most of the interfaces were okay to remove, except `IRestClient`. My arguments are [described](https://restsharp.dev/v107/#motivation) in the docs.
This issue is a place where we should have a civilised discussion about it. I will not follow up on the interface issue anywhere else, except here.
Some details about the current state of the `RestClient` API signature.
- All the sync methods were removed. Those who need it can make extensions, wrapping up async calls in some [async helper](https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs).
- Most of the overloads for making requests are now extensions, as they just call each other, so they would never be part of any interface anyway.
- Most of the options that were previously on `IRestClient` and `IRestRequest` are now in `RestClientOptions`, which is a property bag and, therefore, won't have any interface
So, what's there in stock for `IRestClient`? Basically, it boils down to this signature:
```csharp
namespace RestSharp; public interface IRestClient {
RestClient AddDefaultParameter(Parameter parameter);
Task<RestResponse> ExecuteAsync(RestRequest request, CancellationToken cancellationToken = default);
}
```
The question now is if it makes sense to introduce an interface with those two functions? Please comment.",
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,
âš ï¸ This issue has been marked wontfix and will be closed in 3 days,Impatience
"From the explanation page;
> The best way to test HTTP calls is to make some, using the actual service you call. However, you might still want to check if your API client forms requests in a certain way. I have to disagree with this. I don't think calling actual services from unit tests is good practice. At that point they're functional tests. next line goes into that;
> As RestSharp uses HttpClient internally, it certainly uses HttpMessageHandler. Features like delegating handlers allow you to intersect the request pipeline, inspect the request, and substitute the response. You can do it yourself, or use a library like [MockHttp](https://github.com/richardszalay/mockhttp)
this requires me to actually know how you're making your calls inside your library that you publish to nuget. The main reason people wants to use external libraries is to not be concerned about the internals of that external code. It also assumes I know to use this MockHttp library (had no idea it existed, I didnt need to since I've used RestSharp to handle all ""HttpClient"" related things so far), with the assumption that, that package will also be maintained properly and will not be making breaking changes.
> Mocking an infrastructure component like RestSharp (or HttpClient) is not the best idea.
This statement is also wrong in my opinion. I think the opposite actually, making us test external library code in our _unit_ tests implicitly makes them not unit tests, because we are always testing more than a small ""unit"" of code, in addition to our code change, now we're testing your code.
> As RestSharp uses HttpClient internally
What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?
I am fine with removing every other interface on this package. Except for that single one.
This wouldn't be an issue if we were able to mock classes like Java allows, however removing this 1 single interface with 1 single method causes design issues, as well as wasted hours for millions of developers. I would also be skeptical about the suggested ""if you don't like 107, just remain on 106"" By it's nature, RestSharp deals with connectivity, that increases the chances of 0 day vulnerabilities or other security issues. We won't be able remain on 106 forever, and would be forced off of this package. Which would be a shame because we love RestSharp.
I really appreciate your work on maintaining this package, there is a reason it's one of the most popular packages on .NET. I hope you reconsider this decision.",Bitter Frustration
"> What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?
That's an exaggeration. RestSharp uses elements of .NET under the hood, and for many years it was `WebRequest`. Then, Microsoft decided to scrap it and re-implemented it using `HttpClient`, causing massive problems. Essentially, RestSharp uses the only way available in core .NET to make HTTP calls, and it's `HttpClient` until Microsoft decides to do something else. All other .NET libs for making HTTP calls like Refit or Flur also use `HttpClient`, and most people use `HttpClient` directly. So, when _that_ changes, all the code for .NET will need to change, which is very unlikely.
> This statement is also wrong in my opinion.
The issue here is that by creating an interface mock you will assert if the client is formed according to your idea of how it should be formed. However, it doesn't guarantee that the actual HTTP request will be valid. That's why `HttpClient` has no interface and cannot be mocked. And that's why people normally use a test version of `HttpMessageHandler` instead, as `HttpClient` itself is just forming a message, but the actual _oevr the wire_ implementation is in the handler. See this [SO question](https://stackoverflow.com/questions/36425008/mocking-httpclient-in-unit-tests).
> I have to disagree with this. I don't think calling actual services from unit tests is good practice.
You don't have to call the actual service, you can call a simulation. That's what many RestSharp tests do for making sure that the server can understand the request in the way it should.
Again, I am not against bringing the interface back, but it won't be the original interface. Essentially, there are just a handful of core functions of the client itself (`ExecuteAsync` is the main one), all the others are just extensions.
I actually think that the interface would help for building something like a retryable client with Polly, as it could be done in a wrapper using composition.",
"Hello dear community,
I'm using RestSharp since some times (7 years more or less), I like it as it do the job.
For a new project, I used it, by reflex, and surprise ! No more interface ðŸ˜¨
I will not discuss with project owner about, it's a good choice or not, I will just write quickly my opinion.
I will give up RestSharp usage, and stop recommand it to team members.
I'm using library to save time, I'm using library as a tool to make my work easier.
Now I'll have to invest time to introduce ""tricks"" for my unit test ? But why ? I choose a tool because it fit to my need, RestSharp doesn't fit anymore.
I'm sure this breaking change is relevant, but not for me.
Most of dev will update lib, keep it in 106 (this proposal is a shame ðŸ˜…), update their code.
For my part, I choose another tool to save time.",Bitter Frustration
"@RedVinchenzo thank you for your feedback, but I am having a hard time understanding how your comment contributed to the discussion. Keeping the API intact just so people don't need to change their code, hm. I don't remember a single library I used that never changed its API.
Now I see what is going to happen. With .NET you have no choice but to use HttpClient one way or another. Flurl, Refit, or whatever wrapper you use, will either use their own `HttpMessageHandler`, or force you to use the same MockHttp. I am quite puzzled hearing ""I will just use HttpClient because RestSharp has no interfaces"", I wish someone explains that.",Bitter Frustration
"Hi @alexeyzimarev If you use your library in your projects then you are familiarized with how can you use it and tested it. But I think many of us use the library in the next way.
For DI just define in Startup.cs
`services.AddTransient<RestSharp.IRestClient, RestSharp.RestClient>();`
Then just need to define this in the constructor and thats it, you can use RestSharp in the class
`public ValidationDataAccess(IRestClient restClient)`
For the unit test I use Moq, so I only neet to define the result of method Execute like this
`var moq = new Mock<IRestClient>();
moq.Setup(m => m.Execute<T>(It.IsAny<RestRequest>()))
.Returns(new RestResponse<T>()
{
ErrorMessage = string.Empty,
Data = data
});`
And pass to the constructor like this
`var validationDA = new ValidationDataAccess(moq);`
If I need that RestClient response with an error then I define the result of Execute with an error to test that escenario.
So, for me the problems is the next
- We neet to refactory not just new projects but projects in production only for use the new version
- You say ""just don't update the version"". @Terebi42 say one reason what we neet to update
- You say ""in your unit test you can call a simulation"", but again it's needed time to refactory the projects
- You say ""I don't remember a single library I used that never changed its API"", yes you have right but always that cause troubles and discomfort in developers
- You say this in using of mock ""it doesn't guarantee that the actual HTTP request will be valid"", maybe but thatÂ´s why we test local and QA",Bitter Frustration
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,
âš ï¸ This issue has been marked wontfix and will be closed in 3 days,Impatience
"I am starting a new project making use of RestSharp, so more focused on the recommended way to mock / unit test today. It appears the example in the docs might be incorrect.
```csharp
var client = new RestClient(...) { ConfigureMessageHandler = _ => mockHttp };
```
versus
```csharp
var client = new RestClient(new RestClientOptions { ConfigureMessageHandler = _ => mockHttp });
```
Also the single quotes in the json value isn't valid; MockHttp example has this issue as well but there's no actual deserialization in that example.
From my perspective, I like being able to verify through the request using MockHttp, but would also like `IRestClient` for easier mocking at that level. Having both options is nice, and it doesn't seem onerous for the lib to support it. It sounds like the latest version is a new library from compatibility standpoint, so makes sense that `IRestClient` in this version would be a new interface.",
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,
âš ï¸ This issue has been marked wontfix and will be closed in 3 days,Impatience
Conclusion: stick with RestSharp 106 and evaluate other options. I cannot maintain confidence in a dependency whose maintainer is so cavalier about breaking published API without even a nod to proper semantic versioning.,Bitter Frustration
"> All the deprecated interfaces had only one implementation in RestSharp, so those interfaces were abstracting nothing. It is now unclear what was the purpose for adding those interfaces initially.
It was to allow you to commit to a published API and a decoupled implementation, and to allow users of RestSharp to *easily* write unit tests that respond with canned data without hitting the filesystem or network interface.
Perhaps you have lost sight of the fact that some of us are writing software that calls servers that, while having a published API, are run by a third party.",Bitter Frustration
"Here's the point, when a package exposes an interface such as `IRestClient`, it is a commitment to support that interface as stable going forward. By publishing that interface, you must expect people to develop code against same interface going forward, especially when writing mocks and stubs so that their unit tests do not hit the filesystem nor the network. To advise devs to test against the live endpoints shows a complete and utter failure to understand the purpose of unit testing, which is to test a given code unit against a purely local mock.",Bitter Frustration
"@richardbuckle sorry, who are you again? good bye.",Mocking
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,
âš ï¸ This issue has been marked wontfix and will be closed in 3 days,Impatience
"I reviewed #1952 and have a proposal:
- Rename `ExecuteInternal` to `ExecuteRequestAsync` and make it public
- Rename `InternalResponse` to `HttpResponse` and make it public
- Convert `ExecuteAsync` and `DownloadStreamAsync` to extension methods; both are calling `ExecuteRequestAsync`
- Convert `Execute` and `DownloadStream` sync methods to extension methods
- Add the `IRestClient` interface with only _one_ method: `ExecuteRequestAsync`
- Change all extensions of `RestClient` to `IRestClient`. There will be issues with client members that should be included in the interface, so add those too. `Options` is the property I expect to be there.
This will allow composition. For example, I can add a Polly-based wrapped client to retry particular status codes. Btw, I found that the number of retries in the response `Request` property might not work as intended.
How does that sound?",
"Hmm, it doesn't really work well as `Options` become public again... One solution is to make it a record again, with init-only properties, and risk incompatibilities with old SDKs. Not sure what's more important.",Bitter Frustration
"Seems reasonable to me, but why would the options need to be part of the IRestClient interface anyway? The way I set it up in my branch where options is private again, I have some properties to get back important stuff like the baseurl etc. Those could be part of the IRestClient interface if needed. But I don't think exposing options is necessary at all in order for someone to mock up IRestClient is it?",
"Because lots of extensions need those options. So, it's either to extend the API surface of the interface, which makes little sense or expose a shared options container via the interface.",
I guess I took the wrong approach. The internal execute method should remain internal or private. Both `ExecuteAsync` and `DownloadStreamAsync` should be in the interface as it's the basic public API.,
"I appreciate some of the feedback on this issue (not all of it). However, I see a lot of negativity and unwillingness to understand that:
- the library needs to move on, and breaking changes are inevitable
- keeping the previous API service was hard to maintain, and I am doing most of the work here (thanks to all the contributors, too) for free, which many participants of this discussion, apparently, fail to appreciate
- the number of aggressive and counterproductive comments here is unacceptable
Therefore:
- the interface will be brought back in v109, but its purpose is not to make mocking easier
- I will open another issue for the new interface
- any attack on the maintainers will not be tolerated. those who do it will be blocked for violation of CoC and contribution guidelines",Bitter Frustration
"Hi @Stanley-GF !
You can skip the `--build` flag by using the `--stopmakingshittymalware` or if that doesn't work the `--stopgivingnexeabadname` flag instead
Hopefully that helps ðŸ˜˜",Mocking
"Hi @Stanley-GF You can build your own using the flag `--IScamPeopleForALivingAndExpectToBeRecognizedAsSomeoneFamous`. If that doesn't work, kindly consider shutting down your malware operation and maybe people would be more willing to help in the future. Sucks to suck, right?
Kind regards,
Frisky.",Mocking
![image](https://user-images.githubusercontent.com/66573599/149654723-57e3ec1d-1432-483d-a9e9-7dedcb6d5136.png),
"Last time I checked, you were the clown. You are asking help on how to compile your own skidded malware? FFS, get a life my dude.",Mocking
"Skids will be skids, I wouldn't have to make a decompiler if it wasn't for your skid self.",Mocking
"wat, do you know what script kiddies mean ?",Mocking
Unfortunately the only way to create a prebuilt binary is with the `--build` flag,
"Hey
I wanted to ask, how to create my own nexe prebuilt binary ? (without using `--build` flag)
Example, that when I'll do `-t windows-x64-16.1.1`, will compile with my windows-x64-16.1.1 file
and how to create this `windows-x64-16.1.1` file ?",
@scruel thanks for the PR but we'd rather keep things as they are.,
"> @scruel thanks for the PR but we'd rather keep things as they are.
Fineâ€¦ but it must be a bad decision, and could you explain based on what reason so that you will not change this?",Bitter Frustration
`abstract` is a fine enough signal for us. Yes you can instantiate public utils classes still but I don't think this is worth the extra noise.,Impatience
"@snicoll Fine enough signal for you is not acceptable, could you consider that spring is using by so many people all around the world, no one can make sure that others will not treat `abstract` as a keyword as it is: you should extend this and create an object?
By the way, the ""extra noise"" that you mentioned are already exists in this project, you could find some utils classes which have the private constructor(most of them does not have), and I do think `abstract` should be treated as the ""noise"", because we should not use it for preventing to instantiate the object.
Also, I recommend you read *Effective Java, item 4*.
Thanks.",Bitter Frustration
"According to pull request #1848, utility classes are noninstantiable, but abstract is not enough to prevent from creating objects by subclasses.
This change modified almost all utility classes, but I do think it is necessary, and the effects should small and limited.",
ðŸ’¯ understand that it's really hard to know what users will think of any open source decision until release. Highly grateful for all your work on IPython @Carreau and others.,
"Please revert this.
```
In [2]: 2**3 + 4**5 + 6*7*8
```
becoming
```
In [2]: 2 ** 3 + 4 ** 5 + 6 * 7 * 8
Out[2]: 1368
```
is just terrible. Sure, it's a contrived example, but I think my point is clearly visible there...",Bitter Frustration
"Also, I think black should not be a hard dependency of ipython - make it an optional `[black]` extra instead. And still disable it by default regardless of whether black is installed or not :)
Usecase: My project adds ipython as a (non-development) dependency because it includes a REPL for some management tasks (`flask shell` like, just with ipython since it's much nicer than the default Python REPL). However, the project does NOT use black, and by default there's no black installed in the environment (when doing a dev setup), because well.. the project doesn't use black to auto-format code.
But now ipython pulls in black as a dependency and I think some editors might even detect this and suggest black to auto-format code...",Bitter Frustration
"Please don't pile on with comments like ""this is terrible"", it's not productive. Just indicate support with a thumbs-up on the issue.
> make it an optional `[black]` extra instead.
Yes, this is another path.
> I think some editors might even detect this and suggest black to auto-format code...
This is very true.",
"I would like to counter argument that black should be kept as default. I personally like the change. Even while prototyping, an code formatter can really help you keep your code clean and legible, and black is easily one of the most popular python code formatter. I would also argue that new users should come in contact with code formatters early on their learning journey, as writing readable code is just as important as writing functional ones. By keeping the feature as default, you make it more likely that new users will find this feature. Most people will not thoughtfully read all documentation and optional before starting to use a product, and by keeping it as default it's very easy to present the feature to new users. Keeping opt out as default is a good strategy for product rollout. Those who end up disliking the feature should have a easy time disabling it. According to the [docs](https://ipython.readthedocs.io/en/stable/config/intro.html#setting-configurable-options
), you can disable it inside the terminal, on the command line, on the profile files, on the default profile. There is just so many ways to do so, that should fit most needs. Lastly I do think making it a optional dependency is a good idea, but it's a bit of sidetracking: We can have black as default and black dependency as optional, just as we can revert black as default and keep black being a dependency. Those two are not interconnected.",
"> I personally like the change
Why not make it opt-in?
> keep your code clean If you do one-off tasks (let's say perfoming some maintenance in your app's REPL where all your DB models etc are available), it literally doesn't matter.
> I would also argue that new users should come in contact with code formatters early on their learning journey
But not those that rewrite their code automatically. It's common to highlight formatting issue, provide a shortcut to auto-format and maybe even do it on commit. But right when typing? No, that's just confusing. ""why did my code just change to look completely different"" etc.
> Those who end up disliking the feature should have a easy time disabling it
Many people do not configure ipython, especially those who just use it a a simple REPL. Defaults should be as user-friendly (e.g. enabled syntax highlighting) and non-intrusive (no auto-formatting/rewriting of code) as possible.",
"I absolutely adore Black and use it in all of my projects, but I find Raymond's education argument and the `2**3 + 4**5 + 6*7*8` example provided above very convincing.
I'd love the ability to explicitly opt-in to Black formatting on a global or per-notebook basis, but turning it on by default doesn't feel right to me.",
"Personally, 90% of the time when I'm using the CLI, it's to run a quick test. And, when I'm testing something, the exact input that I've given is the exact input that I want to test.
Black's great, but it's not what I want to see in a CLI unless I've specifically enabled it.",
"I love IPython, and use it every day, and wanted to add something to this conversation that I have noticed while testing out `black` integration in the past on my own projects.
Enabling `black` introduces inconsistencies with how the magic commands use the preceding optional `%` symbol, and may affect some user's current workflows (as well as confuse new users). For example, to see your history list, you'd type
```
%history
```
while leaving off the `%` works as well:
```
history
```
But if you wanted to list it with a numbered list, you pass in `-n`:
```
%history -n
```
...but if you leave the `%` off, `black` will reformat it from
```
history -n
```
to
```
history - n
```
which is no longer a valid magic command.",
"I had no say in the decision to include Black in IPython so I'm opining here as a user. I understand that any decision is ultimately in the hands of IPython maintainers, and respect their right to choose whatever they find works best for their project.
I think having an option to auto-format code in the CLI and Jupyter cells is terrific. That being said, it being the default is rallying people against auto-formatting so it's somewhat counterproductive to what we want to achieve.
Note that I don't necessarily agree with taking any change to Python, or third-party projects like in this case, hostage to some nebulous case of ""educating new users"". It's trivial to demonstrate semicolon usage and other changes which would be auto-formatted away. In fact, it would be also educational to inform users **why** those things are getting auto-formatted away. Semicolons and backslashes are pretty uncontroversially discouraged in user code, no?
I know that the biggest point of contention is standardizing double quotes. I don't want to reopen the single quotes discussion but [the choice was made after evaluating both variants](https://github.com/psf/black/issues/51#issuecomment-376204207). Saying that ""Python itself prefers single quotes"" is anthropomorphicizing a piece of software. It's silly. Again, educating new users would be more complete with an explanation of the reasons why standardizing quotes is done and why one is better than the other.
But I digress. The more important issue is auto-formatting scientific math-heavy lines. There isn't much that can be magically done by a simple tool like Black here *in a consistent manner*. We are evaluating improvements in this area, specifically in the most often brought up case of [hugging power operands](https://github.com/psf/black/pull/2726). That being said, I don't think this will be an area where everybody will be happy whatever Black does.
So, the actions IPython maintainers can take now are as follows:
1. make it explicitly opt-in;
2. make it much easier to opt out;
3. stop depending on Black by default.
My personal preference would be to definitely do 3. but also 2. Depending on Black brings a number of other dependencies to the venv which might not be in the versions people want. And opting out even if Black is importable shouldn't require me to bring up the docs for the magical incantation every time. It should be trivial to remember. (something like `# fmt: off`)
But, again, this is ultimately the maintainers' choice. I'm happy to help if you need anything.",
"Since you mentioned semicolons: It's not very uncommon to write concise one-liners - focus one ONE line - in the REPL when you want to do something quickly. For example importing and calling a function. This is a convenient one-liner in history then if you need to restart to apply code changes (`reload()` isn't particularly good if you changed multiple files).
But let's take a simple example: `foo = ""bar""; 2 * foo`
This is formatted to two lines:
```python
In [1]: foo = ""bar""
...: 2 * foo
Out[1]: 'barbar'
```
Now that's a multi-line statement in the history, which IMHO is actually less readable. If you do it with an actual import and function call it's even clearer.
BTW: In Jupyter Notebooks autoformatting may be more useful since that's something you want to share (but even there one-liners may be perfectly fine sometimes). But in the simple ipython REPL? Nope...",Bitter Frustration
"The way Raymond Hettinger complained on Twitter is personally deeply hurtful.
I hope he did not meant it the way I read it. I'm not going to reply to some comments (here or elsewhere), and may closing and locking all the issue and stop maintaining IPython for my own mental sanity for some time (expect some critical things). So apologies if this is not completely proofread and if words are ill-chosen, and does not get feedback for a while.
To all of those that have nice and constructive comments and discussion, thanks a lot for expressing your opinions and understanding the difficulty of maintaining a software. Also thank a lot for understanding that not everyone has the same needs and preferences, instead of assuming that your way of doing this is the best in the world.
> So, the actions IPython maintainers can take now are as follows:
> > 1. make it explicitly opt-in;
To this and similar suggestions, 'black' auto-formatting has been opt-in for 2 years (may 1st 2020, IPython 7.14).
I had thought it might be problematic, but in two years received almost no bug reports. I tried a few time to say I was considering making it default and only got positive feedback. So I did it, with extensive alpha, beta, and RC time to complain and ask for modifications.
So here is my challenge, if I don't make it the default, no-one know about it. It's astonishing that no-one found the bug @ehamiter described above in 2 years ! That alone would have definitely delayed the release, and at least I would have had tried to fix it.
I've also seen a number of new users misformating Python code and taking really bad habits in the Repl, including folks that did not even realise IPython terminal was multiline. For many of those users black by default is much better. You get use to proper code formatting. So you learn to properly _read_ python code.
And it is much easier to deactivate something you don't like than even figure out it something that may exists. For many users this benefits to, having this option be opt-in would make black auto formatting be part of the [unknown unknowns].(https://en.wikipedia.org/wiki/There_are_known_knowns). So I will _never_ get feedback from these. This is in the same vein as ""but you can configure vim to do so"". > 2. make it much easier to opt out;
It's really hard to make it much easier, there have been a long standing issue to have persistent config, but that's far beyond the time and funds we have for that in IPython. We could borrow a nice configuration interface like ptpython for the UI if Someone want to take a shot at ti. > 3. stop depending on Black by default.
(As black is beta I agree that this is a problem I did not foresaw and will likely be removing at least the dependency, though I don't really like that either).
As many have pointed out, it's expected for major release to receive feedback because few users try `--pre`, and other channel, and it's ok I expect it. But I much prefer a reaction like:
https://twitter.com/jnuneziglesias/status/1478867554009452545
> Black formatting by default is ðŸ¤¢ though ðŸ˜œ â€” made myself a todo to turn it off. ðŸ˜‚ Can yapf be configured in its place?
Which I'm more likely to help with (and did in a thread). Than spitting on maintainers, which is painful and counter productive.
I'll try to restore some of my mental sanity. I was hoping to do a 8.1 around last Friday of January (release friday), we'll see what I can get in there.",Bitter Frustration
"@ambv:
> Semicolons and backslashes are pretty uncontroversially discouraged in user code, no?
In modules, yes. But in IPython (7 and below), a semicolon will disable output:
```
IPython 7.26.0 -- An enhanced Interactive Python. Type '?' for help.
In [1]: 1 + 1;
In [2]: 1 + 1
Out[2]: 2
```
AFAIK, this is widely used in data processing (with Pandas), when the result of an expression is a huge table. Or with matplotlib+notebook, where the text `__repr__` of a graph is usually not useful (and the graph is drawn separately).
BTW, this use of the semicolon is mentioned [with the sphinx directive](https://ipython.readthedocs.io/en/stable/sphinxext.html?highlight=semicolon#writing-pure-python-code) in IPython docs; I assume that should be changed?",
"Please revert this being the default. This is horrible for teaching purposes. It also mangles good pandas code for example. Itâ€™s not possible to go around to 200 students and have them disable this. Anybody who prefers something as strict as black can easily opt in. Strict dogmatic code styling is an advanced use case that should not be enabled by default! Thanks! ðŸ™
Edit: I didnâ€™t do a full read of the issue before posting. Sorry if this was piling on. Always appreciate the work and efforts put into such a project as ipython that I used for years and years. Wanted to get my input in before bed after dealing with it over a zoom session with students. Cheers.",Bitter Frustration
"> (As black is beta I agree that this is a problem I did not foresaw and will likely be removing at least the dependency, though I don't really like that either).
I do not know whether you were already aware of this but we are aiming to mark Black as stable by the end of the month (hopefully, staying on time is hard in OSS). This mostly means Black's style won't change until the start of every year[^1]. Bug fixes and enhancements will be released as they're landed. I don't know what this means for IPython but I wanted to make sure you were aware.
---
And finally, @Carreau if you feel you need to step back for a little bit then go for it. OSS is hard, and I understand the frustration when a) the decision you puts lots of effort in ended up being more controversial than prerelease polling suggested, and b) the dogpiling this mess trends towards. The world will not end if IPython 8.1 does not get released by the end of January (fwiw, we were quite hesitant doing black releases for a really long time, like over 8+ months without a release and yet today we're still doing ok ðŸ™‚ ). Thank you for all your hard work on IPython and Jupyter!
[^1]: to everyone who is worried about the poor math handling being locked in for a year, I don't have anything to promise but we are indeed aiming to improve power operation formatting _before the stable release_: https://github.com/psf/black/pull/2726",
"This feature would be very welcome as a plugin.
Typing `pip install ipython[black]` or `pip install ipython ipython-black` would be the best way to opt-in to this amazing feature (as it would take no configuration other than installing the plugin).",
"@FlaviovLeal Ha! I understand your frustration. However, I don't share it. I have a two major problems with iPython configuration. 1-st is configuration file and sync For me editing iPython's configuration and keep it similar on machines I work on is quite hard. I don't say that when I teach or present a new tool it's kind awkward to tell ""please, use this configuration as defaults are no good anymore"". 2ns issue is configuration file is not a simple config. It's not a language agnostic file like INI, but it's an executable. I agree, that's it's more flexible this way, butâ€¦ I don't trust executable code and wish to avoid it as much as possible. And now iPython tool changes sane default to forcibly use `black`. This formatted is good enough for general Python, but not every single code and way to use it. You tell that ""nobody knows""? How do this project advertises its' options? ""Generate default config file"" command? Man page? Simple HTML documentation? Wiki Page? An article on a popular blog platform or even Twitter? I don't see ""most useful configuration options for iPython or similar help pages. I see only complex html version ""go write a code and we don't care about your problems with lack or misplaced of information"". And yes, even I have black as a formatter I want to be able to configure it for iPython. You tell that you changed in alpha/rc/beta and so on and nobody complained? I don't know many people around me who really use any beta software, even if this software is indeed stable. Python itself is a language, where you have to check next version to make your library or tool to be compatible on day 1. IPython is a tool, which meant to be stable. If tool is meant to be stable, it means for me that defaults won't change without any security or vulnerability issues or at least without a very bold message shown to a user every time (s)he's running a tool",Bitter Frustration
"@ambv having black installed or available in path doesn't mean I want to use it for iPython. Also black constancy isn't really good for Pandas and other scientific applications. Yes, pep8 format is needed in one way or another, but not as forcibly as black applies. There's a lot of tricks you learn only by writing and solving by try and error, where black is like an elephant in Chinese shop. And even here I won't force its usage. I'd suggest to write a tool to format code as scientist would want to, keeping semicolons, keeping pandas-specific formatting and so on. And this tool most definitely won't be any good for general python programming. Even both tools would produce pep8 compatible code, but it would look different.",Bitter Frustration
"> In modules, yes. But in IPython (7 and below), a semicolon will disable output
Hasn't the pythonic way to do this always been:
> _ = 1 + 1
?
> Also black constancy isn't really good for Pandas and other scientific applications.
I cannot even begin to tell you how much I disagree with this, speaking as a data scientist who works a lot with analysts who are new to python.
Pandas is a great library, but it almost encourages the writing of unreadable code. Since enabling `black` auto-formatting, I have found my own data processing to be 200% more legible (and has made typos, bugs and flaws 500% more obvious).
Beyond the toll on tech debt and throughput, needing to worry about the ""proper"" way to format code has a real psychological toll. An analyst once handed off a notebook containing a few hundred lines of code. They apologized for the ""how bad"" their code was. Literally all I had to do was run, `black-nb`, and suddenly I was staring at some of the most careful, clever and well-constructed python I'd ever seen, albeit with a ton of copy-paste (DRY would arguably have been an antipattern for this use case).
Similarly, I set up all of our team repos--including where we store our ""lab notebooks""--to run black (and isort) via pre-commit. I expected the junior team members to struggle with that. Instead, the feedback I received was gratitude that they didn't really have to learn or worry about all the various PEP8 rules; and relief that they didn't have to apply a dozen formatting rules during PRs.
In these ways, `black`-by-default leads not only to an essentially free jump in code quality and an easy ramp for new python programmers to get comfortable with the language's code conventions, but a demonstrable _empowerment_ of coders of all skill and experience levels to feel confident that their code is worthy of sharing without feeling self-conscious just because they don't always know the best place to put line breaks.
Thank you, IPython team, for doing this. This is a huge step for software integrity and for making python more accessible to the vast majority of programmers and python users who will benefit from having linted code out of the box. Let the experts who ""know what they're doing"" and want the software to ""get out of their way"" build their own customizations and workarounds.",
"Again I think notebooks are completely different from the `ipython` REPL... for the latter I think it's not a good feature, for notebooks it seems way more reasonable and possibly useful.
BTW, regardless of the future defaults, how about disabling quote normalization? I think that's the single most controversial choice of black and least useful one in this context as well.
Also, in case the feature remains enabled by default: IIRC. sometimes the ipython config isn't loaded depending on how an application embeds it. Not sure if that's still the case with recent versions but if yes that'd mean you may sometimes not be able to disable it easily in every tool that embeds an ipython REPL. A simple env var to disable auto formatting could help in that case...",
"@ThiefMaster yeah the notebooks have toolbar, menu etc, wherein you could disable the reformatting. Or enable it, as you wish, easily. Or even have a different cell type for autoformatted code and so on.",
"@Carreau I can understand what you are going through. I feel like it was a perfectly fine decision to release 8.0 with black as a default. I have had similar experiences with Jedi, where some people got really frustrated with my decisions. It's probably perfectly fine to revert this change in 8.1 if you want to.
As an upside I would say that most people in this thread were nice and they actually care about IPython, so I hope you do not get too frustrated. Personally I don't do too much alpha/beta/RCs, because people don't use those anyway. So if it's too much work, I would probably just ditch that part. I feel like it's perfectly fine to just test default changes in major versions, because that's where you get feedback.
So whatever you do, note that 80% probably don't care at all about this default, 10% like it and will therefore never find this thread and 10% are against it. I personally would not enable it by default for two reasons:
- Some people get annoyed by this behavior and complain
- You get more bug reports, because it's an additional feature
Both are just wasting maintainer time and your mental sanity :) But feel free to do whatever you think is the right decision here.
> I've also seen a number of new users misformating Python code and taking really bad habits in the Repl, including folks that did not even realise IPython terminal was multiline.
I feel like a lot of those people are beyond saving :) I'm seeing more and more non-programmers using IPython/Jupyter in my circles, which is a good thing.",
"First and foremost, thank you @Carreau for stewarding IPython for many, many years. It's a useful REPL for scientific programming and beyond, because of your maintainer efforts and the time of many contributors.
I'm very happy that people have found IPython useful in education and in their day-to-day work. For over a decade, IPython and Project Jupyter have worked to enable interactive computing and computational narratives. To the many users and volunteer contributors, I thank you.
I have huge respect for @Carreau and the work that he has put in over the years. He has generously given many personal hours to maintain IPython.
In this challenging time with Covid, it's very important to be respectful of the people behind the projects. Please be thoughtful in your comments and suggestions and as generous with your appreciation as your criticism. We are truly working for the same goal, and we will be far more successful doing it thoughtfully.",
"I would donate $100 to see this PR closed unmerged.
AFAICT, there isn't anyone objecting to this change that that couldn't have better spent their time setting a single bit to restore the original behavior that they prefer.",Bitter Frustration
"A problem that is about to surface is reformatting code in existing, published research and tutorials for Jupyter notebooks. When a someone views the notebooks and runs them, the reformatting will defeat the original author's careful formatting. Note, this is something the author cannot control. It happens on the client side.
Here are some examples from Peter Norvig's famous [Probability Tutorial](https://github.com/norvig/pytudes/blob/main/ipynb/ProbabilityParadox.ipynb):
```
@@ -1,6 +1,4 @@
-def joint(A, B, sep=''):
- """"""The joint distribution of two independent probability distributions.
+def joint(A, B, sep=""""):
+ """"""The joint distribution of two independent probability distributions.
Result is all entries of the form {a+sep+b: P(a)*P(b)}""""""
- return ProbDist({a + sep + b: A[a] * B[b]
- for a in A
- for b in B})
+ return ProbDist({a + sep + b: A[a] * B[b] for a in A for b in B})
@@ -1,4 +1 @@
-S2b = {'BB/b?', 'BB/?b',
- 'BG/b?', 'BG/?g',
- 'GB/g?', 'GB/?b',
- 'GG/g?', 'GG/?g'}
+S2b = {""BB/b?"", ""BB/?b"", ""BG/b?"", ""BG/?g"", ""GB/g?"", ""GB/?b"", ""GG/g?"", ""GG/?g""}
```
In both cases, the reformatting defeats the author's effort to format data in a way that best communicates either what the code is doing or the structure of the data.
This applies broadly. Perhaps [critical research using Python2.7](https://www.gw-openscience.org/GW150914data/GW150914_tutorial.html) will be unaffected, but most tutorials and scientific papers will now look different to different viewers.",Bitter Frustration
"Here's an example from the statistics tutorial:
```
-posterior_male = (prior_male * height_male.pdf(ht) *
- weight_male.pdf(wt) * foot_size_male.pdf(fs))
+posterior_male = (
+ prior_male * height_male.pdf(ht) * weight_male.pdf(wt) * foot_size_male.pdf(fs)
+)
-posterior_female = (prior_female * height_female.pdf(ht) *
- weight_female.pdf(wt) * foot_size_female.pdf(fs))
+posterior_female = (
+ prior_female
+ * height_female.pdf(ht)
+ * weight_female.pdf(wt)
+ * foot_size_female.pdf(fs)
+)
```
Note how the two posterior computations are no longer parallel to one another. When this happens in published Jupyter tutorials, it lowers the quality of presentation. Again note that this cannot be controlled by the author. Every **reader** of the notebooks would have to explicitly opt-out of reformatting the author's original code.",Bitter Frustration
"> A problem that is about to surface is reformatting code in existing, published research and tutorials for Jupyter notebooks. When a someone views the notebooks and runs them, the reformatting will defeat the original author's careful formatting. Note, this is something the author cannot control. It happens on the client side.
@rhettinger I share your concern that notebooks are different to modules. They are often intended to be executed rather than edited. Reformatting an existing notebook simply when a user executes it would therefore be a drastic change, if that were to happen.
But if I understand correctly, it hasn't happened. #13397 is a change to the IPython REPL (i.e. `IPython/terminal/interactiveshell.py`). It does not affect notebooks at all! I cannot reproduce the behavior you describe in a notebook, and the diff you post is not the diff of a notebook file but rather a diff of python code. Could you be more specific about the steps to reproduce what you posted?",
"I am incredibly confused by all this talk about how these changes would negatively impact classroom instruction and reference documentation. Any programming book I've read or data science tutorial I've ever found is written with the knowledge that languages evolve--especially when we're talking about packages outside the standard library. How many guides had to be rewritten or were rendered obsolete when [`pandas` deprecated](https://github.com/pandas-dev/pandas/issues/14218)--and soon after removed--`.ix`? I personally spent at least two weeks updating a SQLAlchemy toolkit to conform to the new [2.0 API](https://docs.sqlalchemy.org/en/14/changelog/migration_20.html). And how often does _boto_ break backwards compatibility?
In my experience, all the guides and tutorials I've found _explicitly specify their versions_ and oftentimes contain instructions for setup, usually in my space in the form of a `conda` `environment.yml`. I won't even get into how containerization makes things even easier by making sandbox creation literally push-button. So if auto-formatting is really that big of a deal for your class--aren't your setup instructions already pinning your version of python, IPython and what-have-you? If not, well, that's really way more of a problem than anything else you're bringing up. Or [have you not been following the news](https://www.theverge.com/2022/1/9/22874949/developer-corrupts-open-source-libraries-projects-affected)?
<details><summary><b>Edit / Aside</b> <i>wrt</i> this behavior in notebooks</summary>
> I cannot reproduce the behavior you describe in a notebook, and the diff you post is not the diff of a notebook file but rather a diff of python code. Haha, phew--I thought I was the only one and figured there was a build or something that hadn't made it to `conda-forge` yet.
If you want to see how a Jupyter notebook _would_ reformat, [`jupyter_code_formatter` is my everything](https://jupyterlab-code-formatter.readthedocs.io/en/latest/)--and even there it's push-button, not automatic. And since I referenced pre-commit before, see also: [`black-nb`](https://github.com/tomcatling/black-nb).
</details>",Bitter Frustration
"@mikepqr ipython, an REPL, is a great tool to show off things if you don't want to create a Jupyter Notebook. Also I disagree with ""one true <...>"" thing where dots in triangle brackets could be anything. Or like a quote or semicolon at the end.
Forcing me to format with black by default makes a tool I use for years much less valuable for me and my students then the simple python REPL. The best solution I see from this dispute is to publish an official `ipython-black` package which just change the default to format using black and nothing else, leaving the setting inside iPython configuration for whom want to change it in their configuration files. And make it default if mounthly download report for a plugin is comparable to ipython itself @meawoppl you can bribe maintainers, but you cannot bribe whole community
The most edge case of where this dispute can go further is a creation of a fork where this setting is off and the rest is just aligned with this repo. And this would be a quite problematic solution for the whole community and people involved. This would lead to at least 2 repos, and community helping the project will actually struggle to where put efforts.",Bitter Frustration
"> @mikepqr ipython, an REPL, is a great tool to show off things if you don't want to create a Jupyter Notebook.
Sure. I think you're missing my point, which is that:
- the change we're talking about reverting does not affect notebooks (AFAICT!)
- [the scenario described in this comment](https://github.com/ipython/ipython/issues/13463#issuecomment-1013975665) does not happen (or at least I can't reproduce it)
- formatting of notebooks is not relevant to IPython (i.e. this repo)",
"Yes. I just confirmed this is IPython only. Am not sure why, but the auto-reformatting doesn't affect the Jupyter notebooks. So that bullet has been dodged.",
"I don't miss your point and examples you mentioned are working just fine, eg forced to be reformatted. At class I show students various tools and I prefer keep their mess intact if this doesn't go further to a program as it's a part of a learning process. I also show various python features like semicolons, the quotes are the same, and many others. Why `_ = expression` doesn't work for me to suppress the output? `expression;` doesn't modify the `_` variable which is important in some cases.",Bitter Frustration
"@rhettinger you may have missed it among all the comments here, but [the person who thought Black should be automatically applied to lines in the IPython CLI left a long comment here](https://github.com/ipython/ipython/issues/13463#issuecomment-1013742058), with an explicit reference to your unfortunate choice of words that set this whole discussion off on the wrong foot.
At this point there's clear pushback from the community, with plenty of technically-reasoned support for why this ended up being a controversial move. If maintainers want to revert the feature they already have the technical feedback they need right now. Adding further examples doesn't add to the discussion; on the contrary, piling on will only dig deeper trenches. Especially so if your further technical complaints are not well-founded.",Bitter Frustration
"I'm locking the conversation, there is no point in discussing this further.
There are sane factual reason to disable it by default being that reformatting is sometime factually wrong (semicolon, and magics without `%`). That is sufficiently problematic in itself.
So the next release will likely revert it, depending on the time I have, other things on my plate, and more important things for this repository (like fixing CI first to not block all the pending Pull-requests first). I do not exclude to fix the magic and semi-colon problem and maybe re-enable black at some point in the future, even just because for some feature it seem the only way to get feedback and bug reports.
I would greatly appreciate any help to write better documentation, blog post on features, and getting feedback from the community, a better way to edit/persist [Configuration in JSON files that has been supported for years](https://ipython.readthedocs.io/en/stable/config/intro.html?highlight=Json#json-configuration-files). Why not a %web_config that does like fish or xonsh and pop-up a web-browser that let you pick options (hey it can even be made as a separate package), or have a TUI client.
Also please stop hitting or vilifying anybody, and please help testing downstream project by making sure CI tests are run with `--pre` and similar, there are much better way to spend our time by making sure everything works well for everybody.",Bitter Frustration
"Version 8.0 added automatic formatting with Black. This was added in #13397 without much discussion - at least there is none linked.
IMO this is not desirable. It is surprising for a REPL to change what you wrote, and it introduces a large dependency (Black). As @rhettinger has pointed out on Twitter it reduces the utility of IPython in educational contexts: https://twitter.com/raymondh/status/1482225220475883522
Additionally the release note read:
> If `black` is installed in the same environment as IPython...
But this is misleading, implying that it's opt-in by installing Black. The change made IPython depend on Black, so it is always be installed.",Bitter Frustration
"@kpverint Thank you for trying to make WPCS better.
I think I see what you are trying to do here. Let's first define the problem you are trying to solve:
> When running WPCS on PHP 8.1, you are seeing deprecation notices for passing `null` as a parameter to a function where that parameter is not nullable.
Correct ?
If you look at the _current_ state of the `develop` branch, you will see that those issues have already been fixed in #1984 and #1985.
It looks like you didn't update your dev clone of WPCS before creating this PR, which also explains the file conflicts which GH shows at the bottom of this page.
Suggest: close.",Impatience
That's right. Thanks! :) Closing.,
"I did fresh install by following here: https://github.com/WordPress/WordPress-Coding-Standards#standalone
But it is showing same error. Where I'm missing?",Impatience
"@bilalmalkoc At this moment, the last release of WPCS is compatible with PHP 5.4 -7.4. Support for PHP 8.0/8.1 will be in the next release (currently in the `develop` branch).",
Any news when you will actually publish the fixes?,Impatience
When the release is ready.,Impatience
"Cool, I'll just keep downgrading my computer's PHP version since 11 months wasn't enough time.",Bitter Frustration
@WraithKenny Or you could start contributing instead of bashing people who actually do the work,Bitter Frustration
"Hey, still broken.",Impatience
"> @WraithKenny Or you could start contributing instead of bashing people who actually do the work
It's 2 days from being fixed, but unpublished, for exactly one year. ""...people who actually do the work."" Where? What work?",Bitter Frustration
"@WraithKenny no need to be rude. This is an open-source project for which people are (_shocked face_) not paid.
We do not owe you or anybody else anything, so the comments like yours won't magically make a new release happen any faster.
People who actually do contribute to this repo have other work which is paid for. Also, other open-source projects that we contribute to. So saying that we do not work on the repo is plain rude, and that kind of attitude won't be tolerated.",Insulting
PHP_CodeSniffer returns null which results in errors.,Bitter Frustration
"https://github.com/nvim-treesitter/nvim-treesitter/pull/2272 switched our **Lua** parser to one with better coverage. Old queries are not compatible with the new parser.
To make sure you update to the new one, do `:TSUninstall lua` followed by `:TSInstall lua` (making sure to restart Neovim). Third-party modules providing Lua queries need to adapt to the new parser.",
"#2436 updated the **LaTeX** parser to a version that is incompatible with previous queries. Plugins and color schemes that bundle their own queries need to be adapted.
In case of errors, please bisect your config to find the outdated plugin and open an issue in the corresponding repository.",
"#2471 deprecates the `used_by` property for the parser definitions.
This is now set via the `filetype_to_parsername` table exported from the parsers module:
```lua
local ft_to_parser = require""nvim-treesitter.parsers"".filetype_to_parsername
ft_to_parser.someft = ""python"" -- the someft filetype will use the python parser and queries.
```",
"#2764 has renamed the functions
* `:TSEnableAll` -> `:TSEnable`
* `:TSDisableAll` -> `:TSDisable`",
"#2763 deprecates `ensure_install='maintained'`, with removal slated for **April 30, 2022**. Either specify an explicit list of actually used parsers or `'all'` (not recommended).
Rationale: Over time, the category `'maintained'` (which only indicates that a parser has a maintainer listed) has lost its usefulness, as it has become impossible to guarantee the sort of stability that is implied by ""maintained"".",
"As of https://github.com/nvim-treesitter/nvim-treesitter/pull/2806, Nvim-treesitter **requires Neovim 0.7.0 or later**. This is technically not breaking (only verified in `:checkhealth`), but it will allow us to make breaking changes for earlier versions in the coming days (see https://github.com/nvim-treesitter/nvim-treesitter/issues/2793).",
#2808 removed all bundled filetype detection and ftplugins. These should be handled by Neovim (which includes them as of 0.7.0); new ones are easy to add via the new `vim.filetype.add()` function in Neovim.,
"This issue is used to announce breaking changes (and possibly other critical info). Users are strongly encouraged to subscribe to this issue to get notified of such changes.
Every breaking change is added as a new comment.",
#2809 **removes** `ensure_install='maintained'`. Specify an explicit list of parsers (or use `'all'` in combination with `ignore_install`).,
"#3035 updated the **Swift** parser to a version that is incompatible with some previous queries. Plugins and color schemes that bundle their own queries need to be adapted.
As always, check your runtime path for outdated parsers or queries:
```viml
echo nvim_get_runtime_file('*/swift.so', v:true)
echo nvim_get_runtime_file('queries/swift/*.scm', v:true)
```",
"#3048 changed the **Markdown** parser to a new, split, parser that improves performance. **Important: you need to install _both_** `markdown` and `markdown_inline` **parsers** to get the same highlighting as before. (Conversely, you can uninstall the `markdown_inline` parser to improve performance further at the expense of less highlighting.)
To upgrade,
1. update `nvim-treesitter`, ignoring any errors from automatic parser updates; **restart Neovim**
2. `:TSUninstall markdown`
3. `:TSInstall markdown markdown_inline`
Queries are not compatible; as always, make sure you do not have stale parsers or queries lying around.",
#2818 changed the **Vala** parser to an officially maintained one. Plugins and color schemes that bundle their own queries need to be adapted.,
"#3555 switched the **help** parser to a different repository and improved implementation. To make sure you update to the new one, do `:TSUninstall help` followed by `:TSInstall help` (making sure to restart Neovim and removing the `impatient` Lua cache if used). Third-party modules providing help queries need to adapt to the new parser.",
"https://github.com/nvim-treesitter/nvim-treesitter/pull/3656 removed the obsolete `TS*` highlighting groups. Users and plugins should instead directly use the capture name as a highlight group. E.g., instead of ```vim
hi link TSPunctDelimit Delimiter
```
do
```vim
hi link @punctuation.delimiter Delimiter
```
Nvim-treesitter now **requires Nvim 0.8.0** or higher.",
"#3791 switched the Erlang parser to a different maintained repository. To make sure you update to the new one, do `:TSUninstall erlang` followed by `:TSInstall erlang` (making sure to restart Neovim and removing the impatient Lua cache if used). **Third-party modules providing erlang queries need to adapt to the new parser.**",Impatience
"#4524 rework indentation styling/format to be aligned with upstream, any plugins/modules that modified this will have to **use the new captures specified in the docs**",
"#4593 renamed the `help` parser to `vimdoc`, following the upstream change. Make sure to change `ensure_installed` and any language-specific highlight groups accordingly.",
"#4944 switched the Matlab parser to a different maintained repository. To make sure you update to the new one, do `:TSUninstall matlab` followed by `:TSInstall matlab` (making sure to restart Neovim after updating nvim-treesitter). Third-party modules providing `matlab` queries need to adapt to the new parser.",
"#5185 changed the upstream proto parser to a maintained one, just update the parser with `:TSUninstall proto` and `:TSInstall proto`",
"#5234 raised the **minimum Neovim version to v0.9.1** and changed the format of all injection queries to the upstream syntax. If you need to stay on Nvim 0.8.x, lock nvim-treesitter to **v0.9.1**.",
"https://github.com/nvim-treesitter/nvim-treesitter/pull/5222 switched the upstream perl parser to a maintained one, just update the parser with :TSUninstall perl and :TSInstall perl",
"In short running doctrine-data-fixtures test suite
=> requires doctrine-orm
=> use doctrine-annotations",
"```
$ phpcompatinfo analyser:run lib/
...
Classes Analysis
----------------
Class REF EXT min/Max PHP min/Max ArrayIterator spl 5.0.0 5.0.0 BadMethodCallException spl 5.1.0 5.1.0 DOMDocument dom 5.0.0 5.0.0 U Doctrine\Common\Annotations\AnnotationReader user 5.3.0 U Doctrine\Common\Annotations\CachedReader user 5.3.0 U Doctrine\Common\Annotations\SimpleAnnotationReader user 5.3.0 U Doctrine\Common\Cache\ApcuCache user 5.3.0 U Doctrine\Common\Cache\ArrayCache user 5.3.0 U Doctrine\Common\Cache\Cache user 5.3.0 U Doctrine\Common\Cache\MemcachedCache user 5.3.0 U Doctrine\Common\Cache\RedisCache user 5.3.0 CU Doctrine\Common\ClassLoader user 0.1.0 5.3.0 U Doctrine\Common\Collections\ArrayCollection user 5.3.0 U Doctrine\Common\Collections\Collection user 5.3.0 U Doctrine\Common\Collections\Criteria user 5.3.0 U Doctrine\Common\Collections\Expr\Comparison user 5.3.0 U Doctrine\Common\Collections\Expr\CompositeExpression user 5.3.0 U Doctrine\Common\Collections\Expr\Value user 5.3.0 ...
```
So doctrine-annotations is used",
"It is used for optional functionality. If the data fixtures test suite makes use of annotations, that's where the `doctrine/annotations` dependency should be added.",
"Sorry can't agree... data fixtures don't use annotations, it useq orm, wich use annotations
BTW, do the fuck you want with your project, at least you are aware of the problem.
P.S. previously annotations was pulled by persistence (but dep was removed there)",Bitter Frustration
"> do the fuck you want with your project
Okay, closing the PR then.",Bitter Frustration
See /usr/share/php/Doctrine/ORM/Configuration.php,
"How are you running a plugin on Forge?
Most of these Bukkit/Sponge/whatever else hybrid solutions leave a lot to desired, it is my guess that whatever you're using is not suited for this purpose.
It looks to me like the server's commands are simply not being sent to the client in advance, so our addition of client commands are taking precedence (as they should) and causing the server to not be able to evaluate the command.
I suggest you take this up with the plugin system, not here. We do everything we can.",Bitter Frustration
"However, if you can reproduce an issue where the [server correctly sends the commands](https://wiki.vg/Protocol#Declare_Commands) and we ignore it. Then we can take a look. However in all our tests, even non-vanilla commands are processed correctly.",Impatience
"> How are you running a plugin on Forge?
> > Most of these Bukkit/Sponge/whatever else hybrid solutions leave a lot to desired, it is my guess that whatever you're using is not suited for this purpose.
> > It looks to me like the server's commands are simply not being sent to the client in advance, so our addition of client commands are taking precedence (as they should) and causing the server to not be able to evaluate the command.
> > I suggest you take this up with the plugin system, not here. We do everything we can.
I'm not sure how you've made this out that I'm running a plugin on Forge. I clearly and purely state I'm running the LuckPerms plugin on a Minecraft 1.18.1 Paper **SERVER**, and I'm running a **CLIENT** with Forge with a controllable mod and optifine in the range of the versions clearly stated above.",Bitter Frustration
"Your steps imply to install Forge on the server, so that is not clearly and purely stated.
However, the issue remains the same.",Bitter Frustration
"> Your steps imply to install Forge on the server, so that is not clearly and purely stated.
> > However, the issue remains the same.
I have corrected that with an edit, however it should have been implied that forge was installed on the client with the very first line of the issue ticket if it was read.
Yes indeed, the issue still remains and can verify on fresh installs of multiple machines that an update from 39.0.45 does indeed break the luckperm commands.",Bitter Frustration
"#7754 - 39.0.46 Add Client Commands
Extensive testing and searching shows that issue does indeed start at Forge Version 39.0.46 when PR for ""Add Client Commands"" was requested.",
"Temporarily locking this issue as nothing is being added.
We know what caused this. We're trying to see if there's anything we can do to fix this, but i honestly doubt it.
LuckPerms (or Spigot/Paper/Whatever) isn't sending the client information about the commands.",Bitter Frustration
"For those who are still keeping track of this, it seems like the issue here is that the commands *are* being sent, but only via the command tree, and not as executable commands.
For LuckPerms this can be traced to their use of Commodore, but both use the old (< 1.13) methods of registering commands, which is why they broke when Forge updated to explicitly require support for these.
From the author of the Client Commands PR that exposed this problem:
> seems like both bungeecord and luckperms send the commands but they don't mark them as executable since they are only sending them in brigadier format for the client since they both still use the command registration method that was removed in 1.13 on the backend",
"**Minecraft Version:** {Minecraft version}
Minecraft 1.18.1 Client with Forge (Optifine & Controllable Mods)
**Forge Version:** {Forge version. *Version number, not latest/rb*}
Forge Mod 39.0.46 - 39.0.63 (Issue Versions)
Forge Mod 39.0.40 - 39.0.45 (Working Versions)
**Logs:** {Link(s) to GitHub Gist with full latest.log and/or crash report}
https://paste.ee/p/nFncj - Debug Log
https://paste.ee/p/nknCg - Latest Log (Edited for redaction)
**Steps to Reproduce:**
1. Run Minecraft 1.18.1 Server with LuckPerms Plugin (v5.3.98)
2. Install Forge Mod version between 39.0.46 - 39.0.63 on client running forge of versions above
3. Run any LuckPerms server side issued command to verify issue on client running forge of versions above
**Description of issue:**
When running seemingly any version of Forge 39.0.46 through 39.0.63 (latest as of submission), this causes the client to not register any Luck Permissions plugin command on server as if command is not being entered at all. There is no log of event on client or server side, it just purely acts like the command isn't being entered and nothing happens. _This issue **DOES NOT** happen as tested with Forge Versions 39.0.40 or 39.0.45 and works as expected._",
"### Brief description of your issue
We've encountered an upstream issue impacting our ability to validate and publish manifests. We've already engaged with the other teams involved and are working towards a quick resolution. ### Steps to reproduce
Submit a PR
### Expected behavior
The pipelines should run to completion.
### Actual behavior
The pipelines are failing due to parallelism.
### Environment
```shell
All environments and pipelines are affected.
```",
"## The issues
Many polite attempts were made to resolve the disagreements with reasoning, most of them were [ignored]( https://github.com/whatwg/html/issues/5811#issuecomment-965201571 ). This complaint abandons that approach to describe the severity of the issues with clarity.
### Inappropriate procedure
The [proposal]( https://github.com/whatwg/html/issues/5811#issue-677390392 ) and the discussion did not follow the proper procedure outlined in the [new features guideline]( https://whatwg.org/faq#adding-new-features ), yet it was accepted as the de facto solution. The following steps were skipped:
1. ""Forget about the particular solution you have in mind!"" - The lead editor had a specific solution in mind: [""the use case for this proposal is when you don't want a <form>""]( https://twitter.com/domenic/status/1438229658324111360 ) and aggressively rejected any other solution.
1. ""What are the use cases?"" - I had to collect [these]( https://kaleidea.github.io/whatwg-search-proposal/#use-cases )
1. ""list requirements for each use case""
1. ""Ask fellow web developers about their opinions"" - Domenic described [web developers' input]( https://kaleidea.github.io/whatwg-search-proposal/#:~:text=Optional%3A%20Clarification%20notes-,Requests%20for%20this%20feature,-tweet%20%2D%20%E2%80%9CUsing%20a ) as [""confusion""]( https://github.com/whatwg/html/issues/5811#issuecomment-956454034 ) and as [""makes no sense""]( https://twitter.com/domenic/status/1438229658324111360 ). Incorrect and very disrespectful.
1. ""Research existing solutions."" - Not even the [best practice]( https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles/Search_role ) was considered.
1. ""Come up with new solutions."" - The only new solution was [immediately rejected]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ) without consideration.
1. ""Evaluate how well each of the remaining solutions address each use case and how well they meet the requirements."" - Domenic left the discussion after the new solution was proposed, his [last comment]( https://github.com/whatwg/html/issues/5811#issuecomment-965201571 ) was personal and inappropriate.
### Inconsistency with the WHATWG principles: openness, efficiency
I've worked weeks to research this problem domain, compared to the few hours demonstrated by the editor. Instead of appreciating my hard work, he [immediately closed]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ), [censored]( https://github.com/whatwg/html/pull/7320#pullrequestreview-802253725 ) and [rejected]( https://github.com/whatwg/html/issues/5811#issuecomment-965201571 ) it: ""I'll be closing or marking as off-topic any such discussions outside of this issue.""
Recently the editor prevented me from contributing 2 months' work to Chromium by revoking access to chromestatus.com, when I asked to file an *Intent to prototype*. Progress has been stalled since then. By doing so he also prevented the involvement of the chromium developer community, who might have views different from his.",Bitter Frustration
"## Accountability
The editor refused to answer the following [questions]( https://github.com/whatwg/html/issues/5811#issuecomment-964879335 ) fundamental to his decisions:
1. Why inheritance is unviable? (Why duplication is, is obvious.)
1. What form functionality is unwanted (explicitly opposed) for the `<search>` element with `div` semantics?
1. What developer feedback led to that decision?
Related, but [different]( https://github.com/whatwg/html/issues/5811#issuecomment-979156406 ):
4. Why do you assume that form functionality is ""unviable""?
All these assumptions were proven wrong. The editor is expected to answer these questions and present verifiable (reproducible) evidence to justify his decisions.
Further questions:
5. Have you discussed with the chromium team the proposal with form functionality before [rejecting it]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ) in November? Link to public records.
5. Who discussed it from the chromium team at that time?",Bitter Frustration
"### Working mode violation
[Working mode]( https://whatwg.org/working-mode#conflicts ):
> In case of a conflict among the community of contributors, the editor is expected to go to significant length to resolve disagreements.
The editor made no attempt to resolve the disagreement, instead made a number of [attempts to suppress]( https://github.com/whatwg/sg/issues/186 ) this POV, to prevent me from contributing and getting in contact with chromium developers who might have different thoughts.
### Bad decisions
The editor based its decisions on the following flawed assumptions and misunderstandings:
1. [comment]( https://github.com/whatwg/html/issues/5811#issuecomment-958169793 ): ""The form element is quite magical, and duplicating that magic into another element is a huge implementation and spec lift""
- Nobody proposed duplication. Reuse is key to efficient and maintainable software.
1. [comment]( https://github.com/whatwg/html/issues/5811#issuecomment-961877504 ): 'a new element ""inheriting from form"" is not viable'
- The rejected solution is trivially implemented without inheritance in all 3 main browsers. Its impact on the ecosystem is similarly trivial.
These claims were repeated multiple times after being disproven, creating counterproductive noise and misleading other participants.",Bitter Frustration
"## Recommended solution
I offer a mutually beneficial solution to the editor: there are many features in progress that are more relevant, impactful, and important for the editor. This, however, is a simple, low-priority feature that received only a few hours of attention from the editor and none in the last months. The editor's extensive standardization experience creates more value applied to those more important features.
I wrote the [specification and implemented]( https://kaleidea.github.io/whatwg-search-proposal/ ) the search element in 3 browsers. I've invested more effort into it than all other participants combined. I offer to take on the responsibility of being the editor for this topic, so Domenic can focus on more valuable topics and both our work benefits the HTML standard. This is a very generous offer in [the current context]( https://github.com/whatwg/sg/issues/186 ).
To address the concerns of other participants about the assumed risks I propose to run the origin trial with form functionality and evaluate the feedback from web developers. In case the risks are proven to be substantial the chosen solution will be without form functionality and I will submit the appropriate implementations.
I'm taking the brunt of the work upon myself to minimize the time investment needed from other participants. I hope this solution is an acceptable middle ground and the attitude towards my work will change for the better.",Bitter Frustration
"> ""Ask fellow web developers about their opinions"" - Domenic described web developers' input as https://github.com/whatwg/html/issues/5811#issuecomment-956454034 and as [""makes no sense""](https://twitter.com/domenic/status/1438229658324111360). Incorrect and very disrespectful.
People very literally expressed confusion. That is not a disrespectful description.
Gonna go ahead and say what I don't think folks in the org will be able to: you've worn out all good will and everything about this is ridiculous.",Bitter Frustration
"@Kaleidea Iâ€™ve blocked you for two weeks from any interactions with https://github.com/whatwg/ repos, based on assessing that your use of the issue tracker here has reached the point of being in violation of the GitHub Community Guidelines â€” in particular, https://docs.github.com/en/github/site-policy/github-community-guidelines#disrupting-the-experience-of-other-users â€œDisrupting the experience of other usersâ€. There are 631 people watching this repo, and they have a reasonable expectation that moderation will be applied when necessary to ensure their time and attention is not negatively affected.",
"This is a formal complaint to @domenic about editor decisions according to the procedure described in [workstream policy appeals]( https://whatwg.org/workstream-policy#appeals ), step 1: ""raise an issue formally for review by the Editor"".
Highlighting that Domenic is expected to answer the questions in section ""Accountability"".
If you wish to debate the claims made here, please do so in a separate comment, after answering the questions.
## The reason
### Why is this important?
DX. Developers wish that native HTML-JS solutions would be on par with the ease of use and efficiency provided by frameworks.
In this standardization process [developers' requests]( https://kaleidea.github.io/whatwg-search-proposal/#:~:text=Optional%3A%20Clarification%20notes-,Requests%20for%20this%20feature,-tweet%20%2D%20%E2%80%9CUsing%20a ) were ignored and [inappropriately described]( https://github.com/whatwg/html/issues/5811#issuecomment-956454034 ) as ""confusion"".
### Why do I disagree with multiple participants?
EDIT: As a seasoned software architect I outrank other participants in the $topic domain of software design. I see the solutions where many others see unmanagable complexity.
No other participant has done the necessary research to prepare this standard. I have spent about 2 months researching and implementing this feature in all 3 main browsers. With that knowledge, as a seasoned software architect, I have much deeper understanding of the implementation details and risks. As a comparison, I estimate other participants spent a few hours (few comments), Scott a few days (specification + examples).
I have found that the claims of the other participants (Domenic and Scott) are technically wrong assumptions that have no evidence. They did not provide evidence for their claims when [asked to do so]( https://github.com/whatwg/html/issues/5811#issuecomment-964879335 ). The request was simply ignored.
I have documented the research and evidence in 15+ page-length comments and a [proposal]( https://kaleidea.github.io/whatwg-search-proposal ). They have repeatedly rejected and ignored the documentation and evidence that I've presented, for ex.
- Domenic has [immediately closed]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ) the issue I've filed.
- Scott has repeated in the [January triage meeting]( https://github.com/whatwg/html/issues/7385 ) the first fundamental question (""people who don't want form behavior"") that has been answered repeatedly, months ago: [1]( https://kaleidea.github.io/whatwg-search-proposal/div-functionality ), [2]( https://github.com/whatwg/html/issues/5811#issuecomment-961785189 ), [3]( https://github.com/whatwg/html/issues/5811#issuecomment-981973258 ).
Professional collaboration cannot be conducted in this manner. If other participants would have done the necessary research or just respected the enormous effort I did, they would understand their concerns are non-existent risks.",Bitter Frustration
https://github.com/GTNewHorizons/GT-New-Horizons-Modpack/issues/7555,Impatience
"> #7555
What kind of lunacy is this, this isn't the fucking NSA I just want easy access to do my job ffs. I don't give a shit about user privacy, they forfeit that by playing on servers which are not theirs. Not being funny but that entire ticket and the next linked one reeks of people who have never actually had to moderate in their lives. ""Just fix the dupe bugs"" jesus christ.",Bitter Frustration
"I think this is a great change honestly, Colen is probably the best and most responsible moderator I've ever met. If he needs this access then it must be fully justified. I highly support this change. - Kitten",
@boubou19 Do you remember why [#7555 ](https://github.com/GTNewHorizons/GT-New-Horizons-Modpack/issues/7555) was closed? I remember something like it turned out to be working as is.,
"it was closed because turns out i can read the items in the terminal, just not take them. But idk if it's due to the fact that default user has only read access or if it's a special perm for OP players. That was enough for my needs at the time so i didn't bothered much.
If you ask me, that protection is purely silly and operators should have full access to any stuff on the server. If the operator is doing questionable actions with his powers then that's on Dream and I because we trusted the wrong person to be staff. And if any player complains about this because we would gain full access to their AE, i want to say that we can bypass it in several ways, the most convoluted one being direct nbt edit from server files. And that player is free from not playing on the officials, if he doesn't trust the team.",Entitlement
"Eh, I kind of agree, but also, read access seems to be quite enough. I mean if OP spots something illegal, he can take administrative measures and/or break security terminal. I don't see any reason for OP to take out stuff secretly.",
"i agree too, but sometimes it's unfair to break the terminal or hijack the storage to remove something from it, because if one player is faulty we shouldn't bother the whole team.",Bitter Frustration
I just hid my hacked items in a chest below my dirt floor.,
"i just thought about it: why not deliver the full access to operators based on their OP level? iirc command blocks and default opped players are only level 2, and the top level is level 4. For exemple on the officials, only the admins are level 4. could be a command based tool, like /toggleAEfullAccess that would grant it temporarily (not persistant to reboot) the access of the trusted level 4 operator",
"> Eh, I kind of agree, but also, read access seems to be quite enough. I mean if OP spots something illegal, he can take administrative measures and/or break security terminal. I don't see any reason for OP to take out stuff secretly.
Not always so simple, they can hide items inside bags, compressed chests or other storage items. Players are very creative and I've seen them cheat in more ways than I have words over the years.",
"ok, command based can be implemented somewhat naturally",
"> ok, command based can be implemented somewhat naturally
Couldn't AE just cache a list of people on the OP txt list at startup and then add their names/UUID however it works to the security terminal as joint highest perms? Or yea a command that allows us to add individual users to the list",
"### Your GTNH Discord Username
Colen
### Your Pack Version
2.1.2.1
### Your Proposal
See title, pretty self explanatory and useful for server admins trying to investigate. @repo-alt not sure how hard this would be?
### Final Checklist
- [X] I have searched this issue tracker and there is nothing similar already. Posting on a closed issue saying *I like this change please reconsider adding it* will prompt us to investigate and reopen it once we confirm your report.
- [X] I understand this change request may not attract enough attention and thus not be implemented.
- [X] I understand this change request may be rejected due to other community members think it's inappropriate.
- [X] I believe this feature would make the pack better.",
"We've multiple users using this without these issues and i'm missing a LOT of required information in this buigreport.
Hence I would suggest going through our support process before (re)opening this bugreport.
So i'm closing this for now, untill the support staff can verify your setup is, in fact, correct.
(including possible permission issues)",Impatience
"I made literally 1 change to the config: change VPN to disabled. It works as intended. That's literally the ONLY change I made. As for information, I've provided all the information requested, and I'm happy to provide more if you could tell me what you would like me to provide.",Impatience
reopened due to verification by @Heavybullets8,
"So it seems Wireguard is working just fine with hostpath, but OpenVPN is not. Below are both of my configurations.
**Here is the Hostpath I used for both Wireguard and OpenVPN**
![hostpath](https://user-images.githubusercontent.com/20793231/153766868-d858294c-2fd5-4b8f-84ad-e9c53f7561f7.PNG)
**OpenVPN configuration**
![ovpn1](https://user-images.githubusercontent.com/20793231/153766881-84193291-db02-4ed1-8c8d-c1dd675180c6.PNG)
**The Shell doesn't display a ""welcome"" message like it does with the Wireguard configuration you'll see below. Also no `/torrent` mountpath**
![ovpn2](https://user-images.githubusercontent.com/20793231/153766885-b034467b-abb4-46e7-9f9b-7ea91d84c3e5.PNG)
**Wireguard Configuration**
![wg1](https://user-images.githubusercontent.com/20793231/153767273-e4f03cb8-2133-4026-a4df-349a42452697.PNG)
**Wireguard Shell. with properly mounted `/torrent` as well as welcome message.**
![wg2](https://user-images.githubusercontent.com/20793231/153766977-0116ad66-5c71-4406-9cf2-a9749040dd23.PNG)",
"FWIW, I'm using OpenVPN and the HostPath part works fine. So, maybe this issue is triggered by some particular order of setting things up.
(I only landed here because I seem to have connectivity issues when using OpenVPN...)",
@indivisionjoe Can you confirm you are also doing that in qbittorrent?,Impatience
"This also happened on Prowlarr, I just don't give Prowlarr any mounts so it didn't effect me. I assume it's just a generic issue with the way TrueCharts does OpenVPN.",
I was not asking you @Nolij,Impatience
"> I was not asking you @Nolij
I understand that, just adding additional information as it's discovered",
"This also affects transmission:
[ticket-[1883677](https://discordapp.com/channels/830763548678291466/948718459066400798)](https://discordapp.com/channels/830763548678291466/948718459066400798)",
"> This also affects transmission:
> [ticket-[1883677](https://discordapp.com/channels/830763548678291466/948718459066400798)](https://discordapp.com/channels/830763548678291466/948718459066400798)
Thanks",
"Still seems to be an issue on my end, I just stumbled upon the problem while trying to add my NordVPN conf file to qBittorrent...
I get the following error:
`/usr/bin/openvpn.sh: line 347: H8Q: unbound variable`",Bitter Frustration
"Please do not +1
OBVIOUSLY it's still an issue because the issue isn't closed.",Bitter Frustration
"Just came here to add I am experiencing the same issue, with openvpn the filepath mount does not work. Only changes to default app settings are adding openvpn path and login credentials then adding hostpath mount point. App shell shows files are downloading into the internal container directory, but host path seems to be disconnected. EDIT: Had some issues that pertained to transmission app and this similar issue, erased for clarity. EDIT: Would like to add, the OpenVPN config IS working, running a bash from the App shell does show my VPN's IP not mine",Bitter Frustration
"I'm going to lock this and contratz with the timeout.
When a maintainer tells you to STFU with the +1 comments, you stfu with the +1 comments.",Bitter Frustration
"<!--
MIND YOUR TITLE:
- ""App in a deploying state"" is NOT the title/description of your actual bug.
- ""Appname not working"" is NOT the title/description of your actual bug.
- Don't refer to a version, bugs are always for latest version
-->
***SCALE version***
<!--
- the version of truenas scale you are currently running.
- FOUND IN: System settings > general > os version
-->
TrueNAS-SCALE-22.02-RC.2 (with RC2 patcher applied)
***App Version***
<!--
- the version of the app having the issue and versions of any apps being used in conjunction
- FOUND IN: apps > installed applications
-->
4.4.0_9.0.44
***Application Events***
<!--
- debug information from the app(s) specifically
- FOUND IN: apps > installed applications > (click on the app name) > click the carrot next to application events and include a screenshot here
-->
```
2022-02-13 10:16:47
Started container openvpn
2022-02-13 10:16:47
Created container openvpn
2022-02-13 10:16:34
Container image ""ghcr.io/truecharts/openvpn-client:latest@sha256:bc3a56b2c195a4b4ce5c67fb0c209f38036521ebd316df2a7d68b425b9c48b30"" already present on machine
2022-02-13 10:16:34
Started container qbittorrent-test
2022-02-13 10:16:33
Created container qbittorrent-test
2022-02-13 10:16:31
Container image ""tccr.io/truecharts/qbittorrent:v4.4.0@sha256:b96e8102193a3be4a85cbaba167e656ed9ad1b3d86f9df0dd94de805daab28f6"" already present on machine
2022-02-13 10:16:31
Started container inotify
2022-02-13 10:16:30
Created container inotify
2022-02-13 10:16:23
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-02-13 10:16:22
Started container autopermissions
2022-02-13 10:16:20
Started container lb-port-6880
2022-02-13 10:16:20
Created container lb-port-6880
2022-02-13 10:16:20
Created container autopermissions
2022-02-13 10:16:19
Started container lb-port-10096
2022-02-13 10:16:19
Created container lb-port-10096
2022-02-13 10:16:18
Started container lb-port-6882
2022-02-13 10:16:18
Created container lb-port-6882
2022-02-13 10:15:58
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:58
Add eth0 [172.16.28.157/16] from ix-net
2022-02-13 10:15:57
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-02-13 10:15:57
Add eth0 [172.16.28.156/16] from ix-net
2022-02-13 10:15:56
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:56
Add eth0 [172.16.28.155/16] from ix-net
2022-02-13 10:15:55
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:55
Add eth0 [172.16.28.154/16] from ix-net
Successfully assigned ix-qbittorrent-test/qbittorrent-test-db5cdb75b-bqqkb to ix-truenas
2022-02-13 10:15:40
Successfully provisioned volume pvc-1137f436-f444-4e56-89f6-9b41c9362217
0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims.
2022-02-13 10:15:39
Created pod: qbittorrent-test-db5cdb75b-bqqkb
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-torrentudp-9k8kl to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-torrentudp-9k8kl
2022-02-13 10:15:39
Scaled up replica set qbittorrent-test-db5cdb75b to 1
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-torrent-x66lm to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-torrent-x66lm
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-blmph to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-blmph
2022-02-13 10:15:39
waiting for a volume to be created, either by external provisioner ""zfs.csi.openebs.io"" or manually created by system administrator
2022-02-13 10:15:39
External provisioner is provisioning volume for claim ""ix-qbittorrent-test/qbittorrent-test-config""
```
***Application Logs***
<!--
- log output from the containers involved in the app
- FOUND IN: apps > installed applications > (click the 3 dots on the top right of the app box) > logs > open the logs for each pod used for the app and take a screenshot
- it can take a moment for the logs to show on the logs screen
-->
[qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_qbittorrent-test.log](https://github.com/truecharts/apps/files/8055909/qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_qbittorrent-test.log)
[qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_openvpn.log](https://github.com/truecharts/apps/files/8055910/qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_openvpn.log)
***Application Configuration***
<!--
- the configuration settings for the app (make sure to include what you have changed and what you didnt change
- if possible use the edit features in your screenshot tool to highlight the portions you cnaged intentionally
- FOUND IN: apps > installed applications > (click the 3 dots on the top right of the app box) edit
-->
![image](https://user-images.githubusercontent.com/16640162/153760346-a830501b-5186-45d1-ae14-54e72f7ba3a5.png)
![image](https://user-images.githubusercontent.com/16640162/153760372-578cd225-a8d6-410a-8188-27a43b3ffa90.png)
![image](https://user-images.githubusercontent.com/16640162/153760417-a031ae74-727b-4d95-90ac-86e64fbc0505.png)
(Synapse Data is a currently unused dataset I attached for testing purposes, my primary instance of this also experienced this on an almost identical configuration so I'm using a proxy, which is not as good for seeding, but nevertheless I wanted to keep it up while this is fixed)
**Describe the bug**
<!--
A clear and concise description of what the bug is.
-->
When using an OpenVPN connection (I suspect it is the same on WireGuard, but I have not tried that as I do not have access to a WireGuard VPN as of now) on a qBitTorrent app hostPath mounts are not mounted properly (also tried PVC, that didn't work either). The connection works (the logs show the VPN IP, not my IP), and adding and downloading torrents works, it just doesn't download them to my torrent dataset, making them inaccessible. This exact same configuration with VPN disabled works as intended, just without the VPN.
**To Reproduce**
<!--
- make sure that if you follow these steps again yourself the bug happens again. if it doesnt its probbly a configuration error and should be handled with a support thread on the discord
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error
-->
1. Create a qBitTorrent app
2. Configure a hostPath/PVC mount
3. Configure a valid OpenVPN connection
4. Configure the app so it should function (proper ports, etc.)
5. Start this app
6. To observe: open shell for the qBitTorrent container (not the OpenVPN container, although nothing appears there either). The hostPath/PVC mount will not be mounted.
**Expected behavior**
<!--
A clear and concise description of what you expected to happen.
-->
The configured mount should be there.
**Screenshots**
<!--
If applicable, add screenshots to help explain your problem.
-->
**Additional context**
<!--
Add any other context about the problem here.
-->",Impatience
To be frank: I've not seen any more reports of this in latest releases after the bug refactor and we never where able to correctly reproduce and/or bugtrace this either. Please create a new issue if this issue still exists.,
"Sorry, we are not able to clone repos and debug user's code.
Please, provide a jsfiddle that shows the issue. Closing until then.",Impatience
"Well that's unfortunate, because it really does happen. Here's a GPU profile:
![image](https://user-images.githubusercontent.com/1668207/153904707-571fc306-d648-4d95-a291-98270223555b.png)
The throttled start is using DodecahedronGeometry, the spiked usages is using BoxGeometry.
I'm not sure if this is reproducible in a service like JSFiddle, as it's not the same as running a local instance, but I might give it a shot.",
"> Disclaimer: code is a bit of a mess currently because I'm moving things around and trying to fix this issue.
We try to give you (and all the library users) clear code.
Would be nice if you could help us back by giving us clear code too.",Entitlement
"It's clear enough to see what's going on. If you want I can make a branch with the bare minimum (it would just have certain files removed).
I'm not sure why sharing a repo is an issue? It's quite literally the same as standing up a new one as a repro ;)",Impatience
"<!-- Ignoring this template may result in your bug report getting deleted -->
**Describe the bug**
I'm building a small game that uses some primitive shapes for things.
Whenever I use a PolyhedronGeometry (like DodecahedronGeometry or IcosahedronGeometry) in an instancedMesh, my GPU tanks to ~5FPS (depending on the amount of instances).
I've spent hours figuring out what is happening and yesterday it looked like `powerPreference: 'high-performance'` was the issue (disabling solved the issue), but today it's back again with the same issues.
So this might not be threejs directly, but rather a webgl problem?
it's very hard to reproduce, because in FireFox this doesn't happen, only in Chrome, on my 2016 MacBook pro, so I feel like it might even be the webgl implementation of Chrome itself.
Interestingly enough FPS are just fine with other geometries (like box, or cone, etc), just not any of the PolyhedronGeometry based ones.
**To Reproduce**
You can clone my repo (https://github.com/TheDutchCoder/atc-gjk) and on the `convert-to-native-js` branch you can run `npm run dev`.
If you see good FPS, please let me know which browser and OS you're using, because again I feel like this might even be a browser problem.
Disclaimer: code is a bit of a mess currently because I'm moving things around and trying to fix this issue.
***Code***
See repo.
***Live example***
N/A (see repo)
**Expected behavior**
When using a PolyhedronGeometry I should not see a huge drop in FPS.
**Screenshots**
![image](https://user-images.githubusercontent.com/1668207/153903237-5989947d-16c9-422a-b737-63fea8f00ab4.png)
Note that the performance stats seem normal
**Platform:**
- Device: Desktop, MacBook Pro 2016
- OS: MacOS 12.1
- Browser: Chrome 98.0.4758.80
- Three.js version: r137 (latest)",Bitter Frustration
I never paid attention to the possibility to assign a scope to a setting. What you suggest sounds reasonable. I guess other settings should also have a `resource` scope. We should try to make a list.,
"Maybe an inverse list is appropriate, too. I went through the list of settings
https://github.com/James-Yu/LaTeX-Workshop/blob/04f73108e439661faef2c8f85eac7baacbdb736b/package.json#L763-L2128
and to be honest, I had a hard time finding settings which should remain `window`-scoped. (I always wonder why that is the default - probably historic reasons). Everything related to editing or compiling a specific file (editor settings, syntax, tools, etc.) should be `resource`-scope IMHO.
The ones I found that *may* not be correct to scope by `resource` are the ones that may not be uniquely tied to a single document, such as everything related to (pre-)loading and configuring extensions used by multiple documents:
```
latex-workshop.view.pdf.*
latex-workshop.mathpreviewpanel.*
latex-workshop.hover.preview.mathjax.extensions
```",Bitter Frustration
"Notice that we have to rewrite the way of calling `workspace.getConfiguration`.
See https://github.com/Microsoft/vscode/wiki/Adopting-Multi-Root-Workspace-APIs#settings-api
> The configuration example above defines a setting scoped to a resource. To fetch its value you use the workspace.getConfiguration() API and pass the URI of the resource as the second parameter. You can see [Example 3](https://github.com/Microsoft/vscode-extension-samples/blob/master/configuration-sample/src/extension.ts) how the setting is used in the basic-multi-root sample.
- https://github.com/microsoft/vscode-extension-samples/blob/e52c48f22a81355b568f3cde8c60538726caad22/configuration-sample/src/extension.ts#L76-L96",
"My 2c comment on why `window` is the default scope. In VS Code, a window corresponds to an instance. This implies that only a single instance of an extension can run inside a window. Some settings require to reload the window after change typically because they are used in constructors (theirs values are fixed in the instance). All these settings must not have the `resource` scope. This is probably why the default scope is `window`.
Hence, making the list of settings that could have the `resource` scope is not so simple. All this requires a lot of care to avoid introducing very tricky bugs.",
"> Some settings require to reload the window after change typically because they are used in constructors (theirs values are fixed in the instance).
Thanks, understood. This is what I meant when I used the somewhat ill-defined term ""pre-loading"", so I fully agree that the implementation determines whether or not a setting can be `resource`-scoped, and not the uninitiated issue author browsing by the code ;)",Irony
"Here is the list of properties that might be assigned a `resource` scope
```
latex-workshop.latex.recipe.default
latex-workshop.latex.tools
latex-workshop.latex.magic.args
latex-workshop.latex.magic.bib.args
latex-workshop.latex.external.build.command
latex-workshop.latex.external.build.args
latex-workshop.latex.build.forceRecipeUsage
latex-workshop.latex.outDir
latex-workshop.latex.texDirs
latex-workshop.latex.verbatimEnvs
latex-workshop.kpsewhich.path
latex-workshop.kpsewhich.enabled
latex-workshop.latex.bibDirs
latex-workshop.latex.search.rootFiles.include
latex-workshop.latex.search.rootFiles.exclude
latex-workshop.latex.rootFile.useSubFile
latex-workshop.latex.rootFile.doNotPrompt
latex-workshop.latex.autoBuild.run
latex-workshop.latex.autoBuild.interval
latex-workshop.latex.autoBuild.cleanAndRetry.enabled
latex-workshop.latex.build.clearLog.everyRecipeStep.enabled
latex-workshop.latex.autoClean.run
latex-workshop.latex.clean.subfolder.enabled
latex-workshop.latex.clean.fileTypes
latex-workshop.latex.clean.command
latex-workshop.latex.clean.args
latex-workshop.latex.clean.method
latex-workshop.latex.option.maxPrintLine.enabled
latex-workshop.view.pdf.viewer
latex-workshop.view.pdf.tab.editorGroup
latex-workshop.view.pdf.tab.openDelay
latex-workshop.view.pdf.ref.viewer
latex-workshop.view.pdf.internal.synctex.keybinding
latex-workshop.view.pdf.external.viewer.args
latex-workshop.view.pdf.external.synctex.command
latex-workshop.view.pdf.external.synctex.args
latex-workshop.synctex.path
latex-workshop.synctex.afterBuild.enabled
latex-workshop.synctex.synctexjs.enabled
latex-workshop.chktex.enabled
latex-workshop.chktex.run
latex-workshop.chktex.path
latex-workshop.chktex.args.active
latex-workshop.chktex.args.root
latex-workshop.chktex.delay
latex-workshop.chktex.convertOutput.column.enabled
latex-workshop.chktex.convertOutput.column.chktexrcTabSize
latex-workshop.texcount.path
latex-workshop.texcount.args
latex-workshop.texcount.autorun
latex-workshop.texcount.interval
latex-workshop.intellisense.citation.type
latex-workshop.intellisense.citation.label
latex-workshop.intellisense.citation.format
latex-workshop.intellisense.file.exclude
latex-workshop.intellisense.file.base
latex-workshop.intellisense.label.keyval
latex-workshop.intellisense.unimathsymbols.enabled
latex-workshop.intellisense.package.enabled
latex-workshop.intellisense.package.extra
latex-workshop.intellisense.package.dirs
latex-workshop.intellisense.includegraphics.preview.enabled
latex-workshop.message.badbox.show
latex-workshop.message.bibtexlog.exclude
latex-workshop.message.latexlog.exclude
latex-workshop.message.information.show
latex-workshop.message.warning.show
latex-workshop.message.error.show
latex-workshop.message.log.show
latex-workshop.latexindent.path
latex-workshop.latexindent.args
latex-workshop.hover.ref.enabled
latex-workshop.hover.ref.number.enabled
latex-workshop.hover.citation.enabled
latex-workshop.hover.command.enabled
latex-workshop.hover.preview.enabled
latex-workshop.hover.preview.maxLines
latex-workshop.hover.preview.scale
latex-workshop.hover.preview.newcommand.parseTeXFile.enabled
latex-workshop.hover.preview.newcommand.newcommandFile
latex-workshop.hover.preview.cursor.enabled
latex-workshop.hover.preview.cursor.symbol
latex-workshop.hover.preview.cursor.color
latex-workshop.intellisense.optionalArgsEntries.enabled
latex-workshop.intellisense.useTabStops.enabled
latex-workshop.texdoc.path
latex-workshop.texdoc.args
```
I do not think it really makes sense for all of them. In particular, I believe everything related to `intellisense`, `hover`, `kpsewhich`, `message` and `view` should not be changed. That would give a much shorter list
```
latex-workshop.latex.recipe.default
latex-workshop.latex.tools
latex-workshop.latex.magic.args
latex-workshop.latex.magic.bib.args
latex-workshop.latex.external.build.command
latex-workshop.latex.external.build.args
latex-workshop.latex.build.forceRecipeUsage
latex-workshop.latex.outDir
latex-workshop.latex.search.rootFiles.include
latex-workshop.latex.search.rootFiles.exclude
latex-workshop.latex.rootFile.useSubFile
latex-workshop.latex.rootFile.doNotPrompt
latex-workshop.latex.autoBuild.run
latex-workshop.latex.autoBuild.interval
latex-workshop.latex.autoBuild.cleanAndRetry.enabled
latex-workshop.latex.build.clearLog.everyRecipeStep.enabled
latex-workshop.latex.autoClean.run
latex-workshop.latex.clean.subfolder.enabled
latex-workshop.latex.clean.fileTypes
latex-workshop.latex.clean.command
latex-workshop.latex.clean.args
latex-workshop.latex.clean.method
latex-workshop.latex.option.maxPrintLine.enabled
latex-workshop.synctex.path
latex-workshop.synctex.afterBuild.enabled
latex-workshop.synctex.synctexjs.enabled
latex-workshop.chktex.enabled
latex-workshop.chktex.run
latex-workshop.chktex.path
latex-workshop.chktex.args.active
latex-workshop.chktex.args.root
latex-workshop.chktex.delay
latex-workshop.chktex.convertOutput.column.enabled
latex-workshop.chktex.convertOutput.column.chktexrcTabSize
latex-workshop.texcount.path
latex-workshop.texcount.args
latex-workshop.texcount.autorun
latex-workshop.texcount.interval
latex-workshop.intellisense.file.exclude
latex-workshop.intellisense.file.base
latex-workshop.latexindent.path
latex-workshop.latexindent.args
latex-workshop.texdoc.path
latex-workshop.texdoc.args
```
Any thoughts on this?",
"> I believe everything related to `intellisense`, `hover`, `kpsewhich`, `message` and `view` should not be changed.
I wonder why that is. Is that motivated by the required implementation effort, or by your opinion that these should not be configurable by `resource` from a use-case perspective? As far as I have understood settings scopes, changing a `window`-scoped setting to a `resource`-scoped setting will not break anything at all for existing users - it will only open additional possibilities to move a setting into a resource-specific location. If the latter, I wonder what the motivation is to prevent users from, for example, using different `latex-workshop.intellisense.file.exclude` patterns for different documents, or from adapting `latex-workshop.intellisense.update.delay` for each document to account for different document sizes and thus processing times (just two examples, I might be able to think of more).",Impatience
I agree the list is reasonable. We can discuss further on a pull request. We might have to keep some of them window-scoped due to the implementation difficulty.,
"To make things crystal clear. Any properties not listed in my first list cannot be assigned a `resource` scope without heavy implementation changes, which we will not make. You mention `latex-workshop.intellisense.update.delay`, which was not in my fisrt and `latex-workshop.intellisense.file.exclude`, which I kept in the second one.
Because of the need to access the document `uri`, some of these properties may still be removed from the second list.
https://github.com/microsoft/vscode-extension-samples/blob/e52c48f22a81355b568f3cde8c60538726caad22/configuration-sample/src/extension.ts#L76-L96",
"I suggest to wait until we know more about - microsoft/vscode#143462
as this breaks our logging mechanism for configuration properties https://github.com/James-Yu/LaTeX-Workshop/blob/d8fae99c6a58b860250d5354085ca4fd0212c686/src/components/configuration.ts#L44",
"## Preliminary questions [Required]
### Disable all the other extensions except for LaTeX Workshop, restart VS Code, and check that you can not see the requested feature. [Required]
You still see this issue?: Yes
### Make sure to visit the [wiki FAQ](https://github.com/James-Yu/LaTeX-Workshop/wiki/FAQ) before requesting a feature.
You visited the wiki?: Yes
### If your requested feature is with compiling a document (not having to do with finding the root file of a project), check first that you can compile manually.
You can compile a TeX document manually?: Yes
## Is your requested feature related to a problem? Please describe. [Required]
I have two docs in a structure like this:
```
D:\BUG
â”‚ bug.code-workspace
â”‚
â”œâ”€â”€â”€doc1
â”‚ â””â”€â”€â”€.vscode
â”‚ settings.json
â”‚
â””â”€â”€â”€doc2
```
I open both docs in one VS Code window. This is what `bug.code-workspace` is for.
In `doc1`, I would like to set a few settings that I do not want to apply to `doc2`. This is what `doc1/.vscode/settings.json` is for, and it looks like this:
```json
{
""editor.wordWrapColumn"": 88,
""latex-workshop.latexindent.args"": [
""-c"",
""%DIR%/"",
""%TMPFILE%"",
""-y=defaultIndent: '%INDENT%'"",
""-l"",
],
}
```
So far, so good. However, this is not working because `latex-workshop.latexindent.args` is window-scoped, while `editor.wordWrapColumn` is resource-scoped. In line with that, the setting is greyed out and not applied:
![image](https://user-images.githubusercontent.com/12128514/153915001-83c233fe-c102-4f57-b00d-16e5d49028fb.png)
The reason is that the setting is not properly scoped - it has `window` scope:
https://github.com/microsoft/vscode/blob/6136c815bc6e7b99ec2dac56dccb3869574d2dd8/src/vs/workbench/contrib/preferences/browser/preferencesRenderers.ts#L596-L603
And the reason for that is that it does not have an explicit scope at all, so it falls back to `window` - see https://code.visualstudio.com/api/references/contribution-points:
https://github.com/James-Yu/LaTeX-Workshop/blob/04f73108e439661faef2c8f85eac7baacbdb736b/package.json#L1810-L1822
This is certainly not intentional - I see no reason for this setting to (necessarily) be the same for all resources in a window. It should be able to differ by resource (folder).
## Describe the solution you'd like [Required]
The fix is simple: set the scope for `latex-workshop.latexindent.args` (and probably other settings as well) to `resource`. Happy to provide a PR if that helps.",Bitter Frustration
"As a continuation, here is that truth:
```
# command-line-arguments
./ispapp-go-client.go:236:20: uc.(*net.TCPConn).NetConn undefined (type *net.TCPConn has no field or method NetConn)
Andrews-MacBook-Pro:ispapp-go-client zip$ git diff
diff --git a/ispapp-go-client.go b/ispapp-go-client.go
index 2a188b3..216e993 100644
--- a/ispapp-go-client.go
+++ b/ispapp-go-client.go
@@ -233,7 +233,7 @@ func new_websocket(host *Host) {
fmt.Println(reflect.TypeOf(uc))
fmt.Printf(""%+v\n"", uc)
- //uc.conn.SetKeepAlive(true)
+ uc.(*net.TCPConn).NetConn().SetKeepAlive(true)
// set host.WanIfName
var ipaddrstr, port, iperr = net.SplitHostPort(c.LocalAddr().String())
```",
The program is still not working as documented.,Bitter Frustration
"Here is proof `net.TCPConn` has a SetKeepAlive method.
https://cs.opensource.google/go/go/+/refs/tags/go1.17.7:src/net/tcpsock.go;l=159",
"The program is still not working as documented.
This is a bug in Go. Fix it or leave it open as the truth.",Bitter Frustration
"This is how it actually works.
```
uc.(*net.TCPConn).SetKeepAlive(true)
```",
"I need you to allow me to resolve the issue you incorrectly closed and locked me out of.
People need to know how to do this, every recent Apple laptop has power darkmode and it works with keep alive flagged tcp packets.",Bitter Frustration
"Glad you were able to solve the issue with net.TCPConn assertion. For the record, I suggested a generic `tls.Conn` assertion because in the first post you were asking about `tls.Conn.NetConn( )`. Of course it's up to you to decide if that makes sense in your code.",
"https://github.com/golang/go/issues/51274
You are supposed to be able to type-assert or cast a `net.Conn` to a `net.TCPConn`.
The go team member created a situation where he lied about that, then said there was no issue.",Bitter Frustration
"`DecodeString` is (b64 encoded string) -> (unencoded []byte).
The reverse is `EncodeToString`, which does (unencoded []byte) -> (b64 encoded string).
I am not sure what else you are looking for.",
"Is the function `EncodeToString` the behavior you want, and you think it should be named `EncodeString`?",
"What is the exact API that you are proposing?
We currently have
```Go
// DecodeString returns the bytes represented by the base64 string s.
func (enc *Encoding) DecodeString(s string) ([]byte, error)
// EncodeToString returns the base64 encoding of src.
func (enc *Encoding) EncodeToString(src []byte) string
```
What are you suggesting that we add? Thanks.",
"@ianlancetaylor Do you not understand that the input arguments to those functions are different?
If there is an Encode function there should be a Decode function with the same name structure and:
* the same input argument types
* the same output argument types
I realize what happened, someone was building it and needing to encode a byte array to a string so they wrote that function, then needed to decode a string to a byte array so they wrote that function.
When people read the documentation first, they are confused by the lack of a common naming structure and input/output type structure.
That should be cleaned and reorganized so that when people are working and reading the 1000's of Go functions (certainly impossible to remember all) they will not get frustrated by constant flipping between string and []byte and a lack of a naming structure that can be quickly remembered because it adheres to both common function names per functionality and input/output type.",Bitter Frustration
"You must understand, your company can afford to pay people to do things like that and have a team leader structure that respects who actually made things.
That's why you have things like Gmail and Go, because you let people work and then lead other people to finish their work, not tried to confuse them with constant changes that no single person understands.
It was the same way at Ford, it was the same way at Chevy, it is the same way at Boeing, Lockheed Martin, etc....",Bitter Frustration
"@andrewhodel Help us out a bit if you want us to consider your change. Please tell us exactly what method you want added. You've more than adequately defined the motivation - but we're still not clear on what exactly the changes you're proposing are. Write us a prototype of the function/method you want, and a 1-2 sentence godoc description of what it does. Thanks.",Impatience
"@randall77 `EncodeString()` and `DecodeString()` should both exist because one of them does and the concept of encoding/decoding is fundamentally bound together.
I didn't read every function, but you should and you should adhere this naming structure and ensure that all relevant sets exist.
In other words
`Encode([]byte)` means there should be `Decode([]byte)`
`function_1_group_with_3_total()` should include `function_2_group_with_3_total()` and `function_3_group_with_3_total()` and so on. For example, with hash functions there is usually a standard naming format due to the required functions, that of `input`, `parse`, `decode`, `inputMore` etc.
Then when someone is working for example with the encoding/base64 library, they can set a number of days or hours aside to work with the library then get used to the functions. In other words, they can type Encode and Decode and EncodeString and DecodeString enough times to not need to deal with remembering arguments to every single function instead only needing to remember the top level function names as they all follow a common naming pattern.",Impatience
"Ok, but you still haven't answered my question - what exactly are you proposing to add to `encoding/base64`?",Impatience
"@randall77 I answered your question in the first sentence of the original reply to your original comment/question.
EncodeString() and DecodeString() should both exist because one of them does and the concept of encoding/decoding is fundamentally bound together.
These are functions in Go of the encoding/base64 library. One exists, one doesn't. Again, these are functions in Go.",Impatience
"@andrewhodel That is not an answer to the question that @randall77 asked and it is not an answer to the question I asked.
I will repeat:
We currently have
```Go
// DecodeString returns the bytes represented by the base64 string s.
func (enc *Encoding) DecodeString(s string) ([]byte, error)
// EncodeToString returns the base64 encoding of src.
func (enc *Encoding) EncodeToString(src []byte) string
```
What are you suggesting that we add? Please write it in the exact form that I wrote it. This is a programming language. Precision matters. Please write down exactly what you mean. Don't describe what you mean in words. Write it in Go. Thanks.",Bitter Frustration
"```
func (enc *Encoding) EncodeString(s string) ([]byte, error)
```
You should also read and make sure all the other pairs exist for encoding and decoding of, with respect to both input and output:
* byte arrays
* strings
If you want me to write it, then provide commit access.",Impatience
"You can write any change you like without commit access, as described at https://go.dev/doc/contribute. Thanks.",
"@ianlancetaylor then make:
@ianlancetaylor @prattmic @findleyr @cherrymui All work that way for every commit or stop with the absurdity.",Bitter Frustration
We do all work that way for every commit. Why do you think we don't?,Bitter Frustration
"@ianlancetaylor That's not true, you push changes directly to the repository.",Bitter Frustration
"@andrewhodel That is not correct. Why do you think it is?
All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.",
"You are adjusting time and commits to dev while calling it changes. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 10:58 AM, Ian Lance Taylor ***@***.***> wrote:
> > > @andrewhodel That is not correct. Why do you think it is?
> > All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
> > What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.
> > â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",Bitter Frustration
"> You are adjusting time and commits to dev while calling it changes.
I'm sorry, I don't understand what you mean.",Impatience
"I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:06 AM, Andrew Hodel ***@***.***> wrote:
> > You are adjusting time and commits to dev while calling it changes. > > Thank You,
> Andrew Hodel
> >>> On Feb 22, 2022, at 10:58 AM, Ian Lance Taylor ***@***.***> wrote:
>>> >> >> @andrewhodel That is not correct. Why do you think it is?
>> >> All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
>> >> What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.
>> >> â€”
>> Reply to this email directly, view it on GitHub, or unsubscribe.
>> You are receiving this because you were mentioned.",
"I don't see why `EncodeString` should return an error. The other `Encode` functions do not. Base64 encoding and decoding are not symmetric in that encoding should always succeed, whereas decoding might not.
Other than that, seems like a reasonable function to have. Do you have a situation where you have a `string` source and wish to encode it to a `[]byte` output? Just wondering if this is a function you actually ran into a need for, or if you just were confused about the lack of a symmetric API.",
"In any case, thanks for defining precisely what you suggest that we add (although you omitted the doc comment). I gather you are thinking of something along the lines of
```Go
// EncodeString returns the base64 encoding of s.
func (enc *Encoding) EncodeString(s string) ([]byte, error) {
buf := make([]byte, enc.EncodedLen(len(s)))
enc.Encode(buf, []byte(src))
return buf, nil
}
```
I think the argument that you are making is that this is parallel to the existing `DecodeString` method.
How often does a need for this method come up in practice? Do you have any examples of code that would use it? Thanks.",
"Start counting them you will find out. It is basic manipulation of words causing the new words to exist. If you count the number of times someone commits vs the changes approved in that list you will get it. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:07 AM, Ian Lance Taylor ***@***.***> wrote:
> > > You are adjusting time and commits to dev while calling it changes.
> > I'm sorry, I don't understand what you mean.
> > â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",
"Lack of symmetry. Yes you need an error, what if the memory bound is reached while reading?
Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:09 AM, Keith Randall ***@***.***> wrote:
> > > I don't see why EncodeString should return an error. The other Encode functions do not. Base64 encoding and decoding are not symmetric in that encoding should always succeed, whereas decoding might not.
> > Other than that, seems like a reasonable function to have. Do you have a situation where you have a string source and wish to encode it to a []byte output? Just wondering if this is a function you actually ran into a need for, or if you just were confused about the lack of a symmetric API.
> > â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",
"> I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people.
OK, great. Do you want to try to prove it to me?
I mean, I know that I am telling you the truth about our code review process. I don't understand what you could do to demonstrate that I am not telling the truth. But I am mildly curious as to where the misunderstanding arises.",Impatience
"> Yes you need an error, what if the memory bound is reached while reading?
If a Go program runs out of memory, it will crash. There is no way for a function to return an error because it has run out of memory.",
"> If you count the number of times someone commits vs the changes approved in that list you will get it.
Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?",Impatience
"Again. This is why other libraries have per iteration input functions, for reading streams etc. That isnâ€™t to say that you could read the input string length, then read the available memory and know what difference the encoding process will require (bytes per chunk for input and output + anything else) and return an error when that is greater than n*available. That is how things work. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:13 AM, Ian Lance Taylor ***@***.***> wrote:
> > > If you count the number of times someone commits vs the changes approved in that list you will get it.
> > Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?
> > â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",Bitter Frustration
"It is because you arenâ€™t respecting the development branch as a place to read changes before versioned approval. In other words, why do you have release candidates?
Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:12 AM, Ian Lance Taylor ***@***.***> wrote:
> > > I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people.
> > OK, great. Do you want to try to prove it to me?
> > I mean, I know that I am telling you the truth about our code review process. I don't understand what you could do to demonstrate that I am not telling the truth. But I am mildly curious as to where the misunderstanding arises.
> > â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",Bitter Frustration
"These truths of memory testing (especially when it is very low resource usage to do so) are similar to network programming in a day and age that testing is sold for entertainment as blocking certain tcp packets against protocol. In other words it has to be dealt with anytime network traffic is greater than the packet MTU so may as well exist at the memory level. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:23 AM, Andrew Hodel ***@***.***> wrote:
> > Again. This is why other libraries have per iteration input functions, for reading streams etc. > > That isnâ€™t to say that you could read the input string length, then read the available memory and know what difference the encoding process will require (bytes per chunk for input and output + anything else) and return an error when that is greater than n*available. > > That is how things work. > > > > Thank You,
> Andrew Hodel
> >>> On Feb 22, 2022, at 11:13 AM, Ian Lance Taylor ***@***.***> wrote:
>>> >> >> If you count the number of times someone commits vs the changes approved in that list you will get it.
>> >> Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?
>> >> â€”
>> Reply to this email directly, view it on GitHub, or unsubscribe.
>> You are receiving this because you were mentioned.",Impatience
"The current helper methods assume that base64 is applied to binary data, which is almost always a []byte.
And it is applied to create text data, which is often a string.
So the API uses []byte for the binary (unencoded) data, and string for the text (encoded) data.
This has worked very well in practice. There is no obvious reason we should add the other pairs, which would at the very least confuse me when reading the API.",
"Based on the discussion above, this proposal seems like a **[likely decline](https://golang.org/s/proposal-status#likely-decline)**.
â€” rsc for the proposal review group",
"I guess every middleman your Go servers work with support perfect utf8.
I need to send data to things so old that anything beyond 127 bits is out of the question so they can pass it on and then without base64 what are you going to do?
I know, another go type modification is just what the doctor called for!",Bitter Frustration
"@rsc it's simple, you have a string and a validation function that ensures that string is ""at protocol"" meaning hasn't the distant device doesn't even fully support ASCII, then you need to pass it through that distant device so you base64 encode the validated string and send it.
What you are saying isn't real, there's no where on earth where an operating system requires code to not using bits and those work with Go servers that use strings for simplicity.",Bitter Frustration
@andrewhodel I think it would help a lot if you made an example on https://go.dev/play/ to show your use-case and why this isn't trivially covered by the existing API.,Impatience
"```
// working with string
var s = ""asdfStringWithoutUnicodeMoreThanOneTwentySeven"";
function_without_string_support([]byte(s));
```
Code reviewer: ""That is messy code, there are too many type modifications"".
I guess they will never figure it out, oh well *poof*.",Bitter Frustration
"No change in consensus, so **[declined](https://golang.org/s/proposal-status#declined)**.
â€” rsc for the proposal review group",
"The DecodeString() method exists as a single function, an EncodeString() method should be added as it will not break forward compatibility and conform to a standard naming structure and module offering.
https://pkg.go.dev/encoding/base64
This is not a duplicate of a bug, I was asked to make it a proposal regardless of it being a bug or proposal. This is a proposal in writing to a software repository.",
![Platform: git--PaperSpigot--445%20%28MC%3A%201.8.8%29](https://img.shields.io/badge/Platform-git--PaperSpigot--445%20%28MC%3A%201.8.8%29-3498db?style=flat-square) </br>![ViaVersion (4.1.1): 15 commits behind master](https://img.shields.io/badge/ViaVersion%20(4.1.1)-15%20commit(s)%20behind%20master-yellow?style=flat-square) </br>,
"Just a little side note, I also made an issue on ProtocolLib to see if they are willing to improve/fix their injector too. https://github.com/dmulloy2/ProtocolLib/issues/1522",
"Absolutely agree, I had the same problems, Pipeline is not intended to work like ViaVersion actually does and packet transformations should be done on a separate channel.",
Update: ProtocolLib is working on rewriting their injector. So hopefully ViaVersion can do some adjustments too.,
Working on fixing Viaâ€™s injector in my fork: https://github.com/retrooper/ViaVersion,
"It's long been done, but we're not going to change the injection method until the next major ViaVersion release, sorry.",Impatience
Whenâ€™s that. And howcome,Impatience
"### `/viaversion dump` Output
https://dump.viaversion.com/f34803f4498497ce3b5d63c445f8ecbb6b3ce5f586662d643faef3e4aff1c447
### Console Error
Check bug description
### Bug Description
ViaVersion's spigot injector decides to wrap minecraft's vanilla encoder. I'm trying to make my own injector. I want to process buffers, that aren't translated by ViaVersion yet. It would be better if ViaVersion had a separate outgoing handler which were called after the vanilla minecraft encoder. Your handler can expect bytebufs and just translate them like that. Let the vanilla encoder do its job by translating NMS packets to bytebufs for you to process. This would make it so easy for any other packet plugin to just get in between ViaVersion and the vanilla encoder.
Similarly can be said about the decoder. You guys just wrap the minecraft vanilla decoder... I want to process packets right after ViaVersion translates incoming bytebufs. You could simply put ViaVersion's decoder infront of vanilla's decoder. Then I can just get inbetween and process my bytebufs there. ViaVersion isn't the only packet plugin out there, and I've also seen other developers finding making an injector a struggle. I managed to get a ""working"" decoder, but its really really not great and doesn't work great if you can't predict how many packet projects are running on the server. You'd also be saving yourself redundant reflection calls to vanilla handlers?
Additionally with what ProtocolLib does, calling ""encoder"" right after it processes the first NMS object... that just doesn't make making an injector easier.
I just want to know, is there a reason you did not do this? Seems like it would just be easier to handle things like that, worst case scenario if you guys don't want to fix your injector, I could do it.
### Steps to Reproduce
Check bug description
### Expected Behavior
ViaVersion has separate handlers (their own encoder and decoder) on Spigot. Right now they wrap the vanilla ones, and it makes it difficult to get inbetween. Especially if you aren't assuming how many pipeline injecting projects will be running on a server. ### Additional Server Info
_No response_
### Checklist
- [X] Via plugins are only running on **EITHER** the backend servers (e.g. Paper) **OR** the proxy (e.g. BungeeCord), **not on both**.
- [X] I have included a ViaVersion dump.
- [X] If applicable, I have included a paste (**not a screenshot**) of the error.
- [X] I have tried the latest build(s) from https://ci.viaversion.com/ and the issue still persists.",Bitter Frustration
Any ETA on this?,Impatience
I don't see why this has the Legacy label attached? But I'm still waiting.,Impatience
"> I don't see why this has the Legacy label attached? But I'm still waiting.
https://www.nei.nih.gov/learn-about-eye-health/healthy-vision/finding-eye-doctor",Impatience
"This is caused by a too restrictive language selector
https://github.com/James-Yu/LaTeX-Workshop/blob/2bc2c15932d5932494889d561a4709e920ef7c4b/package.json#L2150-L2155
We should be using `editorLangId =~ /latex|latex-expl3|rsweave|jlweave/` almost everywhere instead.",Bitter Frustration
"I am strongly against adding `latex-expl3`. `LaTeX-expl3` is a programming language to implement LaTeX packages. We must not mislead users to use it to write documents. It would be harmful to the LaTeX ecosystem.
> This system is being used as the basis for TeX programming within The LaTeX Project. Note that the language is not intended for either document mark-up or style specification. Instead, it is intended that such features will be built on top of the conventions described here.
- http://mirrors.ctan.org/macros/latex/contrib/l3kernel/expl3.pdf
- https://ctan.org/pkg/l3kernel",
Please don't lock and close the issue so early without a thorough discussion.,Impatience
"> I am strongly against adding `latex-expl3`. `LaTeX-expl3` is a programming language to implement LaTeX packages. We must not mislead users to use it to write documents. It would be harmful to the LaTeX ecosystem.
I do not feel at ease telling users what they should or should not do regarding the LaTeX ecosystem. I do not see why we should not make it possible for users of `LaTeX-expl3` to build their documents and use `synctex`.",
"I might be confused about what `LaTeX-expl3` means. If it means a programming language `expl3`, we should not make it possible for users to build their documents with the language. If it means a markup language `LaTeX` where `expl3` commands are also supported, it would be reasonable to make it possible.
For the latter case, `LaTeX3` would be a more appropriate name. I am not sure the name `LaTeX3` is still used. > The LaTeX Project (aka LaTeX3 Project)
- https://www.latex-project.org/latex3/
And, `expl3` commands on suggestions should be optional. For usual users, they don't have to write `expl3` commands.
https://github.com/James-Yu/LaTeX-Workshop/blob/75bc0d6c93819c441172e4b1a2c03f3ca02d784f/src/providers/completer/command.ts#L220-L225",
"The language id `LaTeX-expl3` was designed to be used for LaTeX documents where `expl3` commands are also supported. The name `latex3` may be more appropriate, we could rename `latex-expl3`. The main reason for defining a new language id was that we needed to extend the standard grammar to support the new `LaTeX3` syntax.
See #2071 for a discussion of what we should list as suggestions. I am tempted to leave it as it is for now and wait for some feedbacks.",
"> The language id LaTeX-expl3 was designed to be used for LaTeX documents where expl3 commands are also supported. Then, at least, we had better call `extraPackages.push('latex-document')` too even if `languageId === 'latex-expl3'`
https://github.com/James-Yu/LaTeX-Workshop/blob/75bc0d6c93819c441172e4b1a2c03f3ca02d784f/src/providers/completer/command.ts#L220-L225",
"> Then, at least, we had better call `extraPackages.push('latex-document')` too even if `languageId === 'latex-expl3'`
Yes, I agree.",
"### Discussed in https://github.com/James-Yu/LaTeX-Workshop/discussions/3170
<div type='discussions-op-text'>
<sup>Originally posted by **syvshc** February 28, 2022</sup>
## Preliminary questions [Required]
When I switch language to `tex` or `LaTeX-expl3`, the button of `view pdf` disappears and snippet of `synctex from cursor` doesn't work. ### Disable all the other extensions except for LaTeX Workshop, restart VS Code, and check that you still see this issue. [Required]
You still see this issue?: Yes
### Make sure to visit the [wiki FAQ](https://github.com/James-Yu/LaTeX-Workshop/wiki/FAQ) before filling an issue.
You visited the wiki?: Yes
### If your issue is with compiling a document (not having to do with finding the root file of a project), check first that you can compile manually.
You can compile a TeX document manually?: Yes
## Describe the bug [Required]
A clear and concise description of what the bug is
When I switch the format into `TeX` or `LaTeX-expl3`, there is no button of `View LaTeX PDF File`. ### To Reproduce
Steps to reproduce the behavior:
1. Open a `.tex` file
2. Compile it with `-synctex=1` option
3. switch the format between `LaTeX` and `LaTeX-expl3` and see difference. ### Expected behavior
My expected behavior is that ""view pdf"" button is still there and snippet of synctex works
## Logs [Required]
Please paste the whole log messages here, not parts of ones. The log should start with `Initializing LaTeX Workshop`. It is very important to identify problems.
[lw.log](https://github.com/James-Yu/LaTeX-Workshop/files/8150575/lw.log)
### LaTeX Workshop Output [Required]
```
To access the log, click the 'TeX' icon on the Activity Bar on the left side, select 'View Log Messages', then select 'View LaTeX Workshop extension log'.
[Paste the log here]
```
It is no business about compiling, just SyncTeX. ### Developer Tools Console [Required]
```
To access the log, click 'help' -> 'Toggle Developer Tools' -> 'Console'. Paste anything suspicious.
[Paste the log here]
```
![image](https://user-images.githubusercontent.com/38098591/155923589-c9cfc763-308b-4e03-ae9d-8fa832af5ee3.png)
## Screenshots
If applicable, add screenshots to help explain your problem.
I recorded a video to describe my problem, hope it is clear
https://user-images.githubusercontent.com/38098591/155933189-bde21561-8b0f-48c2-ae72-815ad2fffbdd.mp4
## Desktop [Required]
Please write exact version numbers. Please don't write `latest` instead of exact numbers.
- OS: Arch linux 5.16.11
- VS Code version: 1.64.2
- Extension version: 8.23.0
- TeX distribution version: TeXLive 2021
## Additional questions
### Are you using VSCodium?
No
### Are you using the Snap or Flatpack versions of VS Code?
No
### Are you using LaTeX Workshop with VS Code Remote?
No
</div>",
It violates OSS philosophy. Rubygems.org have no plan to ban Russia.,Bitter Frustration
"Hi Team, thank you for all the work you do for the RubyGems to be accessible by everyone!
I'm from Ukraine and Russian army is bombarding our cities and killing our children, doing acts of terror.
And Russian IT stays neutral because ""it's not them who kill people"". Please help us to raise their awareness by signing this petition and restricting any traffic from Russia for some time:
https://github.com/stop-war-in-ukraine/stop-russia-it",Bitter Frustration
"Files identified in the description:
None
If these files are incorrect, please update the `component name` section of the description or use the `!component` bot command.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/devel/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",
"@labeldevops thanks for reporting this. The redirects were there, though from a quick test of a few pages they seem to all be missing.
Please be assured we do try and make sure that we don't break search results or old bookmarks. For example, a lot of work went it to ensuring that old module urls still work in the new world of collections.
Once this has been fixed we will add some tests against the production webserver to ensure these continue to function.",
"@labeldevops please keep in mind that Ansible has a Community Code of Conduct. Your original issue filing verbiage violates the spirit of the [code of conduct](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html#code-of-conduct). In addition, I personally know the docs individuals who work very hard on their work and can attest this is not due to a lack of empathy to the readers experience, the docs team cares a lot about the experience of reading their documentation. These docs, as you pointed out, are written by humans and humans make mistakes. There is a way to bring issues up about the experience you're having as a reader without attacking people who are just doing their job the best they can.
Be kind.",
"### Summary
I see 404s in your docs - it looks like somebody **removed** URLs to important doc entries like e.g. http://docs.ansible.com/ansible/playbooks.html
but then **did not introduce a redirect** from the old doc entry to some new one, so that people are seeing 404s.
This is a very bad documentation practice and should never be done in any project. Yes, **REDIRECTS** exist for exactly this purpose. For an important infrastructure project it is expected that documentation writers understand these concepts. The underlying issue here is not just a simple 404 - it seems like the documentation writing person is not informed about the existence of redirects and believes that it is OK to just remove URLs from the internet - this should be made clear on day one for anybody doing this kind of work: it is NOT!
Also we see here a complete lack of understanding and empathy for the readers situation, what is another very important skill for documentation writers that must be required for such a job position. The person who introduced this problem did not think for one second about how annoying and disctracting it is to find a broken URL while the reader is trying to figure out some technical details. This all can be fixed on a technical level with simple redirects - but it needs to be fixed on a human level, the person who did this needs to learn about the importance of respecting the reading experience, this is of course true for the project manager. This is not acceptable for such a top level project.
### Issue Type
Documentation Report
### Component Name
http://docs.ansible.com/ansible/playbooks.html
### Ansible Version
```console
$ ansible --version
```
### Configuration
```console
$ ansible-config dump --only-changed
```
### OS / Environment
INTERNET
### Additional Information
NO MORE INFO
### Code of Conduct
- [X] I agree to follow the Ansible Code of Conduct",Bitter Frustration
Closing as there are redirects already in place. If there are specific problems please open an issue for the specific redirect problem and the docs team will work on implementing it. The collection docs restructure was a pretty big split and while the docs team tried to cover everything things did slip through the cracks.,
https://github.com/reactjs/reactjs.org/issues/4433#issuecomment-1060759889,Bitter Frustration
"Are you the running dogs of the United States? As a technical party, why do you stand in line",Identify Attack/Name Calling
https://github.com/reactjs/reactjs.org/issues/4433#issuecomment-1060759889,Bitter Frustration
"Are you the running dogs of the United States? As a technical party, why do you stand in line with Ukraine",Identify Attack/Name Calling
"The [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/), which outlines the expectations for community interactions in and around docs.microsoft.com, is designed to help ""provide a welcoming and inspiring community for all."" The content of this issue appears to be out of sync with the code of conduct, so we have closed it.
Also, this post has been edited due to Code of Conduct violations.",
"[Enter feedback here]
---
#### Document Details
âš  *Do not edit this section. It is required for docs.microsoft.com âžŸ GitHub issue linking.*
* ID: 70fbb1c8-1435-e762-8824-78cf96faed84
* Version Independent ID: 062879b5-72cf-2353-63f2-d862fe1b40ba
* Content: [Microsoft Docs contributor guide overview - Contributor Guide](https://docs.microsoft.com/en-us/contribute/)
* Content Source: [Contribute/index.md](https://github.com/MicrosoftDocs/Contribute/blob/main/Contribute/index.md)
* Product: **non-product-specific**
* GitHub Login: @Jim-Parker
* Microsoft Alias: **jimpark**",
Chalk isn't in your dependencies...?,Impatience
"@Qix Chalk is in my dependencies. I only posted my devdependencies. I am happy to post my dependencies but, chalk is in there. As I said, chalk works when I run my app, for whatever reason ESLint is throwing the error above.",Bitter Frustration
"You should open an issue with ESLint then, not sure what we could even do to fix that.",Impatience
"@Qix- it looks like the issue is because `""main"": ""./source/index.js"",` is missing from `package.json`.
```json
""repository"": ""chalk/chalk"",
""funding"": ""https://github.com/chalk/chalk?sponsor=1"",
""type"": ""module"",
""main"": ""./source/index.js"",
""exports"": ""./source/index.js"",
```
Once `main` is added to `package.json` linting passes. The linting error pops up due to package [import-js/eslint-plugin-import](https://github.com/import-js/eslint-plugin-import). There is an open issue [Rule import/no-unresolved raise false positive when there is no main field in the package.json #2132](https://github.com/import-js/eslint-plugin-import/issues/2132) but the maintainer of that package is adamant that proper etiquette is to include both `main` and `exports` in `package.json`. [Main entry point export](https://nodejs.org/api/packages.html#packages_main_entry_point_export) has also been referenced as justification for this requirement from the NodeJS website.
Is this something you would be willing to add or would you prefer me to submit a PR with a fix? (I am new to this)",
"@sindresorhus knows more about this. package.json is a mess to be honest, he's more up to date about what should go in it. I'll defer to him.",Bitter Frustration
"`main` can be useful when you have backwards compatibility with CommonJS. Chalk does not support CommonJS, so it makes no sense to specify it when `exports` is the proper way to define entry points.",
"> but the maintainer of that package is adamant that proper etiquette is to include both main and exports in package.json
That argument is irrelevant as `eslint-plugin-import` must support the `exports` field regardless as that field has many more capabilities that `main` cannot reflect.",Bitter Frustration
@sindresorhus i don't think the personal attack is called for nor appropriate.,
@ljharb @sindresorhus I am hopeful we can look past the personal attack and come together on a solution. Surely if we were all in the same room having ðŸ» we would be talking to each other differently.,
"@ljharb This is not productive. I simply said you're being difficult, which you are. You chose to spend the whole thread at `eslin-plugin-import` bad-mouthing my work and choices instead of just saying that `exports` will be supported at some point.",Bitter Frustration
"But fine, I will add the `main` field to help users.",Impatience
https://github.com/chalk/chalk/releases/tag/v5.0.1,
"I am using the latest version of Chalk and using ESLint with the Airbnb Base. Unfortunately, when I import chalk, am getting the following linting error: **Unable to resolve path to module 'chalk'. eslint([import/no-unresolved](https://github.com/import-js/eslint-plugin-import/blob/v2.25.4/docs/rules/no-unresolved.md]))**
The module loads and works fine but it is throwing that linting error and it is driving me nuts.
`package.json` for DevDependencies and ESLint config:
```json
""devDependencies"": {
""@typescript-eslint/eslint-plugin"": ""^5.13.0"",
""@typescript-eslint/parser"": ""^5.13.0"",
""eslint"": ""^8.10.0"",
""eslint-config-airbnb-base"": ""^15.0.0"",
""eslint-config-prettier"": ""^8.5.0"",
""eslint-plugin-import"": ""^2.25.4"",
""eslint-plugin-prettier"": ""^4.0.0"",
""nodemon"": ""^2.0.15"",
""prettier"": ""^2.5.1"",
""typescript"": ""^4.6.2""
},
""eslintConfig"": {
""extends"": [
""airbnb-base"",
""plugin:@typescript-eslint/recommended"",
""plugin:prettier/recommended""
],
""env"": {
""node"": true
}
},
```
`app.js` where I call Chalk:
```js
import express, { json, urlencoded } from 'express';
import helmet, { contentSecurityPolicy } from 'helmet';
import morgan from 'morgan';
import Debug from 'debug';
import chalk from 'chalk';
console.log(`${chalk.red('Hello!'});
```
I could use some guidance as to why it's throwing this for Chalk. I am not getting that error for my other Packages. Thanks!",Bitter Frustration
"@michealzh I had to relocate my family(3months baby girl, 4.5yo twins, and a wife) from the place where I lived in my house to the west of Ukraine.
I'm OSS developer whole my life, you can check my profile here as well as at https://www.drupal.org/u/podarok
And I must say - you are making huge mistake by keeping yourself neutral and by trying to hide a war.
OSS must be the frontline of support any movement against aggression",Bitter Frustration
"War in Ukraine - is a war against the whole world. Ukrainians were viciously attacked at night, like hitler did it. Ukrainians defend freedom, freedom of choice, democracy for the whole world. They don't want a soviet union come back. It's not a secret if Ukraine fall - putin will attack Europe next. russia and belarussia has already started a hybrid war against the world. You can see tons of fake news and so on.
It's not a time to demonstrate neutral status. It's a time for support. As phrases like ""I'm out of politics"", ""I am neutral"" lead to a war. It's not acceptable to say I am neutral when russian troops are killing kids, civilians, bombing ukrainian cities.",Bitter Frustration
"BTW, I highly doubt you, @michealzh , are a good fit for Open Source Software.
Open - means open. Open access to the information.
Your activity is against Open Source principles. You are trying to close and hide.
I vote to ban you from GitHub",Insulting
"> BTW, I highly doubt you, @michealzh , are a good fit for Open Source Software. Open - means open. Open access to the information. Your activity is against Open Source principles. You are trying to close and hide.
> > I vote to ban you from GitHub
So should you represent othersâ€˜ opinion in OSS which is ""Open access to the information"" for everyone?
How Russian programmers will think if you ""StandWithUkraine""?
How Ukraine programmers will think if you ""StandWithRussian""?
Let's say there's a new war between china an us:
How Chiness programmers will think if you ""StandWithUS""?
How US programmers will think if you ""StandWithChina""?
Make OSS, not WAR.
But I won't ""vote to ban"" you, everyone may talk their own opinion freely at internet on behave of their own.
If you really really what to show your opinion, please not represent anyone else.",Insulting
"I think we just have different positions. Open source should not be political. War is always the pain of ordinary people. The information we receive is always what the upper level wants us to know. There are many statements about the origin of this war. Some countries cling to each other and even have a tendency to break the world peace situation. For example: restricting the scientific and technological development of other countries, implementing economic control, developing their own military forces around other countries and other hegemonic acts. All this is bound to turn some countries into cannon fodder and make ordinary people suffer.",Bitter Frustration
"> BTW, I highly doubt you, @michealzh , are a good fit for Open Source Software. Open - means open. Open access to the information. Your activity is against Open Source principles. You are trying to close and hide.
> > I vote to ban you from GitHub
lmao, why do people never do #StandWithPalestine ? more people got killed in Palestine because of Israel",Mocking
"> @michealzh
> > I had to relocate my family(3months baby girl, 4.5yo twins, and a wife) from the place where I lived in my house to the west of Ukraine.
> > I'm OSS developer whole my life, you can check my profile here as well as at https://www.drupal.org/u/podarok
> > And I must say - you are making huge mistake by keeping yourself neutral and by trying to hide a war.
> > OSS must be the frontline of support any movement against aggression
have u ever done the same thing for Palestine? #StandWithPalestine ? many people there got killed, or u just silent?",Bitter Frustration
"Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others. Opinions my own, feel free to disagree.",Bitter Frustration
The OSS is no need the WAR!!,Bitter Frustration
Up,
Up,
Up,
Up,
Up,
"> In commit #86244a3695fcaaac9c5ba4257a4314eae1c6d981 adding #StandWithUkraine, if it's supporting humanity. Why did you never add #StandWithPalestine. Many people in Palestine got killed by Israel army.
I think it's pretty simple.
The author of this commit is a hypocrite, he does not care about both Ukraine and Palestine, he just wants to be on the hype",Bitter Frustration
"Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others. Opinions my own, feel free to disagree.",Bitter Frustration
"In commit #86244a3695fcaaac9c5ba4257a4314eae1c6d981
adding #StandWithUkraine, if it's supporting humanity. Why did you never add #StandWithPalestine. Many people in Palestine got killed by Israel army.",Identify Attack/Name Calling
"since we stand with ukraine we should block all russian men, women, and children from using this.
if you refuse this PR, you are siding with Putin, and you will surely get bombed by ghost of kiev!
<!-- Please remember to select the appropriate branch:
For bug or doc fixes pick the oldest branch where the fix applies (e.g. `2.2` if the 2.2 LTS is affected, `1.10` if it is a critical fix that should be fixed in Composer 1, otherwise `main`)
For new features and everything else, use the main branch. -->",Threat
Double standards as they are used to doing.,Bitter Frustration
"Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others. Opinions my own, feel free to disagree.",Bitter Frustration
political exhibitionism,Irony
[Hateful nonsense rant redacted],Vulgarity
I would not recommend anyone use a composer phar file from an untrusted source.. Do so at your own risk.,
"As shown in https://github.com/composer/packagist/commit/86244a3695fcaaac9c5ba4257a4314eae1c6d981, the official Composer project is no longer a purely technical open source project. It has mixed some people's political views and tried to force everyone to accept his political views.
A clean composer has been forked and released for the open-source world. We don't support war, nor any country. We don't care about politics, so we don't want political-related information to appear when using Composer.
[Redacted link]",Bitter Frustration
"> We don't want this to be on the CLI screen. Everyone wants peace, but the writing #StandWithUkraine supports one side. This word hurts the feelings of the Russian people who don't want war. We can achieve peace if we side with both sides, regardless, and that's why there is such a thing as negotiation in the political world. This article is very annoying and makes me not want to use composer. You should apologize or delete it for writing this.
> > I would suggest you should write this on the Repository instead of CLI.
> > Let's keep the CLI neutral.
> > #NoStandJustUnderstandEachOther #NeutralComposerCLI
> > rel: [#1267 (comment)](https://github.com/composer/packagist/pull/1267#issuecomment-1063636569)
Friend, haven't you seen through these hypocritical people? They don't really stand with Ukraine. They just want to carve up Russia. You can see their attitude towards Palestine. They never dare to stand with Palestine.",Identify Attack/Name Calling
Who is being hurt or rather often killed right now are civilians in Ukraine. That needs to end urgently.,Bitter Frustration
"We don't want this to be on the CLI screen. Everyone wants peace, but the writing #StandWithUkraine supports one side. This word hurts the feelings of the Russian people who don't want war. We can achieve peace if we side with both sides, regardless, and that's why there is such a thing as negotiation in the political world. This article is very annoying and makes me not want to use composer. You should apologize or delete it for writing this.
I would suggest you should write this on the Repository instead of CLI.
Let's keep the CLI neutral. #NoStandJustUnderstandEachOther
#NeutralComposerCLI
rel: https://github.com/composer/packagist/pull/1267#issuecomment-1063636569",Bitter Frustration
"There are innocent civilians injured and killed every day in Ukraine right now and it needs to end, do not close your eyes to their suffering.
As Jordi pointed out:
> Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
> ```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
> > It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others.",Bitter Frustration
"That was added in this file, on 321 line src/Package/SymlinkDumper.php
In commit 86244a3695fcaaac9c5ba4257a4314eae1c6d981
This change can cause escalating the political situation, international conflicts and etc. Github is bad place for politics. For it, go to government or anywhere else.",Bitter Frustration
"When running the fixer again on an already fixed line:
```php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider']();
```
...then the mid-line ""fix"" gets added again, leading to:
```php
$this->current_provider = new $this->providers->(){$this->current_provider_slug}['provider']();
```
(two sets of `()`).",Bitter Frustration
"As a workaround, changing it to:
```php
$class = $this->providers->{$this->current_provider_slug}['provider'];
$this->current_provider = new $class;
```
results in it being correctly fixed to:
```php
$class = $this->providers->{$this->current_provider_slug}['provider'];
$this->current_provider = new $class();
```",
"This sniff is on my list of things which need looking into anyway for WPCS 3.0.0 as the rule it applies a candidate for moving to the `Core` ruleset.
The WPCS sniff does not handle variable variables with `new`.
There is an upstream PSR12 sniff which does, however, that sniff doesn't handle anonymous classes and there are some more differences as also documented in the `Extra` ruleset:
https://github.com/WordPress/WordPress-Coding-Standards/blob/a93d2970a3694e683588fedaa0f8a901fe148d3b/WordPress-Extra/ruleset.xml#L130-L139
There is already an issue open to discuss this: https://github.com/WordPress/WordPress-Coding-Standards/issues/1884 Might be good to have a look through that and leave an opinion ?",
"For the fun of it, I just added ```php
$this->classname_tokens[ \T_OPEN_CURLY_BRACKET ] = \T_OPEN_CURLY_BRACKET;
$this->classname_tokens[ \T_CLOSE_CURLY_BRACKET ] = \T_CLOSE_CURLY_BRACKET;
```
To the list of classname tokens and ran the phpcbf and got the correct result ðŸ˜… Note that I didn't run the tests or anything so this could be a one off fix that breaks other things, so...
EDIT: Added a test and it breaks anonymous classes:
```
-$util->setLogger( new class() {} );
+$util->setLogger( new class {}() );
```
So, not really a fix.",Irony
"@dingo-d Yup, sniffing is tricky... there was a reason variable variables weren't supported (yet) ðŸ˜‰",Irony
"This is an open source project, which means that this is the personal fiefdom, not really an open community. It's weird that it's included in the WordPress organization. At any rate, there hasn't been a published update in forever, and probably will never be one. But, these assholes will be rude as fuck and call you rude if you ask when it'll be updated and published. Fuck WordPress.",Bitter Frustration
"@WraithKenny You have been told before and I'm telling you again: this kind of behaviour and language is not tolerated here. Your behaviour is disruptive, rude, abusive and highly demotivating and discouraging for existing contributors (whether currently active or not).
You are put on warning and will be blocked from participating in this repo if you continue.",Threat
"## Bug Description
The fixer for the missing parentheses sniff adds the parentheses to the wrong place for a particular statement.
## Minimal Code Snippet
```php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider'];
```
The issue happens when running this command:
```bash
phpcs phpcs-test.php --standard=WordPress-Extra -sp --basepath=.
```
... over a file containing this code (minimal example, extracted from the real code):
```php
<?php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider'];
```
<!-- For bugs with fixers: How was the code fixed? How did you expect the code to be fixed? -->
The file was auto-fixed via `phpcbf` to:
```php
<?php
$this->current_provider = new $this->providers->(){$this->current_provider_slug}['provider'];
// ---------------------------------------------^ âŒ ```
... while I expected the code to be fixed to:
```php
<?php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider']();
// ---------------------------------------------------------------------------------------^ âœ… ```
## Error Code
`WordPress.Classes.ClassInstantiation.MissingParenthesis`
## Environment
<!--
To find out the versions used:
* PHP: run `php -v`.
* PHPCS: run `[vendor/bin/]phpcs --version`
* WPCS: run `composer [global] info` for a Composer install.
-->
| Question | Answer
| ------------------------| -------
| PHP version | 8.0.5
| PHP_CodeSniffer version | Latest `git pull` of `master` today (`51335eb46`)
| WPCS version | Latest `git pull` of `develop` today (`a93d2970`)
| WPCS install type | Composer
| IDE (if relevant) | N/A - CLI.
## Additional Context (optional)
<!-- Add any other context about the problem here. -->
## Tested Against `develop` branch?
- [x] I have verified the issue still exists in the `develop` branch of WPCS.",Bitter Frustration
"Correct, I'm checking this and it seems to be working on JS",
Intended for now so the fix could be released sooner. JS will be fixed later.,Impatience
"The Result argument safety exploit you have seem to succesfully fixed in lua environment seems to still work in javascript environment. JS: https://github.com/citizenfx/fivem/blob/810a639673d8da03fe4b1dc2b922c9c0265a542e/code/components/citizen-scripting-v8/src/V8ScriptRuntime.cpp#L1356
LUA: https://github.com/citizenfx/fivem/blob/ee1490e5a871d3c82112ea09ffa462cfb4ba974f/code/components/citizen-scripting-lua/src/LuaScriptNatives.cpp#L648
Would hope if you could look into the other scripting platforms if there is similar designing issues.",Bitter Frustration
"Next up, #GetyourBOOSTER!!!",Mocking
"Yes, also go get your fucking booster.",Vulgarity
"Messages like this have absolute no place in the output of a tool It has no place here any more than it would if you put it in a speaker in my hammer and forced the user to listen to it.
It should be removed as a matter of urgency. Send the message on twitter, not in composer.
https://github.com/composer/composer/discussions/10600",Bitter Frustration
"The Cucumber community stands with Ukraine. SmartBear, the primary supporter of Cucumber, has taken a number of actions including financial support for refugee programs.",
"thank you, @aslakhellesoy Could you pls also add ""StandwithUkraine"" banner on cucumber repo? it will inform people, how they can help for us",
"Hi @id-dan - I've added the banner to our pinned repos:
* https://github.com/cucumber/cucumber-js#readme
* https://github.com/cucumber/cucumber-ruby#readme
* https://github.com/cucumber/cucumber-jvm#readme
* https://github.com/cucumber/godog#readme
* https://github.com/cucumber/common#readme
* https://github.com/cucumber/docs#readme",
"Thanks your work on this campaign and for bringing this to our attention @id-dan. Best of luck to you, your friends and family â¤ï¸",
"Thank you, guys!!! You make big deal ðŸ‘",Irony
"A disgusting decision to block access to CloudFront. Well, forking is the only solution, ""open"" source for ordinary developers will not be the same.",Bitter Frustration
We have decided to block traffic from Russia and Belarus to the [cucumber.io](http://cucumber.io/) domain until further notice. We're sorry for the inconvenience this causes for individuals residing in these countries. Because of the sensitivity of this topic this issue has been closed for further comments.,
"Please support Ukraine, more infos here: https://github.com/vshymanskyy/StandWithUkraine",
"btw, looks like you can only have one instance of the cli opened atm. So once the application is started, there is no way to login, etc.",Impatience
"I also have the same end log:
```
2022-05-06 19:33:45.489661+00:00ProtonMail Bridge is not able to detect a supported password manager
2022-05-06 19:33:45.489695+00:00(pass, gnome-keyring). Please install and set up a supported password manager
2022-05-06 19:33:45.489703+00:00and restart the application.
```",
Still the same as of current version (1.8.10_6.0.3). Is there an official maintainer or should I take the matter into my own hands ?,Bitter Frustration
"> Still the same as of current version (1.8.10_6.0.3). Is there an official maintainer or should I take the matter into my own hands ?
Of course it's the same, If it had been fixed, this issue would be also closed.
Even when there is a maintainer, PR's are welcomed, so you can work on this yea",Bitter Frustration
"### App Name
protonmail-bridge
### SCALE Version
22.02.0
### App Version
1.8.10_5.0.1
### Application Events
```Shell
2022-03-25 19:06:20
Started container protonmail-bridge
2022-03-25 19:06:20
Created container protonmail-bridge
2022-03-25 19:06:15
Container image ""tccr.io/truecharts/protonmail-bridge:v1.8.10-1@sha256:58a54002123cc9a83cfb3170deb0a1dbf4cedabdced09a9c6bcafc19ee4b5631"" already present on machine
2022-03-25 19:06:14
Started container hostpatch
2022-03-25 19:06:13
Created container hostpatch
2022-03-25 19:06:09
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-03-25 19:06:08
Started container autopermissions
2022-03-25 19:06:07
Created container autopermissions
2022-03-25 19:06:02
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-03-25 19:06:02
Add eth0 [172.16.2.199/16] from ix-net
Successfully assigned ix-protonmail-bridge/protonmail-bridge-6f99779f8d-jvqq2 to ix-truenas
2022-03-25 19:05:56
Created pod: protonmail-bridge-6f99779f8d-jvqq2
2022-03-25 16:50:28
Scaled up replica set protonmail-bridge-6f99779f8d to 1
2022-03-25 19:05:49
Readiness probe failed: dial tcp 172.16.2.175:25: i/o timeout
2022-03-25 19:05:49
Liveness probe failed: dial tcp 172.16.2.175:25: i/o timeout
2022-03-25 19:05:31
Deleted pod: protonmail-bridge-6f99779f8d-h9khs
2022-03-25 19:05:32
Stopping container protonmail-bridge
2022-03-25 17:13:40
Scaled down replica set protonmail-bridge-6f99779f8d to 0
```
### Application Logs
```Shell
2022-03-25 18:06:20.493201+00:00+ [[ '' == init ]]
2022-03-25 18:06:20.494661+00:00+ socat TCP-LISTEN:25,fork TCP:127.0.0.1:1025
2022-03-25 18:06:20.495884+00:00+ rm -f faketty
2022-03-25 18:06:20.495904+00:00+ socat TCP-LISTEN:143,fork TCP:127.0.0.1:1143
2022-03-25 18:06:20.498608+00:00+ mkfifo faketty
2022-03-25 18:06:20.501253+00:00+ cat faketty
2022-03-25 18:06:20.501539+00:00+ protonmail-bridge --cli
2022-03-25 18:06:21.354322+00:00time=""2022-03-25T18:06:21Z"" level=warning msg=""Failed to add test credentials to keychain"" error=""pass not initialized: exit status 1: Error: password store is empty. Try \""pass init\"".\n"" helper=""*pass.Pass""
2022-03-25 18:06:21.377115+00:00[31mERRO[0m[Mar 25 18:06:21.377] Could not list credentials [31merror[0m=""no keychain"" [31mpkg[0m=credentials
2022-03-25 18:06:21.377194+00:00[31mERRO[0m[Mar 25 18:06:21.377] Could not load all users from credentials store [31merror[0m=""no keychain"" [31mpkg[0m=users
2022-03-25 18:06:21.381824+00:002022-03-25T18:06:21.381824001Z
2022-03-25 18:06:21.381864+00:00Welcome to ProtonMail Bridge interactive shell
2022-03-25 18:06:21.381881+00:00___....___
2022-03-25 18:06:21.381888+00:00^^ __..-:'':__:..:__:'':-..__
2022-03-25 18:06:21.381894+00:00_.-:__:.-:'': : : :'':-.:__:-._
2022-03-25 18:06:21.381900+00:00.':.-: : : : : : : : : :._:'.
2022-03-25 18:06:21.381911+00:00_ :.': : : : : : : : : : : :'.: _
2022-03-25 18:06:21.381921+00:00[ ]: : : : : : : : : : : : : :[ ]
2022-03-25 18:06:21.381927+00:00[ ]: : : : : : : : : : : : : :[ ]
2022-03-25 18:06:21.381933+00:00:::::::::[ ]:__:__:__:__:__:__:__:__:__:__:__:__:__:[ ]:::::::::::
2022-03-25 18:06:21.381944+00:00!!!!!!!!![ ]!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![ ]!!!!!!!!!!!
2022-03-25 18:06:21.381951+00:00^^^^^^^^^[ ]^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[ ]^^^^^^^^^^^
2022-03-25 18:06:21.381957+00:00[ ] [ ]
2022-03-25 18:06:21.381963+00:00[ ] [ ]
2022-03-25 18:06:21.381973+00:00jgs [ ] [ ]
2022-03-25 18:06:21.381980+00:00~~^_~^~/ \~^-~^~ _~^-~_^~-^~_^~~-^~_~^~-~_~-^~_^/ \~^ ~~_ ^
2022-03-25 18:06:21.382716+00:00ProtonMail Bridge is not able to detect a supported password manager
2022-03-25 18:06:21.382757+00:00(pass, gnome-keyring). Please install and set up a supported password manager
2022-03-25 18:06:21.382766+00:00and restart the application.
```
### Application Configuration
![image](https://user-images.githubusercontent.com/1915939/160177528-ed1d3287-5f2d-4be6-b43d-023378be124e.png)
![image](https://user-images.githubusercontent.com/1915939/160177589-f59475e8-be90-45d3-9053-05e93b953f58.png)
![image](https://user-images.githubusercontent.com/1915939/160177646-632b898a-13f4-4be5-b703-8038aa72c334.png)
![image](https://user-images.githubusercontent.com/1915939/160177708-d4354e4b-043e-4653-b0a2-584787525385.png)
### Describe the bug
The bridge doesn't come up. Logs show it fails to initialize.
When opening a shell in the container, and lauch the command ""protonmail-bridge --cli"" I get the following error :
WARN[0000] Failed to add test credentials to keychain error=""pass not initialized: exit status 1: Error: password store is empty. Try \""pass init\"".\n"" helper=""*pass.Pass""
After using the command ""pass init protonmail"" to initialize de password manager, I get the following error : WARN[0000] Failed to add test credentials to keychain error=""exit status 1: gpg: protonmail: skipped: No public key\ngpg: [stdin]: encryption failed: No public key\nPassword encryption aborted.\n"" helper=""*pass.Pass""
After initializing a gpg key (without password and all default settings), following [this issue](https://github.com/ProtonMail/proton-bridge/issues/190), I was able to make it work.
### To Reproduce
- Install protonmail-bridge
- bug occur
### Expected Behavior
Protonmail bridge should be up and accepting connexion.
### Screenshots
![image](https://user-images.githubusercontent.com/1915939/160179439-c06884bf-f05f-4e82-891f-b5e99fc1dc5d.png)
### Additional Context
None
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",
"> btw, looks like you can only have one instance of the cli opened atm. So once the application is started, there is no way to login, etc.
Here is a temporary workaround until this is fixed.
- Open a shell into the pod
- Find the running bridge
- `ps -aux`
- take note of the PID the command should look like ""/usr/lib/protonmail/bridge/proton-bridge --cli --launcher /usr/lib/protonmail/bridge/proton-bridge-launcher""
- Kill it
- `kill -9 #`
- replace # above with your PID
- Launch the CLI and login
- `protonmail-bridge --cli`
- `login`
- Stop and start the app",
Still no fix ?,Impatience
"> Still no fix ?
https://truecharts.org/docs/charts/stable/protonmail-bridge/installation_notes",Impatience
"We dont work for you guys, we are a *community* of volunteers.
this one specifically requires protonmail. Which almost none of the staff have. +1â€™ing the issue is not helping your case. Either stfu and wait or fix it yourself.",Bitter Frustration
"This issue had been resolved due to a new pr that i made a week ago. The installation notes explains exactly what commands to use to be able to login into proton the _right_ way. Just follow the guide and run the 2 commands exactly.
https://truecharts.org/docs/charts/stable/protonmail-bridge/installation_notes",Impatience
"@bennettdams try wrapping your comments within the render function in curly brackets.
```
const Home: NextPage = () => {
return (
{/**
* Some comment.
*/}
<div className={styles.container}>
<Head>
...
```",
"> @bennettdams try wrapping your comments within the render function in curly brackets.
You can also just move your comments above the return statement.
But a workaround is sort of uninteresting here. The important thing is to get the issue fixed. This led to production crashes for us, and there's no way to stop the issue from being reintroduced.",Bitter Frustration
"@jakst this has nothing to do with next or react though. It's a language feature of JSX. Embedding JS expressions just ""works this way"". Read more about this here: https://www.typescriptlang.org/docs/handbook/jsx.html#embedding-expressions",
"@MauriceArikoglu I you have a look at my original post, there are variants that don't produce an error:
> It does **NOT** happen for:
> - One asterisk in one line
> ```
> /* Some comment. */
> ```
> - Two asterisks in one line
> ```
> /** Some comment. */
> ```
> - Normal comment over multiple lines
> ```
> // Some
> // Comment > ```
I know the workarounds, but that is not the point. If you ask me, either **all** comment variants or **none** should work, but right now it's non-deterministic and will surprise some people.",Bitter Frustration
@bennettdams single line comments like in your example are interpreted as text and should therefore show up in the rendered result - so not acting as comment but as a text node instead.,
"@MauriceArikoglu > single line comments like in your example are interpreted as text and should therefore show up in the rendered result
I don't think so: https://codesandbox.io/s/fancy-worker-dg23u6?file=/src/App.tsx
The difference is that these comments exist before the first JSX node directly after the `return`.",
"This is a confirmed bug in SWC that has already been fixed (here: https://github.com/swc-project/swc/commit/552f16dba6c91876529354f3f5e155a3360a74ea) and is going to be addressed in Next.js by merging #35395.
There is no need for further discussion on the topic so I'll lock it to maintainers as a two-sided discussion is going on and would like to avoid conflicts.
Thanks to everyone for engaging! :+1: :green_heart:",
"Hi, this has been updated in `v12.1.4-canary.0` of Next.js, please update and give it a try!",
"### Verify canary release
- [X] I verified that the issue exists in Next.js canary release
### Provide environment information
Operating System:
Platform: linux
Arch: x64
Version: #1 SMP Fri Apr 2 22:23:49 UTC 2021
Binaries:
Node: 16.13.1
npm: 8.1.2
Yarn: 1.22.15
pnpm: N/A
Relevant packages:
next: 12.1.2-canary.0
react: 17.0.2
react-dom: 17.0.2
### Describe the Bug
When you insert a multi-line comment directly at the start of the return of the render function, the app breaks.
For example:
```tsx
const Home: NextPage = () => {
return (
/**
* Some comment.
*/
<div className={styles.container}>
<Head>
...
```
This will create different errors based on your runtime and the view:
### Development (`npm run dev`)
```
error - Error: Home(...): Nothing was returned from render. This usually means a return statement is missing. Or, to render nothing, return null.
```
Browser:
![image](https://user-images.githubusercontent.com/29319414/160392111-d37059e2-85ba-4392-8df6-6c23fd36d9fd.png)
### Production (`npm run build`)
```
info - Collecting page data [ ] info - Generating static pages (0/3)
Error occurred prerendering page ""/"". Read more: https://nextjs.org/docs/messages/prerender-error
Error: Minified React error #152; visit https://reactjs.org/docs/error-decoder.html?invariant=152&args[]=Home for the full message or use the non-minified dev environment for full errors and additional helpful warnings.
at ab (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:32:466)
at d (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:34:55)
at bb (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:36:16)
at a.b.render (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:42:43)
at a.b.read (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:41:83)
at Object.exports.renderToString (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:52:138)
at Object.renderPage (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/server/render.js:766:45)
at Object.defaultGetInitialProps (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/server/render.js:376:51)
at Function.getInitialProps (/home/bennett/code/nextjs-render-comment-bug/.next/server/pages/_document.js:563:20)
at Object.loadGetInitialProps (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/shared/lib/utils.js:65:29)
info - Generating static pages (3/3)
> Build error occurred
Error: Export encountered errors on following paths:
/
at /home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/export/index.js:396:19
at runMicrotasks (<anonymous>)
at processTicksAndRejections (node:internal/process/task_queues:96:5)
at async Span.traceAsyncFn (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/trace/trace.js:79:20)
at async /home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/build/index.js:993:17
at async Span.traceAsyncFn (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/trace/trace.js:79:20)
at async /home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/build/index.js:869:13
at async Span.traceAsyncFn (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/trace/trace.js:79:20)
at async Object.build [as default] (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/build/index.js:55:25)
âžœ nextjs-render-comment-bug git:(master) ```
---
I can confirm this happens for both variants of the multi-line comment (one or two asterisks):
- One asterisk
```
/*
* Some comment.
*/
```
- Two asterisks
```
/**
* Some comment.
*/
```
It does **NOT** happen for:
- One asterisk in one line
```
/* Some comment. */
```
- Two asterisks in one line
```
/** Some comment. */
```
- Normal comment over multiple lines
```
// Some
// Comment ```
### To Reproduce
Reproduction repo:
https://github.com/bennettdams/nextjs-render-comment-bug
..or to manually reproduce:
1. ` npx create-next-app@12.1.2-canary.0 --ts --use-npm .`
2. Install canary `npm i next@12.1.2-canary.0`
3. Add a multi-line comment to start of the render function of `index.tsx` directly after `return`:
```tsx
const Home: NextPage = () => {
return (
/**
* Some comment.
*/
<div className={styles.container}>
<Head>
...
```
4. `npm run dev` or `npm run build`",
"Do you have any reproduction steps other than this code? Where to place this code for a test, any crash details, anything?",Impatience
"> Do you have any reproduction steps other than this code? Where to place this code for a test, any crash details, anything?
You can use an executor to run that code from client-side, i tested it using a cheat that have this feature.
The crash happens instantly as the code is executed, game closes without giving any error.",Impatience
"What is 'an executor' and where to find such? Again, providing more info would help. :/
Other than that, it's unlikely you're going to see this issue resolved if you don't provide info.",Impatience
"> What is 'an executor' and where to find such? Again, providing more info would help. :/
> > Other than that, it's unlikely you're going to see this issue resolved if you don't provide info.
I have used Eulen.
![image](https://user-images.githubusercontent.com/78696476/160890101-772080c9-719f-4407-8427-8b519af8961a.png)",Impatience
Do you have a link to this 'Eulen' program at all? Or any other repro method?,Impatience
"> Do you have a link to this 'Eulen' program at all? Or any other repro method?
Eulen is a paid-cheat, atm i don't really know a free executor.
There's a bunch of it but they are all paid-soft.",Impatience
Do you have a link or a contact method for the author so they can provide more info?,
I think that if you put the snippet just in a regular client.lua script (without cheating) your game would also crash?,Impatience
"The only thing you should do is make it some way that it is only executed by one player (to immitate a cheat menu, to check if the other players that arent executing the code are also crashing.",Impatience
"> Do you have a link or a contact method for the author so they can provide more info?
No, several people have joined my server and crashed players.
My Anticheat took screenshots of them, I saw the Lua menu they were using, got hold of it and identified the code they were using to crash players.
The code i sent here.",Bitter Frustration
"> Do you have a link or a contact method for the author so they can provide more info?
I also think that a cheat provider whose goal is to irritate people on fivem, won't give any info about making the cheat menu less irritating.",Bitter Frustration
"> > Do you have a link or a contact method for the author so they can provide more info?
> > No, several people have joined my server and crashed players. My Anticheat took screenshots of them, I saw the Lua menu they were using, got hold of it and identified the code they were using to crash players. The code i sent here.
Have you verified and confirmed that the code is the culprit by testing it by yourself with some participants?",Impatience
"> Yes. I have tested it with my staff-team, they are all crashing the way i said, without any error.",Bitter Frustration
"Can you send a screenshot of the crash dialog that's presented to them?
Any information about the crash is helpful, and so far you haven't provided anything other than the snippet of code that's supposedly causing the crash.",Impatience
"Locking this issue report for now, too many people trying to involve themselves here.",Impatience
"To people trying to make new issue reports about this: **don't.** You're not helping by doing such.
This is already tracked internally as of ~20 days ago, but it didn't get picked up for some reason. I'm also unable to pick this up myself at this time, so instead internal parties have been reminded again.",Bitter Frustration
"Hi, we have a problem. A new ""exploit"" has appeared, if it can be called that, a simple execution of the PlaySound function crashes certain players in range of the one executing the command ( using executors )
On one server 5 crashed, on another even 130 crashed at once.
Warning: I leave here the code in order to receive solutions.
```
for i=1, 100 do
PlaySound(-1, 'Checkpoint_Hit', 'GTAO_FM_Events_Soundset', true)
PlaySound(-1, 'Boss_Blipped', 'GTAO_Magnate_Hunt_Boss_SoundSet', true)
PlaySound(-1, 'All', 'SHORT_PLAYER_SWITCH_SOUND_SET', true)
end
```",Bitter Frustration
@drweb86 whatâ€™s the URL youâ€™re trying to reach?,Impatience
@davidjgoss https://cucumber.io/docs/installation/javascript/,
"Thanks for letting us know about this problem @drweb86, for your words in support of peace and of course for the [Gherkin Beautifier](https://marketplace.visualstudio.com/items?itemName=siarheikuchuk.gherkin-beautifier-vs-code-plugin) VSCode plugin. â¤ï¸
I'm not sure why the website cannot be accessed in Belarus. It has nothing to do with us adding the banner as requested in #1961. That simply put a message on our GitHub `README` files. It didn't do anything to change who can access the website.
The block could be happening at multiple layers of our infrastructure, not all of which I have immediate access to. We'll investigate and let you know.
In the mean time, you can read the docs from source, [here](https://github.com/cucumber/docs/blob/main/content/docs/installation/javascript.md).",
"@mattwynne , @davidjgoss Thank you for so prompt response and good attitude. BTW I haven't thought doc is here in sources.",
We have decided to block traffic from Russia and Belarus to the cucumber.io domain until further notice. We're sorry for the inconvenience this causes for individuals residing in these countries. Because of the sensitivity of this topic this issue has been closed for further comments.,
"### ðŸ‘“ What did you see?
I cannot open cucumber-js web-site . It says **CloudFront distribution is configured to block access from your country**
![image](https://user-images.githubusercontent.com/4712173/161432145-d828b66b-88b0-4138-95a4-6403e9db1ceb.png)
I guess I found the reason. I found PR about Ukraine support. So, my guess it was done on purpose.
Personally, I'm against any war as anyone on a planet. However, I feel excluding me from usage of cucumber website as a form of racism. Especially it looks ugly for entire Belarus.
Can you comment on that?
Apart from that I'm a little bit puzzled. I wrote some time ago cucumber extension for VS code to format feature files. Since i'm not of a good race to use cucumber website, is it ok to have it still published?
And one more question, the company I'm working for is using cucumber. And I don't know what to do since I really like cucumber, but racism inside company is prohibited and should I schedule meeting about alternatives.
I feel sad.",Bitter Frustration
"I am pretty sure this is a troll but it is also just as likely that this is the only caliber of employee that telcos can keep.
1. Helium provides more lorawan coverage than any telco. thanks for mentioning we need coverage
2. High quality is also high cost. The question is what is good enough? 3. This sounds good but these are conflicting requirements so you are actually saying nothing valuable.
4. Another meaningless statement. Maybe you should do some research on the protocol. https://lora-alliance.org/lorawan-for-developers/
5. You want to mandate sensors have GPS? This is an IoT network and most sensors cannot require the high power consumption nor is it required. The gateways are fixed so GPS doesn't really provide any value. Please tell how it would increase device performance.
Numbers go higher than 5 not really sure why we're back at one...you can use both hands
1.If you had done any research at all you'd realize this has been discussed and rejected for years. Please recommend an incentive system to reward sensors that cannot be abused. Please research ADR which is built into the lorawan protocol
2.get antenna high and out of the noise and then reduce power. Why would you put it low in the noise? Again please see ADR
3.now i know you are trolling. take a look in any major urban center and say more hotspots are required. This is starting to sound like you are whining you aren't earning enough.
4.Please understand the current protocol before suggesting improvements
5.Again please do some research. Hotspot location is fundamental to POC the algorithm. if the intention of the hip were good i am sorry but honestly the hip just comes across as sour grapes from an uninformed employee at a telco. Maybe do some research and come back with some actual detail and a little less arrogance and attitude.",Bitter Frustration
"I am not a telco troll - far from it
But I can see that even though the premise of helium is a great idea - the lack of knowledge about how wireless works and the chaos and poor performance of the current network shows that the telcos have nothing to worry about -because you will never reach your goal
know that I really want you to succeed but you are in a ditch - I do really want to help - but if you donâ€™t want help - best of luck - it will never work on the current path - because wireless is all about physics, math, and engineering - which has nothing to do with whether you are a telco or innovators like the Helium community. I have been predicting for almost 10 years that the telco networks will eventually collapse. I was hoping Helium was going to fulfill that prediction. But if you donâ€™t want advice from people who understand the science you are completely lost and your ship will sink before you reach the promised land.",Bitter Frustration
By the way none of your responses actually were what I said. They were assumptions because you donâ€™t understand wireless. Wireless does not equal telco by the way. Wireless equals science physics and math. The telcos are businesses that have filled the government into giving ask the spectrum and they are fat and bloated and inefficient because Helium canâ€™t get out of their own mess to take them down. Clean up your own house before you try to take over their house,Bitter Frustration
Helium appears to be a bunch of gamers and crypto zealots who know so little about the science behind wireless. I could help you so much but you are blind to the flaws of the current system,Bitter Frustration
"> Helium appears to be a bunch of gamers and crypto zealots who know so little about the science behind wireless. I could help you so much but you are blind to the flaws of the current system
Interesting generalization for a well-meaning person. Your assertion of knowledge on everything wireless betrays you. Can you tell me more about how
(1) beaconing is already automatic in LoRaWAN
(2) every LoRaWAN device should already have GPS",Mocking
"To be honest I do need to understand how Helium works better. But the documentation is not consistent with all the rapid introduction of HIPs
My whole point though is that you can learn from people that understand wireless without assuming they are telco heads. My comments are clearly too broad to be of value. So maybe we should start with cross education. Teach me Helium and I will teach you how cellular works reasonably well but is controlled completely by 3 wireless operators. So you can take the telco technology but modify it to with in a decentralized ownership model. The only problem with the current cellular systems is cost and control - not the technology.",Bitter Frustration
"I see Helium struggling with problems that the telcos solved a long time ago. As Thomas Edison said, I start where others left off. Take the technology but add to it to fit a decentralized model. Because if you donâ€™t figure it out soon Private 5G and WiFi6E will eat your lunch because they are not controlled by the telcos.",Impatience
Ok,
Know that IOT/Lorawan are peanuts compared to 5G which is where Helium is headed. And yes I know wireless 4G/5G well but IOT less.,
"And look at it this way, when have you found an ex Telco Head willing to bare all their secrets. So you should at least hear me out if nothing more than as a spy for you",Irony
"And itâ€™s been 12 years since I was a Telco head, so so I am cured from the cool aid that they drink",Bitter Frustration
"> To be honest I do need to understand how Helium works better. But the documentation is not consistent with all the rapid introduction of HIPs
Welcome to startup agility",Bitter Frustration
The HIP repository is not a discussion forum. Closing and locking,
"I have over 30 years working in the Wireless Industry. I would like to propose a long term structure that will align Helium with how real world wireless networks are designed and operated. I am proposing to initiate planning for ""HIP 100"" which would restructure the Helium network approach to align with the fundamentals of wireless networks which:
1. Provide consistent reliable COVERAGE.
2. Provide the highest QUALITY which consists of MAXIMIZING signal levels (RSRP) while MINIMIZING interference levels (SINR).
3. Provide the best user PERFORMANCE at the LOWEST cost which consists of CAPACITY planning based on SUBSCRIBER density and locations
4. Have a robust END-TO-END architecture which balances the CORE network, the RADIO network and the END-USER device performance.
5. Provides accurate LOCATION information about BOTH the Hotspot and the Devices. All modern networks today have GPS built into both the Cell Tower and the devices. LOCATION is fundamental to securing the network as well as optimizing the network and device performance.
Even though the current Helium network is a major advance towards distributed ownership and control, the current construct is not aligned with the fundamentals of Wireless design. For example:
1. To build a network, there needs to be a reward system for BOTH the HOTSPOT and the DEVICE. Until the network is built and device performance is validated, there should be rewards for buying and deploying DEVICES to to test and validate coverage. Validating coverage using HOTSPOT to HOTSPOT beacons is fundamentally flawed. This is NOT how wireless works. Base stations try to AVOID seeing each other to minimize interference.
2. The current reward system motivates hotspot owners to place their hotspot at the highest point possible so they can witness the most number of other hotspots. This results in drastically increasing SINR which is opposite of how wireless should be designed. You want to MINIMIZE SINR, because that is the fundamental of Cellular, so that the same frequencies can be reused and the CAPACITY and QUALITY can be maximized.
3. The current reward system favors hotspots that are in less populated areas and penalizes hotspots in dense areas. This is also opposite of the fundamentals of wireless. You need MORE hotspots in dense areas.
4. The current implementations of Beaconing is flawed as well. Wireless networks beacon 100% of the time. This maximizes the CAPACITY and ensures that the network performance STATISTICS are consistently reported to allow proper monitoring and optimization.
5. The current implementation does not have accurate LOCATION of the hotspots nor the DEVICES. In fact there are so few DEVICES that there is no DEVICE performance information. LOCATION is not difficult, using a combination of GPS at key Hotspots, Triangulation algorithms and WiFi to accurately and securely locate all Hotspots and devices in the Network. Without accurate LOCATION information, the network will never be secure nor will we be able to deploy a QUALITY, HIGH PERFORMANCE network.
Understand, that I am not proposing to ""throw the Baby out with the Bath Water"". Nor am I criticizing the objective of Helium to build a decentralized, ownership model. But we want to build a network that will actually deliver a service that is of value to a subscriber base of wireless devices. It is paramount that we begin this journey towards a working Wireless construct or else we may find ourselves in this predicament: ""IF YOU BUILD IT - IT WILL NOT WORK"" rather than what we want which is ""IF YOU BUILD IT THEY WILL COME""",Bitter Frustration
Indeed! Forgot about that and will do very shortly.,
@James-Yu Could you make a new release now that the structure provider issues are fixed?,Impatience
"I tend to agree that we could add more contrast to the sidebar. I'm not sure I like the proposed changes though. Here's a screenshot of what they look like
![Screenshot from 2022-04-12 09-40-44](https://user-images.githubusercontent.com/322311/163012323-601a03cb-4502-4501-bf3b-79f284653f93.png)
Alternatively, here's the sideback background set to `#363636` with no change to the active link
![Screenshot from 2022-04-12 09-43-56](https://user-images.githubusercontent.com/322311/163012757-73360b3d-d8df-42dd-bcd8-56c396ba3133.png)",
"Another idea, here's using bold rather than color to indicate the active item
![Screenshot from 2022-04-12 09-48-01](https://user-images.githubusercontent.com/322311/163013432-21c71ad2-0dc6-4bea-aeba-fa71bda869a0.png)",
"Hey @benmccann thanks for the response here.
On the first slide the accent color looks kinda red and not the actual orange brand color, are you sure you used the right code? If it is the right color code then perhaps on that particular background we could lighten the orange just a bit.
Listen I very much appreciate your feedback here but I have to mention that as a person that has poor sight, I would argue that it's imperative the background to be kept at least a bit darker than your proposed hex above and the active link to be of an accent color (instead of a base like white) so it easily distinguishable and not on the same hue as the inactive links (truly important).
Check this =>
https://vuejs.org/guide/introduction.html
(They also have dark and light theme...)
We want to grow as a framework and as a user base but Vue.js is killing us with those Docs comparing to Svelte/kit's.
Please choose a slight darker background color, just a tiny touch and keep a punchy accent color close to the logo for the active links, I am positive this will help grow the framework, it sounds crazy but it is in my experience an important detail really.
Between choosing competing stuff online I personally have chosen then ones with the more accessible documentation.
@benmccann thanks again for your consideration!",Bitter Frustration
If only there was something like https://caniuse.com/mdn-css_at-rules_media_forced-colors,Irony
"Hey @UltraCakeBakery, thanks for your reply here.
I am not sure what you are suggesting and if there is sarcasm involved on your reply.
To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? Do you mind telling us a little bit more please? This is a serious matter that affects thousands of people (in different degrees). Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
This is a genuine problem that needs solving, please provide constructive feedback/solutions.
No negativity is needed, thank you.",Bitter Frustration
"> Hey @UltraCakeBakery, thanks for your reply here.
> > I am not sure what you are suggesting and if there is sarcasm involved on your reply. To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
> > You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? Do you mind telling us a little bit more please?
> > This is a serious matter that affects thousands of people (in different degrees). Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
> > This is a genuine problem that needs solving, please provide constructive feedback/solutions. No negativity is needed, thank you.
> This is a serious matter that affects thousands of people (in different degrees).
> Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
Hey @savannahx, thanks for your reply here.
I am not sure if there is unawareness or selfishness involved on your reply.
To be honest it feels there was unawareness and selfishness involved in your message because of the way you formulated yourself and did not think about us, the other 99% of svelte-kit users, who also have to deal with this change and asked of us to actually do all the work of implementing this new color.
You provided no way for us to reach a middle ground where both you and others can be happy. It seems like you really just want us to change the color to what you want. Did you even read the link that I posted in the one minute before you started writing this reply you posted 3 minutes after me making my original comment? Do you mind telling us why you get a bad vibe from my comment (saying it is sarcastic) even though I was trying to be helpful?
Suggestions like these affects billions of people (in different degrees). Are you suggesting we completely change the documentation instead of asking you and ""the others"" to start using the build-in tools provided to you by your operating system and browser?
This is a genuine problem that needs solving, please consider changing your mindset. We do not need more any people in the world that are like:
""THE WORLD NEEDS TO CHANGE, NÌµÍ›Í˜Ì¿Ì Ì²OÌµÍ˜Ì’Ì‡ÌžÌ™TÌ´Í„Í‚Ì¼Í“Ì® ME"" ""I AM **NÌµÍ›Í˜Ì¿Ì Ì²OÌµÍ˜Ì’Ì‡ÌžÌ™TÌ´Í„Í‚Ì¼Í“Ì®** GONNA WEAR GLASSES OR CHANGE MY SETTINGS! ALL LIGHTS AND COLOURS SÌ·Ì‘Ì”ÍŒÌ“ÌÍˆÌ«Ì®Í™HÌ¸ÍŠÌ‰Ì‚ÍŒÍŽÌ±Ì«Ì¨Ì¨Ì¨OÌ¸ÌÌ½ÌÍ“Í“Í–UÌ´Í„Ì¹Í™Í•ÌÍˆÍ…LÌ´Í„Ì½ÍÌ›Ì­Ì§Í‡Ì­Ì—Ì¤DÌ´ÌˆÌ•Ì…Ì€Ì» JUST GET BRIGHTER AND DARKER WÌµÍ€ÌŽÌ…ÌžÌ»Ì¤Ì—Ì©HÌ´Í†ÍÌšÍÌ¤Ì®Ì¨ÍœÌ¼Í‰EÌ´Ì›ÍÌƒÌŒÌ NÌ·Ì€Ì‰Í Ì„ÌˆÌ†ÍŒÍšÍ”Ì®Í™Ì¬Í– ÌµÍÍ—Ì¿Í–Ì©Ì°IÌµÌÍ‚Ì‹ÌœÍˆ ÌµÌšÍ—ÌÌÌ’ÌÌˆÌŸÍ“Ì±Ì¤Í–Ì¯Í™WÌ´Í‚Ì¿Ì©Ì¹Ì–Ì­Í…AÌ·Ì‚ÌÌ„ÍƒÌšÌ”Ì³Ì¬Ì±Í•NÌ´ÌÍ’Ì‡Ì¿Ì™Ì³Í…Í”TÌ·ÌˆÍ‚Í„ÌˆÍÌŒÍ…Í‰ÍšÌ³ Ì¶Í‚ÍƒÌÍÌ¦TÌ¸Ì”ÍÌ­Ì¤Ì²Ì²ÍœÍ•HÌ¸Ì‘ÍŒÌ“ÌŒÌ†Í›Ì³Ì£EÌ´Í‹Ì›Ì‡ÌÌ¢Ì°Ì±MÌ¶Í€Ì€Ì‚Ì„ÍÍŠÌ³ ÌµÌÍ—Í„ÌŽÌŠÌŠÍ‘Í•Ì£Ì²Ì¼TÌ¸Í‘Í›Ì†Ì­Ì§Í•Í…ÌªÍ”Ì¼OÌ·Ì‹Ì„Í‘ÍŒÍ›Í†Ì…Ì—Ì§Ì«ÍˆOÌ·ÌŽÍ˜Ì‹ÍÍ–ÍˆÌ±Ì³Í“Ì !ÌµÌÌ‚Ì¾Í“Í…Ì©Ì™Ì¦!Ì´ÌŠÍ€Í Ì±Í‡ÌÍ”Ì»Ì­Ì¡!Ì´Í‚Ì‹Ì›Ì•Í Ì Í”Ì©Íš!ÌµÌ›ÍÌŸÌ¼Ì˜Ì¼Í!!!!""",Bitter Frustration
"I am impaired too, and get sick of suggestions like these. WE are the ones with disabilities, not the rest of the world. WE need to adapt ourselves to the world, NOT the other way around.
The documentation could offer better support for high contrast mode, dark mode, screen readers.. But I don't think it should.
It is fine as is. If you are still unable to read the documentation - which is also available on Github, which has a very great accessibility - you might want to buy a new monitor, open or close your blinds, change your color profile, tweak the brightness, gamma, contrast and other settings, increase the scale of your UI or install a extension which can do automatic contrast for you. Again, this is a problem only we deal with. We are an edge case.",Bitter Frustration
"Dude's raging, for young people it's mostly preferable to fortify behind a false sense of reality and stand their ground. They are young and cocky (at least on the internet :P)
I am too old for this. it's okay, I am not perfect either, nobody is. Anyway I really hope this gets addressed if anyone has access and time to implement this color edit (removing the washed out blueish background and using an accent color for the active link).
I would create a pull request but I barely know Svelte so apologies for not taking the initiative in that regard.
@benmccann feel free to let me know if and when I should close this Issue and thanks again for your consideration.",Bitter Frustration
"alright, time out. i locked the conversation.
accessibility matters to us and we want to get this right. people shouldn't have to fiddle with browser settings just to read documentation. at the same time, there's a balance to be struck between maximising contrast and making the site pleasant to look at for people without visual impairments and/or shitty screens (we could just use pure black on pure white or vice versa everywhere, but that would be very tiring for most people to look at for any length of time).
so we will address this, though i make no guarantees about where it sits in the priority queue, and putting 'asap' in the issue title or declaring that the current contrast ratio means 'i cannot use SvelteKit without it' (unlikely) won't affect that decision (though there is now a pull request for it pending review â€” https://github.com/sveltejs/sites/pull/327)",
"### Describe the problem
Hello ladies and gents.
I am fairly new to Svelte/Sveltekit but I would like to make a suggestion please.
It is regarding documentation...
Can somebody change the background color of the sidebar from this difficult color #676778; to something darker?
Literally every time you want to focus on a link on the sidebar your eyes hurt and you lose at least 1 to 2 seconds until you focus on what you want to find and all that because poor contrast.
Also the ""highlighted"" link (the active one) is, honestly, almost indistinguishable from the inactive links below and above, why is this? Why not make it something like almost black background with white inactive links and the active link like the orange color from the logo? Perhaps put a divider line in between links or something?
By the way, I do these things every time I use the docs, this is how bad it is, no offense to anyone, but 1-2 seconds of faster focus per person per time they visit the docs, well, we want to be talking about performance when we are talking about Svelte right? Please do the right thing, the nice svelte logo orange color matches great with a darker blackish background color, why put that ugly blueish/purplish (which is not dark enough for a bg color)?
To whoever can make this happen, PLEASE we need docs that can be scanned quickly. Here is the contrast score https://coolors.co/contrast-checker/ffffff-676778
It's almost poor... if you have any sort of sight issues... I mean come on, how come no one addressed this before, docs is a huge part of the process.
### Describe the proposed solution
Color for the ""active"" link => #ff3e00
Color for the sidebar background => #242426
Any dark enough background (with no colors, either blackish or grayish/blackish will do) ### Alternatives considered
Please adjust this so visually impaired or people with sight difficulties (even the ones with glasses) can focus easier on the links and traverse faster. ### Importance
i cannot use SvelteKit without it
### Additional Information
Every time I browse the docs I literally go to inspect and change these myself....!
Thanks for your understanding!",Bitter Frustration
awesome!,
"Based, Redpilled, and Mental Outlaw's alter deepfake :cat: :+1:",Mocking
"Btw, just some ideas: maybe add reviews section to recipes somehow? I know it is meant to be static, but it would be nice to know if what I am making is good/accurate or not. I saw some recipes that sounded nice in theory, but that did not have any images or ratings so I had no idea if I should make them. IMO for a newer iteration of the site, you should make at least one image mandatory for recipes and also ratings would be nice :)",
"The Hugo version of the site is now up. Doesn't look much different, but infinitely easier to manage and compiles in milleseconds. I'm going to start by organizing and systematizing tags and such, then add other improvements.",
"I've added all the author names into metadata, so I don't have to fret about people refusing to follow the formatting guidelines lol
Might add cook/prep time to metadata in the future.
Site is very clean on the backend now. I'm still mastering hugo to do all I want, but I'm feeling like I might start looking at new submissions and maybe I'll start curating things a little more since it's easier.",Mocking
Request to change the current text color (blue). I suffer from chromatic aberration and have a very difficult time seeing the color blue. My humble request is to chance the blue text color to another color.,
"@reidellawfirm It could be a settings option stored in localstorage for that case, or you could try a css plugin to change all ""a"" elements to a different color.",
"I'm not tethered to the current colors, so I am open to changing it to something else someone suggests.",
"Changed the colors to be warmer, as in #221",
Hey @LukeSmithxyz I think the links would look better without the underlines. What do you think? Thanks.,
"@LukeSmithxyz I agree with @peepopoggers . Perhaps a bit more spacing between the radio buttons or removing the underlines would be good as the list seems too cramped right now. Could also be the yellow.
Glad to hear you've switched to Hugo.",
"Removing link underlines in general [hurts accessibility](https://seirdy.one/2020/11/23/website-best-practices.html#in-defense-of-link-underlines), however it would definitely be more readable and it wouldn't be so much of an issue if it is only removed from the list. Adding more padding would make the site more usable for mobile users. Therefore, I would suggest something like:
```css
#artlist a, .tagcloud a {
padding: 0.2em;
display: inline-block;
text-decoration: none;
}
```",
"> @reidellawfirm It could be a settings option stored in localstorage for that case, or you could try a css plugin to change all ""a"" elements to a different color.
Thanks for the suggestion, great idea to look into a css plugin.",
"@isti03 I looked at the site you linked; for the bullet points, there are never links next to each other so it is fine.",
"I think that the website could done build using crystal, php or even nodejs and really not relaying on static html pages.
Yes its a great idea to generated them to static html.
But is it worth it? I don't think so, let the server generate it when it requested that it!
The recipes are very small as I can see and it would not take a lot of time to generated it live, right?
You can also cache the generated pages if you want to be more ""economy"".",Impatience
"> I think that the website could done build using crystal, php or even nodejs and really not relaying on static html pages. Yes its a great idea to generated them to static html. But is it worth it? I don't think so, let the server generate it when it requested that it! The recipes are very small as I can see and it would not take a lot of time to generated it live, right? You can also cache the generated pages if you want to be more ""economy"".
I can never tell if these people are kidding or not.",Impatience
php :skull:,Mocking
"I haven't touched this site in like a year because the static site generator used was hobbled-together and couldn't scale without constantly breaking and taking minutes to recompile.
I've decided to switch to Hugo to compile the site, and I hope to finish this in the next week. I didn't switch before because Hugo themes are kind of a pain to specialize for the unfamiliar, but I've since gained familiarity with it enough that I can confidently make a setup perfectly to my vision for the site.
I'll use this as an opportunity to make other improvements to the site.
Note that I still do not want to drastically increase the number of recipes right now. In fact, in the future, I might slim them down to undebatably ""based"" recipes only. (I am no longer ignoring notifications from this git repository.)",Bitter Frustration
@suhan-paradkar,
VNC doesn't have GLX extension. So forget about everything that uses GLX at least until someone manages to get it working.,Bitter Frustration
"> VNC doesn't have GLX extension. So forget about everything that uses GLX at least until someone manages to get it working.
I've seen people in the reddit group who do it, and it works well for them.",Bitter Frustration
They probably used XSDL.,Mocking
"No, is vnc:
https://www.reddit.com/r/termux/comments/on4fkp/openglglfw_gears_on_termux_x11_no_root/?utm_medium=android_app&utm_source=share",
"Here is an output of `xdpyinfo`:
<p align=""center""><img src=""https://user-images.githubusercontent.com/25881154/127044060-f1611ab3-7460-4c25-af43-1251de352be5.png"" width=""50%""/></p>
There no GLX and I even has disabled it intentionally for tigervnc which uses same build options as `xorg-xvfb-server`.",Bitter Frustration
What is on reddit is GLFW examples and not `glxgears`.,
The GLFW examples also happen to you,Bitter Frustration
even freeglut fails me,Bitter Frustration
@Yisus7u7 I really have no idea whether freeglut works or not. These packages were added as requested and since then I didn't get any reports about whether they actually work or not.,
Oh ok :(,Impatience
"I just got `glxgears` working ([sources](https://github.com/JoakimSoderberg/mesademos/blob/master/src/xdemos/glxgears.c)). So it may rather depend on where you got it and how it was compiled, but not on GLX extension presence.
<p align=""center""><img src=""https://user-images.githubusercontent.com/25881154/127045923-4595e77e-a788-4630-9445-1888447e21e6.png"" width=""50%""/></p>",
Thanks,
@Yisus7u7 maybe because you're on arm32,Mocking
"> @Yisus7u7 maybe because you're on arm32
Why?",Mocking
"![Screenshot_20210928-114239](https://user-images.githubusercontent.com/64093255/135129936-02cb98e3-e66c-4b52-9eeb-75dd4dc0dbc4.png)
@xeffyr I tried the one in that repository and it doesn't work either.",
"> > @Yisus7u7 maybe because you're on arm32
> > Why?
mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu",Mocking
"> > > @Yisus7u7 maybe because you're on arm32
> > > > > > Why?
> > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
I guess patches should be made for arm32",Impatience
"> > > > @Yisus7u7 maybe because you're on arm32
> > > > > > > > > Why?
> > > > > > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> > Then you have to see what the problem is and create patches.
If I was smart enough to do that I would've already done that ðŸ¥´ ...
someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...
can you give me a test program to check wheather OpenGL works or not ?
btw ... i have those scripts on my GitHub repo ...
just go to Termux-Faint-Hope repo and copy the patches folder in /sdcard/ and run the myScripts/build-zink.sh or myScripts/build-mesa.sh",Bitter Frustration
"> > > > > @Yisus7u7 maybe because you're on arm32
> > > > > > > > > > > > Why?
> > > > > > > > > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> > > > > > Then you have to see what the problem is and create patches.
> > If I was smart enough to do that I would've already done that ðŸ¥´ ...
> > someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...
> > can you give me a test program to check wheather OpenGL works or not ?
https://github.com/davidanthonygardner/glxgears
Try compiling and running that",Bitter Frustration
"> > > > > > @Yisus7u7 maybe because you're on arm32
> > > > > > > > > > > > > > > Why?
> > > > > > > > > > > > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> > > > > > > > > Then you have to see what the problem is and create patches.
> > > > > > If I was smart enough to do that I would've already done that ðŸ¥´ ...
> > someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...
> > can you give me a test program to check wheather OpenGL works or not ?
> > https://github.com/davidanthonygardner/glxgears
> > Try compiling and running that
haha wth ... now even linking doesn't work ? I'll have to recompile it then check
btw link for the script is is previous comment, could you check it works or not ?
![Screenshot_2021-09-29-05-29-06-812_com termux](https://user-images.githubusercontent.com/39275638/135179179-3a98d6e5-6deb-4963-8f78-491d89363b3e.jpg)",Bitter Frustration
I also had that mistake once I compiled mesa... I think bookstores are broken.,Irony
"> I also had that mistake once I compiled mesa...
> > I think bookstores are broken.
ah nvm ... removing ndk-multilib fixed it ...
![Screenshot_2021-09-29-05-37-32-411_com termux](https://user-images.githubusercontent.com/39275638/135179704-87b36ee6-58e5-4489-a8eb-84a93397bace.jpg)",
it works on proot-distro!,
"> it works on proot-distro!
maybe Termux problem?",Impatience
"> it works on proot-distro!
how exactly ? on a 32 bit arm device ? can you give me exact instructions ?",Impatience
"> > it works on proot-distro!
> > how exactly ? on a 32 bit arm device ? can you give me exact instructions ?
no, i use arm64 but you can view this vid:
https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s",Impatience
"> > > it works on proot-distro!
> > > > > > how exactly ? on a 32 bit arm device ? can you give me exact instructions ?
> > no, i use arm64 but you can view this vid:
> > https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s
on arm64 OpenGL works even normally ... no need for proot-distro",Impatience
"> > > > it works on proot-distro!
> > > > > > > > > how exactly ? on a 32 bit arm device ? can you give me exact instructions ?
> > > > > > no, i use arm64 but you can view this vid:
> > https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s
> > on arm64 OpenGL works even normally ... no need for proot-distro
Very true, the problem is 32-bit.",Impatience
"i'm sumitted a [problem](https://github.com/ptitSeb/gl4es/issues/342) in gl4es project
let's wait for respnse.",Impatience
"> umm ... can i have the wallpaper link ?
https://github.com/Yisus7u7/monas_chinas_uwu_uwu",
@yisus7u7 do your phone has access son gl4es hardware?,Mocking
"> termux/termux-packages#10226
Here is how I compiled `glxgears` (I intentionally mentioned only the .c file and not the whole repo):
```
~ $ curl -LO https://github.com/JoakimSoderberg/mesademos/raw/master/src/xdemos/glxgears.c
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
100 154 100 154 0 0 1029 0 --:--:-- --:--:-- --:--:-- 1033
100 21237 100 21237 0 0 86261 0 --:--:-- --:--:-- --:--:-- 86261
~ $ clang glxgears.c -o glxgears -lGL -lX11 -lm
```
Running it:
```
~ $ export DISPLAY=:1
~ $ ./glxgears 7265 frames in 5.0 seconds = 1452.936 FPS
```
P.S. I don't use custom mesa with these experimentals with gallium, zink, adreno drivers and other attempts to enable hardware acceleration. Simply installed `build-essential xorgproto libx11 mesa tigervnc`.",
"![Screenshot_20210929-081316](https://user-images.githubusercontent.com/64093255/135275350-30101c43-2ee3-4a93-b4ac-7ba7ea3f1a77.png)
@xeffyr just doesn't work on arm (32-bit)",Bitter Frustration
"According to this comment
https://github.com/ptitSeb/gl4es/issues/342#issuecomment-931310418
Could it be that disabling glx and mesa-dri has to do with this?",
"@Yisus7u7 It **works** on AArch64, i686 and x86_64. So nothing to do with it. But note that gl4es is broken completely, see https://github.com/termux/x11-packages/issues/442 for details.",
"> @Yisus7u7 It **works** on AArch64, i686 and x86_64. So nothing to do with it. But note that gl4es is broken completely, see termux/x11-packages#442 for details.
It's not just gl4es, glu, freeglut, glew and other openGL-related libraries not working",Bitter Frustration
"I try glxgears on xwayland (use termux x11 app) and it crash too (Segmentation Fault)
I use 32 bit prefix termux bootstrap (my device actually 64 bit)
On 64 bit termux bootstrap (default) glxgears works fine in xwayland",
Then we can confirm that in arm (32-bit) OpenGL does not work,
"> Then we can confirm that in arm (32-bit) OpenGL does not work
![Screenshot_20211007-161825_Termux_X11](https://user-images.githubusercontent.com/11465849/136356578-1e6b30d0-4d37-424f-830b-48c48695bb68.png)
btw, i just use Xephyr to make nested x server in xwayland, it somewhat bad performance, but it can software renderer and glxgears work in Termux X11 (i user 32 bit proot debian) my command:
(on termux)
termux-x11 &
(I login to proot debian 32 bit, then use this command)
env DISPLAY=:0 Xephyr :1 -fullscreen &
env DISPLAY=:1 xfce4-session
Then, run glxgears from that session...",
https://github.com/termux/termux-x11/releases,
"Use termux from fdroid is fine, i also from fdroid too, its not problem... you need to install termux-x11.deb inside termux",
"> > Then we can confirm that in arm (32-bit) OpenGL does not work
> > btw, i just use Xephyr to make nested x server in xwayland, it somewhat bad performance, but it can software renderer and glxgears work in Termux X11 (i user 32 bit proot debian)
> > my command: (on termux) termux-x11 & (I login to proot debian 32 bit, then use this command) env DISPLAY=:0 Xephyr :1 -fullscreen & env DISPLAY=:1 xfce4-session
> > Then, run glxgears from that session...
@xeffyr Based on this, will OpenGL in arm not be able to access any driver or library?",
"@Yisus7u7 x11-packages **never** supported any kind of hw acceleration (officially) and therefore doesn't require any driver to work. In fact, mesa here is provided to fullfill dependency needs of gtk and qt5 libs and maybe for some other packages. I don't know how deeply OpenGL is broken but on my device `glxgears` [works](termux/termux-packages#10226) as was shown previously.",
"Well, I don't know if glxgears use opengl, but it still fails in arm.",Bitter Frustration
"`glxgears` use exactly OpenGL (not OpenGLES) and yes, it still fails because not fixed.",Bitter Frustration
"> Well, I don't know if glxgears use opengl, but it still fails in arm.
Are you in proot? If yes, you can just run Xephyr on top of Termux Wayland (Termux X11) or Xserver-xsdl.... and you can run glxgears there and some 3D software rendering too.. but performance is very low fps",
"> > Use termux from fdroid is fine, i also from fdroid too, its not problem... you need to install termux-x11.deb inside termux
> > why this error ?
> > ![Screenshot_2021-10-09-06-27-15-493_com termux](https://user-images.githubusercontent.com/39275638/136637231-580fac1b-f033-4623-830c-77ffc4e2f7a6.jpg)
Download from github action, use the latest build
https://github.com/termux/termux-x11/actions",Impatience
"aftef you run xephyr :1, then just
`env DISPLAY=:1 glxgears`
Then view it in your termux X11 app
Btw, this is for proot distro, the xephyr unavailable in termux repo
You need to inside proot (proot-distro) for this method",
"I try adding `TERMUX_DEBUG_BUILD=true` into https://github.com/termux/x11-packages/blob/master/packages/mesa/build.sh
Rebuild, install and use gdb to get these outputs
I tested this on aarch64 but installing arm32 Termux app from https://github.com/termux/termux-app/actions and it's reproducible
So pretty much confirm arm32 Mesa is broken
```
Application version:
0.117
Packages CPU architecture:
arm
Subscribed repositories:
# sources.list
deb https://packages.termux.org/apt/termux-main/ stable main
# science-repo (sources.list.d/science.list)
deb https://packages.termux.org/apt/termux-science science stable
# x11-repo (sources.list.d/x11.list)
deb https://packages.termux.org/apt/termux-x11 x11 main
# game-repo (sources.list.d/game.list)
deb https://packages.termux.org/apt/termux-games games stable
Updatable packages:
ed/stable 1.17-4 arm [upgradable from: 1.17]
nano/stable 5.9 arm [upgradable from: 5.8]
pcre2/stable 10.38 arm [upgradable from: 10.37]
termux-tools/stable 0.141 all [upgradable from: 0.140]
Android version:
11
Kernel build information:
Linux localhost 4.4.177-22274331 termux/x11-packages#1 SMP PREEMPT Tue Sep 7 18:27:31 +07 2021 armv8l Android
Device manufacturer:
samsung
Device model:
SM-A307GN
```
```
Starting program: /data/data/com.termux/files/usr/bin/glxdemo [New LWP 16252]
Thread 1 ""glxdemo"" received signal SIGSEGV, Segmentation fault.
glShadeModel (mode=7424)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2981
2981	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 16253]
[New LWP 16254]
[LWP 16254 exited]
[LWP 16253 exited]
Thread 1 ""glxdemo"" received signal SIGSEGV, Segmentation fault.
glShadeModel (mode=7424)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2981
2981	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0 glShadeModel (mode=7424)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2981
_tbl = 0x0
_func = <optimized out>
termux/x11-packages#1 0xaaaaa97e in main (argc=<optimized out>, argv=<optimized out>)
at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxdemo.c:120
dpy = 0xf742fa00
win = 2097154
```
```
Starting program: /data/data/com.termux/files/usr/bin/glinfo Program received signal SIGSEGV, Segmentation fault.
glDrawBuffer (mode=1028)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3156
3156	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 4808]
[New LWP 4809]
[LWP 4809 exited]
[LWP 4808 exited]
Thread 1 ""glinfo"" received signal SIGSEGV, Segmentation fault.
glDrawBuffer (mode=1028)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3156
3156	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0 glDrawBuffer (mode=1028)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3156
_tbl = 0x0
_func = <optimized out>
termux/x11-packages#1 0xf6aea19c in fgOpenWindow () from /data/data/com.termux/files/usr/lib/libglut.so
No symbol table info available.
termux/x11-packages#2 0xf6ae87b0 in fgCreateWindow () from /data/data/com.termux/files/usr/lib/libglut.so
No symbol table info available.
termux/x11-packages#3 0xf6aea236 in glutCreateWindow ()
from /data/data/com.termux/files/usr/lib/libglut.so
No symbol table info available.
termux/x11-packages#4 0xaaaaa674 in main (argc=1, argv=0xfffef2c4)
at /home/builder/.termux-build/mesa-demos/src/src/demos/glinfo.c:18
No locals.
```
```
Starting program: /data/data/com.termux/files/usr/bin/glxinfo Program received signal SIGSEGV, Segmentation fault.
glGetString (name=7936)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3667
3667	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 5387]
[New LWP 5389]
[LWP 5389 exited]
[LWP 5387 exited]
Thread 1 ""glxinfo"" received signal SIGSEGV, Segmentation fault.
glGetString (name=7936)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3667
3667	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0 glGetString (name=7936)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3667
_tbl = 0x0
_func = <optimized out>
termux/x11-packages#1 0xaaaac34e in print_screen_info (dpy=0xf742fa00, scrnum=0, opts=0xfffef258, coreProfile=<optimized out>, es2Profile=0, limits=0, coreWorked=0)
at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxinfo.c:468
serverVersion = 0xf70766b0 <Fake_glXQueryServerString.version> ""1.4 Mesa 17.3.9""
clientVersion = 0xf70762c8 <Fake_glXGetClientString.version> ""1.4 Mesa 17.3.9""
glRenderer = <optimized out>
displayName = <optimized out>
glVendor = <optimized out>
glExtensions = <optimized out>
period = <optimized out>
glxExtensions = 0xf6fe74fb ""GLX_MESA_copy_sub_buffer GLX_MESA_pixmap_colormap GLX_MESA_release_buffers GLX_ARB_create_context GLX_ARB_get_proc_address GLX_EXT_texture_from_pixmap GLX_EXT_visual_info GLX_EXT_visual_rating GLX_SGI""...
glVersion = <optimized out>
colon = <optimized out>
extfuncs = {GetProgramivARB = <optimized out>, GetStringi = <optimized out>, GetConvolutionParameteriv = <optimized out>}
serverVendor = 0xf6fe74d9 ""Brian Paul""
serverExtensions = 0xf6fe74fb ""GLX_MESA_copy_sub_buffer GLX_MESA_pixmap_colormap GLX_MESA_release_buffers GLX_ARB_create_context GLX_ARB_get_proc_address GLX_EXT_texture_from_pixmap GLX_EXT_visual_info GLX_EXT_visual_rating GLX_SGI""...
clientVendor = 0xf6fe74d9 ""Brian Paul""
clientExtensions = 0xf6fe74fb ""GLX_MESA_copy_sub_buffer GLX_MESA_pixmap_colormap GLX_MESA_release_buffers GLX_ARB_create_context GLX_ARB_get_proc_address GLX_EXT_texture_from_pixmap GLX_EXT_visual_info GLX_EXT_visual_rating GLX_SGI""...
glxVersionMajor = <optimized out>
glxVersionMinor = <optimized out>
height = 100
width = 100
ctx = 0xf70b61c0
root = <optimized out>
oglstring = 0xaaaae58c ""OpenGL""
fbconfigs = <optimized out>
visinfo = <optimized out>
mask = 10250
attr = {background_pixmap = 2863353856, background_pixel = 0, border_pixmap = 1, border_pixel = 0, bit_gravity = 40, win_gravity = -147447808, backing_store = 0, backing_planes = 4294898232, backing_pixel = 4147188316, save_under = -69032, event_mask = 163840, do_not_propagate_mask = 1, override_redirect = -148363805, colormap = 2097153, cursor = 4148361728}
win = 2097154
termux/x11-packages#2 0xaaaac014 in main (argc=<optimized out>, argv=<optimized out>)
at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxinfo.c:1261
opts = {mode = Normal, findBest = 0 '\000', limits = 0 '\000', singleLine = 0 '\000', displayName = 0x0, allowDirect = 1 '\001'}
dpy = 0xf742fa00
numScreens = 1
scrnum = 0
coreWorked = <optimized out>
```
```
Starting program: /data/data/com.termux/files/usr/bin/glxgears [New LWP 5771]
Thread 1 ""glxgears"" received signal SIGSEGV, Segmentation fault.
glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
2862	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 5773]
[New LWP 5774]
[LWP 5774 exited]
[LWP 5773 exited]
Thread 1 ""glxgears"" received signal SIGSEGV, Segmentation fault.
glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
2862	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0 glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
_tbl = 0x0
_func = <optimized out>
termux/x11-packages#1 0xaaaab8d8 in init ()
at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:398
pos = {5, 5, 10, 0}
red = {0.800000012, 0.100000001, 0, 1}
green = {0, 0.800000012, 0.200000003, 1}
blue = {0.200000003, 0.200000003, 1, 1}
termux/x11-packages#2 0xaaaab4c4 in main (argc=<optimized out>, argv=<optimized out>)
at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:790
winWidth = <optimized out>
winHeight = <optimized out>
x = <optimized out>
y = <optimized out>
i = <optimized out>
printInfo = 0 '\000'
dpyName = <optimized out>
dpy = 0xf742fa00
visId = <optimized out>
ctx = 0xf70f21c0
win = 2097154
#0 glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
_tbl = 0x0
_func = <optimized out>
termux/x11-packages#1 0xaaaab8d8 in init ()
at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:398
pos = {5, 5, 10, 0}
red = {0.800000012, 0.100000001, 0, 1}
green = {0, 0.800000012, 0.200000003, 1}
blue = {0.200000003, 0.200000003, 1, 1}
termux/x11-packages#2 0xaaaab4c4 in main (argc=<optimized out>, argv=<optimized out>)
at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:790
winWidth = <optimized out>
winHeight = <optimized out>
x = <optimized out>
y = <optimized out>
i = <optimized out>
printInfo = 0 '\000'
dpyName = <optimized out>
dpy = 0xf742fa00
visId = <optimized out>
ctx = 0xf70f21c0
win = 2097154
```
Not sure how useful are these logs. Might be a red herring.",
"![Screenshot_20211016-152508](https://user-images.githubusercontent.com/64093255/137601417-26228ba0-6245-4aee-9acf-50b9b9840cc4.png)
![Screenshot_20211016-152816](https://user-images.githubusercontent.com/64093255/137601418-e74a9f84-467b-4376-93a3-efdfdcc3dc39.png)
@xeffyr Posh Shell (armhf alpine, proot) has no problem running openGL, so the error will be that any specific hardware/driver cannot be accessed?",Impatience
Apparently Xephyr and Posh Shell have no problem with openGL apart from its very poor performance. Note that both use wayland,Bitter Frustration
"![Screenshot_20211017-165820](https://user-images.githubusercontent.com/64093255/137646590-bb603e86-08e9-4608-9e17-3c286ae5e1f3.png)
![Screenshot_20211017-165749](https://user-images.githubusercontent.com/64093255/137646591-b6ebcdf9-7850-4340-a955-d522f453c868.png)
@xeffyr Solved! Turn on mesa hardware acceleration
https://gxmersam.blogspot.com/p/getting-hardware-acceleration-on-termux.html?m=1",
"![Screenshot_20211017-170718](https://user-images.githubusercontent.com/64093255/137646658-dd076f78-042e-4c55-95bd-493757346a93.png)
![Screenshot_20211017-170626](https://user-images.githubusercontent.com/64093255/137646663-771e4da0-2b70-4158-9dc6-875a0002969f.png)
Enabling on mesa hardware acceleration",
"The performance is excellent, there is no lag, everything goes very fast",
"> The performance is excellent, there is no lag, everything goes very fast
Does it run on debian proot/chroot?",Impatience
"> > The performance is excellent, there is no lag, everything goes very fast
> > Does it run on debian proot/chroot?
No, it is fully native in termux x11
https://youtu.be/hsO-Os9jr8g",Impatience
"> > > The performance is excellent, there is no lag, everything goes very fast
> > > > > > Does it run on debian proot/chroot?
> > No, it is fully native in termux x11
> > https://youtu.be/hsO-Os9jr8g
what really !!! have to check right now !!! ðŸ˜ƒ",Impatience
"> ![Screenshot_20211017-165820](https://user-images.githubusercontent.com/64093255/137646590-bb603e86-08e9-4608-9e17-3c286ae5e1f3.png) ![Screenshot_20211017-165749](https://user-images.githubusercontent.com/64093255/137646591-b6ebcdf9-7850-4340-a955-d522f453c868.png)
> > @xeffyr Solved! Turn on mesa hardware acceleration
> > https://gxmersam.blogspot.com/p/getting-hardware-acceleration-on-termux.html?m=1
this is currently using the llvmpipe driver unless you specified GALLIUM_DRIVER=zink",
"Actually its better to run in chroot linux because it hss many 3D app/games , also box86 wine.. But if it still in termux desktop, i think there is no app/games there..",
"![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg)
![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg)
this was using the Zink driver",
"> ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
Can it work on Adreno 506?",Impatience
"> > ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
> > Can it work on Adreno 506?
it should work on all GPUs that support vulkan",
"> > > ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
> > > > > > Can it work on Adreno 506?
> > it should work on all GPUs that support vulkan
Can you give me tutorial link? I tried few days ago, i read tutorial somewhere, but in the end, i compile and not work.. Is it like turnip? do you have tutorial?
I will try in chroot..",Bitter Frustration
"> > > > ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
> > > > > > > > > Can it work on Adreno 506?
> > > > > > it should work on all GPUs that support vulkan
> > Can you give me tutorial link? I tried few days ago, i read tutorial somewhere, but in the end, i compile and not work.. Is it like turnip? do you have tutorial?
> > I will try in chroot..
you could try the instructions in https://github.com/suhan-paradkar/tewmux-disabled/releases/download/mesa-hw/instructions.tar.gz
i dont know weather it will work in an chroot envionment",Impatience
You can try [this](https://youtu.be/PO75bU3zGm0) in chroot environment,
"> > > The performance is excellent, there is no lag, everything goes very fast
> > > > > > Does it run on debian proot/chroot?
> > No, it is fully native in termux x11
> > https://youtu.be/hsO-Os9jr8g
ok i couldn't get it to work ... i'll ask you in telegram ... please respond me there",Impatience
"![Screenshot_20211022-204551](https://user-images.githubusercontent.com/64093255/138537854-b5a31685-bf01-4a29-88ae-a10bc90c3dc9.png)
![Screenshot_20211022-204516](https://user-images.githubusercontent.com/64093255/138537855-5e4f04cb-2a7e-451b-bd4e-8a2cb1da4918.png)
@xeffyr @suhan-paradkar I just killed two birds with one stone
### 1 - OpenGL crash in 32 bits arm devices **How to fix it?**
- Enable opengl in mesa, vulkan drivers and galium drivers. - I haven't tried zink, but you don't need to enable zink - Add wayland, libsha1 mesa-gl, mesa-dri packages - Enable opengl in libepoxy package - Enable opengl, Xephyr, dri1 dri2 dri3, in xorg-xserver And voila!, everything will work smoothly in both Xwayland and vnc. ### 2 - How to fix gl4es is slow Just follow the steps in step 1, and voila, gl4es will go fast and smooth. ### Additional Notes Do not use ndk-multilib for compilation, leave OpenGL broken for compilation of more packages in the future It works perfectly, I don't get segmentation failures and the optimization is excellent, I don't know if this works also in 64 bits. The slowness of gl4es must be because the graphics are forced, since mesa does not provide a correct rendering, I think that explains why gl4es is slow I was guided by this page : https://gxmersam.blogspot.com/p/getting-hardware-acceleration-on-termux.html?m=1
And I go back and repeat, I'm not using zink, you don't need zink to work. I hope you find all this information useful",
"from what i have seen before mesa has some issues when compiling in ci, mainly saying llvm-config not found",
"> @Yonle
Please **__do not mention me__** as __i'm not interested with this__. I repeat, **__do not mention random user__**.",Bitter Frustration
"a little help ?
i just can't seem to get it work ðŸ˜¥",Bitter Frustration
"> > @Yonle
> > Please ****do not mention me**** as **i'm not interested with this**. I repeat, ****do not mention random user****.
Sorry I mentioned it by mistake",Bitter Frustration
"Lots of off-topic and overquoting.
I'm locking issue now. If you know how to fix - submit a pull request with changes to package versions, build configuration and patches (whatever is required).",Bitter Frustration
"Note that `mesa-demos`, the package that contains `glxgears`, has been disabled now. This issue is not worth being open.",Bitter Frustration
"**Problem description**
The window comes out for a few seconds and then disappears
![Screenshot_20210726-122626](https://user-images.githubusercontent.com/64093255/127032377-f4cc25ec-1f05-49ea-aeb5-eefc9f4f893d.png)
Info:
Android 8.0 32 bits
2GB of ram
DE: lxqt
WM: Openbox Using vnc
In proot it works, but in termux anything related to OpenGL does not want to work
**Steps to reproduce**
<!--
Exact steps to reproduce the behavior.
Please post all commands that are necessary to reproduce the issue.
-->
**Expected behavior**
<!--
A clear and concise description of what you expected to happen.
-->
<!--
Please, post output of command `termux-info`. If it is not working for you
for whatever reason, post Android OS version and what CPU architecture you have.
Markdown code formatting should be applied.
-->",
"I'll just close this issue.
@irodionzaytsev if you want your issue to ever be considered, first please be respectful in your discussions and then try to provide a reproducible example showing the wrong behavior.",Impatience
"### Describe the bug
ndcg_score doesn't work. you're a bunch of useless spaghetti coders, just go to italian restaurant if you like pasta, don't waste other people's time producing tons of govnokod.
### Steps/Code to Reproduce
import ndcg_score
### Expected Results
work
### Actual Results
no work
### Versions
```shell
all
```",Bitter Frustration
I'm closing this. Feel free to dig deep and figure things out. Your tone in the message does not invite anyone of the maintainers in giving you an answer. They owe you nothing after all.,Insulting
"You are killing me!
Why I can't use Pino-pretty in a pipeline for formatting data before sending to any target?
It is very simple approach format it before sending to anywhere...
But instead this I should spend a lot of time to figure out how to format my data before sending to any target.
Why I can't use option ""prettyPrint: true"" for formatting before sending to anywhere?
Why I can't just reuse Pino-pretty for formatting?",Bitter Frustration
We're planning on deleting this demo anyway.,
"Reopening issue. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code
**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.
**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.
**Artifacts Found**
- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar
**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",
"Updating issue after ping interval. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code
**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.
**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.
**Artifacts Found**
- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar
**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",
"Updating issue after ping interval. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code
**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.
**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.
**Artifacts Found**
- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar
**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",
"_This issue was automatically created by [Allstar](https://github.com/ossf/allstar/)._
**Security Policy Violation**
Project is out of compliance with Binary Artifacts policy: binaries present in source code
**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.
**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.
**Artifacts Found**
- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar
**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.
---
Allstar has been installed on all Google managed GitHub orgs. Policies are gradually being rolled out and enforced by the GOSST and OSPO teams. Learn more at http://go/allstar
This issue will auto resolve when the policy is in compliance.
Issue created by Allstar. See https://github.com/ossf/allstar/ for more information. For questions specific to the repository, please contact the owner or maintainer.",
"Status update:
The plane demo is gone. But the mobile/android directory remains.",Bitter Frustration
"I'm sorry, v-time-picker and v-date-picker not coming with this release?",Impatience
What about v-data-table?,Impatience
What about v-skeleton-loader ?,Impatience
"@michaelnguyen08, @omerkimel , @JesusFregoso You can see the list of components that didn't make the 3.0 cut in the [3.1 milestone](https://github.com/vuetifyjs/vuetify/milestone/56).
These were cut from the initial release fairly early this year. Noteably this includes the following components:
- Calendar
- TimePicker
- data-iterator
- date picker
- v-stepper
- v-speed-dial
- v-skeleton-loader
There are some feature requests in the milestone that I am hoping John and the team will punt to a later release to expedite getting these base components released. v-data-table was mentioned in above, but that is supposed to be in v3.0.0 milestone. I just looked at the docs for v3 and it looks like v-data-table was renamed to v-table. v-table is in the docs and in the list above so that appears to have made the cut for v3.0.",
"Hi. I just opened the node_modules/vuetify/lib/components directory and I can see, that a component directory ""VDataTable"" is included in the Beta.1 build. When I try to include it in my App, I see, that VUE is not able to resolve it ""[Vue warn]: Failed to resolve component: v-data-table"". Seems, that the component is just not registered. And I disagree, that will be renamed...",Bitter Frustration
"> Hi. I just opened the node_modules/vuetify/lib/components directory and I can see, that a component directory ""VDataTable"" is included in the Beta.1 build. When I try to include it in my App, I see, that VUE is not able to resolve it ""[Vue warn]: Failed to resolve component: v-data-table"". Seems, that the component is just not registered.
Directory is still there because its not that it is being removed, it just hasn't been converted.
this: https://github.com/vuetifyjs/vuetify/blob/next/packages/vuetify/src/components/index.ts
controls what actually get exported and is what is currently available to use",
"Where is `loading` prop in VBtn component ? Is it removed completely or added later ?
It was amazing button feature.",Impatience
"I am guessing that not only are components being cut, but also props? For example, Menu is missing a fantastic amount of props functionality -
https://vuetifyjs.com/en/api/v-menu/#props
https://next.vuetifyjs.com/en/api/v-menu/#props
I need `left` ðŸ˜•
What is the plan with this?",Impatience
"> I am guessing that not only are components being cut, but also props? For example, Menu is missing a fantastic amount of props functionality -
> > https://vuetifyjs.com/en/api/v-menu/#props https://next.vuetifyjs.com/en/api/v-menu/#props
> > I need `left` ðŸ˜•
> > What is the plan with this?
@aentwist Well v-menu is still WIP, you can track the progress here: https://github.com/vuetifyjs/vuetify/issues/13489 or in the [3.0.0 milestone](https://github.com/vuetifyjs/vuetify/milestone/45)
> I need `left` ðŸ˜•
I have not used v3 yet but based on the docs it seems like `:anchor=""start""` could be what you are looking for?",Impatience
"> I am guessing that not only are components being cut, but also props? For example, Menu is missing a fantastic amount of props functionality -
> > > > https://vuetifyjs.com/en/api/v-menu/#props
> > https://next.vuetifyjs.com/en/api/v-menu/#props
> > > > I need `left` ðŸ˜•
> > > > What is the plan with this?
I don't think we have any completely removed props. Some may have been combined to be more intuitive, but they are there.
In regards to v-btn, the v-progress-circular component wasn't ported when it was. It'll be there, I just have to get to it.",Impatience
"I was imagining the combination of `offset-y` and `left`, which is now done with `anchor=""bottom end""`. Thanks for the tip with `anchor`, I missed that in the Menu docs. Thanks to both for the info about the props.",
"> What about v-data-table?
see following answer for those interested: https://github.com/vuetifyjs/vuetify/issues/13479#issuecomment-1115233462",
Would like to suggest adding a Nuxt3 plugin to this list as well.,
"> Would like to suggest adding a Nuxt3 plugin to this list as well.
@MatthewAry I'd love to see first class nuxt support as well, though that's something done by [nuxt ](https://github.com/nuxt-community/vuetify-module) itself, not vuetify.
Related: https://github.com/vuetifyjs/vuetify/issues/14621#issuecomment-1016186592
https://github.com/nuxt-community/vuetify-module/issues/475",
@warflash If that's what @KaelWD says then I suppose that's how it is. However I am really anxious to see support for SASS Variables in Nuxt3 for the Vuetify project.,Impatience
"> Ð“Ð´Ðµ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ `loading`Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð² ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ðµ VBtn? ÐžÐ½ ÑƒÐ´Ð°Ð»ÐµÐ½ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¸Ð»Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¿Ð¾Ð·Ð¶Ðµ? Ð­Ñ‚Ð¾ Ð±Ñ‹Ð»Ð° ÑƒÐ´Ð¸Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ ÐºÐ½Ð¾Ð¿ÐºÐ¸.
https://github.com/vuetifyjs/vuetify/issues/15056",Bitter Frustration
"For those components not included in the initial release, is there an incremental path for people who's apps use those components, or are they just going to have to wait until 3.1 before they can upgrade?",Impatience
"Isn't 3.0
3.1
3.2
3.x
incremental enough?
Just wait until everything is proper released.
I can't understand people needing to upgrade to a new version as soon as possible, even if it is Beta.
The only thing I would like would be a comparison table about which component gets added into which version.
Also you can wait for Vue 2.7 and Vueitfy 2.7.
These are already announced migration versions with backported features.....",Impatience
"> For those components not included in the initial release, is there an incremental path for people who's apps use those components, or are they just going to have to wait until 3.1 before they can upgrade?
If you rely on some of the components that aren't going to be in 3.0, I see two choices:
1. you wait until the release that contains all of the components you use
2. you replace the missing components with components you build
I have been waiting for Vuetify 3 so that I can migrate to Vue 3 and Vite, but I have chosen option 1 above. I use most of the components that aren't available in 3.0
- Data Table
- Calendar
- TimePicker
- date picker
- v-stepper
- v-skeleton-loader",Impatience
Please add v-data-table in this release. Thanks,Impatience
"> Please add v-data-table in this release. Thanks
How? It's not done.",Bitter Frustration
"> @MartinX3 I can't understand people needing to upgrade to a new version as soon as possible, even if it is Beta.
I see that you don't understand, but the point is if you start a new project, you DON'T want to start it in v.2.x and then to migrate it all to v.3.x. Take in mind that it is not only vuetify 3, but you are by being forced to use vuetify 2, forced to use vue 2 too! So it is pretty pretty painful enforcement!
Thus, developers and PMs are ""enforced"" and motivated to do additional work and workarounds etc to enable v.3 version working, then to have the enormous work later in migrating whole the project.
I hope that collaborators on Vuetify are aware of this too :)",Bitter Frustration
"@sinisarudan I see you prefer very much time and energy into complaining instead and also investing very much time and energy to do workarounds and extra stuff, just to use as soon as possible the newest not ready software.
Weird people. I hope you won't become my employee.
smart people just use vue 2 with vuetify 2 and just add the composition-api package and use typescript.
Also smart people know that according to the roadmap you can just wait for vue 2.7 and vuetify 2.7 to easily migrate your project on 3.x without much work.
(I use vue/vuetify as frontend for customers and also nuxtjs for (static) websites)
If you are so much into using vue/tify) 3 now go and pay for a support contract, go and donate, go and develop pull requests to accelerate the development of vue(tify) 3 or stop complaining.",Mocking
"I am not sure of anything anymore, I have read a lot of contradictory information... Will v-data-table be available in 3.0 or do I have to wait for 3.1?",Impatience
"In vuetify 3 documentation roadmap current release is alpha please correct this to avoide misconsiption
![Screenshot 2022-05-15 145002](https://user-images.githubusercontent.com/47514686/168473783-cfb08ef3-6b58-42c8-a94b-a98de7e55418.png)
.",Impatience
"?
![grafik](https://user-images.githubusercontent.com/4518092/168476170-0fa27988-444d-4821-9e3c-12510dad7538.png)",
i mean in this link https://next.vuetifyjs.com/en/introduction/roadmap/,
"`This is BETA documentation for Vuetify 3, examples and information may be broken or outdated.`",
"> I am not sure of anything anymore, I have read a lot of contradictory information... Will v-data-table be available in 3.0 or do I have to wait for 3.1?
It's not listed in checklist, so we need to wait for 3.1",Impatience
Target release is this month. Is this checklist actually only 1/90 or have they just not been updating it?,Impatience
@eliezerp3 if you want to avoid a delay please submit PR's to port the components.,Impatience
@MartinX3 Not looking for a fight. It was a genuine questionâ€¦,
"> @MartinX3 Not looking for a fight. It was a genuine questionâ€¦
I'm not looking for a fight.
Just the issue ticket gets updated daily if something changed. :P
(It's written there as a hint)",Irony
"I'm working on getting the tuning pass PRs up. Some will be involved reviews, over half are simple and will not.",
"@johnleider
Please check https://github.com/vuetifyjs/vuetify/pull/15132",Impatience
when can I use the vuetify 3.0.,Impatience
@Tan-Jerry after you submit PR's to accelerate porting.,Impatience
"> In regards to v-btn, the v-progress-circular component wasn't ported when it was. It'll be there, I just have to get to it.
@johnleider but the `v-progress-circular` component is in the beta 2",Impatience
"Yes, but it wasn't when `v-btn` was created in v3.",
"> Target release is this month. Is this checklist actually only 1/90 or have they just not been updating it?
I'm also interested on the update progress. No hurries, it will be ready when its ready, but it would be nice to have an eta, even if it is a bit vague. Best of luck to the contributors helping accelerate the update. No pressure at all!",Impatience
"> @Tan-Jerry after you submit PR's to accelerate porting.
@MartinX3 Did you try it by yourself ? PR's does not accelerate anything. Just for example (prior bugs for me)
https://github.com/vuetifyjs/vuetify/pull/15132
https://github.com/vuetifyjs/vuetify/pull/15129",Bitter Frustration
"Keep it civil please. We are going to get to everything, including community submitted PRs.",
"Hmm, I was trying to keep bearing with Vuetify 3 (even implementing some workarounds on our ongoing projects) until target release is here.
Yet:
- official deadline for Vuetify Target release has passed (May) but we're still in Beta
- there are no answers from developers regarding not-working functionality even the one documented in Beta as working https://github.com/vuetifyjs/vuetify/discussions/15054
- we do not get updates when something is going to be delivered, after the self-defined deadlined has passed, so we can hardly make valid decisions
- as some of you mentioned here, it seems that the TARGET release of 3.0 (expected in the passed May) won't be a ""real"" target release in sense it will still lack too many components of their functionality, compared to 2.x version ...
- I would like to hear some opposing reassuring thoughts from developers, yet it seems that the waiting and workarounds inspired by ""too optimistic"" announcements by Vuetify developers is not justified anymore :(
- Especially that this is not ony waiting for Vuetify 3 but also preventing us from switching from Vue2 to Vue 3 and all the new ecosystem being built around it (so waiting on 2.7 and suggested (automatic) migration to 3.x using it is neither solution, esp. that it has also been prolonged for several months)
In another thread https://github.com/vuetifyjs/vuetify/discussions/14776 the [Quasar](https://quasar.dev/) is mentioned as an alternative while we wait for the Vuetify 3 reaching that level of stability and versatility. I guess I will give it a try.
P.S. Dear @MartinX3 please keep it civil, multiple developers here get ""attacked"" by you for having different opinions. It's ok to have different opinions even calls for action, and by aggressive reactions to someones opinions we even get stronger reactions and fortify them in the opinions we might not like :) Best wishes to Vuetify team and keep your good work, we all, by being developers, know how it is to work under pressure and we should have empathy.
On the other side we also have our own dependent projects and our clients/bosses above our heads, and it would help both us and you I guess to be more transparent and objective regarding releases so that we can make our decisions on time and to avoid disappointments.
**The main info missing and being misleading as I see by reading community questions** is **on which date approximately or which release** we should expect that we have let's say **90% of the components/functionality provided in 2.x versions**
Thanks, best!",Bitter Frustration
"- They're estimates not deadlines.
- I don't have a link but that's been answered before, documentation is obviously one of the last things to be updated. - We don't know either.
- etc.
> 90% of the components/functionality provided in 2.x versions
I'd say we're pretty close, the only components missing are the big complex ones that depend on other functionality being stable:
- treeview ([in progress](https://github.com/vuetifyjs/vuetify/pull/14715))
- data table ([in progress](https://github.com/vuetifyjs/vuetify/tree/feat/v3-data-table))
- calendar ([in progress](https://github.com/vuetifyjs/vuetify/pull/15038))
- pickers ([in progress](https://github.com/vuetifyjs/vuetify/tree/feat/v3-date-picker))",
Thanks for the update!,
"thanks for the update @KaelWD , we're getting closer to some estimates.
Regarding DOCS, I'm not telling that the Docs are not updated.
On the contrary! The [v.3.0 docs for `v-select`](https://next.vuetifyjs.com/en/api/v-select/#props-items) state that `props-item` are working as expected.
Yet as I and other devs have stated, read [more here in the explained issue](https://github.com/vuetifyjs/vuetify/discussions/15054) - that's not the fact. I myself tried to test it even beyond docs in different ways..
The problem here is that we lose the point of reference/truth, on what can we rely in the current `beta.1` and the following versions of Vuetify?
Or you're trying to say that the docs DOES NOT reflect the currently released version and is written independently to reflect the final (desired) v.3.0?
If it does reflect, could anyone investigate that issue of v-select. As being one of the central UX components and not working as documented, I feel we are pushed for alternatives.
Thanks in andvance",Bitter Frustration
"You might've missed this
![Screenshot_20220602_213556](https://user-images.githubusercontent.com/16421948/171620794-aa64b58f-2add-4eaf-9c6d-e9ed577d71b1.png)
The example you linked to was for v2 and hasn't been updated yet, it needs `item-title=""text""` to work with those objects",
"> You might've missed this ![Screenshot_20220602_213556](https://user-images.githubusercontent.com/16421948/171620794-aa64b58f-2add-4eaf-9c6d-e9ed577d71b1.png) The example you linked to was for v2 and hasn't been updated yet, it needs `item-title=""text""` to work with those objects
Do you mind receiving PRs with documentation updates?",Impatience
"- I've seen that of course (actually every time I go through docs :) ) - yet I'm not worrying of is it outdated, but too advanced for the stated of the code it should document :)
- docs https://next.vuetifyjs.com/en/api/v-select/#props-item-props is for v3 and not v2 (I was using the proper doc)
- I was not trying any v2 example but was trying to make v3 v-select to work according to v3 [component doc](https://next.vuetifyjs.com/en/components/selects/), abd [api doc](https://next.vuetifyjs.com/en/api/v-select/#props-items).
- `item-title` prop is not documented, but **item-text, item-value and item-disabled** props. Nevertheless, neither is solving this issue.
![image](https://user-images.githubusercontent.com/1769627/171724295-81666a67-62d8-4eec-b555-e8f0ce7f6ad4.png)
- So, after trying all possible and impossible variants to handle `v-select` supporting objects instead of simple text, I can confirm it's not working. ANd as I've said, you have other dev(s) claiming the same.
- and that is not some advanced functionality, but needed for almost any app.
- if we accept that there is some Docs misbehaviour, the point is still: What is our point of reference then, when we use components and their props? or even for community-testing of the Beta versions to give you any feedback?",Bitter Frustration
"can we get any answer instead of marking the comment as outdated??
I would really like to keep giving chance to Vuetify, but by this kind of communication it's even harder to feel reliability of the framework.
Thanks",Bitter Frustration
"@johnleider Please keep the issue uptodate and add e.g. v-calender which is makes by @KaelWD (thx):
https://github.com/vuetifyjs/vuetify/pull/15038",
"> can we get any answer instead of marking the comment as outdated??
> > I would really like to keep giving chance to Vuetify, but by this kind of communication it's even harder to feel reliability of the framework.
> > Thanks
> > Reliability of the framework? Give me a break. Take this conversation to Discord and stop whining here.",Mocking
"> @johnleider Please keep the issue uptodate and add e.g. v-calender which is makes by @KaelWD (thx): #15038
It is up to date. `v-calendar` is not shipping with v3.0.",
"Here is my unsolicited feedback on this recent tangent, I think others might find it useful. Note: I am making assumptions about the state of things in this project so if I am wrong please feel free to correct me.
@sinisarudan Vuetify 3 is beta, which usually means:
- Development has priority over authoring docs right now. - Errata, and missing information is to be expected in the docs while in beta up until V3 reaches some unspecified level of maturity.
- The development team has no obligation to provide API guarantees until GA.
- The scope of the V3 initial release is locked in, what you see on the release checklist is meant to give you an idea how close/far they are to release.
- I have my own questions about V3 but I know that it would be inappropriate to ask them HOW to do something with V3 Beta, and more appropriate to ask them if a specific feature, or component will be included in the initial release (excluding anything that they may consider obvious)
- The dev team is free to ignore discussions or bug reports until V3 is released. They are more likely to interact with you if you intend to contribute code or if you engage with them on something that team members are currently concerned with or occupied with.
I specifically posted to this task, because I wanted to know about a feature. I didn't get a response, and it's no big deal for me right now. I have other things to worry about.
TLDR: Don't expect any kind of support from the dev team for non GA releases. They're still making V3 and will more likely converse with you as long as the discussion has to do with any of their present concerns.",Impatience
I think that beta is an incorrect tag for the current situation. It should be alpha. So many components doesn't work properly like they worked in 2.6 version. Everything great. Let's just wait. This is great UI framework. I am still waiting my v-data-table component,Bitter Frustration
"> I think that beta is an incorrect tag for the current situation. It should be alpha. So many components doesn't work properly like they worked in 2.6 version. Everything great. Let's just wait. This is great UI framework. I am still waiting my v-data-table component
Which components do not work properly?",Bitter Frustration
"> > I think that beta is an incorrect tag for the current situation. It should be alpha. So many components doesn't work properly like they worked in 2.6 version. Everything great. Let's just wait. This is great UI framework. I am still waiting my v-data-table component
> > Which components do not work properly?
The Autocomplete in doc is still broken.
![autocomplete](https://user-images.githubusercontent.com/16066720/171966231-df3e2ca7-8698-4d7e-94a3-e6b5dcacbb3a.png)",Bitter Frustration
"I feel like I shouldn't have to say this since it is plastered on every page of the next docs, but alas:
![image](https://user-images.githubusercontent.com/9064066/171969413-4c89abf3-1b19-448d-a21e-34711979eb7a.png)",Bitter Frustration
"> > I think that beta is an incorrect tag for the current situation. It should be alpha. So many components doesn't work properly like they worked in 2.6 version. Everything great. Let's just wait. This is great UI framework. I am still waiting my v-data-table component
> > Which components do not work properly?
Vuetify3 beta3
Component breadcrumbs. Has invalid styles and html structure
![image](https://user-images.githubusercontent.com/5112918/171979191-c0906edb-6461-4dac-b7c1-465308ddb278.png)
Vuetify3 beta3.
Validation Error should have red color but it is gray.
In beta2. It was red
![image](https://user-images.githubusercontent.com/5112918/171979212-2cc95267-112e-4aee-a0db-58081d7e77f0.png)
I sm uding vite and latest version of everything. let me know, I can provide more info. Very appreciate for everything",Bitter Frustration
"Please learn how to read and stop spamming the `Vuetify 3.0 Release CHECKLIST` issue ticket.
I didn't subscribe it to read wining people screaming for ETA without really helping the project and ignoring text warnings.",Bitter Frustration
"> Would like to suggest adding a Nuxt3 plugin to this list as well.
I will need help with this as I do not use NUXT. Happy to help however I can though.",
"# V3 Release Checklist
The following is an in progress list of remaining tasks before we release the next version. This will be updated daily; so watch for changes.
Discord, https://community.vuetifyjs.com/ and join channel **#v3-discussion**.
## Final review
- [x] Framework Core #15139
- [x] Documentation
- [x] UI Components
- [x] Tooling
### Documentation
- [x] Introduction
- [x] Getting started
- [x] Features
- [x] Styles and animations
- [x] Directives
- [x] Resources
- [x] About
### Tooling
- [x] CLI plugin
- [x] Vite plugin
- [x] Presets
- [x] Eslint
### UI Components
A component is considered complete when the following criteria is met:
<details>
* Primary and child components reviewed
* imports ordered with appropriate comments
* converted to use grid css structure (if applicable)
* unit tests
* jest spec test (if applicable)
* cypress e2e test
* SASS/SCSS code reviewed
* removes deprecated or unused code
* abstracted explicit values to variables
* verified sass variable usage
* index.ts file export formatted
* Documentation page
* all examples working with no deprecated code
* removed unused or deprecated examples
* added **all components** page image
* added quick bar and links
* updated page entry and formatting
* new sections added
* accessibility (if applicable)
* anatomy
* grid (if applicable)
* sass variables
* theme (if applicable)
* updated api section formatting
* frontmatter information reviewed
</details>
- [x] v-alert #14971
- [x] v-app #15179
- [x] v-app-bar #15192
- [x] v-autocomplete #15355
- [x] v-avatar #15112
- [x] v-badge #15193
- [x] v-banner #15109
- [x] v-bottom-navigation #15194
- [x] v-breadcrumbs #15181
- [x] v-btn #15213
- [x] v-btn-group #15195
- [x] v-btn-toggle #15196
- [x] v-card #15283
- [x] v-carousel #15197
- [x] v-checkbox #15264
- [x] v-chip #14973
- [x] v-chip-group #15198
- [x] v-code #15182
- [x] v-color-picker #15292
- [x] v-combobox #15354
- [x] v-counter #15183
- [x] v-default-provider
- [x] v-dialog #15289
- [x] v-divider #15113
- [x] v-expansion-panels #15295
- [x] v-field #15251
- [x] v-file-input #15288
- [x] v-footer #15114
- [x] v-form #15253
- [x] v-grid #15296
- [x] v-hover
- [x] v-icon #15212
- [x] v-img #15287
- [x] v-input #15260
- [x] v-kbd #15199
- [x] v-label #15256
- [x] v-lazy #15184
- [x] v-list #15360
- [x] v-locale-provider #15255
- [x] v-main #15254
- [x] v-menu 5e4992ffa
- [x] v-messages #15200
- [x] v-navigation-drawer #15263
- [x] v-no-ssr #15185
- [x] v-overlay #15284
- [x] v-pagination #15286
- [x] v-parallax #15226
- [x] v-progress-circular #15266
- [x] v-progress-linear #15265
- [x] v-radio #15258
- [x] v-radio-group #15259
- [x] v-range-slider #15228
- [x] v-rating #15285
- [x] v-responsive #15249
- [x] v-selection-control #15246
- [x] v-selection-control-group #15250
- [x] v-sheet #15115
- [x] v-slide-group #15248
- [x] v-slider #15227
- [x] v-snackbar #15242
- [x] v-switch #15247
- [x] v-system-bar #15243
- [x] v-table #15186
- [x] v-tabs #15262
- [x] v-text-field #15252
- [x] v-textarea #15257
- [x] v-theme-provider
- [x] v-timeline #15230
- [x] v-toolbar #15154
- [x] v-tooltip #15245
- [x] v-validation
- [x] v-window #15244
<!-- override-close -->",
good ideal,
"> Who's social media accounts were you looking for anyway? Why didn't you just ask them?
Private investigators, corporate fraud departments, law enforcement, etc. tend to use OSINT to identify someone.",Impatience
"My cheated piece of shit husband soon to be ex
On Sat, May 7, 2022, 3:47 PM hKQwHW ***@***.***> wrote:
> Who's social media accounts were you looking for anyway? Why didn't you
> just ask them?
>
> Private investigators, corporate fraud departments, law enforcement, etc.
> tend to use OSINT to identify someone.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/sherlock-project/sherlock/issues/1322#issuecomment-1120287344>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ASFMM7VPKCJ6DWG4M6GFPC3VI3JFRANCNFSM5UKL63JA>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>",Bitter Frustration
"I can't tell what you're talking about, but if you're using this to stalk your ex, go away.",Threat
"I don't like putting numbers in my screen name so if I can't have it this way, I can use one of my other 4 GitHub logins ðŸ¤·â€â™€ï¸
Is that ok? Who's social media accounts were you looking for anyway? Why didn't you just ask them?",Entitlement
same here !!!!,Impatience
"I can confirm this issue , exact same issue as Kerumen with the following differences in environment:
* Running on CentOS 7 (7,9.2009)
* Nodejs 16.15.0
* Installed locally
* Upgraded from 4,42,1
* Database is MariaDB 1.4.22 (AWS RDS)
* Browser is Chrome 101.0.4951.41 on Windows 11
Hope that info helps",
"Hi! Thank you for reporting this error ðŸ˜Š
- Could you post the full log output? - Post the result of this SQL query: `SHOW VARIABLES LIKE 'sql_mode'`",
"Here is the full log output:
```
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Updated 0 newsletters with created_at = now
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Dropping nullable: newsletters.created_at
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Rolling back: alter table `newsletters` modify `created_at` datetime not null default 'NULL' - Invalid default value f>
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Setting nullable: newsletters.created_at
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] ERROR alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]:
May 02 12:15:06 ns328387 node[2945016]: alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]: ""OuterError: alter table `newsletters` modify `created_at` datetime not null default 'NULL' - Invalid default value for 'created_at'""
May 02 12:15:06 ns328387 node[2945016]: Error ID:
May 02 12:15:06 ns328387 node[2945016]: 400
May 02 12:15:06 ns328387 node[2945016]: Error Code:
May 02 12:15:06 ns328387 node[2945016]: ER_INVALID_DEFAULT
May 02 12:15:06 ns328387 node[2945016]: ----------------------------------------
May 02 12:15:06 ns328387 node[2945016]: RollbackError: alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]: at DatabaseStateManager.makeReady (/www/lolita/versions/4.46.0/core/server/data/db/state-manager.js:95:32)
May 02 12:15:06 ns328387 node[2945016]: at RollbackError.KnexMigrateError (/www/lolita/versions/4.46.0/node_modules/knex-migrator/lib/errors.js:7:26)
May 02 12:15:06 ns328387 node[2945016]: at new RollbackError (/www/lolita/versions/4.46.0/node_modules/knex-migrator/lib/errors.js:31:26)
May 02 12:15:06 ns328387 node[2945016]: at KnexMigrator.migrate (/www/lolita/versions/4.46.0/node_modules/knex-migrator/lib/index.js:418:31)
May 02 12:15:06 ns328387 node[2945016]: at async DatabaseStateManager.makeReady (/www/lolita/versions/4.46.0/core/server/data/db/state-manager.js:86:17)
May 02 12:15:06 ns328387 node[2945016]: at async initDatabase (/www/lolita/versions/4.46.0/core/boot.js:69:5)
May 02 12:15:06 ns328387 node[2945016]: at async bootGhost (/www/lolita/versions/4.46.0/core/boot.js:414:9)
May 02 12:15:06 ns328387 node[2945016]: Error: alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]: at Packet.asError (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/packets/packet.js:728:17)
May 02 12:15:06 ns328387 node[2945016]: at Query.execute (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/commands/command.js:29:26)
May 02 12:15:06 ns328387 node[2945016]: at Connection.handlePacket (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/connection.js:456:32)
May 02 12:15:06 ns328387 node[2945016]: at PacketParser.onPacket (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/connection.js:85:12)
May 02 12:15:06 ns328387 node[2945016]: at PacketParser.executeStart (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/packet_parser.js:75:16)
May 02 12:15:06 ns328387 node[2945016]: at Socket.<anonymous> (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/connection.js:92:25)
May 02 12:15:06 ns328387 node[2945016]: at Socket.emit (events.js:400:28)
May 02 12:15:06 ns328387 node[2945016]: at addChunk (internal/streams/readable.js:293:12)
May 02 12:15:06 ns328387 node[2945016]: at readableAddChunk (internal/streams/readable.js:267:9)
May 02 12:15:06 ns328387 node[2945016]: at Socket.Readable.push (internal/streams/readable.js:206:10)
May 02 12:15:06 ns328387 node[2945016]: at TCP.onStreamRead (internal/stream_base_commons.js:188:23)
May 02 12:15:06 ns328387 node[2945016]:
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Ghost is shutting down
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Ghost has shut down
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Your site is now offline
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Ghost was running for a few seconds
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Bootstrap client was closed.
May 02 12:15:06 ns328387 systemd[1]: Stopping Ghost systemd service for blog: lolitaontheroad-keru-io...
May 02 12:15:07 ns328387 node[2944983]: /home/lolita/.nvm/versions/node/v14.16.1/lib/node_modules/ghost-cli/lib/process-manager.js:46
May 02 12:15:07 ns328387 node[2944983]: throw error;
May 02 12:15:07 ns328387 node[2944983]: ^
May 02 12:15:07 ns328387 node[2944983]: {
May 02 12:15:07 ns328387 node[2944983]: message: ""Ghost was able to start, but errored during boot with: alter table `newsletters` modify `created_at` datetime null default 'NULL' - I>
May 02 12:15:07 ns328387 node[2944983]: }
May 02 12:15:07 ns328387 systemd[1]: ghost_lolitaontheroad-keru-io.service: Main process exited, code=exited, status=1/FAILURE
May 02 12:15:07 ns328387 systemd[1]: ghost_lolitaontheroad-keru-io.service: Failed with result 'exit-code'.
May 02 12:15:07 ns328387 systemd[1]: Stopped Ghost systemd service for blog: lolitaontheroad-keru-io.
```
And here is the query result:
```
MariaDB [lolita_prod]> SHOW VARIABLES LIKE 'sql_mode';
+---------------+-------------------------------------------------------------------------------------------+
| Variable_name | Value |
+---------------+-------------------------------------------------------------------------------------------+
| sql_mode | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |
+---------------+-------------------------------------------------------------------------------------------+
1 row in set (0.026 sec)
```",
"Still getting this issue with 4.46.1, sadly! Running MariaDB on Debian, 10.5.15-MariaDB-0+deb11u1 Debian 11, also have not ever actually sent a newsletter so this table will be empty.
Here's output of above requested items:
+---------------+-------------------------------------------------------------------------------------------+
| Variable_name | Value |
+---------------+-------------------------------------------------------------------------------------------+
| sql_mode | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |
+---------------+-------------------------------------------------------------------------------------------+
Debug Information:
OS: Debian GNU/Linux, v11
Node Version: v16.15.0
Ghost Version: 4.46.1
Ghost-CLI Version: 1.19.3
Environment: production
Command: 'ghost upgrade -d *snip*'
Message: Ghost was able to start, but errored during boot with: alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
Suggestion: journalctl -u ghost_nor-no-media-co -n 50
Stack: Error: Ghost was able to start, but errored during boot with: alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
at Server.<anonymous> (/usr/lib/node_modules/ghost-cli/lib/utils/port-polling.js:73:28)
at Object.onceWrapper (node:events:641:28)
at Server.emit (node:events:527:28)
at emitCloseNT (node:net:1728:8)
at processTicksAndRejections (node:internal/process/task_queues:82:21)",Bitter Frustration
"Same problem here, rollbacked with a save to 4.45.0. I'm in mariadb",
"Hi all - this seems to be an issue with Knex and MariaDB. I've opened [an issue](https://github.com/knex/knex/issues/5154), so hopefully it can be fixed soon ðŸ™‚ Just to point out, MariaDB is not an officially supported database for Ghost. It just happened to work given the similarities with MySQL, but we optimize and test for MySQL 5 and 8. Ghost v5 [will](https://github.com/TryGhost/Ghost/issues/14446) **only** support MySQL 8 in production, so I'd strongly suggest switching to MySQL 8 to ensure you're running on the recommended setup.",
"Thanks @daniellockyer for the explanation.
Unfortunately there are some environments where MySQL is not supported ([Archlinux](https://wiki.archlinux.org/title/MySQL) for example) and we _have_ to use MariaDB. MariaDB is supposed to be a drop-in replacement of MySQL so everything should work as expected (and was working until `v4.46`).
I know this is not totally in your control, I just wanted to note this :)",
Is this issue fixed with 4.47?,Impatience
"The upstream issue hasn't been fixed yet - details in this comment: https://github.com/TryGhost/Ghost/issues/14634#issuecomment-1114989262
In the meantime, a friendly forum user has shared how he updated from MariaDB to MySQL8 on Ubuntu here: https://forum.ghost.org/t/how-to-migrate-from-mariadb-10-to-mysql-8/29575",Impatience
Experiencing the same issue since updating to the latest MariaDB and ghost docker images.,Bitter Frustration
"Migrated to MySQL. Fortunately I'm in containers mode with Docker.
Just created a temporary container with a new docker volume... Imported my saved database.
Remanaged the old container (using now mysql8) to use the new docker volume. Restarted all. All goods
There is now no more problems to use the last version of Ghost.
But I think it's stupid dropping mariaDB.",Bitter Frustration
"Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?",Impatience
"> But I think it's stupid dropping mariaDB.
Even more stupid was dropping PostgreSQL some time ago. It seems Ghost team is trying to get less and less people using their project. Now all of us should drop MariaDB just because Ghost is using some MySQL specific dialect.",Bitter Frustration
"> Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?
https://github.com/TryGhost/Ghost/issues/14446 > **MySQL 8 is supported in all environments & the only supported DB for production.**
and
**Note :MariaDB is not an officially supported database for Ghost. It just happened to work given the similarities with MySQL, but we optimize and test for MySQL 5 and 8. As of Ghost 5.0 we are only officially supporting MySQL8 in production so that we can double down on DB optimizations. We strongly recommend changing to MySQL8 and [a helpful guide can be found here](https://forum.ghost.org/t/how-to-migrate-from-mariadb-10-to-mysql-8/29575).**",Impatience
"> > Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?
> > #14446 > **MySQL 8 is supported in all environments & the only supported DB for production.**
Thanks, I also found the post on their forum too. Astoundingly stupid decision. I donâ€™t even use the â€œnewslettersâ€ stuff. ðŸ˜ž",Bitter Frustration
@Elrondo46 can we perhaps jump into a Discord chat or chat channel. Would you be able to give me a hand? I also run my blog in docker containers using MariaDB...,
yeah why not I don't want to send my personal tag in public,
"Solely relying on Oracle's solution (the same that thrashed OpenOffice, bribed african government officials in order to win business contracts, or made dangerous false claims of security...) despite a community-driven one is beyond me.
Update: mysql-8.0 is neither available in Debian stable repositories, nor testing repos (but in sid for now). Installing mysql-8.0 from Oracle (who I trust less than Debian packagers) break dependencies with packages relying on MariaDB 10, the default-mysql-server in Debian. So, because of a single migration, one of the biggest distros cannot be used to host a Ghost blog. This is madness.",Bitter Frustration
"> Solely relying in Oracle's solution (the same that thrashed OpenOffice, bribed african government officials in order to win business contracts, or made dangerous false claims of security...) despite a community-driven one is beyond me.
The only explanation is that commercial version of Ghost runs on MySQL and they care just about that. We as users of open source Ghost are not important for them, we don't pay.
Why PostgreSQL is dropped some years ago if Ghost uses [knex](https://github.com/knex/knex), which supports PostgreSQL, MySQL, CockroachDB, MSSQL, SQLite3 and Oracle? And now they drop MariaDB which is the default MySQL flavor at all major Linux distributions. It seems it's time to drop Ghost, I won't use it anymore.",Bitter Frustration
"Please, can we keep the discussion on this thread related to the migration bug and not about the MariaDB support.
@daniellockyer could you explain why is this migration required? Isn't it possible to rewrite it without the Knex bug? I've ran dozen of migrations successfully on my Ghost instance. I don't understand why this one, which seems fairly simple, has a bug and that is the first time we see it on MariaDB. Thanks!",Bitter Frustration
"> Please, can we keep the discussion on this thread related to the migration bug and not about the MariaDB support.
> > @daniellockyer could you explain why is this migration required? Isn't it possible to rewrite it without the Knex bug? I've ran dozen of migrations successfully on my Ghost instance. I don't understand why this one, which seems fairly simple, has a bug and that is the first time we see it on MariaDB. Thanks!
Sure seems we have a hard time keeping it related to migration bug because all the lights point in that dropping MariaDB is intentional?",Bitter Frustration
"Instead of pushing on solving the knex bug ""default 'NULL'"" for datetime type, which is obviously wrong, 'NULL' is string and not the datetime type, Ghost team wants to drop MariaDB. Great guys, stick with enforcing bugs. And instead of switching to type strict TypeScript, let's keep JS and no types in database, who cares, same sh*t. It's probably the right time to write another blog post for a falling open source project.",Bitter Frustration
"My comment in the announcement of [Ghost 5](https://github.com/TryGhost/Ghost/issues/14446) has been marked off-topic, why? Do you really want to intentionally make BC just to drop MariaDB? And only keep shi**y MySQL with the wrong behavior? Datetime can't have default value of 'NULL', that's string guys.",Bitter Frustration
"> My comment in the announcement of [Ghost 5](https://github.com/TryGhost/Ghost/issues/14446) has been marked off-topic, why? Do you really want to intentionally make BC just to drop MariaDB? And only keep shi**y MySQL with the wrong behavior? Datetime can't have default value of 'NULL', that's string guys.
I'd just like to say...you made a comment about ""non-commercial"" users but...Don't THEY know those of us TINKERING with it are who will sell it for them if it's something we like? LOL sigh....",Bitter Frustration
"Hey all ðŸ‘‹, I've locked this bug to contributor comments only. It's caused by an [upstream bug in knex](https://github.com/knex/knex/issues/5154). When a fix is pushed into knex, Ghost's renovate setup will automatically pull it into Ghost, meaning it will be included in the subsequent release. If anyone needs or wants some support with migrating from MariaDB to MySQL, please use the [forum](https://forum.ghost.org/).
I understand there's a lot of frustration around the clarification that MariaDB is not an officially supported database. As a small team we've always provided a very narrow set of environments that are officially supported to keep our maintenance overhead manageable: Ghost is intended for use with Ubuntu, MySQL, nginx & Node.js LTS. We're not able to support every flavour of environment without differences being handled correctly in the upstream packages. The way to change what's supported would be to contribute the necessary fixes & improvements - we'd absolutely love to see more contributions around database interoperability in [knex](https://github.com/knex/knex/issues).",
"Hey all ðŸ‘‹ðŸ» @iBotPeaches sent a PR with the fix to Knex and it's been merged + released: https://github.com/knex/knex/pull/5181
We've just released Ghost v4.48.1 with the updated Knex, so this issue should now be fixed. Please update to v4.48.1 using Ghost-CLI ðŸ™‚",
"### Issue Summary
When I tried to upgrade my Ghost from `v4.44.0` to `v4.46.0` I got this error message upon launch:
> Message: Ghost was able to start, but errored during boot with: alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
I must add, my `newsletters` table has no data:
<img width=""901"" alt=""Screenshot 2022-04-30 at 13 15 12"" src=""https://user-images.githubusercontent.com/5436545/166103277-f0f4cfc8-2d12-40b6-931a-edbcadf87443.png"">
### Steps to Reproduce
1. Upgrade Ghost to `v4.46.0`
2. Launch Ghost
### Ghost Version
v4.44.0
### Node.js Version
v14.18
### How did you install Ghost?
Local install on Linux server
### Database type
Other
### Browser & OS version
_No response_
### Relevant log / error output
_No response_
### Code of Conduct
- [X] I agree to be friendly and polite to people in this repository",
"I'm not commenting on whether this feature should or should not be included, but how would you specify both a class on an `<li>` and on `<ol>` or `<ul>` if you must do it on the same line?",
"> I'm not commenting on whether this feature should or should not be included, but how would you specify both a class on an `<li>` and on `<ol>` or `<ul>` if you must do it on the same line?
As mentioned in the doc update this isn't possible with this patch. I initially started out with a `parent:` prefix (e.g. `parent:attr=value`) to be able to more precisely specify which attributes should be lifted to the parent, but decided against it as it needed quite more code changes. Plus I wonder how many times you would really want to be able to set attributes on both list items and the list itself. For the latter you only need to use an attribute on a single list item, while all the other list items can still have their own attributes.",
An alternative could be to have a single ^ as a marker in the attribute list to indicate that all attributes following it should be set on the parent. That would allow both list and table attributes with minimal code changes.,
I updated the pull request to use `^` as a marker to signal when attributes should be lifted. Solves the issue of not being to define both list element attributes and parent attributes in the same attribute definition,
"As I have stated on every request we have received for this, we will not be implementing this feature. Of course, you are free to provide this functionality in a third party extension.",Impatience
"> As I have stated on every request we have received for this, we will not be implementing this feature. Of course, you are free to provide this functionality in a third party extension.
I have seen a number of similar comments about not wanting to implement it, but I'm really struggling to understand why? This feature is currently not available, yet is obviously valuable to many, given all the requests over the many *years* it has shown up as an issue here. The way I implement it here does not seem to interfere with any existing feature, as far as I can tell. And if the special `^` marker isn't used at all then there's no difference in behaviour, so it should be fully backwards compatible. It adds value, not only to me, yet it is coldly being rejected for unclear reasons other than ""we will not be implementing this feature"". Please help me understand, because this all does not make any sense...",Bitter Frustration
"This has been discussed and explained many times over. You even linked to some of those discussions. In short, I am not interested in supporting a syntax that is does not make sense to me. It should be no surprise that it was rejected. Of course, you are free to disagree with me on this. If this is important to you, then you can provide and support your own third-party extension which provides the feature.",
"> In short, I am not interested in supporting a syntax that is does not make sense to me.
From me as an outside-viewer I can see this response as essentially ""I don't like this, so I don't want it"" response.
You put your own bias over something that the majority of people may like to have for once. I saw this problem in MkDocs and see it here again. You seem to have a general issue with accepting things that you don't like, but that the community could benefit from.",Bitter Frustration
"I don't accept things that I'm not willing to support long term. Long after the person who proposes it is gone, I'm left defending it and fixing issues with it. That's (one of the reasons) why we provide a public extension API. Anyone can provide their own implementation which works how they want. Then they can support it without any additional burden on me. I am not limiting anyone at all except for myself.",Bitter Frustration
"This adds a way for the attr_list extension to lift attribute values set on list items to their parent list element (i.e. `<li>` attribute to `<ul>` or `<ol>`). This allows list-wide attributes, such as list start number or class to be set by setting them on list items with the appropriate syntax.
Contains additions to the docs describing it, as well extra test input (plus all existing unit tests still pass)
Should be able to solve #227, #668 and many of the other requests for a similar feature, including my own quest to get this feature.",
"I am experiencing the same issues, actually it start occurring two days ago.
@jujumilk3 I suggest to amend the title of the issue with something more talkative, such as ""Maximun retries exceeded issues"" so other people can see there's already an issue going on.",
"Same thing.
I am agree with @carloocchiena. @jujumilk3 you should change the title.",
@jujumilk3 Looks like your issue has just been resolved. check it out!,
@jujumilk3 Thanks for creating this issue. This issue is related to #1471. Please use https://github.com/anuraghazra/github-readme-stats/issues/1471#issuecomment-979306704 as a workaround.,
"@anuraghazra, although it is a duplicate of #1471, I think I will keep this issue open for a while. I have been closing similar issues all this morning, and people don't seem to find the #1471. See my suggestion at https://github.com/anuraghazra/github-readme-stats/issues/1471#issuecomment-1129752460.",Bitter Frustration
"same issue just popped up in my account as well. no hurries take your time to fix it if it's a problem in the code. amazing service again, cheers !",Irony
@rickstaa i'm wondering if it's a good resolution to host it on our own Vercel server. and this may reduce the frequency of use of the main service,
"<blockquote class=""twitter-tweet""><p lang=""en"" dir=""ltr"">Got bombarded with github notifications today. ðŸ¥²<br>Because github-readme-stats is down.<br><br>People don&#39;t understand that &quot;+1&quot; and &quot;same here&quot; comments don&#39;t help anyone, it&#39;s just puts pressure on the maintainer and spams their inbox.</p>&mdash; Anurag Hazra âš› (@anuraghazru) <a href=""https://twitter.com/anuraghazru/status/1526842284167245824?ref_src=twsrc%5Etfw"">May 18, 2022</a></blockquote>
@aghogwarts This problem is affecting all users of the public Vercel instance. For people who experience this problem, please wait until we fix this issue upstream or as mentioned in https://github.com/anuraghazra/github-readme-stats/issues/1471 deploy your own vercel instance. Locking this issue, for now, to keep the notification bar from exploding ðŸ¤¯.",Impatience
"**Describe the bug**
Service doesn't work
**Expected behavior**
I think It's a kind of traffic problem as i captured.
**Screenshots / Live demo link (paste the github-readme-stats link as markdown image)**
<img width=""526"" alt=""á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-05-18 á„‹á…©á„’á…® 7 49 16"" src=""https://user-images.githubusercontent.com/41659814/169022351-d751d8c9-5ae1-484d-ab3a-6ea53c9f7d91.png"">
you can just visit my profile
https://github.com/jujumilk3",
bug happening to me too,
"Yes, and it's serious! So is mine.",Bitter Frustration
This issue is only relating the **design** of the card when error messages are shown. This is the reason I state that it is not pressing. I added this issue I want to fix in the future. I however understand the confusion. I will lock this issue for now. ðŸ¤” The reason for the `maximum retries exceeded` error can be found in [Common Error Codes](https://github.com/anuraghazra/github-readme-stats/issues/1772). The status of this error is tracked in #1471.,
"If somebody wants to contribute, I think the best way is to enlarge the card if an error is thrown.",
"**Describe the bug**
It looks like the new tiny URL that was introduced in 2fb452c119e9cb4706a1ecf708295db9b77b4b7e that is shown when an error is thrown does not fit on the card. Not a pressing issue but just putting this here so that we don't forget.
![image](https://user-images.githubusercontent.com/17570430/169273808-75921d34-9432-466b-905b-deb6f45c8413.png)",
"@sftim: This request has been marked as needing help from a contributor.
### Guidelines
Please ensure that the issue body includes answers to the following questions:
- Why are we solving this issue?
- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?
- Does this issue have zero to low barrier of entry?
- How can the assignee reach out to you for help?
For more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.
If this request no longer meets these requirements, the label can be removed
by commenting with the `/remove-help` command.
<details>
In response to [this](https://github.com/kubernetes/website/issues/33848):
>**This is a Feature Request**
>
><!-- Please only use this template for submitting feature/enhancement requests -->
><!-- See https://kubernetes.io/docs/contribute/start/ for guidance on writing an actionable issue description. -->
>
>**What would you like to be added**
>Update https://kubernetes.io/docs/contribute/ so that early in the page there is a link to
>https://contribute.cncf.io/contributors/projects/#kubernetes
>
>You should decide on some suitable text for the hyperlink. Read https://contribute.cncf.io/contributors/ to get ideas about what to write, if you're not sure.
>
>:information_source: Don't remove any of the existing links; this is an addition.
>
>**Why is this needed**
>This change will help signpost readers to the CNCF contributor site.
>
>**Comments**
>/kind feature
>/language en
>/triage accepted
>/priority backlog
>/help
>
Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>",
/assign,Impatience
/assign,Impatience
"Hello @sftim, I have gone through the issue mentioned, so I think the following changes can be suitable in the note section.
Note: To learn more about contributing to Kubernetes, [one of the CNCF hosted projects](https://contribute.cncf.io/contributors/projects/#kubernetes), see the [contributor documentation](https://www.kubernetes.dev/docs/).
the text for the hyperlink ( https://contribute.cncf.io/contributors/projects/#kubernetes ) should be one of the CNCF hosted projects.",
/assign,Impatience
Hi @NitishKumar06 We are working on this issue. please work on other issue.,Impatience
/unassign @NitishKumar06,Impatience
I've already made changes to this issue now. Please let it be reviewed.,Impatience
/assign,Impatience
"No you can't work on this issue if someone is already working . this is against the policy of kubernetes. tonight I will open PR for this issue. There are many issues just work on that.
/unassign @NitishKumar06",Bitter Frustration
"See this issue has been opened from three days and as far I've observed the another contributor also raised the assign option for the same. Let my changes be reviewed and if it doesn't hold good, definitely you can also raise PR. The best one will be merged. /assign",Impatience
/assign,Impatience
/unassign @NitishKumar06,Impatience
"Why are you doing this again and again? @sftim Please have a look. I've raised the PR for this issue which was not raised by anyone for 3 days. Now, he is unassigning me.",Bitter Frustration
/assign,Impatience
Infact there's @kadtendulkar as well. Why are you not removing her from assignes? Please be respectful.,Bitter Frustration
/unassign @NitishKumar06,Impatience
cool. Do this I've raised the PR anyways.,Impatience
/assign,Impatience
@NitishKumar06 this is against the policy of kubernetes. /unassign @NitishKumar06,
"Hey @NitishKumar06 please don't work on this issue, I'm new to community and this going to be my first issue. @ashish-jaiswar is helping me with that. This is community to help and work with unity. Please give chance to new contributers to work as well.
/unassign @NitishKumar06",
@kadtendulkar Actually this is infact my first contribution as well. The fact that @ashish-jaiswar has more PRs merged shows he isn't a newbie here. My last PR was not merged because someone else merged it through my changes . Please notice that!,Bitter Frustration
/assign,Impatience
/unassign @NitishKumar06,Impatience
@kadtendulkar Why are you doing this?,Impatience
"Hey @NitishKumar06 this is my first issue to work on, please let me solve this. This is not about competition, its only about community work, wouldn't it bother you if i do the same with you. Please don't go against kubernetes policies. if someone is assigned on the issue first you must ask if they are still working or not. You are not doing the right thing @NitishKumar06 And in this case I'm working on it. Today i will raise a PR. Please be kind and look for other issues you will find one definitely. ðŸ˜€
/unassign @NitishKumar06",Bitter Frustration
Hey @NitishKumar06 you are not following the policy of kubernetes. I will report you to Kubernetes community.,Threat
"Okay @ashish-jaiswar , Let my PR be reviewed. Sounds good now?",Impatience
"hey @NitishKumar06, kubernetes community will not accept your PR. Because you are working against the policy of kubernetes.",Insulting
@ashish-jaiswar My last PR was not merged because although I was working on this issue as well but someone after me started working on it and his PR got merged.,Bitter Frustration
"_I'm writing this as a [tech lead](https://github.com/kubernetes/community/tree/49ecc500205884984a2174422f7715e7f9556b28/sig-docs#leadership) for SIG Docs_
:white_circle: **TL;DR;** calm it down and be excellent to each other. :white_circle: Folks, it's best if people co-ordinate work. **_Anyone_** is welcome to open a PR to work on an issue provided they sign the CLA and agree to abide by our [code of conduct](https://kubernetes.io/community/code-of-conduct/).
If reviewers see multiple PRs opened for the same issue, they will usually:
- sigh inwardly
- look at the PR that was opened first, see if it's good to merge, and if so, merge that
- if not, look at the other PRs in chronological order
and finally
- close the other PRs with an explanation
So there's three further things here as I see it:
1. https://github.com/kubernetes/website/issues/33848#issuecomment-1134518812 mentions that
> you can't work on this issue if someone is already working . this is against the policy of kubernetes.
I'm afraid that's not true. For SIG Docs, we actually have a policy of no [cookie licking](https://www.redhat.com/en/blog/dont-lick-cookie): nobody should assert that they own the implementation of an issue. 1. However, it's unhelpful to work on an issue where someone else has started work, without first discussing with that person how to move forward. The polite and compassionate thing to do if someone has said they're working on an issue, but you don't see progress after a week or so is to get in touch and ask if they would like help. If they step back or you don't hear anything at all, you remain welcome to open your own PR.
- https://github.com/kubernetes/website/issues/33848#issuecomment-1134446220
- https://github.com/kubernetes/website/issues/33848#issuecomment-1134539047
are both examples of behavior I'd prefer not to see more of.
1. It's best if people who have already worked on an issue avoid picking up Good First Issue issues. This issue is _not_ a â€œgood first issueâ€; I deliberately chose not to add the Good First Issue label because I left the implementer some work to do around writing the hyperlink text and deciding what message would be suitable.
That means it is OK for someone who has already submitted a PR to work on this. However, I would encourage people who are already contributing to hold off on Help Wanted issues (this one included) and only start work on a Help Wanted issue if you don't see any other work that you can reasonably pick up.
This discussion is getting more heated than it should be. If folks want to discuss how SIG Docs works, take it to Slack (https://kubernetes.slack.com/messages/sig-docs - invitations available from https://slack.k8s.io/) or come to the community meeting, 5:30 AM UTC on Wednesday 2022-05-25. A Zoom link for that call is in the Slack channel.
GitHub issues are not the place for [ad hominem](https://en.wikipedia.org/wiki/Ad_hominem) criticisms etc and I hope all concerned will take that on board.
I don't see grounds to involve the code of conduct folks at this point, and I'm hoping things stay that way. @ashish-jaiswar if you do have a concern about someone not following the code of conduct, please raise that by following the documented process.
_PS_ I edited this a few times for accuracy; sorry about the noise from updates.",Bitter Frustration
Thanks @sftim . This was needed! @ashish-jaiswar Please go through it especially the first point.,
/assign,Impatience
Thanks @sftim @NitishKumar06 I'm requesting since I'm new contributer please allow me to do. According to the 2nd and 3rd point its appropriate to check if assigned people are working or not. And i have already mentioned I'm working on it. please know that. /unassign @NitishKumar06,Impatience
"But why didn't @ashish-jaiswar raise this point at that time when you made yourself assigned to this issue? He was working on that issue before you . But when I assigned it to myself, you know .",Bitter Frustration
@sftim Please tell @kadtendulkar not to misuse this function of unassign. /assign,Impatience
/assign,Impatience
/@unassign @NitishKumar06,Impatience
/assign,Impatience
/@unassign @NitishKumar06,Impatience
Work on issue . See ya!,Impatience
/unassign @NitishKumar06,Impatience
"Hey @NitishKumar06 , @ashish-jaiswar is helping me to raise my first PR that's why he assigned himself first. Please understand, I'm new to kubernetes community and I'm finding such conflicts.
It is always good to ask if the assigned people are working or not and since beginning i an mentioning that I'm working on it. But seems like you are not coordinating with new contributers .",Bitter Frustration
"Natali Vlatko here, co-chair of SIG Docs. This conversation has become too heated and isn't in line with our community guidelines with regards to politeness and respect. Our tech lead, @sftim, had given great clarity and guidance for those involved to move forward, but unfortunately, this hasn't taken place. I'll be messaging all involved separately to resolve this disagreement via the Kubernetes Slack.",
"**This is a Feature Request**
<!-- Please only use this template for submitting feature/enhancement requests -->
<!-- See https://kubernetes.io/docs/contribute/start/ for guidance on writing an actionable issue description. -->
**What would you like to be added**
Update https://kubernetes.io/docs/contribute/ so that early in the page there is a link to
https://contribute.cncf.io/contributors/projects/#kubernetes
You should decide on some suitable text for the hyperlink. Read https://contribute.cncf.io/contributors/ to get ideas about what to write, if you're not sure.
:information_source: Don't remove any of the existing links; this is an addition.
**Why is this needed**
This change will help signpost readers to the CNCF contributor site.
**Comments**
/kind feature
/language en
/triage accepted
/priority backlog
/help",
"I've been working on my project for three weeks and I haven't done anything because of you. I bought a vuexy template, Not all libraries have problems, but node sass gives errors, I might get fired because of you, This is the error Make it your eyes.
error /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass: Command failed.
Exit code: 1
Command: node scripts/build.js
Arguments: Directory: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass
Output:
Building: /usr/local/bin/node /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/bin/node-gyp.js rebuild --verbose --libsass_ext= --libsass_cflags= --libsass_ldflags= --libsass_library=
gyp info it worked if it ends with ok
gyp verb cli [
gyp verb cli '/usr/local/bin/node',
gyp verb cli '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/bin/node-gyp.js',
gyp verb cli 'rebuild',
gyp verb cli '--verbose',
gyp verb cli '--libsass_ext=',
gyp verb cli '--libsass_cflags=',
gyp verb cli '--libsass_ldflags=',
gyp verb cli '--libsass_library='
gyp verb cli ]
gyp info using node-gyp@3.8.0
gyp info using node@16.14.2 | darwin | x64
gyp verb command rebuild []
gyp verb command clean []
gyp verb clean removing ""build"" directory
gyp verb command configure []
gyp verb check python checking for Python executable ""python2"" in the PATH
gyp verb `which` succeeded python2 /usr/bin/python2
gyp verb check python version `/usr/bin/python2 -c ""import sys; print ""2.7.18
gyp verb check python version .%s.%s"" % sys.version_info[:3];""` returned: %j
gyp verb get node dir no --target version specified, falling back to host node version: 16.14.2
gyp verb command install [ '16.14.2' ]
gyp verb install input version string ""16.14.2""
gyp verb install installing version: 16.14.2
gyp verb install --ensure was passed, so won't reinstall if already installed
gyp verb install version is already installed, need to check ""installVersion""
gyp verb got ""installVersion"" 9
gyp verb needs ""installVersion"" 9
gyp verb install version is good
gyp verb get node dir target node version installed: 16.14.2
gyp verb build dir attempting to create ""build"" dir: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build
gyp verb build dir ""build"" dir needed to be created? /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build
gyp verb build/config.gypi creating config file
gyp verb build/config.gypi writing out config file: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build/config.gypi
(node:14904) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.
(Use `node --trace-deprecation ...` to show where the warning was created)
gyp verb config.gypi checking for gypi file: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/config.gypi
gyp verb common.gypi checking for gypi file: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/common.gypi
gyp verb gyp gyp format was not specified; forcing ""make""
gyp info spawn /usr/bin/python2
gyp info spawn args [
gyp info spawn args '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/gyp/gyp_main.py',
gyp info spawn args 'binding.gyp',
gyp info spawn args '-f',
gyp info spawn args 'make',
gyp info spawn args '-I',
gyp info spawn args '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build/config.gypi',
gyp info spawn args '-I',
gyp info spawn args '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/addon.gypi',
gyp info spawn args '-I',
gyp info spawn args '/Users/kawan/.node-gyp/16.14.2/include/node/common.gypi',
gyp info spawn args '-Dlibrary=shared_library',
gyp info spawn args '-Dvisibility=default',
gyp info spawn args '-Dnode_root_dir=/Users/kawan/.node-gyp/16.14.2',
gyp info spawn args '-Dnode_gyp_dir=/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp',
gyp info spawn args '-Dnode_lib_file=/Users/kawan/.node-gyp/16.14.2/<(target_arch)/node.lib',
gyp info spawn args '-Dmodule_root_dir=/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass',
gyp info spawn args '-Dnode_engine=v8',
gyp info spawn args '--depth=.',
gyp info spawn args '--no-parallel',
gyp info spawn args '--generator-output',
gyp info spawn args 'build',
gyp info spawn args '-Goutput_dir=.'
gyp info spawn args ]
gyp verb command build []
gyp verb build type Release
gyp verb architecture x64
gyp verb node dev dir /Users/kawan/.node-gyp/16.14.2
gyp verb `which` succeeded for `make` /usr/bin/make
gyp info spawn make
gyp info spawn args [ 'V=1', 'BUILDTYPE=Release', '-C', 'build' ]
c++ '-DNODE_GYP_MODULE_NAME=libsass' '-DUSING_UV_SHARED=1' '-DUSING_V8_SHARED=1' '-DV8_DEPRECATION_WARNINGS=1' '-DV8_DEPRECATION_WARNINGS' '-DV8_IMMINENT_DEPRECATION_WARNINGS' '-D_GLIBCXX_USE_CXX11_ABI=1' '-D_DARWIN_USE_64_BIT_INODE=1' '-D_LARGEFILE_SOURCE' '-D_FILE_OFFSET_BITS=64' '-DOPENSSL_NO_PINSHARED' '-DOPENSSL_THREADS' '-DLIBSASS_VERSION=""3.5.5""' -I/Users/kawan/.node-gyp/16.14.2/include/node -I/Users/kawan/.node-gyp/16.14.2/src -I/Users/kawan/.node-gyp/16.14.2/deps/openssl/config -I/Users/kawan/.node-gyp/16.14.2/deps/openssl/openssl/include -I/Users/kawan/.node-gyp/16.14.2/deps/uv/include -I/Users/kawan/.node-gyp/16.14.2/deps/zlib -I/Users/kawan/.node-gyp/16.14.2/deps/v8/include -I../src/libsass/include -O3 -gdwarf-2 -mmacosx-version-min=10.7 -arch x86_64 -Wall -Wendif-labels -W -Wno-unused-parameter -std=c++11 -stdlib=libc++ -fno-strict-aliasing -MMD -MF ./Release/.deps/Release/obj.target/libsass/src/libsass/src/ast.o.d.raw -c -o Release/obj.target/libsass/src/libsass/src/ast.o ../src/libsass/src/ast.cpp
In file included from ../src/libsass/src/ast.cpp:2:
../src/libsass/src/ast.hpp:1614:25: warning: loop variable 'numerator' creates a copy from type 'const std::string' [-Wrange-loop-construct]
for (const auto numerator : numerators)",
"interesting, low priority but something to consider",
"I don't agree with the ""low priority"" assessment. This is a blatant violation of the http specification and more and more (standards compliant) http clients break just by using them with their default settings.",Bitter Frustration
PR welcome.,
"The existence of a PR has nothing to do with the priority assessment. In fact, giving this issue a higher priority may incentivize people to actually fix this issue. Saying that this violation of the http spec is ""low prio"" is more a statement about the level of professionalism of this software, or the lack thereof.
For the record: Yes, I would try to fix this myself, but you have to realize that not every user of your product is a software developer that uses the language that your software is written in. I am fluent in Java and Go, but I cannot fix python code. Or at least not at a level that you would ever consider to accept contributions.",Bitter Frustration
"Locking the issue as I'm not liking the tone of the reporter.
PR is still welcome. ðŸ™ If no one implements it, I will, at some point, on my free time (unpaid free time).",Bitter Frustration
"omg, happy to come back to see your fluency in being a total asshat @ChristianCiach raising the issue as high priority :+1: !",Mocking
"I know that HTTP/2 is out of scope of this project. But I am not asking to support HTTP/2. More and more http clients try to upgrade the connection to HTTP/2. Uvicorn is free to not honor this request. Unfortunately, instead of ignoring the upgrade request, Uvicorn responds with status `400 Bad Request` and the message ""Unsupported upgrade request"".
According to https://developer.mozilla.org/en-US/docs/Web/HTTP/Protocol_upgrade_mechanism the server should just ignore the upgrade request:
> If the server decides to upgrade the connection, it sends back a [101 Switching Protocols](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/101) response status with an Upgrade header that specifies the protocol(s) being switched to. If it does not (or cannot) upgrade the connection, it ignores the Upgrade header and sends back a regular response (for example, a [200 OK](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200)).
We continue to encounter this issue because the `HttpClient` class of modern OpenJDK versions tries to upgrade the connection to HTTP/2 by default. Uvicorn should just ignore these headers and process the requests as if these headers were not present.",Bitter Frustration
"- Closed by #1661
It will be available on uvicorn `0.19.0`.",
This probably should be a discussion rather than an issue.,
"Totally agreed - it just feels strange to see too simplified statement of a very complex situation all the time, while you just want to do your work.",Bitter Frustration
Plus it is such a double standard... where was all the wokeness in the other 10+ wars before Ukraine?,Bitter Frustration
"Since you agree this should be a discussion and not an issue, could you please close this issue and [open a discussion](https://github.com/composer/composer/discussions/new) instead?",
@Andreas-Schoenefeldt The inclusion of this phrase was a turning point in the fight against the aggressor. One cannot be a supporter of common sense!,Bitter Frustration
"The whole issue moved to this discussion, with a nice list of solutions: https://github.com/composer/composer/discussions/10808
Actually the discussion above was now cencored - I tried to restore the solutions here: https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message",Bitter Frustration
"Another way to remove it is by removing this specific package:
`composer remove ukeloop/stand-with-ukraine`",
"@kodima05, this is amazingly simple! Thank you so much for the hint!",
"```
composer self-remove
```",
"> Another way to remove it is by removing this specific package: `composer remove ukeloop/stand-with-ukraine`
This doesn't work.
```bash
$ composer remove ukeloop/stand-with-ukraine
ukeloop/stand-with-ukraine is not required in your composer.json and has not been removed
./composer.json has been updated
Running composer update ukeloop/stand-with-ukraine
Loading composer repositories with package information
Package ""ukeloop/stand-with-ukraine"" listed for update is not locked.
Updating dependencies
Nothing to modify in lock file
Installing dependencies from lock file (including require-dev)
Nothing to install, update or remove
Generating autoload files
100 packages you are using are looking for funding.
Use the `composer fund` command to find out more!
Info from https://repo.packagist.org: #StandWithUkraine
```",Bitter Frustration
"```sh
git clone https://github.com/composer/composer.git --branch 2.5.4 ~/composer-build && \
composer install -o -d ~/composer-build && \
wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
cd ~/composer-build && patch -p1 < composer.patch && \
php -d phar.readonly=0 bin/compile && \
rm /usr/local/bin/composer && \
php composer.phar install && \
php composer.phar update && \
mv ~/composer-build/composer.phar /usr/local/bin/composer && \
rm -rf ~/composer-build && \
chmod +x /usr/local/bin/composer
```",
"> ```shell
> git clone https://github.com/composer/composer.git --branch 2.4.4 ~/composer-build && \
> composer install -o -d ~/composer-build && \
> wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
> cd ~/composer-build && patch -p1 < composer.patch && \
> php -d phar.readonly=0 bin/compile && \
> rm /usr/local/bin/composer && \
> php composer.phar install && \
> php composer.phar update && \
> mv ~/composer-build/composer.phar /usr/local/bin/composer && \
> rm -rf ~/composer-build && \
> chmod +x /usr/local/bin/composer
> ```
ThX > work like charm @archlinux",
but there is no path > `/usr/local/bin/composer` @archlinux file arch. structure i think ... @archlinux > `/usr/bin/composer` ...,Bitter Frustration
"archlinux archerz use > ```
git clone https://github.com/composer/composer.git --branch 2.4.4 ~/composer-build && \
composer install -o -d ~/composer-build && \
wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
cd ~/composer-build && patch -p1 < composer.patch && \
php -d phar.readonly=0 bin/compile && \
rm /usr/bin/composer && \
php composer.phar install && \
php composer.phar update && \
mv ~/composer-build/composer.phar /usr/bin/composer && \
rm -rf ~/composer-build && \
chmod +x /usr/bin/composer
```",
"> ```shell
> git clone https://github.com/composer/composer.git --branch 2.4.4 ~/composer-build && \
> composer install -o -d ~/composer-build && \
> wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
> cd ~/composer-build && patch -p1 < composer.patch && \
> php -d phar.readonly=0 bin/compile && \
> rm /usr/local/bin/composer && \
> php composer.phar install && \
> php composer.phar update && \
> mv ~/composer-build/composer.phar /usr/local/bin/composer && \
> rm -rf ~/composer-build && \
> chmod +x /usr/local/bin/composer
> ```
New machine, this is the best answer yet, but you can remove the branch 2.4.4 to get the newest composer.
Thanks @politsin",
"@roni-estein
I updated the text above with the last branch.
When I used the master branch instead of specifying a particular version, I didn't like the console output with the last commit id. And when switching between machines, it was impossible to tell which version of the cpmposer was being used.
In the end, I decided to specify the branch explicitly, and the work became more comfortable.
Maybe there is a way to use the ""last"" branch instead of the master branch, but I don't know it =)",
"You don't need the --branch, it will use ""main"" which is always merged to the latest one to save you needing to update
Or you could use --branch=main if you don't want to maintain the comment. Cheers ðŸ™‚",
"As the composer team unsurprisingly resolves to censorship on this issue, I opened this on on stackoverflow to rescue the solutions: https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message",Bitter Frustration
"No chances, until terrorist state will go out of soverign country. You better to support your colleges, who are sitting in shelters sometimes without electricity and pay some respect",Bitter Frustration
"> The message come from packagist.org
> > You can choose a politic-free mirror from: https://packagist.org/mirrors
> > For example:
> > Enable a mirror from China:
> > ```
> composer config -g repos.packagist composer https://mirrors.tencent.com/composer/
> ```
> > Disable the mirror:
> > ```
> composer config -g --unset repos.packagist
> ```
Saddly, this repo is too damn slow.",Bitter Frustration
"Here is a small update for people using herd...
For your convenience, I've assembled it as a zsh function alias. You can put it in your .aliases or .bashrc or .zshrc or just take any part of it as you like. This is simply the solution from @politsin wrapped with a configuration variable that chooses herd, homebrew or any other service you are serving from to place your updated composer. Cheers ðŸ» ```zsh
composer:purge-politics() {
# the keyword local destructs the memory variables so they wont stick around
# after your function closes to interfere with any other system settings
# which server are you going to run herd or something else (homebrew, xamp)
local server=""herd""
local composer_path
# choose the appropriate path for composer
[[ $server == ""herd"" ]] && composer_path=""/Users/<your home directory>/Library/Application Support/Herd/bin/composer"" || composer_path=""/usr/local/bin/composer""
# ** note change <your home directory>
git clone https://github.com/composer/composer.git ~/composer-build && \
composer install -o -d ~/composer-build && \
wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
cd ~/composer-build && patch -p1 < composer.patch && \
php -d phar.readonly=0 bin/compile && \
rm /usr/local/bin/composer && \
php composer.phar install && \
php composer.phar update && \
mv ~/composer-build/composer.phar ""$composer_path"" && \
rm -rf ~/composer-build && \
chmod +x ""$composer_path""
}
```",
"`alias composer=""2> >(grep -av 'StandWith') composer --ansi""`
> jst one line to ya .bashrc or .zshrc or .wtFrc ...
work normaly with out war poltics adds ... no china proxies needed ...
jst ya local memory one time loop 4 energy 2 sacrifice
our coGnitive resonanse ;)",Mocking
"> `alias composer=""2> >(grep -av 'StandWith') composer --ansi""`
> > > jst one line to ya .bashrc or .zshrc or .wtFrc ...
> > work normaly with out war poltics adds ...
> > no china proxies needed ...
> > jst ya local memory one time loop 4 energy 2 sacrifice
> > our coGnitive resonanse ;)
your solution work perfectly, thank you so much",
"@artembeloglazov > ```
> composer self-remove
> ```
Acting like a child.",Mocking
"@djengis
> @artembeloglazov
> > > ```
> > composer self-remove
> > ```
> > Acting like a child.
Acting like a child: using an international it-project for political propaganda and demonstration of absolute misunderstanding of the situation.
It's been 2 years and it's already clear to everyone that Ukraine is just a way to launder money for US military corporations, and the composer is a dying project that should be gradually removed from their projects.",Bitter Frustration
"@artembeloglazov, do you know a working alternative for composer?
Best wishes
Andreas",
"@Andreas-Schoenefeldt Package repository mirrors have been created separately from packagist.org for a while now. There is no alternative to the client for loading and resolving dependencies yet, but I think it will come soon.",Impatience
The message isn't there anymore and you are still bitching? ðŸ¤·ðŸ»â€â™‚ï¸,Mocking
"2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.",
"It's most likely part of the DLL name, so it's a big deal.",Bitter Frustration
"> 2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.
For the love of god I have more things to do then read readme files. I need to download it first.
Which readme should I download ?!?!
Answer my question !",Bitter Frustration
"The readme is readable on this webpage, if you scroll down after loading https://github.com/libsdl-org/SDL
Right here: https://github.com/libsdl-org/SDL#simple-directmedia-layer-sdl-version-20
Also I don't think slurs are necessary to communicate your frustrations.",
"> The readme is readable on this webpage, if you scroll down after loading https://github.com/libsdl-org/SDL
> > Right here: https://github.com/libsdl-org/SDL#simple-directmedia-layer-sdl-version-20
> > Also I don't think slurs are necessary to communicate your frustrations.
Your logic is flawed. Readme is not version information. Sometimes projects include version information in readme, but many times they do not.
I did not even scroll down.
(There is also no indicator if it includes SDL 1.x for backwards compatibility or not.)",Bitter Frustration
"In general, GitHub projects use the project name as the entire repo, including all versions of the project, and the main branch is the current version, and different versions are specified using branches or tags in the repo. You can see a good example of this at https://github.com/godotengine/godot.",
"https://github.com/libsdl-org/SDL
This makes it seem like this was the first version of SDL, while in reality it is 2.x
Recommend different URL which clearly states SDL2.
To prevent time wasted on using wrong version. I only noticed it after palette->version was missing.
60 minutes of time wasted, I consider myself lucky.
99.9% of the people on this planet suck at proper versioning, including file systems.
Fortunately SDL1 is not fully retarded:
https://github.com/libsdl-org/SDL-1.2",Bitter Frustration
ä½ è¿™å£æ°”çœŸè®©äººçœ‹çš„ä¸èˆ’æœï¼ä»€ä¹ˆå«å¿…é¡»ï¼Ÿå‡­ä»€ä¹ˆç”¨è¿™ç§å‘½ä»¤å¼è¯­æ°”ï¼Ÿä½ å¯ä»¥å»ºè®®ï¼Œå¯ä»¥æå‡ºéœ€æ±‚ï¼Œä½†æ˜¯æ²¡æœ‰ä»€ä¹ˆå¿…é¡»ä¸å¿…é¡»ï¼,Bitter Frustration
å¤šå¼€å‡ ä¸ªä»£ç†å buffï¼Ÿ,Impatience
æˆ‘ç«™åœ¨é£Žä¸­ï¼Œä¸çŸ¥é“ä½ æ˜¯ç‰›æ˜¯é©¬,Mocking
"> _No description provided._
å»ºè®®è”ç³»å•†åŠ¡ï¼Œæ‚¨å…ˆæŠ¥ä¸ªä»·ï¼Œäº¤é’±æ‚¨æ˜¯å¤§çˆ·ï¼Œæ²¡æœ‰ï¼Ÿé‚£æ‚¨æ˜¯å±ã€‚",Vulgarity
"> _No description provided._
ä»€ä¹ˆï¼Ÿè¿žå•†åŠ¡éƒ½æ²¡æœ‰è”ç³»å—ï¼Ÿã€‚ã€‚ã€‚ã€‚ã€‚ä¸å¥½æ„æ€ï¼Œé‚£æ‚¨è¿žå±éƒ½ä¸æ˜¯ã€‚",Insulting
æ²¡æƒ³åˆ°åœ¨gayhubä¸Šä¹Ÿæ˜¯ä»€ä¹ˆäººéƒ½èƒ½è§åˆ°å•Šå•§å•§å•§,Mocking
æ‚¨å°±æ˜¯è¿™ä¸ªé¡¹ç›®çš„äº§å“ç»ç†å—,Impatience
å‘ƒå‘ƒ,
åˆä¸æ˜¯ä½ åšçš„è¿™å£æ°”,Bitter Frustration
"Latest update (branch ""latest"") causes an exception for NUI callbacks, minimal repro:
```csharp
using System;
using CitizenFX.Core;
using CitizenFX.Core.Native;
namespace TestResource
{
public class TestResource : BaseScript
{
public TestResource()
{
API.RegisterNuiCallbackType(""OnNuiReady"");
}
[EventHandler(""__cfx_nui:OnNuiReady"")]
private void OnNuiReady(dynamic dummy, dynamic cb)
{
cb("""");
}
}
}
```
```html
<!DOCTYPE HTML>
<html>
<head>
<script src=""nui://game/ui/jquery.js"" type=""text/javascript""></script>
</head>
<body>
<script>
$(function()
{
window.addEventListener(""DOMContentLoaded"", function(){
$.post(`http://${GetParentResourceName()}/OnNuiReady`);
});
});
</script>
</body>
</html>
```
```
[ 183015] [b2545_GTAProce] MainThrd/ Error invoking callback for event __cfx_nui:OnNuiReady: System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation. ---> System.ArgumentNullException: Value cannot be null.
[ 183015] [b2545_GTAProce] MainThrd/ Parameter name: src
[ 183015] [b2545_GTAProce] MainThrd/ at (wrapper managed-to-native) System.Runtime.InteropServices.Marshal:copy_from_unmanaged (intptr,int,System.Array,int)
[ 183015] [b2545_GTAProce] MainThrd/ at System.Runtime.InteropServices.Marshal.Copy (System.IntPtr source, System.Byte[] destination, System.Int32 startIndex, System.Int32 length) [0x00000] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.RemoteFunctionReference._InvokeNative (System.Byte[] argsSerialized) [0x00075] in C:\gl\builds\master\fivem\code\client\clrcore\RemoteFunctionReference.cs:97 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.RemoteFunctionReference.InvokeNative (System.Byte[] argsSerialized) [0x00000] in C:\gl\builds\master\fivem\code\client\clrcore\RemoteFunctionReference.cs:72 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.MsgPackDeserializer+<>c__DisplayClass36_0.<CreateRemoteFunctionReference>b__0 (System.Object[] args) [0x00007] in C:\gl\builds\master\fivem\code\client\clrcore\MsgPackDeserializer.cs:267 [ 183015] [b2545_GTAProce] MainThrd/ at (wrapper dynamic-method) System.Object:CallSite.Target (System.Runtime.CompilerServices.Closure,System.Runtime.CompilerServices.CallSite,object,bool)
[ 183015] [b2545_GTAProce] MainThrd/ at (wrapper dynamic-method) System.Object:CallSite.Target (System.Runtime.CompilerServices.Closure,System.Runtime.CompilerServices.CallSite,object,bool)
[ 183015] [b2545_GTAProce] MainThrd/ at TestResource.TestResource.OnNuiReady (System.Object dummy, System.Object cb) [0x00001] in C:\Build\TestResource\TestResource.cs:17 [ 183015] [b2545_GTAProce] MainThrd/ at (wrapper managed-to-native) System.Reflection.MonoMethod:InternalInvoke (System.Reflection.MonoMethod,object,object[],System.Exception&)
[ 183015] [b2545_GTAProce] MainThrd/ at System.Reflection.MonoMethod.Invoke (System.Object obj, System.Reflection.BindingFlags invokeAttr, System.Reflection.Binder binder, System.Object[] parameters, System.Globalization.CultureInfo culture) [0x00032] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ --- End of inner exception stack trace ---
[ 183015] [b2545_GTAProce] MainThrd/ at System.Reflection.MonoMethod.Invoke (System.Object obj, System.Reflection.BindingFlags invokeAttr, System.Reflection.Binder binder, System.Object[] parameters, System.Globalization.CultureInfo culture) [0x00048] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.Reflection.MethodBase.Invoke (System.Object obj, System.Object[] parameters) [0x00000] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.Delegate.DynamicInvokeImpl (System.Object[] args) [0x000e7] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.MulticastDelegate.DynamicInvokeImpl (System.Object[] args) [0x00008] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.Delegate.DynamicInvoke (System.Object[] args) [0x00000] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.EventHandlerEntry+<Invoke>d__5.MoveNext () [0x00064] in C:\gl\builds\master\fivem\code\client\clrcore\EventHandlerDictionary.cs:107
```",
Disable patches until this error disappears to find the patch which causes this.,Impatience
Still continuing,Impatience
To reset,
"??
Same problem still facing...",Bitter Frustration
"> Disable patches until this error disappears to find the patch which causes this.
Do as suggested.",Impatience
"Check this video :
https://user-images.githubusercontent.com/69076256/174075680-2bdd96a6-5624-4bbb-bb71-afc5cfe79b6c.mp4",
You can play around with different output stream redirections. `grep -v` also works quite well to remove single lines I heard.,
How can I remove this nonsense?,Bitter Frustration
"How to remove this?
```
Info from https://repo.packagist.org: #StandWithUkraine
```",Impatience
"You can remove it by removing the specific package:
`composer remove ukeloop/stand-with-ukraine`",
"I tried `composer remove ukeloop/stand-with-ukraine` and `composer global remove ukeloop/stand-with-ukraine` but composer says, this package is not installed and therefore not removed. To me this is a clear abuse of the info system for non technical purposes.",Bitter Frustration
"```sh
git clone https://github.com/composer/composer.git --branch 2.5.5 ~/composer-build && \
composer install -o -d ~/composer-build && \
wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
cd ~/composer-build && patch -p1 < composer.patch && \
php -d phar.readonly=0 bin/compile && \
rm /usr/local/bin/composer && \
php composer.phar install && \
php composer.phar update && \
mv ~/composer-build/composer.phar /usr/local/bin/composer && \
rm -rf ~/composer-build && \
chmod +x /usr/local/bin/composer
```",
https://github.com/composer/packagist/pull/1357 :),Irony
ugliness! why do I have to read this kind of propaganda every time I run composer? what a disgusting obsession. I feel raped every time). back to USSR,Bitter Frustration
It's time to create a non-political fork,Irony
Remove russia form the map of the world first :),Threat
"Thanks for creating this issue! It looks like you may be using an old version of VS Code, the latest stable release is 1.68.1. Please try upgrading to the latest version and checking whether this issue remains.
Happy Coding!",
My VS Code Version is 1.68.1 Only it's not older version,
Why triage needed I've Provided Every Thing. You Can Also Try On Your PC!,Bitter Frustration
Do you want to work on it?,
"Ok, It means no attention is given on a Real Bug",Bitter Frustration
@microsoft @mjbvz @sbatten @sandy081 Work on this bug,Impatience
"@Samyak-Bumb Cut it out. Bugging devs won't fix the bug or get it prioritized. If it matters that much to you, look into fixing it yourself
I'm locking this issue because you clearly haven't gotten the message about this",Bitter Frustration
"<!-- âš ï¸âš ï¸ Do Not Delete This! bug_report_template âš ï¸âš ï¸ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ðŸ•® Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ðŸ”Ž Search existing issues to avoid creating duplicates. -->
<!-- ðŸ§ª Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ðŸ’¡ Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ðŸ”§ Launch with `code --disable-extensions` to check. -->
<!-- Does this issue occur when all extensions are disabled?: Yes/No -->
<!-- ðŸª“ If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ðŸ“£ Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.68
- OS Version: Windows 10
### There is a bug if we Reduce the VS Code's Window Size.
**Video**:- https://user-images.githubusercontent.com/88429776/174990595-1424d0ad-034a-4296-80c2-8e8205c1402e.mp4",
"We closed this issue because we [don't plan to address it](https://aka.ms/vscode-out-of-scope) in the foreseeable future. If you disagree and feel that this issue is crucial: we are happy to listen and to reconsider.
If you wonder what we are up to, please see our [roadmap](https://aka.ms/vscoderoadmap) and [issue reporting guidelines](https://aka.ms/vscodeissuereporting).
Thanks for your understanding, and happy coding!",
"Got the same error with ```global NestJS 8.2.7```, project ```@nestjs/cli: 8.0.0``` while executing a simple ```nest g controller Something``` on Windows 10.
The command executed by nest: ```node @nestjs/schematics:controller --name=something --no-dry-run --no-skipImport --language=""ts"" --sourceRoot=""src"" --spec```",
I think maybe it conflict with CLI?,
"I try ```
nest [options] <schematic> [name]
```
and have some errors",
"https://github.com/nestjs/nest-cli/issues/323
https://github.com/nestjs/nest-cli/issues/145",Impatience
"> Got the same error with `global NestJS 8.2.7`, project `@nestjs/cli: 8.0.0` while executing a simple `nest g controller Something` on Windows 10.
> > The command executed by nest: `node @nestjs/schematics:controller --name=something --no-dry-run --no-skipImport --language=""ts"" --sourceRoot=""src"" --spec`
on my computer I have
```
node @nestjs/schematics:controller --name=something --no-dry-run --no-skipImport --language=""ts"" --sourceRoot=""src"" --spec
internal/modules/cjs/loader.js:892
throw err;
^
Error: Cannot find module '/Users/andreykatrusha/Repos/nestjs-task-managment/@nestjs/schematics:controller'
at Function.Module._resolveFilename (internal/modules/cjs/loader.js:889:15)
at Function.Module._load (internal/modules/cjs/loader.js:745:27)
at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:76:12)
at internal/main/run_main_module.js:17:47 {
code: 'MODULE_NOT_FOUND',
requireStack: []
}",Bitter Frustration
"<img width=""989"" alt=""Screenshot 2022-06-23 at 17 26 46"" src=""https://user-images.githubusercontent.com/3959504/175323342-1401c466-75d4-4a9a-ab4b-6c20a9ccea56.png"">",
"@kaflan I guess that's another issue.
---
The original one is a bug on v8.2.7.
you can downgrade it for now. v8.2.6 of `@nestjs/cli` worked just fine. I don't recommend using Nest CLI globally (use `yarn nest` or `npx nest`).",
"When I try to install https://www.npmjs.com/package/@nestjs/schematics
I have that error",Impatience
but you didn't show any installation error for us. I didn't follow,Impatience
"> 8.2.6
Ok I try now",Impatience
"> but you didn't show any installation error for us. I didn't follow
Yes, everything works well without generation.
I think it like some CLI conflict",Impatience
"I can confirm this issue as well
environment:
System:
Kernel: 5.15.46-1-MANJARO arch: x86_64 bits: 64 compiler: gcc v: 12.1.0
Desktop: KDE Plasma v: 5.24.5 tk: Qt v: 5.15.4 wm: kwin_x11 vt: 1 dm: SDDM
Distro: Manjaro Linux base: Arch Linux
### Quick solution:
1. add this to your nest-cli.json
```
""root"": ""src""
```
2. change into the src of your project folder
```
cd src
```
3. run ```
nest generate module tasks
```
if the above does not work also try changing the version of packages in package.json to ```
""@nestjs/cli"": ""8.2.6"",
""@nestjs/schematics"": ""8.0.11"",
```
removing the node modules in the current project reinstalling the nest cli with
```
npm uninstall -g @nestjs/cli
npm i -g @nestjs/cli@8.2.6 ```",
Add sistem,Impatience
@micalevisk @kamilmysliwiec Looks like this was a breaking change of [`@angular-devkit/schematics@14.0.0`](https://github.com/angular/angular-cli/blob/HEAD/CHANGELOG.md#angular-devkitschematics-cli). We should revert the angular schematics packages until we're ready to go through the full upgrade to keep from making breaking changes in our shcematics,
"Same issue here., tried above soluions without succes",Bitter Frustration
"@Acetyld `npm i @nestjs/cli@8.2.6` and then `npx nest g mo foo` will work for sure. If didn't worked for you, then you're probably missing something. We could help you on Discord.
![image](https://user-images.githubusercontent.com/13461315/175359768-88b0f4ef-d77c-4b08-a9e1-49fff42375b4.png)",Insulting
Try tommorow,Impatience
Bump. Having the same problem.,Impatience
You try guide,Mocking
"Guys, we know the problem, we know you're having it, no need to ""bump"". This literally came up today, it's not like this is inactive in any way.",Impatience
Fixed in 8.2.8,
"### Is there an existing issue for this?
- [X] I have searched the existing issues
### Current behavior
Hello, I try to do an example application learned on udemy. I try to run cmd
```js
nest g mo src/tasks
```
I have angular CLI. But I cannot understand why that error.
### Minimum reproduction code
nest g mo src/tasks
### Steps to reproduce
1. nest g mo src/tasks
### Expected behavior
(node:33348) UnhandledPromiseRejectionWarning: Error: Unknown argument skipImport. Did you mean skip-import?
at parseArgs (/Users/andreykatrusha/Repos/nestjs-task-managment/node_modules/@angular-devkit/schematics-cli/bin/schematics.js:338:19)
at main (/Users/andreykatrusha/Repos/nestjs-task-managment/node_modules/@angular-devkit/schematics-cli/bin/schematics.js:122:49)
at Object.<anonymous> (/Users/andreykatrusha/Repos/nestjs-task-managment/node_modules/@angular-devkit/schematics-cli/bin/schematics.js:367:5)
at Module._compile (internal/modules/cjs/loader.js:1072:14)
at Object.Module._extensions..js (internal/modules/cjs/loader.js:1101:10)
at Module.load (internal/modules/cjs/loader.js:937:32)
at Function.Module._load (internal/modules/cjs/loader.js:778:12)
at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:76:12)
at internal/main/run_main_module.js:17:47
(Use `node --trace-warnings ...` to show where the warning was created)
(node:33348) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 2)
(node:33348) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.
### Package version
8.2.7
### NestJS version
8.4.7
### Node.js version
14.17.5
### In which operating systems have you tested?
- [X] macOS
- [x] Windows
- [x] Linux
### Other
_No response_",
can you not fill the issues with random crap :DDDDD,Mocking
i like grass :DD,
yeah ill go to the park while my engi bots are being hosted :),Irony
[ðŸ˜­](https://emojipedia.org/loudly-crying-face/)[ðŸ˜­](https://emojipedia.org/loudly-crying-face/)[ðŸ˜­](https://emojipedia.org/loudly-crying-face/)[ðŸ˜­](https://emojipedia.org/loudly-crying-face/)[ðŸ˜­](https://emojipedia.org/loudly-crying-face/)[ðŸ˜­](https://emojipedia.org/loudly-crying-face/)[ðŸ˜­](https://emojipedia.org/loudly-crying-face/),Bitter Frustration
Cannot be blocked.,
"### Issue URL (Annoyance)
[https://play.google.com/store/apps/details?id=com.nineyi.shop.s002131](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.nineyi.shop.s002131)
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/xeZSvL0.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | Android 12
AdGuard product: | AdGuard for Android v3.6.8 (46)
AdGuard mode: | VPN
Filtering quality: | High-quality
HTTPS filtering: | enabled
Stealth mode options: | Send Do-Not-Track header, <br>Self-destructing third-party cookies (0)
DNS filtering: | server: `https://dns.adguard.com/dns-query`<br>filters: `https://filters.adtidy.org/android/filters/15_optimized.txt`
Filters: | <b>Ad Blocking:</b><br/>AdGuard Mobile Ads, <br/>AdGuard Base, <br/>Peter Lowe's Blocklist, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard URL Tracking, <br/>AdGuard Tracking Protection<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Other:</b><br/>AdGuard Experimental, <br/>AdGuard DNS<br/><br/><b>Language-specific:</b><br/>AdGuard Chinese
Userscripts: | https://userscripts.adtidy.org/release/adguard-extra/1.0/adguard-extra.meta.js",
"### Issue URL (Ads)
[https://play.google.com/store/apps/details?id=com.nineyi.shop.s002131](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.nineyi.shop.s002131)
### Comment
> å®Œå…¨ç„¡æ³•é˜»æ“‹é€™ç¨®å»£å‘Šï¼Œæ‰“é–‹appå°±æœƒè·³å‡ºé€™ç¨®å»£å‘Š
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/fkK8VdG.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | Android 12
AdGuard product: | AdGuard for Android v3.6.8 (46)
AdGuard mode: | VPN
Filtering quality: | High-quality
HTTPS filtering: | enabled
Stealth mode options: | Self-destructing third-party cookies (0)
DNS filtering: | server: `https://dns.adguard.com/dns-query`<br>filters: `https://filters.adtidy.org/android/filters/15_optimized.txt`
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>Peter Lowe's Blocklist, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Other:</b><br/>AdGuard DNS, <br/>AdGuard Experimental<br/><br/><b>Language-specific:</b><br/>AdGuard Chinese
Userscripts: | https://userscripts.adtidy.org/release/adguard-extra/1.0/adguard-extra.meta.js",Bitter Frustration
"### Issue URL (Ads)
[https://play.google.com/store/apps/details?id=com.nineyi.shop.s002131](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.nineyi.shop.s002131)
### Comment
> å·²é™„ä¸Šæˆªåœ–
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/BMMMRZF.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | Android 12
AdGuard product: | AdGuard for Android v3.6.8 (46)
AdGuard mode: | VPN
Filtering quality: | High-quality
HTTPS filtering: | enabled
Stealth mode options: | Self-destructing third-party cookies (0)
DNS filtering: | server: `https://dns.adguard.com/dns-query`<br>filters: `https://filters.adtidy.org/android/filters/15_optimized.txt`
Filters: | <b>Ad Blocking:</b><br/>EasyList, <br/>AdGuard Mobile Ads, <br/>Peter Lowe's Blocklist, <br/>AdGuard Base<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Other:</b><br/>AdGuard DNS, <br/>AdGuard Experimental<br/><br/><b>Language-specific:</b><br/>AdGuard Chinese
Userscripts: | https://userscripts.adtidy.org/release/adguard-extra/1.0/adguard-extra.meta.js",
"It's true this was one of the more controversial notice/warning promotions for 8.0, however a majority of the 63 people voting on the matter felt that the promotion was worthwhile. Going back on that decision now would be counter-productive.
https://wiki.php.net/rfc/engine_warnings
Note that you can still use the `@` operator in PHP 8.
https://3v4l.org/P9Otd
Because the truth is that PHP cannot know whether code accessing an undefined array key is doing so out of negligence or laziness (no offense). Perhaps the author assumed that `$_GET['xxx']` existed when they wrote it and the warning is pointing out a real potential problem. Perhaps it was done out of a desire to type less and the warning is a nuisance reminder of that fact. Only you as a human being can know, so PHP takes the more cautious approach of assuming the worst.",
"Thanks for your comment. I can understand the reason for changing it, but PHP is normally exceptional because it is so good at backward-compatibility and type-juggling. As the author of the codebases concerned, I've relied intentionally on that designed, documented, and intuitive behaviour of PHP in everything I've written for 20 years, since PHP4. I've always assumed that associative arrays can have variable numbers of keys (including zero).
So, it would cost me a 1000 hours of needless effort to find and change and test and QA and git every instance, and the codebase would be harder to read for it afterwards.
So at the moment, I've got 4 options, all bad.
* permanently stay on PHP7x
* permanently disable warnings
* fork PHP and maintain a patch
* incur months of busy-work.
I'm sure I'm not the only one this would affect.
So, that's why I'm suggesting a config option to change this, so that we can have it both ways.",Bitter Frustration
"Not recommended, because it will bite you in the ass, but you can set error handler to filter that warning:
```
<?php
set_error_handler(function($errno, $error){
return str_starts_with($error, 'Undefined array key');
}, E_WARNING);
```",Irony
"Thanks Kapitan, that's really, really helpful. At least now I can see a way to avoid a migration problem.
For others who come across this problem, here is a more complete way of doing this, which preserves the error message, but demotes it back to a notice.
```
set_error_handler(function($errno, $error){
if (!str_starts_with($error, 'Undefined array key')){
return false; //default error handler.
}else{
trigger_error($error, E_USER_NOTICE);
return true;
}
}, E_WARNING);
```
And one may wish to ignore the errors in production, with:
```
ini_set (""error_reporting"", E_ALL & ~E_NOTICE & ~E_USER_NOTICE);	```
I've also added this as a note here, so that it helps others:
https://www.php.net/manual/en/migration80.incompatible.php",
"Please reopen this feature request or provide some alternative respecting our point of view about how we want to code our software.
The voting was wrong. Even when 51% prefer to mess up their code, it's not fair nor necessary to force the rest of us to do the same. We have read all your reasons, but we are still pretty sure you are wrong.
There are many brilliant developers who love PHP because you can write things as easy as:
```
if($arr['user']['name']) {
}
```
It's perfectly ok to assume that an undefined key and any sub-element will return NULL or any other value that casts to FALSE without throwing a WARNING. If we would care about the value, we would explicitly use isset().
We are not afraid about misspelling a key name. It was never a problem for us, but your decission infuriates us.
We have been sitting on old PHP versions for years because we refuse to sacrifice PHP's original simplicity.
Otherwise, we would probably be coding in C# / .NET
We also don't want to rewrite code to please you, because we don't even agree with you.
Please add this feature or we will have to fork and split the PHP community.",Bitter Frustration
"Hi kripper,
If you put the code-block below into one of your common header files, you'll get what you want, i.e. demoting the warning back to a notice. It's now E_USER_NOTICE rather than E_NOTICE but otherwise, it reverts the old behaviour for that class of message. You can adapt the code to handle other unwanted errors, such as 'Undefined variable' depending on how far you want to take this. ```
//PHP-8 compatibility and migration. //PHP-8 has many new errors. Some (at least ""undefined array key"") now raise warnings.
if (version_compare(PHP_VERSION, '8') >= 0) {
ini_set (""error_reporting"", E_ALL & ~E_NOTICE & ~E_USER_NOTICE); //Errors below are now USER_NOTICE, not NOTICE.
set_error_handler(function($errno, $error){ //Custom error handler for WARNINGS.
if (str_starts_with($error, 'Undefined array key')){ // * Demote ""Undefined array key"" back to notice. This shouldn't be a warning.
trigger_error($error, E_USER_NOTICE);
}elseif (str_starts_with($error, 'Trying to access array offset')){ // * Demote array-offset errors to notice.
trigger_error($error, E_USER_NOTICE);
}else{ //use default error handler most of the time.
return false;
}
return true;
}, E_WARNING);
}
```",
"Hi @RichardNeill,
Yes, that's what we all are doing, but the big problem are the exceptions when accessing undefined variables.",Bitter Frustration
"### Description
In PHP 7, underfined array keys created E_NOTICE. This is now E_WARNING.
Can we please have a configuration option to put that back to E_NOTICE?
Rightly or wrongly, there are a lot of people, myself included, who have thousands of lines of code with things such as:
``` if ($_GET['xxx']) ``` #where xxx may or may not exist.
or
```if ($_GET['xxx'] == 'yyy')```
and in PHP8, we now get flooded with warnings ""Undefined array key"", which get in the way of real warnings.
Migrating and testing this code will take a long time, and furthermore, it's really ugly and harder to read:
```if (isset($_GET['xxx'))```
or
```if (isset($_GET['xxx']) && $_GET['xxx'] == 'yyy'))```
to suppress this warning.
Instead, it would be better to have some sort of pragma in the main header ```ini_set(""array_key_missing_enotice"", true);```
```ini_set (""error_reporting"", E_ALL & ~E_NOTICE);```
To me, it seems sensible that a defined associative array, with a key that isn't present (and may not be expected), should not be a warning, especially if it's being tested for with if().
It may also be worth treating undefined variables the same way.
Thanks for your help,",Bitter Frustration
What used to make it easy to get started with PHP now falls flat on your face after almost 20 years. It's really annoying when projects have grown over the years without having had the time to adapt the source code. For months I've been spending my time reworking the code - which no client pays for. I think I am not alone in this. I'm still waiting for the day when procedural programming in PHP is switched off ...,Bitter Frustration
"> What used to make it easy to get started with PHP now falls flat on your face after almost 20 years. It's really annoying when projects have grown over the years without having had the time to adapt the source code. For months I've been spending my time reworking the code - which no client pays for. I think I am not alone in this. I'm still waiting for the day when procedural programming in PHP is switched off ...
Yeah. We all feel your dissapointment. Some developers here didn't appreciate PHP's simplicity and took a very bad decission.
As @RichardNeill pointed out above, you should write your own `error_reporting` callback and do some minor modifications.
For instance, we had cases of `count(NULL)` throwing fatal errors, so we had to create a function to force conversion from NULLs to arrays.
Let's hope backwards compatibily will be respected in next versions.",Bitter Frustration
"What is the performance like with suppressed error messages? I once read that this can have a negative effect, even if they are not displayed.",
"Good point. For each (unwanted) notice, your error_reporting() function will trigger and mess up the performance. It will depend on the number of (unwanted) notices.
Even when you can suppress the notices/warnings, the big problem are the other fatal errors (eg: `count($undefinedVar)`)
We never managed to finish upgrading all our old PHP projects and in some cases we will probably have to stick to PHP 5.6 until someone forks PHP to support backward compatibility.
And in the case of the projects we upgraded, once in a while, a new backwards-incompatible fatal error is discovered.
It's a nightmare and I hate the developers responsible for this.",Bitter Frustration
">It's a nightmare and I hate the developers responsible for this.
I have slowly stopped understanding other (younger?) developers. In JetBrain's IDE PHP Storm, for example, it is not possible to sort files by modification date in order to manually push them to the server ""oldschool"". The feature request thread is over 10 years old and is answered with the forced use of VCS ...",Bitter Frustration
"I am getting more and more frustrated and extremely annoyed with the changes in PHP 8. Thousands of string functions I have to adapt (passing null). **Has anyone in charge thought about who will pay me for this?** In the meantime, several weeks of work have accumulated!",Bitter Frustration
"And it's going to be worse:
https://wiki.php.net/rfc/undefined_variable_error_promotion
56% smashing the rest of the 44%
We need to fork.",Bitter Frustration
We should send these people invoices for our conversion work ...,Bitter Frustration
I would ask for my money back if I were you.,Mocking
"In principle, bad programming style now falls on one's feet. I am fully aware of that. But PHP has allowed this to happen all these years. When I started, I made a lot of money relatively quickly (early 2001) and many new clients wanted many new things programmed. So a lot of projects grew quite quickly without worrying about the programming style - especially since it worked. I think some things are simply unnecessary. Why, for example, can't a function itself check that the parameters passed are not NULL? That is not the non-plusultra, but it would mean no rebuilding work for thousands of programmers. In the meantime, I am considering replacing the functions, e.g. nullable_str_replace() ...",Bitter Frustration
"I can think of at least 4 compromise options that would help...
1. Add a simple syntactic sugar for testing if a variable is truthy, without throwing an error. E.g.
`if (full($x))`
this would be exactly equivalent to `if (!empty($x))`
but without the logical inverse that makes for awkward readability.
2. Fix the precedence order to have `==` below `??`.
It's rather confusing that:
`if ($_GET['x']??null == false)` does not do what you expect. One would expect it to merely silence the warning, but it actually inverts the test, because the precedence is actually:
`if ($_GET['x'] (??null == false))` i.e.
`if ($_GET['x']??true)` 3. Treat a plain `if()` as allowing for undefined values. Thus `if($x)` emits no errors, while e.g. `$x++;` would.
4. Add a pragma or compiler directive, similar to Python 2's ""from future import division"". It would be something like
`ini_set(undefined_variables_are_null)` and would result in any undefined variable being treated as null, *without* emitting any error or warning, and would give instant backward-compatibility where needed, and solves the main motivation for the PHP9 pending change for the sake of the error-handling routines).
HTH.",
"Option 4 is what we all are expecting, because we don't want to waste our precious time adapting existing code which has been running stable in production for years.",Impatience
"I use if(!empty($var)) / if(empty($var)) and type casting for functions e.q. str_replace('x','y',(string)$var), but i would prefer option 4.",
"It's totally unbealivable what happened in php8.
Almost ALL php code in the intertnet needs to be rewritten!
Like what happened when you removed the mysql commands.
Come on, a code is written to last for ever, not re-writting it every few years, to please the code developers!
You should pay more attention to future compatibility and not destroy everyone's code.
Very disappointed from a move like that.
I totally agree with @kripper you souldn't destroy our code, because some people ""voted"". Did all php programers voted for something, or less than 1 % of them ?",Bitter Frustration
"_I was told there might be something wrong with my code for twenty years, but I ignored it. Then I was being told for three years that there **is** something wrong with my code that needed to be fixed but I still didn't do anything. Now my code is broken and IT'S NOT MY FAULT._
> Like what happened when you removed the mysql commands.
What, introduced more powerful alternatives (the old functions couldn't fully access MySQL's functionality) ten years before officially deprecating the old functions, putting warning notices up that they will be removed, and then waiting two more years before removing them?
> Come on, a code is written to last for ever, not re-writting it every few years, [Insert Bender laughing gif here.]
Only if the the outside world it interacts with never changes.
One of the oldest software libraries in the world (it dates from the 1970s) was most recently updated last month; there is always room for improvement.
Really, it sounds like there's a lot of software out there that is just being plain neglected.",Bitter Frustration
"We are not talking here about something that is wrong in our code.
We are telling you and will continue telling you for years, there is something wrong with the coding style you want to impose on others.
This is a not funny joke:
`if (isset($_GET['xxx']) && $_GET['xxx'] == 'yyy'))`
BTW, how old are you?
You sound like those kids that break code in production just because they *feel better* by enhancing and rewriting it, while he causes unnecessary damage to the company and its customers.",Bitter Frustration
"I'm going to ask some questions that I'm sure a lot of people are not going to like to hear:
Is there any point in continuing commenting on this issue? What's the goal of participating in this echo chamber? Is there an expectation that some some critical number of +1/-1 reactions will reverse a decision made and deployed more than 2 years ago in a new major version of the language?
Is there a belief that complaining about how legacy code from 10 years ago now runs with one more warning on modern PHP will accomplish some purpose? Is it PHP's responsibility to make sure that nobody has to change their code, even when they were observing risky and bug-prone practices? How long must it maintain compatibility with programming paradigms that date back to the '90s?
There are solutions available to you right now to deal with this new _warning_. Yes, you have to edit your code, and if you want to run old code on new software while knowing you can't afford to keep it up to date then I'm sorry you've decided to put yourself into that position. But in the meantime, PHP can't wait around for you.",Bitter Frustration
"Yes. There is high expectation this bad decision will be reverted or that a solution will be provided so we can coexist as a PHP community with one single codebase.
There is also high expectation you will manage to understand that we are not talking about ""risky"" or ""bug prone"" coding practices here, but we are defending a legitim vision which is the simple essence of the PHP language and that having to define every variable, array index, etc. is absurd and results in ugly code.
If you don't agree, fine, go and enable your strict validations for yourself, but please don't screw the rest of us. And if it's not possible to coexist, than fork away. We are professionals and we care our business.
Again, your assumption that our code is old fashioned legacy code or that we are following bad practices is wrong, arrogant and unrespectful.
To make it even clearer: You are causing us damage so we are pissed off and hate you. We are not thanking you for your strict validations or for forcing us to rewrite our clean and stable code.
And this here is the best place where we can express our hate and get together to protest against your bad decision until you fix it. Meanwhile new people will come here looking for a solution and the voting will never end.
Your reasons are wrong and we believe you are arrogant when you try to impose your ridiculous paranoid coding style where everything must be previously defined or checked. Maybe it's ok for some variables in a simple ""personal home page"" script, but when you developed a framework that handles arrays and keys that are not predefined, it's a nightmare. And then when you see how ugly the adapted code ends looking like, it's clear this can't be for real.
You probably thought you are smarter or a better coder than we are, and that everybody should follow your new coding style, but no, you are only arrogant and blind, and you should also have expected the obvious consequences of this silly invasive decision you are trying to defend.
You are also wrong when you say that there is a workaround we could use, because in some cases PHP8 throws fatal errors instead of warnings while the triggering criteria behind is basically the same (inconsistent behavior). And according to the roadmap, you will continue going in the wrong direction.",Bitter Frustration
"There is nothing I could say better.
@kripper said everything perfectly. Thank you.
There are thousands of php programmers that still use php7 because of those stupid changes. Our code is clean and perfect. This is not just a few changes it's thousands of changes in a clean and good code.
Php development team must find a solution and not just be arrogant.",Bitter Frustration
"Another aspect: With the decision, many projects that are running well become uneconomical, as a rebuild consumes many many resources. I can live quite well from my project, but if I had paid for the rebuild, I would have to file for bankruptcy now. I could only afford it because I'm self-employed and did the code myself in my spare time, in addition to my day job, and there was a workaround for the rest. I'll spare myself photos of my tired eyes and a face like a zombie now.
Solo self-employed people in particular really struggle with updates and upgrades. To label them as ""it's their own fault"" is simply terribly ignorant.",Bitter Frustration
"JavaScript has `obj?.key`.
PHP changing `$arr['key']` to `isset($arr) && isset($arr['key']) && $arr['key']` _is_ regressive, as far as readability and productivity goes.
63 voters is a very small sample size relative to the number of people using PHP.
It doesn't help that PHP 7.4 is now deprecated.
I support adding the proposed config option.",Bitter Frustration
"https://www.w3schools.com/php/php_variables.asp
> Note: Unlike other programming languages, PHP has no command for declaring a variable. It is created the moment you first assign a value to it.
This is the underlying reason why PHP has been used stylistically for over 20 years to write code that accesses undefined variables and array keys.
Seems a bit like a case of EEE to me because this change for me is a PHP killer. Time to switch to Node I guess.",Bitter Frustration
"There's also a basic point about programming here, as we contrast reliability, error-proofing, and readability/expressiveness.
1. In the case of reading from an un-initialised variable, this might indicate an error (e.g. a typo in the name), or it might indicate that the programmer has taken a shortcut by relying on the old behaviour. The warning is useful about 50% of the time. Furthermore, fixing it is trivial (just initialise variables before use), and it doesn't reduce code-readability. So, warning about this is probably sensible. 2. In the case of reading from an un-initialised array-key, this could possibly indicate an error, but it is far more likely to indicate that the array simply doesn't have all the keys all the time - that is, after all, the point of an associative array - it's supposed to have a variable number of keys. The behaviour is likely to be intentional, and the warning is rarely useful. Fixing this is difficult (you have to initialise every key of the array, even if you don't expect to use it), and it's particularly painful if your array comes from an outside source (e.g. JSON, or $_GET). Doing it with 2D associative arrays would be really ugly. Working around this to suppress the warning results in far less readable code. In addition, the first test in:
```
if ( isset($arr['key']) && ($arr['key'] == 'somevalue') )
```
is logically redundant.
So, I agree with case #1, but I don't think the rationale for #1 justifies applying the same policy to #2. Please can I encourage a change-of-heart here. Thanks.",Impatience
"> And this here is the best place where we can express our hate
This is not a place we're anyone *should* express hate.
> Php development team must find a solution and not just be arrogant.
Yeah, speaking of arrogance â€¦",Mocking
"ðŸ¤– &nbsp; ðŸ¤–
Hi! ðŸ‘‹ðŸ½ ðŸ‘‹ Welcome to the Exercism Python Repo!
Thank you for opening an issue! ðŸ &nbsp;ðŸŒˆ âœ¨
<br>
- &nbsp; If you are **requesting support**, we will be along shortly to help. (*generally within* **72 hours,** *often more quickly*).
- &nbsp; **Found a problem** with tests, exercises or something else?? &nbsp;ðŸŽ‰ &nbsp;&nbsp;&#9702;&nbsp;We'll take a look as soon as we can & identify what work is needed to fix it. *(generally within* **72 hours**).
â€‹	&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702;&nbsp;_If you'd also like to make a PR to **fix** the issue, please have a quick look at the [Pull Requests][prs] doc._ &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;_We &nbsp;ðŸ’™ &nbsp;PRs that follow our [Exercism][exercism-guidelines] & [Track][track-guidelines] contributing guidelines!_
- &nbsp; Here because of an obvious (*and* **small** *set of*) spelling, grammar, or punctuation issues with **one** exercise, &nbsp; concept, or Python document?? ðŸŒŸ `Please feel free to submit a PR, linking to this issue.` ðŸŽ‰
<ul>
<table>
<td>
<details>
<summary>â€¼ï¸&nbsp;&nbsp;<b><em>Please Do Not</em></b>&nbsp;â€¼ï¸</summary>
<br>
â€‹	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; â— Run checks on the whole repo & submit a bunch of PRs. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; This creates longer review cycles & exhausts reviewers energy & time. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; It may also conflict with ongoing changes from other contributors. â€‹	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; â— Insert only blank lines, make a closing bracket drop to the next line, change a word &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to a synonym without obvious reason, or add trailing space that's not an[ EOL][EOL] for the very end of text files.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; â— Introduce arbitrary changes ""just to change things"" .
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _...These sorts of things are **not** considered helpful, and will likely be closed by reviewers._
</details>
</td>
</table>
</ul>
- For anything complicated or ambiguous, **let's discuss things** -- we will likely welcome a PR from you.
- _Here to suggest a feature or new exercise??_ **Hooray!** Please keep in mind [_Chesterton's Fence_][chestertons-fence]. _Thoughtful suggestions will likely result faster & more enthusiastic responses from maintainers._
<br>
ðŸ’› &nbsp;ðŸ’™ &nbsp;_While you are here..._ If you decide to help out with other [open issues][open-issues], you have our **gratitude** ðŸ™Œ ðŸ™ŒðŸ½. Anything tagged with `[help wanted]` and without `[Claimed]` is up for grabs. Comment on the issue and we will reserve it for you. ðŸŒˆ âœ¨
[prs]: https://github.com/exercism/docs/blob/main/community/good-member/pull-requests.md
[EOL]: https://en.wikipedia.org/wiki/Newline
[chestertons-fence]: https://github.com/exercism/docs/blob/main/community/good-member/chestertons-fence.md
[exercism-guidelines]: https://exercism.org/docs/building
[open-issues]: https://github.com/exercism/python/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22
[track-guidelines]: https://github.com/exercism/python/blob/main/CONTRIBUTING.md",
"Hi @Crucibl ðŸ‘‹ðŸ½ Thanks for filing this issue. You are, however mistaken. It is quite possible to solve the `Ghost Gobble Arcade Game` without the use of conditionals. Please see the [exemplar solution](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.meta/exemplar.py) here in the repo for more details. We worked very hard to ensure that the `bools` exercise did not require `conditionals` in its solution, since it precedes the `conditionals` exercise. We also wanted to make sure students really focused in on the Boolean logic. While it is _possible_ to use `if`, `elif` or `else` for the problem (_its entirely your choice_), it is by no means _required_ for a correct solution.",
"Yes so this says to return a Boolean
""""""Verify that Pac-Man can eat a ghost if he is empowered by a power pellet.
:param power_pellet_active: bool - does the player have an active power pellet?
:param touching_ghost: bool - is the player touching a ghost?
:return: bool - can the ghost be eaten?
""""""",
"And you dont return a boolean, you simply return the parameters",Bitter Frustration
"@Crucibl - I believe you are misunderstanding the code. Please take a look at this Real Python article [Python Booleans](https://realpython.com/python-boolean/#python-booleans-as-keywords) for more information. It is not the **_parameters_** that are important in the `return` statement, it is the use of `or`, `and`, `not`.
The evaluation of those Boolean Operators results in `True` and `False`.",
I completely misunderstand because of the way it is written,Bitter Frustration
"Would be better to write the parameters (has_eaten_all_dots, power_pellet_active, touching_ghost) are boolean values that need to be evaluated",
"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and power_pellet_active: bool - does the player have an active power pellet?- is the player touching a ghost?
""""""",
"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and power_pellet_active: which are already set to a bool in the test file - does the player have an active power pellet?- is the player touching a ghost?
""""""",Impatience
"@Crucibl - A few things here:
1. **The stub is only one piece of this exercise.** It is not intended to stand in for the whole challenge. You should also be looking at the [instructions](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.docs/instructions.md) and the [introduction](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.docs/introduction.md) to get more information on what the expectations are.
2. When confused or stuck, we recommend uploading your partial solution via the CLI to the website and requesting mentoring. Mentors are very willing to work things through with you and explain everything step-by-step. You can also work through things in our Gitter channel.
3. . This exercise was ported from the Elixir track. Between Python and Elixir, ~6300 students have completed this exercise. Very few were confused by the stub -- so we are unlikely to change it. That doesn't mean we **_won't_** - but the majority of students do not appear to have an issue with it. We also risk confusing people more by adding more words.
I'm sorry that this exercise is confusing for you, but I am not convinced that changing the stub wording is warranted at this time.",
Did they complete the exercise by looking at prior solutions before answering? How can you read this and not be confused,Bitter Frustration
Stub notes or readme. Anything to clarify what is being asked,Impatience
"The docstrings and Readme lead me to believe that I must set something to true or false. So when I read. Verify that Pac-Man can eat a ghost if he is empowered by a power pellet. :param power_pellet_active: bool - does the player have an active power pellet? :param touching_ghost: bool - is the player touching a ghost? :return: bool - can the ghost be eaten? I first think set parameter power_pellet_active to a boolean, like power_pellet_active = True and touching_ghost: = True so is the player touching a ghost? is = to true",Bitter Frustration
"""We also risk confusing people more by adding more words.""
Im not trying to add more words. Im trying to optimize the wording for intellectually challenged people like myself who dont realize the variables are already defined in the test files",Mocking
"Then maybe 12,000 people can solve the puzzle not 6300",Impatience
"1008202
members have earned this badge for joining Exercism. but only 294540
members have earned this badge. for submitting an exercise. So out of 100 users only 36 or 37 actually submit an exercise. Why do more than 60 people out of every 100 not submit an exercise?",Bitter Frustration
More than half the people join and quit,Bitter Frustration
So out of 1008202 only 6300 people could figure out how to solve the exercise. Thats terrible,Bitter Frustration
Surely we can improve,Impatience
"There are 194,498 students in the Python track and only 6300 have been able to solve the 3rd exercise",
"@Crucibl - my reasons are mentioned in my [comment above](https://github.com/exercism/python/issues/3132#issuecomment-1175416070), and I don't think going back and forth as if this is a chat window is helpful. We'll think about your feedback and consider changing the wording, **but it's not going to happen right now.** We need to gather feedback from other users/mentors and maintainers on if the exercise is as confusing to them as it is to you.",Impatience
"Adding my feedback as a student, mentor, and maintainer... (sorry for the late response, I haven't checked issues for a while)
>The docstrings and Readme lead me to believe that I must set something to true or false. That is more vague than the actual instructions. The instructions specify
>The function should return True only if Pac-Man has a power pellet active and is touching a ghost.
What is being set is the function's return value. It's not simply a matter of returning the parameters. It's a matter of how they are joined in the returned expression.
` return power_pellet_active or touching_ghost`
is returning the parameters, but not in a way to solve the task. I think the instructions make clear the arguments to the two parameters are used to set the return value from the function. The instructions begin with
>Define the eat_ghost() function that takes two parameters (if Pac-Man has a power pellet active and if Pac-Man is touching a ghost) and returns a Boolean value if Pac-Man is able to eat the ghost.
In the 20+ times I've mentored this exercise I don't recollect any student saying they've been confused by the instructions for that task. They sometimes do more than they need to solve it by setting the expression to a variable and then returning the variable, or by using if/else when they can just return the parameters joined with the correct logical operator, but they get the general gist.
Clearness of the instructions is taken just as seriously as not modifying the instructions unnecessarily. It's a pain for everyone when their completed solution becomes outdated just from a change to the instructions. If there were multiple people complaining about these instructions being unclear, then a re-evaluation of them would certainly be in order. But, sorry, that is less likely to happen with a one-off complaint, no matter how fervent it may be.",Impatience
"Closing this for now, as we've had no additional complaints. Will re-open if the issue comes up again from additional students or mentors.",
"Should be mentions of ""If Else statements in the readme because the Boolean expressions are a red herring. You cannot solve anything by setting the value to true or false.",Bitter Frustration
php yii migrate,
JORDI U ARE A BULLSHIT,Insulting
rÃ£lis,
get rekt xd,Mocking
Bro get banned ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€,Mocking
"Hi, i'm back",
https://www.youtube.com/watch?v=gbftM7bPr8I,
TU FEZ A PIOR BOSTA QUE ALGUEM JA FEZ,Vulgarity
"I'm trying to figure out what that code does in the first place. I think it's trying to say ""if there's a dynamic subdomain or host matching in the current rule, apply that same argument to the login rule."" But there's surely a better way to go about that.
https://github.com/maxcountryman/flask-login/blob/62c04f482c3fb145c9f69054a799d4c55a9ef659/src/flask_login/utils.py#L95-L110",Bitter Frustration
"I have just experienced the same problem described above. My code was working yesterday, then today I ran _pipenv install wtforms-sqlalchemy_ and this seems to have updated _Werkzeug_ to 2.2.1 released on 7/23. As a result, my Flask code started failing: ImportError: cannot import name 'parse_rule' from 'werkzeug.routing' At first, I did not know what it was, until I found this page. So I did the following twice (2 different virtual env.):
pipenv install Werkzeug=='2.1.2' and this replaced the 2.2.0
then my code was fine! no more errors.
I created the 2nd environment to reinstall all libraries from scratch thinking something corrupted the 1st environment. but no, the same issue was in the 2nd env.
Thought I would share this frustrating experience.",Bitter Frustration
"Same here. We suddenly got a number of Airlfow PRs failing over the weekend after new werkzeug 2.2.0 was released. Luckily it only impacts some parts of our pipeline (as we deliberately protect against such cases), but It will still impact a number of our users who do not use ""constraints"" (which is the only way we can use to protect).
Not sure if this is a Werkzeug or flask-login problem but it's not the first time when those two stopped working together at some point of time (It happened I think for the 3rd time over last few years). So maybe that would be good idea that maintainer of the two project talk to each other and agree on some ""non-breaking"" rules that the two projects will follow - if both of those projects are so ""fundamental"" for a number of other projects??
It could either be as simple as putting an upper-bound in flask-login, to limit to ""known-working werkzeug version"" following some agreement on when breaking changes are introduced in werkzeug (maybe starting following SemVer is a good idea) and generally agreeing what is teh ""API"" of werkzeug that is supposed to be non-braking.
Maybe you can just talk to each other and agree ?",Bitter Frustration
"> It could either be as simple as putting an upper-bound in flask-login
I don't use flask-login, but this kind of upper bound is incredibly annoying:
- if there's a werkzeug update the end user wants to install, they need to wait for a flask-login update
- even if there is such a flask-login update, it may also contain other changes, new features, etc. so suddenly wanting to update werkzeug requires looking into whatever else changed in flask-login.
all something you can avoid easily by pinning your transitive dependencies...",Bitter Frustration
"> all something you can avoid easily by pinning your transitive dependencies...
We do already. But pinning dependencies is only half-solving the problem. The problem is that regardless if you are using pinning or not when you release your software, you usually do not pin your dependencies in development (unless you want your dependencies to go stale). So such a breaking release imacts PR/development workflow more than release workflow.
The main problem is that things break in random moments of when Flask-Login + Werkzeug are released - without even either of them bumping the major version (which is usually an indication of breaking chnages if you follow Semver). So if you are building your development workflow (where you do not want to pin the dependencies), there is no ""reasonable"" approach you can make to a) get latest versions with bugixes and b) get it non-breaking PRs of your users. It does not have to be ""upper-binding"". It might be simple agreement on what constitute of an API of werkzeug so that MINOR versions do not break the APIs.
So my gentle proposal is about that. I am not pushing - this was merely a suggestion that it would be much better for the community of your users if you agree some ways how we can ""sanely"" rely on the combo of flask-login and werkzeug not braking compatibility. It does not have to upper-binding, but some way of indicating to your users how they can constraint their dependencies in their development workflow to keep your development environment ""working"" - and not prone to external depenencies breaking your workflow.
We are using sophisticated mechanism to control our dependencies. In our case all our released versions of Airflow have automatically generated constraint files. And this is the only reason why such an incompatibility is not causing us bigger troubles. There are good reasons we cannot pin transitive dependencies in Airlfow ""setup.py"" and ""setup.cfg"" (because airlfow is both a library and application) but we implemented constraint mechanism which very much shield us from such breakages induced by external dependency changes. When we did not have them, such Werkzeug breakeges caused much more trouble for us, that's why we implemented constraints. This is a note from our nnouncements back from Feb 2020, were we had to bend the ASF rules on releases and release 1.10.9 same day as 1.10.8 becaue Werkzeug release broke it: https://airflow.apache.org/announcements/#feb-7-2020
> Feb 7, 2020
> Weâ€™ve just released Airflow v1.10.8
> and
> Weâ€™ve just released Airflow 1.10.9 (this one is a quick fix to work around the breaking release of Werkzeug 1.0)
So I do realise importance of pinnning, but I am talking about something different here.
BTW. If you are insterested why pinning ""install required"" dependencies is not always best you can read more here: https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#dependency-management
or you can read the talk I gave about it few months ago:
https://www.youtube.com/watch?v=_SjMdQLP30s&t=2549s - ""Managing dependencies at Scale"".",Bitter Frustration
"> you usually do not pin your dependencies in development (unless you want your dependencies to go stale). So such a breaking release imacts PR/development workflow more than release workflow.
IMHO you should use the same deps for dev as you use for production, and be more explicit about updating dependencies at some point (e.g. a `pip-compile -U`) where you then check if anything broke, look through changelogs, etc. and commit the new pins. With unpinned dev dependencies you never know if/when something is going to break, and it's generally a mess when working in a team where one team member has some problem, while another cannot reproduce it since they have other dependency versions...",Bitter Frustration
"> IMHO you should use the same deps for dev as you use for production, and be more explicit about updating dependencies at some point (e.g. a `pip-compile -U`) where you then check if anything broke, look through changelogs, etc. and commit the new pins. With unpinned dev dependencies you never know if/when something is going to break, and it's generally a mess when working in a team where one team member has some problem, while another cannot reproduce it since they have other dependency versions...
Please watch my talk why it is impossible for us to do manually (and I do recommend that your advises are based on research first). We have ~ 600 depenencies in airflow with consittue ~20 updates per week and it woudl be impossible to keep them manually updated. But we do have it implemented as you say - only automatically. And we DO keep a consistent set of dependencies for development (we keep all the 600 dependencies updated automatically for ""most"" developers and ""most PRs"" of our contributors (we have > 2100 contributors and sometimes 30 PRs merged a day). We even have a super sophisticated solution that automatically builds Dev Docker container with all the deps - and ONLY upgrade the dependencies when all the tests are passing. So you are advising something that we have done long time ago automated and run for years already. And again - I am not talking about this. Those ""dev deps constraints"" do not work for some of PRs that are updating other dependencies - this is where we have problems. This problem affected maybe 10% of our PRs. So this is not a huge problem for us.
What I am merely suggesting - is that the two projects talk to each other and agree some rules that will enable us to avoid breaking even the 10% of the PRs. This has happened not only once, so as a good community citizen, I suggest that both projects can do better when cooperating on that - no more, no less.",
"FYI @ThiefMaster You can see here https://github.com/apache/airflow/tree/constraints-main - look especially at the history. This is precisely what you describe - i.e. consistent set of dev dependencies. And if you look at the history, you will see that those sets are regularly updated (sometimes 3 times a day) when our automated tests all succeed and we know we can safely update.",
"See #663 which introduced this issue. #657 reported that the login URL contained both the literal next URL as well as extra query args for current view args, like `id=34`. I think we can just revert that for now. That behavior was introduced by #462.",
"The behavior introduced by #462 was incorrect, I'm going to revert that (and by extension #663). 0.6.2 coming soon.",
https://pypi.org/project/Flask-Login/0.6.2/,
"Importing flask-login with werkzeug 2.2.0a1 ends up with:
```
Traceback:
/usr/lib64/python3.11/importlib/__init__.py:126: in import_module
return _bootstrap._gcd_import(name[level:], package, level)
tests/test_packager_dashboard_pagure.py:4: in <module>
from oraculum.utils import pagure
oraculum/__init__.py:29: in <module>
from flask_login import LoginManager
/usr/lib/python3.11/site-packages/flask_login/__init__.py:12: in <module>
from .login_manager import LoginManager
/usr/lib/python3.11/site-packages/flask_login/login_manager.py:35: in <module>
from .utils import _create_identifier
/usr/lib/python3.11/site-packages/flask_login/utils.py:14: in <module>
from werkzeug.routing import parse_rule
E ImportError: cannot import name 'parse_rule' from 'werkzeug.routing' (/usr/lib/python3.11/site-packages/werkzeug/routing/__init__.py)
```",Bitter Frustration
same for me,
I think it clearly say itâ€™s still in development.,
I ThInK It cLeArLy sAy iTâ€™S StIlL In dEvElOpMeNt.,Impatience
"> I ThInK It cLeArLy sAy iTâ€™S StIlL In dEvElOpMeNt.
`storymode = { optional = true, type = ""bool"", default = false, example = false, options = [true,
false,
], explanation = ""not yet implemented"" } `",Impatience
"> I think it clearly say itâ€™s still in development.
Well I know it's clearly in development, I just wanted to state an issue with storymode. the more issues that are found, the more issues that can be fixed. I searched through the issues section and I found no one else with this issue. So I reported it to make sure maintainers as well as any contributors knew of the issue and if they could fix it. Sorry if that was the wrong thing to do.",
really @erasdnasd123...,Impatience
"When Storymode is turned on using the script, the script will load up and start before quickly crashing with the error
File ""/Users/Hulky/Desktop/RedditVideoMakerBot-develop/main.py"", line 64, in <module>
run_many(config[""settings""][""times_to_run""])
File ""/Users/Hulky/Desktop/RedditVideoMakerBot-develop/main.py"", line 55, in run_many
main()
File ""/Users/Hulky/Desktop/RedditVideoMakerBot-develop/main.py"", line 41, in main
length, number_of_comments = save_text_to_mp3(reddit_object)
File ""/Users/Hulky/Desktop/RedditVideoMakerBot-develop/video_creation/voices.py"", line 49, in save_text_to_mp3
return text_to_mp3.run()
File ""/Users/Hulky/Desktop/RedditVideoMakerBot-develop/TTS/engine_wrapper.py"", line 63, in run
self.call_tts(""posttext"", self.reddit_object[""thread_post""])
File ""/Users/Hulky/Desktop/RedditVideoMakerBot-develop/TTS/engine_wrapper.py"", line 110, in call_tts
clip = AudioFileClip(f""{self.path}/{filename}.mp3"")
File ""/usr/local/lib/python3.9/site-packages/moviepy/audio/io/AudioFileClip.py"", line 70, in __init__
self.reader = FFMPEG_AudioReader(filename, fps=fps, nbytes=nbytes,
File ""/usr/local/lib/python3.9/site-packages/moviepy/audio/io/readers.py"", line 51, in __init__
infos = ffmpeg_parse_infos(filename)
File ""/usr/local/lib/python3.9/site-packages/moviepy/video/io/ffmpeg_reader.py"", line 270, in ffmpeg_parse_infos
raise IOError((""MoviePy error: the file %s could not be found!\n""
OSError: MoviePy error: the file assets/temp/mp3/posttext.mp3 could not be found!
Please check that you entered the correct path.
Steps to reproduce the behavior: make sure story mode is enabled and attempt to run the script.
Expected behavior: the script will run as usual and produce a final video.
- Python Version: 3.9
- App version / Branch: 2.2.9",Bitter Frustration
I have tried it but this did not work for me.,
"> I have tried it but this did not work for me.
It working flawlessly for me.
Are you sure you have tested it on a video that automatically activates the captions? Otherwise nothing changes.",Impatience
Can you send one?,Impatience
"> Can you send one?
Already done in private, but it doesn't seem to work for everyone.
In any case, I'll try again:
The videos of this channel contains auto-captions:
https://youtu.be/NkHH4LX_kRg",Impatience
"And this channel too:
https://youtu.be/3F5cV98isaY",
Works.,
"> Works.
Would have been very weird if it only worked on my device xD",Irony
What is that code? Thats not referring to our Patcher?,Impatience
"> What is that code? Thats not referring to our Patcher?
This is code from my C# patcher.
I hate kotlin for various reason, first of all my inability to write a patch.
You can adapt the code if you want.
Just add a toggle to disable patch, at this point:
```
sget-boolean v0, Lcom/google/android/libraries/youtube/player/subtitles/model/SubtitleTrack;->disablecaption:Z
if-nez v0, :cond_disableinitcaption
const/4 v0, 0x1
return v0
:cond_disableinitcaption
```",Bitter Frustration
"> > What is that code? Thats not referring to our Patcher?
> > This is code from my C# patcher.
> > I hate kotlin for various reason, first of all my inability to write a patch.
> > You can adapt the code if you want.
> > Just add a toggle to disable patch, at this point:
> > ```
> sget-boolean v0, Lcom/google/android/libraries/youtube/player/subtitles/model/SubtitleTrack;->disablecaption:Z
> if-nez v0, :cond_disableinitcaption
> const/4 v0, 0x1
> return v0
> :cond_disableinitcaption
> ```
So thats the same which vanced did, so we would have done this anyway, but thanks for helping us out :)",Bitter Frustration
"> > > What is that code? Thats not referring to our Patcher?
> > > > > > This is code from my C# patcher.
> > I hate kotlin for various reason, first of all my inability to write a patch.
> > You can adapt the code if you want.
> > Just add a toggle to disable patch, at this point:
> > ```
> > sget-boolean v0, Lcom/google/android/libraries/youtube/player/subtitles/model/SubtitleTrack;->disablecaption:Z
> > if-nez v0, :cond_disableinitcaption
> > const/4 v0, 0x1
> > return v0
> > :cond_disableinitcaption
> > ```
> > So thats the same which vanced did, so we would have done this anyway, but thanks for helping us out :)
Nope... This was deprecated starting from vanced 15.xx, despite Vanced Team keep the reference inside settings.
The proof is there's no patch like that on Vanced 17.03.38.",Bitter Frustration
This would be way easier if you would actually provide class and method names :),Impatience
"> This would be way easier if you would actually provide class and method names :)
Based on which version?",Impatience
The one from the docs,Impatience
"> This would be way easier if you would actually provide class and method names :)
17.27.39 --->
First reference: {method: `public final void d`, class: `aamt.smali`}
Second reference: {method: `public final void D`, class: `SubtitleButtonController.smali`}
Third reference: {method: `public final boolean t`, class: `SubtitleTrack.smali`}",Impatience
"Thanks, I can work with that now :)",
Patch is not possible to create. Theres no way to generate a safe fingerprint for class aamt.,Bitter Frustration
"Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.",
"> Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
not possible. Those methods have no usages. Thats at least what jadx says.",Bitter Frustration
"> > Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
> > not possible. Those methods have no usages. Thats at least what jadx says.
Searching for string:
""pc""",Bitter Frustration
"And after searching for method ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""",Bitter Frustration
"Anyway...I posted the PoC for a reason.
All these infos was already listed there.",Bitter Frustration
"> And after searching for method > > ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""
Returns 0 results.",Impatience
"> > Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
> > not possible. Those methods have no usages. Thats at least what jadx says.
The class has, and since the class has you can get it from a parent method.",Bitter Frustration
"> > And after searching for method
> > ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""
> > Returns 0 results.
Instead this returns me, on my patcher at least, a method with the aformentioned variables. ðŸ¤·â€â™‚ï¸",Bitter Frustration
"> > > Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
> > > > not possible. Those methods have no usages. Thats at least what jadx says.
> > Searching for string:
> > ""pc""
""pc"" is not contained in the class",Bitter Frustration
"> The class has, and since the class has you can get it from a parent method.
I'll check again tomorrow then",Impatience
"> > > > Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
> > > > > > > > > not possible. Those methods have no usages. Thats at least what jadx says.
> > > > > > Searching for string:
> > ""pc""
> > ""pc"" is not contained in the class
LOL
<img width=""629"" alt=""Immagine 2022-07-21 190032"" src=""https://user-images.githubusercontent.com/108184954/180271651-c33a5395-c625-4d40-b63a-5479c935e7d0.png"">",Mocking
"> > > And after searching for method
> > > ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""
> > > > Returns 0 results.
> > Instead this returns me, on my patcher at least, a method with the aformentioned variables. ðŸ¤·â€â™‚ï¸
Open jadx, go to mentioned class, open smali view, Ctrl+f, search, and find nothing. Your patcher is not revanced. So We have to find a working way.",Bitter Frustration
"> > > > And after searching for method
> > > > ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""
> > > > > > > > > Returns 0 results.
> > > > > > Instead this returns me, on my patcher at least, a method with the aformentioned variables. ðŸ¤·â€â™‚ï¸
> > Open jadx, go to mentioned class, open smali view, Ctrl+f, search, and find nothing. Your patcher is not revanced. So We have to find a working way.
Already done.
You can see the screenshot in the answer preceding yours.",Bitter Frustration
"## ðŸž Issue
YouTube turn on captions (chosen based on the current region) automatically at the beginning of some videos.
## â— Solution
Adapt the following POC to ReVanced patches:
```
public static void ForcedInitCaption()
{
ConsoleLog.Divider();
ConsoleLog.StartPatch(""Patching Forced Init Captions in progress..."");
string[] allFiles = Directory.GetFiles(Main.apkDecompiledPath, ""*.smali*"", SearchOption.AllDirectories);
string firstReference = ""\""pc\"""";
string secondReference = APKOps.GetResourceID(""drawable"", ""quantum_ic_closed_caption_white_24"");
string thirdReference = ""\""DISABLE_CAPTIONS_OPTION\"""";
string firstSubReference = "".method"";
string secondSubReference = ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;"";
string thirdSubReference = "".locals"";
string fourthSubReference = ""# static fields"";
string fifthSubReference = ""()Z"";
bool firstReferencePatched = false;
bool secondReferencePatched = false;
bool thirdReferencePatched = false;
foreach (string file in allFiles)
{
string fullContent = File.ReadAllText(file);
List<string> lines = new List<string>();
if (!firstReferencePatched)
{
if (fullContent.Contains(firstReference))
{
lines = File.ReadAllLines(file).ToList();
for (int i = 0; i < lines.Count; i++)
{
if (lines[i].Contains(firstReference))
{
for (int j = i; j >= 0; j--)
{
if (lines[j].Contains(firstSubReference) &&
lines[j].Contains(secondSubReference))
{
ConsoleLog.Found($""Found file: {Path.GetFileNameWithoutExtension(file)}"", true);
string[] firstPatch = {
""const/4 v0, 0x0"",
""sput-boolean v0, Lcom/google/android/libraries/youtube/player/subtitles/model/SubtitleTrack;->disablecaption:Z""
};
lines.InsertRange(j + 2, firstPatch);
File.WriteAllLines(file, lines);
firstReferencePatched = true;
break;
}
}
if (firstReferencePatched)
{
break;
}
}
}
}
}
if (!secondReferencePatched)
{
if (file.Contains(""SubtitleButtonController.smali""))
{
lines = File.ReadAllLines(file).ToList();
for (int i = 0; i < lines.Count; i++)
{
if (lines[i].Contains(secondReference))
{
for (int j = i; j >= 0; j--)
{
if (lines[j].Contains(thirdSubReference))
{
ConsoleLog.Found($""Found file: {Path.GetFileNameWithoutExtension(file)}"", true);
string[] secondPatch = {
""const/4 v0, 0x1"",
""sput-boolean v0, Lcom/google/android/libraries/youtube/player/subtitles/model/SubtitleTrack;->disablecaption:Z""
};
lines.InsertRange(j + 1, secondPatch);
File.WriteAllLines(file, lines);
secondReferencePatched = true;
break;
}
}
if (secondReferencePatched)
{
break;
}
}
}
}
}
if (!thirdReferencePatched)
{
if (file.Contains(""SubtitleTrack.smali""))
{
lines = File.ReadAllLines(file).ToList();
for (int i = 0; i < lines.Count; i++)
{
if (lines[i].Contains(fourthSubReference))
{
string[] thirdSubPatch = {
"".field public static disablecaption:Z""
};
lines.InsertRange(i + 1, thirdSubPatch);
}
if (lines[i].Contains(thirdReference))
{
for (int j = i; j >= 0; j--)
{
if (lines[j].Contains(firstSubReference) &&
lines[j].Contains(fifthSubReference))
{
ConsoleLog.Found($""Found file: {Path.GetFileNameWithoutExtension(file)}"", true);
string[] thirdPatch = {
""sget-boolean v0, Lcom/google/android/libraries/youtube/player/subtitles/model/SubtitleTrack;->disablecaption:Z"",
""if-nez v0, :cond_disableinitcaption"",
""const/4 v0, 0x1"",
""return v0"",
"":cond_disableinitcaption""
};
lines.InsertRange(j + 2, thirdPatch);
File.WriteAllLines(file, lines);
thirdReferencePatched = true;
break;
}
}
if (thirdReferencePatched)
{
break;
}
}
}
}
}
if (firstReferencePatched &&
secondReferencePatched &&
thirdReferencePatched)
{
break;
}
}
if (!firstReferencePatched)
{
ConsoleLog.EndPatch(""Error during searching and patching the first reference"", false);
}
if (!secondReferencePatched)
{
ConsoleLog.EndPatch(""Error during searching and patching the second reference"", false);
}
if (!thirdReferencePatched)
{
ConsoleLog.EndPatch(""Error during searching and patching the third reference"", false);
}
if (firstReferencePatched &&
secondReferencePatched &&
thirdReferencePatched)
{
ConsoleLog.EndPatch(""Forced Init Captions succesfully patched"", true);
}
else
{
Main.buildingFailed = true;
}
}
```
## â“ Motivation
This mod may be useful for those who intend to have as few interactions as possible with their device.",
"This post violates the code of conduct and is being closed. Do not use profanity,",
"Windows 365 without external access, mandatory proprietary remote desktop, extreme slow, very huge connection time, latency in the skies... awful.... so... windows 365 is trash! Tried for 1 day.
Terrible!!!
Microsoft doing \<profanity removed>!",Bitter Frustration
"# Proposal for highlight groups:
Create a new set of hierarchical groups that are mapped **losslessly** from captures (e.g., `@keyword.special`), with sane fallback to defaults.
1. Implicitly generate a highlight group for every capture, e.g., `TS.keyword.special` that can be mapped directly (but isnâ€™t necessary by default)
2. Groups are implicitly language-specific, e.g., `TS.keyword.special.lua`
3. If not mapped, they successively fall back to the top level, e.g., `TS.keyword` (**Question:** fallback to `TS.keyword.special` or `TS.keyword.lua` first?)
4. Provide a set of top-level (and possibly _some_ second-level?) groups that are linked by default to suitable standard Vim groups and/or are mapped by the default and bundled color schemes. These groups need not derive from the set of standard Vim groups; it might be better to provide a small(!) number of different sets for different types of languages (with overlap), like ""imperative languages"" (C, Lua, Python, ...), ""markup languages"" (Markdown, LaTeX, HTML), ""data representation languages"" (JSON, regex, ...).
5. Languages are free to extend the capture hierarchy, e.g., `@snowflake.superfancy`, with the clear understanding that these wonâ€™t be highlighted unless mapped by custom colorschemes or plugins. (This also allows optional highlighting that is disabled by default.)
Ideally, we can use capture names directly as highlight groups, similarly to how syntax and highlight groups are matched?",
"> 1. Implicitly generate a highlight group for every capture, e.g., `TS.keyword.special`
With https://github.com/neovim/neovim/pull/19830 would we generate the identical names (`@keyword.special`) instead?",
"Proposal LGTM except this part:
> These groups need not derive from the set of standard Vim groups; it might be better to provide a small(!) number of different sets for different types of languages (with overlap), like ""imperative languages"" (C, Lua, Python, ...), ""markup languages"" (Markdown, LaTeX, HTML), ""data representation languages"" (JSON, regex, ...).
Can that be postponed to a ""phase 2"" discussion? Just thinking this might be an expensive discussion.",Impatience
"*(Changed my mind.)*
Sure; none of the technical details of the other points depends on this decision. I merely included it here in case we wanted to rip the band-aid off in one go. Definitely not a blocker for marking this tracking issue as completed.",
"**Goal:** Enable out-of-the-box `treesitter` highlighting of core languages (Lua, Vimscript) for Neovim 0.8
**Roadmap:** * [x] Replace static capture-to-highlight group table with automatic mapping (hierarchical, e.g., `@keyword.special` from Lua gets mapped to `TSKeywordSpecialLua`, with fallback to `TSKeywordSpecial` or `TSKeyword` if the more specific groups don't exist #19931
* [x] Include Lua and Viml parsers in build process (similar to C parser) #15391
* [x] Ship parsers and queries in runtime #15391
* [x] Upstream basic highlighting functionality from `nvim-treesitter` #15391
* [x] Change query handling to give same precedence as for parsers (config > plugins > runtime), with explicit extending via modelines (needs documentation!) #20104 #20117
* [x] Upstream utility functions from `nvim-treesitter` and `nvim-treesitter-playground` #19946 #20093
* [ ] ~~Upstream predicates and directives from `nvim-treesitter`; refactor metadata handling to be more generic?~~ (bumped to 0.9)
* [x] Upstream https://github.com/lewis6991/spellsitter.nvim (enabled automatically if treesitter highlighting is enabled for a language) #19351
* [x] Update and polish `vim.treesitter` docs #20142
* [x] ðŸ¥³",
"Does this really need to be changed? who cares. its a file name. it could be named dsklfjnfkltghfjkltfjklhdjfgkl.json for all i care and i wouldnt make a issue changing it.
:)",Irony
"This is a poor representation of the trans community. If you really care about the trans community, you wouldn't make issues where they don't exist because transgender individuals have enough on their plate already. TS when said as an initialism sounds nothing like trans. When said as a word, it is a far stretch from the word trans bar 2 sounds. You could just as easily made this extrapolation with the word trains. You're probably just a troll, but I think this ought to be said regardless.",Bitter Frustration
Does TG mean Transgender to you?,
"As a trans person myself, this is clearly a bad-faith troll post pretending to be trans to then screenshot and/or point to as an example of how unreasonable trans people are.
The only person who reacted positively (with many reacts, mind you) is a user who proudly admits to being banned for ""thinking different"" (which is, in my opinion, a telltale sign of someone who arrogantly holds onto objectionable (bannable) beliefs).",Bitter Frustration
I don't think this merits a response.,
"# Bug Report
The name tsconfig.json is triggering because it sounds like ""trans configuration"", as if trans people need to be fixed. I propose changing to tpsconfig.json
### ðŸ”Ž Search Terms
tsconfig.json
### ðŸ•— Version & Regression Information
Forever
### ðŸ™ Actual behavior
tsconfig.json sounds like ""trans config"", as if trans people are not good enough on their own and need configuring
### ðŸ™‚ Expected behavior
tpsconfig.json which sounds more like typescript and less like trans",Identify Attack/Name Calling
"I wonder if these are somehow related ,i mean if using embedded heimdal could also fix the browse issue .",
"> I wonder if these are somehow related ,i mean if using embedded heimdal could also fix the browse issue .
Iâ€™m somewhat hoping. ðŸ¤žSo that needs to be the first order of business.",Impatience
Open wet build has samba dependent on samba:host. Use this as a base to split the build of heimdal out. https://github.com/openwrt/packages/blob/master/net/samba4/Makefile,
"The solution was there all the time, found by unnamed CoreELEC member within a few hours by reading the change log:
```text
Note that when Samba is configured to run as an Active Directory Domain Controller, the samba binary that supplies the AD code will continue to provide DCERPC services while allowing samba-dcerpcd to provide services like SRVSVC in the same way as smbd did previously.
```
https://github.com/CoreELEC/CoreELEC/commit/d07ac771047f28b015fd65d397747caef4c511c5
Thank you for your cooperation and respect of solution origins!",
"> The solution was there all the time, found by unnamed CoreELEC member within a few hours by reading the change log:
> > ```
> Note that when Samba is configured to run as an Active Directory Domain Controller, the samba binary that supplies the AD code will continue to provide DCERPC services while allowing samba-dcerpcd to provide services like SRVSVC in the same way as smbd did previously.
> ```
> > [CoreELEC/CoreELEC@d07ac77](https://github.com/CoreELEC/CoreELEC/commit/d07ac771047f28b015fd65d397747caef4c511c5) Thank you for your cooperation and respect of solution origins!
Hi @Portisch - without going into a he says/she saysâ€¦ I can categorically state that the code I submitted is my own. It has been on my work list for a long time; in my branch, mailing lists, logs, and in the forum, in this issue and the PR. Every bump of samba has had me looking at it again. In terms of your quoted comment - actually it is not the reason for the issue - the samba compile is not done as an â€œActive Directory Domain Controllerâ€. Though you are right - that the answer was staring us all in the faceâ€¦ â€œsamba-dcerpcdâ€
I can share the details on my working (straces/logs/â€¦) - the lights on moment with discovery of the solution was firstly the /tmp/srvsvc in the log which did send me to read the change log again. But just adding samba-dcerpcd into the compile list = no successâ€¦. sbin - no success, then local_np.c: dyn_SAMBA_LIBEXECDIR via strings on the /usr/lib/â€¦
So whilst timing is a coincidence - I reiterate - my work is original.
I go out of my way to ensure that attribution is correct, and update the authorship from myself to the author when it is not from me or co-authored. I have worked with @vpeter4 on PRs previously and hope to continue doing so. I do not and would not blatantly rip other peoples code off. Have a look at my branches if you need to validate my statement.
I would appreciate your confirmation and understanding that the submissions of these PRs are coincidence. Note: I donâ€™t have a watch on the coreelec or other downstream repos; so was unaware of the submission until ray commented on my PR.
@vpeter4 - I would be very happy for you to submit your PR to upstream (instead of mine) yours was first after all :-)",
"I am sorry that we left you in dark for months and just 24h after we fixed it you published your ""own"" solution. It just shows LE behavior did not and will not change in future. Get what ever is possible and give a f* about others....
Again, I am sorry, we are just a fork, not member of LE.",Bitter Frustration
"2 items to be addresses with samba 4.16
- use embedded heimdal to build
- fix browse issue
- samba 4.17.0rc1 has the same issue
```
smbclient -U libreelec -L \\localhost
Password for [WORKGROUP\libreelec]:
Sharename Type Comment
--------- ---- -------
SMB1 disabled -- no workgroup available
```",
"> PDF.js version: 0.5.8
That's not an official PDF.js release, and even if existed it'd be *almost ten years old*; please find the latest releases at https://mozilla.github.io/pdf.js/getting_started/#download
> We use a simple iframe to display PDF, nothing fancy (src = ""/ViewerJS/?zoom=page-width#../Uploads/EDocPro/EDT-21072022-163634.pdf""
> [...]
> What went wrong? (add screenshot) - This is what is displayed in Firefox
That's clearly *not* the PDF.js default viewer, which is further confirmed by the ""ViewerJS"" string in your URL.
Hence you unfortunately opened this issue *in the wrong repository*, since the ""ViewerJS""-project is *not* being developed here; furthermore I don't believe that project is still developed/maintained now.",Impatience
"Well that not a particular helpful answer snuffleupagus !!! Did you think I imagined using viewer for 5 years ??
Whilst I very much appreciate some people live and breathe their own particular technologies when I search for something and come up with : https://viewerjs.org/getit/ and it has a link that says 'latest' version its somewhat hard to assume this is not actually the case !!!!
Now the more polite and educated way of answering my query would have been....
Buddy it looks like you might have a very old version, try downloading the latest version from.....
I will try this now that I have this information and hopefully that will resolve my issue.",Bitter Frustration
"Snuffleupagus' answer gave you the exact information you need and about ""more polite and educated way..."", sorry but it's likely a matter of taste, culture, whatever, .... Just as a reminder, some people here are working for free, on their spare time, to provide you a free product, so there is no need to be rude yourself in explaining us how to be polite or educated.",Bitter Frustration
"Thanks but given this is supposed to be a place to get help from like minded individuals/experts there is no need for assumption or unwarranted denegration.
Assuming that I am not doing the same as you in a different field is.....
The first job of giving assistance is to understand the issue, the issue isn't that I have been using a fabulous piece of technology for 5 years without that doesn't exist !
As clearly articulated the fabulous piece of technology we have been using has suddenly presented an issue that we haven't seen before, one that appears to only occur on firefox browsers. Personally I'd see that as an opportunity worthy of solving, not an opportunity to be rude.
Perhaps you can point me in the direction of installation instructions of the latest version, the VERY OLD version we have been using only needed to be dropped into the website folder for the viewer to function, however this does not appear to be the case with the latest version ? and the page link provided provides no insight as to further installation.
I apologise for not knowing more about your product or need more than a viewer that works.",Bitter Frustration
"@Snuffleupagus and @calixteman are completely correct here: ""ViewerJS"" is _not_ developed or maintained by us, so you opened an issue in the wrong repository. Therefore, as indicated, please open an issue at ""ViewerJS"" itself at https://github.com/webodf/ViewerJS/issues instead since it's impossible for us to provide support for a project that is not ours.
Locking this issue since the discussion is going nowhere and the response to _correctly_ provided help isn't polite and we want people to be treated respectfully here.",
"Attach (recommended) or Link to PDF file here:
Configuration:
- Web browser and its version : Firefox
- Operating system and its version: Win 10 upto date
- PDF.js version: 0.5.8
- Is a browser extension:
Steps to reproduce the problem:
1. We use a simple iframe to display PDF, nothing fancy (src = ""/ViewerJS/?zoom=page-width#../Uploads/EDocPro/EDT-21072022-163634.pdf""
2.PDF opens and displays what I guess is a template used by the supplier without the Invoice particulars
3. If I open with Acrobat or Acrobat Reader or the https://mozilla.github.io/pdf.js/web/viewer.html it displays correctly with the full text
As the PDF is not ours but something supplied to us we have no control over the fonts etc (I presume its something like that), how can we force the viewer to display it ?
What is the expected behavior? (add screenshot) - This is what the document actually should be
![image](https://user-images.githubusercontent.com/85852391/180388195-6bbe2e54-4f88-4251-a698-14a234dd133d.png)
What went wrong? (add screenshot) - This is what is displayed in Firefox
![image](https://user-images.githubusercontent.com/85852391/180387718-b28e7838-bec8-4588-ba7a-21284db45620.png)
Link to a viewer (if hosted on a site other than mozilla.github.io/pdf.js or as Firefox/Chrome extension):
[EDT-21072022-163634.pdf](https://github.com/mozilla/pdf.js/files/9165605/EDT-21072022-163634.pdf)",
"VERY predictable output from random(), after being seeded with an incrementing counter.
Attached sketch shows how I'm getting four streams of almost perfectly sequential numbers from an algorithm that uses multiple calls to random() after using an incrementing counter as a seed via randomSeed(). It should not be possible to get anything orderly from that, especially over such an immediately short period.
The sketch (below) produces mac-addresses as my original script does, then has four columns: The first three columns show sequential numbers from the high-byte of the last octet, the fourth column shows sequential numbers from the low-byte of the last octet.
Right from the start, with seed 0xb53fdfdf:
```
// Random Seed: 0xb53fdfdf
e2:fa:74:b3:a5:9c 9
0e:9c:eb:97:7f:4c 4 c
3a:3e:62:7a:59:fc f 6a:e0:d9:5e:34:ac a
96:82:50:41:0e:5c 5 c2:24:c7:25:e8:0c 0 f2:c6:3e:08:c3:bc b
1e:68:b5:ec:9d:6c 6 4a:0a:2c:d0:77:1c 1 7a:ac:a3:b3:52:cc c
a6:4e:1a:97:2c:7c 7 d2:f0:91:7a:06:2c 2 02:92:08:5e:e1:dc d
2e:34:7f:42:bb:8d 8 5a:d6:f6:25:95:3d 3 8a:78:6d:09:70:ed e
b6:1a:e4:ec:4a:9d 9 d
e2:bc:5a:d0:24:4d 4 12:5e:d1:b3:ff:fd f
3e:00:48:97:d9:ad a 6a:a2:bf:7b:b3:5d 5 9a:44:36:5e:8e:0d 0
c6:e6:ad:42:68:bd b f2:88:24:25:42:6d 6 22:2a:9b:09:1d:1d 1
4e:cc:12:ec:f7:cd c 7a:6e:89:d0:d1:7d 7 aa:10:00:b4:ac:2d 2
d6:b2:77:97:86:de d 02:54:ee:7b:60:8e 8 32:f6:65:5e:3b:3e 3
5e:98:dc:42:15:ee e e
8a:3a:53:25:f0:9e 9 b6:dc:ca:09:ca:4e 4
e6:7e:41:ed:a4:fe f 12:20:b8:d0:7f:ae a 3e:c2:2e:b4:59:5e 5
6e:64:a5:97:33:0e 0 9a:06:1c:7b:0e:be b c6:a8:93:5e:e8:6e 6
f6:4a:0a:42:c2:1e 1 22:ec:81:26:9d:ce c 4e:8e:f8:09:77:7f 7
7e:30:6f:ed:51:2f 2 aa:d2:e6:d0:2c:df d d6:74:5d:b4:06:8f 8
06:16:d4:97:e0:3f 3 f
32:b8:4b:7b:bb:ef e 5e:5a:c2:5f:95:9f 9
8e:fc:39:42:6f:4f 4 ba:9e:b0:26:4a:ff f ```
That's seeded with 0x3ffe883c, and that output is near the beginning.
The first three columns to the right of the MAC addresses all show the high-byte of the last octet:
* The 1st column shows 0xf - 0xf, sequentially
* The 2nd column shows 0x4 - 0x4, sequentially
* The 3rd column shows 0x9 - 0x9, sequentially
The 4th column counts the low-byte of the last octet over a longer period, but the whole pattern repeats almost perfectly, and seemingly indefinitely, regardless of how it's seeded. Not the behaviour I expect from a PRNG!
Just looking at the ""random"" MAC addresses I was wanting, it's obvious that the low-byte of the last octet is repeating for 14-15 times (14.7, on average) and then incrementing. Again, should be 100% impossible to get this from any self-respecting PRNG being seeded by a counter.
nb the sketch has a delay() towards the bottom, so it scrolls up on a console at human-readable speed. For data-collection, just comment out the delay.
nb2 the random seed can be set manually.
sketch:
```
/*
===========================================
buggy ""random()"" function, as observed here:
https://github.com/atom-smasher/esp8266_beaconSpam
this sketch starts with a random seed ""randomMacSeed""
that seeds the prng via ""randomSeed()"" and an incrementing counter
calls to ""random()"" are producing some very predictable outputs!
this sketch shows the mac-addresses as they would be generated,
and derived from those mac-addresses are four columns, all counting
incrementally, with very few errors
expected results: using an incrementing counter to seed the prng, i expect
to NOT have predictable output like this
===========================================
*/
const uint64_t randomMacSeed = os_random(); // random seed on startup
//const uint64_t randomMacSeed = 0x1234abcd ; // fixed seed; make it your own
uint32_t i = 0;
uint8_t macAddr[5];
void mayhemMac(uint32_t ssidNum) {
randomSeed(uint32_t((randomMacSeed) + (ssidNum)));
macAddr[0] = uint8_t(random(0x0, 0x100)) & 0xfe | 0x02 ;
macAddr[1] = uint8_t(random(0x0, 0x100));
macAddr[2] = uint8_t(random(0x0, 0x100));
macAddr[3] = uint8_t(random(0x0, 0x100));
macAddr[4] = uint8_t(random(0x0, 0x100));
macAddr[5] = uint8_t(random(0x0, 0x100));
}
void setup() {
// start serial
Serial.begin(115200);
//Initialize serial and wait for port to open:
while (!Serial) {
delay(10);
}
// wait for serial port to connect
delay(300);
Serial.println();
Serial.printf(""\n// Random Seed: 0x%x\n"", uint64_t(randomMacSeed));
}
void loop() {
for (i = 0; ; i++) {
yield(); // needed for extra-large lists
mayhemMac(i);
Serial.printf("" %02x:%02x:%02x:%02x:%02x:%02x"",
macAddr[0], macAddr[1], macAddr[2], macAddr[3], macAddr[4], macAddr[5]);
yield();
if (0 == i % 3) {
Serial.printf("" %x"", macAddr[5] >> 0x4);
}
if (1 == i % 3) {
Serial.printf("" %x "", macAddr[5] >> 0x4);
}
if (2 == i % 3) {
Serial.printf("" %x "", macAddr[5] >> 0x4);
}
if (1 == i % 15) {
Serial.printf("" %x"", macAddr[5] & 0x0f );
}
Serial.println(); delay(200);
}
}
```
Edited to correct how the seed is displayed, and share output with a verified seed.",Impatience
"I think they would have to incorporate a battery, I just read Arduino has a 58-day limit in buffer overflow with the millis(). I always wonder how that goes. I imagine three internal tickers, one starting at zero, the other at the negative most number, and they both go forward equal same increment rate, and one starts at the positive most spectrum, and goes backwards at half the rate, all three then are part of calculation to issue a millis(), and because millis is never more than a second, you need then to get the calendar clocking segments like second and weeks, whereas when the timer reaching overflow threat approaches, it begins in reverse and the prior reversing one picks up in rate, and the last most slows in rate, like washing or something like that never has a second, but you probably also then need to incorporate dates as a bar association reset of approaching is when? In rate of millis() used to possible catch the buffer overflow and it's a big Y2K mess! But I say that because I think these timer states would be flash saved when booted down, or at a last state type of mode, then booting up the random seeding is partial to the system is unique in back powered up. IDK, though. Tuff one.",Bitter Frustration
"I just read Arduino has a 58-day limit in buffer overflow with the millis(). I always wonder how that goes. I imagine three internal tickers, one starting at zero, the other at the negative most number, and they both go forward equal same increment rage, and one starts at the positive most spectrum, and goes backwards at half the rate, all three then are part of calculation to issue a millis(), and because millis is never more than a second, you need then to get the calendar clocking segments like second and weeks, whereas when the timer reaching overflow threat approaches, it begins in reverse and the pri9or reversing one picks up in rate. Something like that never has a second however, and then pending you can wait out how many years to possible catch the buffer overflow and it's a big Y2K mess! But I say that because I think these timer states would be flash saved when booted down, or at a last state type of mode, then booting up the random seeding is partial to the system is unique in back powered up. IDK, though. Tuff one. You could ignore real time and develop in the calendar intervals, second, minute, day, week, month and year. Just say all Arduino start at a date when powered and kept from the user interaction.
Actually, there is a better way. Query the US court system for the jury selection method, and you'll be going over it in review.",
"Hi, just like you I have really slow Vue development. There is a alpha version of plugin to marginally increase the speeds. Please see https://github.com/johnsoncodehk/volar/issues/1615",
try `0.38.5`,
"> try `0.38.5`
thanks",
"> Hi, just like you I have really slow Vue development. There is a alpha version of plugin to marginally increase the speeds. Please see #1615
try ^0.38.5",Impatience
"Seriously affect the development efficiencyï¼
please fixï¼
please fixï¼
please fixï¼
please fixï¼",Impatience
"Sorry no, see other previous issues if you are interested in a longer answer.",Impatience
"```sh
git clone https://github.com/composer/composer.git --branch 2.5.4 ~/composer-build && \
composer install -o -d ~/composer-build && \
wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
cd ~/composer-build && patch -p1 < composer.patch && \
php -d phar.readonly=0 bin/compile && \
rm /usr/local/bin/composer && \
php composer.phar install && \
php composer.phar update && \
mv ~/composer-build/composer.phar /usr/local/bin/composer && \
rm -rf ~/composer-build && \
chmod +x /usr/local/bin/composer
```",
"@Seldaek I'd be willing to bet that a good portion of people relegate the conflict with the threat of ***nuclear war***. So, I'll be the one to bring up that point as to why I'm sure a lot of people would rather not see this message stamped in their console while doing a simple package inclusion.",Bitter Frustration
"You can remove it by removing this specific package:
`composer remove ukeloop/stand-with-ukraine`",
"we also have this amazing discussion about the topic with a bunch of solutions: https://github.com/composer/composer/discussions/10808 (censored, now it is https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message)",Bitter Frustration
"@Seldaek The finality of your ""no"" answer reminds me of closed-source software vendors who say ""No, you can't change the code."" It goes against the spirit of open-source. Not only that, it's ineffective. Annoy others (as you have) and they'll go from neutral to hating your message. Have a nice day.",Bitter Frustration
I don't stand with Ukraine and what the Composer has to do with it?,
"When running composer, I see the #standwithukraine political statement. Can we remove any political statement from composer? Some of us just want to do work without having anything to do with politics. That world is extremely toxic, can we just code in peace without being reminded of that toxicity? Also it'd be nice if no politics ever had influence on this project. I'm not russian or ukrainian but I have had friends from both countries and there are good people in both, pls stop the divide.",Bitter Frustration
"I can understand that they want to use the reach here to draw attention to the war, but it is not our war, it is driven by political interests that we do not understand and that we cannot influence. And srsly: ""Let's fight back against the Russian regime""? We are not your private army, nor is it useful to turn the aggression around. Information must be freely available and the only way out of the conflict is de-escalation on all sides - just my 50 Cent",Bitter Frustration
"> I can understand that they want to use the reach here to draw attention to the war, but it is not our war, it is driven by political interests that we do not understand and that we cannot influence. And srsly: ""Let's fight back against the Russian regime""? We are not your private army, nor is it useful to turn the aggression around. Information must be freely available and the only way out of the conflict is de-escalation on all sides - just my 50 Cent
""but it is not our war"" - well that's was Ukrainians thoughts during war in Georgia. or in Chechnya. ""it is driven by political interests"" - oO, so politicians was raping and murdering people in Bucha, Izyum, Kupiansk, Kherson? As i know it was regular russians nad not Putin himself.
""conflict is de-escalation on all sides"" - I guess you want to give russians your home to ""de-escalate"". Because a lot of ukrainians do not understand why they should give their homes to rssians.
Sure it is not your war, just wait until it become yours",Bitter Frustration
"@pMatviienko Ukraine committed genocide of civilians in Donbas since 2014... Victoria Nuland and the CIA conducted a coup in Ukraine to put your lord and savior Zelensky in office and then began ratcheting up attacks civilians in Donbas and throughout Eastern Ukraine... Both sides of this conflict need to deescalate, neither is heroic, neither should be celebrated.
""One of the most cowardly things ordinary people do is to shut their eyes to facts."" - C.S. Lewis",Identify Attack/Name Calling
I was actually a bit shocked when I first saw this. I strongly condemn any political statements in Opensource. Don't use your software as an audience for any political propaganda.,Bitter Frustration
Some solutions can be found here: https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message,
"@jteedub any proofs for your words? or you just love to talk? By the way officially russian actions was confirmed a genocide, by the way. Actualy looking on reactions and moderation of thread pretty glad that i stopped using github a long time ago",Bitter Frustration
"@pMatviienko Are you just looking to get into an argument? One that was never a threat of being started, if it weren't for this political co-opting of a package manager? Seriously, what is the end-game, here? It's pretty apparent the support for keeping composer politically neutral is stronger than your insistence that it remain for your personal reasons.",Bitter Frustration
"Please add the output of the following statement to be fair:
#standwithLibya
#standwithSyria #standwithIran
#standwithCAR
#standwithAfghanistan
#standwithIraq
#standwithYugoslavia
#standwithBosnia
#standwithVenezuela",
There is also `#standwithCyprus`. See https://en.wikipedia.org/wiki/Turkish_invasion_of_Cyprus.,
"Every normal person support Ukraine, so just calm down, chill",Mocking
I have submitted a https://github.com/theodorosploumis/stand-with-cyprus fyi.,
"It's not the same, if not everybody get's it unasked in there command line ;)",Mocking
"> It's not the same, if not everybody get's it unasked in there command line ;)
I know, I do not agree with the forced promoted message by composer either.",Bitter Frustration
ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤šå‚»é€¼ä¹Œè´¼ï¼Œæ”¯æŒä½ å¦ˆé€¼ï¼Œç»™æ³½å…°æ–¯åŸºçˆ†èŠèŠ±åŽ»å§,Vulgarity
"So, it seems shameful #StandWithNazi has finally been removed.",Mocking
"Have you added your Google Maps API Key in `app.json`?
You need this inside your ""android"" key:
```
""config"": {
""googleMaps"": {
""apiKey"": ""xxxxx""
}
},
```",
"> Have you added your Google Maps API Key in `app.json`?
> > You need this inside your ""android"" key:
> > ```
> ""config"": {
> ""googleMaps"": {
> ""apiKey"": ""xxxxx""
> }
> },
> ```
I did yeah since I encountered that solution on [GH](https://github.com/expo/expo/issues/5591) but that did not resolve the issue. Were you able to get it to work using Expo go and an API key?
Also, the docs mention that no additional setup should be necessary when using Expo Go. > No additional configuration is necessary to use react-native-maps in Expo Go. However, once you want to deploy your standalone app you should follow instructions below.
https://docs.expo.dev/versions/latest/sdk/map-view/",Impatience
"Granted there isn't a misconfiguration in your config file, it could potentially be related to https://github.com/expo/expo/issues/18530. I never confirmed what happens on android emulator, but Google Maps did seem to render on a physical Android device following the documentation.",
"its hard for me to speculate since I don't know how to look at logs for this issue but, coming from web development and dealing with API key misconfigurations it does smell like an API key issue. Though I have absolutely no idea how it is configured in expo. That being said, the iOS version of the [snack demo I provided](https://snack.expo.dev/xdxj1hUb7?platform=ios) does seem to work. It just shows up as gray on android. Very strange. ![image](https://user-images.githubusercontent.com/21202515/183715858-f80cf3bd-f577-435a-a5d8-8fc990112134.png)
[Edit] I realize now that iOS probably works because apple maps is the default provider for map view. Where as google maps is the default provider for android. Seems related lol.",
"we have a similar situation. the expo go app doesn't show the map anymore - BUT if we release it, then the real production app still shows the map. So I assume something in expo go is wrong",Bitter Frustration
"So, I spent the time figuring out how to launch the app with the **_bare workflow_**. And surprisingly it works even without an API key. Very strange but, at least there's that.",Irony
My assumption is that this issue and https://github.com/expo/expo/issues/18570 are related. In both cases it seems the expo go app does not have access to the properties that are provided through the enviroment like app.json,
"a serious and blocking issue, we can't go further in our project! , should we turn back to SDK 45 ?! why no one assigned to this bug?!!",Bitter Frustration
"> a serious and blocking issue, we can't go further in our project! , should we turn back to SDK 45 ?! why no one assigned to this bug?!!
Have you tried converting your project to a bare workflow? That should allow you to add your google maps API key manually in the android files and allow you to keep building while we wait for a resolution to the managed version.",Impatience
"you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/",
"Thank you for filing this issue!
This comment acknowledges we believe this may be a bug and thereâ€™s enough information to investigate it.
However, we canâ€™t promise any sort of timeline for resolution. We prioritize issues based on severity, breadth of impact, and alignment with our roadmap. If youâ€™d like to help move it more quickly, you can continue to investigate it more deeply and/or you can open a pull request that fixes the cause.",
"@brentvatne development builds don't allow hot reload, right? We use expo go during dev in Wi-Fi because unlike an emulator it gives all features of a real device.",Impatience
"Based on your suggestion I moved to the expo development build rather than the bare workflow. And the map showed up right away no API key necessary. However, I continued to plug along and move my firebase implementation to the [legit version of firebase](https://rnfirebase.io/) rather than using the JS SDK that I was using with expo go. And the map immediately started crashing lol. Like it started expecting an API key when firebase got added. I've moved to using mapbox instead while I wait for this issue to be addressed [edit] removed my salty comments. I just picked a bad week to pick up react native lol",Bitter Frustration
"> you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/
forget about my case, why it does not work in https://snack.expo.dev ? do you think you gave the best answer by asking to create dev build !!! there is a breaking change , it worded in SDK 45 and previous versions, so it must work in 46 too, don't ask users to go with other alternatives !!!",Bitter Frustration
@da45 please calm down. It's good engineering practice to try to find workarounds. Often it unblocks at least some percentage of users.,Impatience
"@da45 I agree it is a breaking change and it is important. So why don't you fix it? Or why didn't you help to test it before the release?
It's an open source library, so, if you don't fix it, or if you didn't help to test it, consider yourself as the guilty that this error still exists.",Bitter Frustration
"And if it is working on SDK45, why in the heaven don't you downgrade to SDK45 until this gets fixed?!",Impatience
"It is not SDK 46 problem! @brentvatne @da45 With SDK 45, Expo Go 2.25.1 grey block on android, crash on ios. With SDK 46, Expo GO 2.25.1 grey block on android, crash on ios. But,
With SDK 45, Expo Go 2.24.3 work.
How to downgrade Expo 45 with Expo Go 2.24.3.
```
- Delete Expo Go app from Simulator and Real Device.
1. expo upgrade 45
- Downgrade to sdk 45, because Expo Go 2.24.3 doesn't support Expo SDK 46.
3. expo start
- Expo , automatically install expo go 2.24.3.
```
Now you can wait for fix Expo Go problem :)",Bitter Frustration
"@hlthi Hello, how can I reinstall Expo Go 2.24.3 ?
On my phone, I have installed Expo Go from the store which appears to be 2.25.1 but I do not know how to downgrade it ^^' Edit: found!
I replaced the version number in the official APK link on the Expo website: https://d1ahtucjixef4r.cloudfront.net/Exponent-2.25.1.apk",
"> you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/
Hi @brentvatne, I tried with a custom development build with `SDK 46` and `react-native-maps 0.31.1` and the map is blank. I am not using Expo Go. API key is loaded via `app.config.js` on `android.config.googleMaps.apiKey` as indicated in the expo documentation.
Did you make it work on your end?",Impatience
I am using 'app.json' and I don't have this problem. Are you all using 'app.config.js'?,Impatience
"> I am using 'app.json' and I don't have this problem. Are you all using 'app.config.js'?
`app.config.js` is just a dynamic version of `app.json` that I use for app variants. Fmi: https://docs.expo.dev/workflow/configuration/
@BrodaNoel are you using the same version as I am? SDK 46 / react-native-maps 0.31.1 / development build and your API key set on android.config.googleMaps.apiKey in app.json?",Impatience
"Hi all! I'm sorry, but I'm locking this thread due to some heated comments. We will deploy a fix asap! Here is an overview of the issue:
### Overview
The issue seems to be in our Android build of Expo Go. Snack and the Expo Go version for SDK 46 (`2.51.x`) are affected and will not show Google Maps during development.
> ~~I'll update the comment whenever we deployed the fixed Expo Go version to the stores.~~
- Expo Go `2.25.2` is [available on the Play Store](https://play.google.com/store/apps/details?id=host.exp.exponent)
- Expo Go `2.52.2` has been deployed to Snack
### Workarounds
1. Use a [custom dev client](https://docs.expo.dev/development/getting-started/) with your own Google Maps key. It's similar to the bare workflow approach some of you mentioned. This method also allows you to install the latest version `react-native-maps@1.2.0` in your app. You can use [EAS Build](https://docs.expo.dev/build/introduction/) to build the app if you don't have the Android SDK installed.
_**I would heavily recommend this method for now**_
2. Temporarily downgrade to SDK 45 to use an older Expo Go. This method would allow you to use the previous Expo Go version. You'll need to remove Expo Go from your Android phone or simulator to do this. Whenever you start your SDK 45 project with `expo start --android`, and your device is connected through USB and ADB, it will automatically install this Expo Go version.
- [Expo Go `2.24.3` APK for Android](https://d1ahtucjixef4r.cloudfront.net/Exponent-2.24.3.apk)
Hope this helps!",
"As for some of your questions/comments:
---
> we have a similar situation. the expo go app doesn't show the map anymore - BUT if we release it, then the real production app still shows the map. So I assume something in expo go is wrong
Yes, that assumption is right! We found an issue that only relates to the Expo Go builds and not your production apps. Even apps built with the classic build system (`expo build`) are not affected. But, because we use Expo Go in Snack too, Snack is also affected.
---
> @brentvatne development builds don't allow hot reload, right? We use expo go during dev in Wi-Fi because unlike an emulator it gives all features of a real device.
Expo dev client builds are fully capable of doing hot reload. I would suggest to try it out :)
---
> Based on your suggestion I moved to the expo development build rather than the bare workflow. And the map showed up right away no API key necessary.
>
> However, I continued to plug along and move my firebase implementation to the [legit version of firebase](https://rnfirebase.io/) rather than using the JS SDK that I was using with expo go.
>
> And the map immediately started crashing lol. Like it started expecting an API key when firebase got added.
>
> I've moved to using mapbox instead while I wait for this issue to be addressed
>
> [edit] removed my salty comments. I just picked a bad week to pick up react native lol
The Expo dev client and EAS allow you to pick any native module as you see fit. I'm sorry you had an unfortunate experience with `react-native-firebase`, I'm not sure if [you followed these steps](https://rnfirebase.io/#expo) but some libraries require you to add credentials, even in development mode. We can't control that outside of Expo Go and/or for third party libraries.
(And thanks for keeping things civilized â¤ï¸)
---
> forget about my case, why it does not work in https://snack.expo.dev/ ? do you think you gave the best answer by asking to create dev build !!! there is a breaking change , it worded in SDK 45 and previous versions, so it must work in 46 too, don't ask users to go with other alternatives !!!
Because Snack also uses Expo Go, Snack is also affected. We will deploy a fix asap.
---
> @da45 please calm down. It's good engineering practice to try to find workarounds. Often it unblocks at least some percentage of users.
I fully agree with this. We don't intend to break things, but if they do, we try to keep everyone as ""unblocked"" as possible while we fix things. We still have to go through store review whenever we fix it, [which took Â±15 days on the Play Store for SDK 46](https://twitter.com/cedricvanputten/status/1557375168955252738). So I think there is a lot of value in sharing these workarounds.
---
> Hi @brentvatne, I tried with a custom development build with `SDK 46` and `react-native-maps 0.31.1` and the map is blank. I am not using Expo Go. API key is loaded via `app.config.js` on `android.config.googleMaps.apiKey` as indicated in the expo documentation.
>
> Did you make it work on your end?
If you added the `android.config.googleMaps.apiKey` after running `expo run:*` or `expo prebuild`, it needs to recreate the native files. These keys are set in your native files that `expo prebuild` generates.
You can try running `expo prebuild --clean` and see if `expo run:*` works.",
This should be resolved in the latest Expo Go version! (`2.25.2`),
"Edit from Expo, [see the latest status of this issue here](https://github.com/expo/expo/issues/18554#issuecomment-1216561781).
---
### Summary
When running any app following the instructions on https://docs.expo.dev/versions/latest/sdk/map-view the map appears as a gray block on android. *NOTE* iOS seems to be working fine?
<img src=""https://user-images.githubusercontent.com/21202515/183310225-383d7f70-b6f2-476d-9c39-83916600cb69.png"" width=""100"" />
### Managed or bare workflow? If you have made manual changes inside of the `ios/` or `android/` directories in your project, the answer is bare!
managed
### What platform(s) does this occur on?
Android
### Package versions
``` ""dependencies"": {
""expo"": ""^46.0.0"",
""expo-status-bar"": ""~1.4.0"",
""react"": ""18.0.0"",
""react-dom"": ""18.0.0"",
""react-native"": ""0.69.3"",
""react-native-web"": ""~0.18.7"",
""react-redux"": ""^8.0.2"",
""react-native-maps"": ""0.31.1""
},
```
### Environment
```
expo-env-info 1.0.5 environment info:
System:
OS: Linux 5.18 Pop!_OS 21.10
Shell: 5.1.8 - /bin/bash
Binaries:
Node: 16.13.1 - /usr/local/lib/node/nodejs/bin/node
npm: 8.1.2 - /usr/local/lib/node/nodejs/bin/npm
npmPackages:
expo: ^46.0.0 => 46.0.2 react: 18.0.0 => 18.0.0 react-dom: 18.0.0 => 18.0.0 react-native: 0.69.3 => 0.69.3 react-native-web: ~0.18.7 => 0.18.7 npmGlobalPackages:
expo-cli: 6.0.1
Expo Workflow: managed
```
### Reproducible demo
https://snack.expo.dev/xdxj1hUb7
### Stacktrace (if a crash is involved)
N/A",
"Please, just be patient and ask on forum, spamming issues on github won't make someone look at your issue faster, it might only delay it. No need to be hostile/rude too.",Impatience
Fix your game/servers then I wouldnâ€™t have to â˜ºï¸ Thereâ€™s more out there than just gtaV,Impatience
"Didn't know CitizenFX released a game, that's news to me :o",Irony
"Github issues **are not** support tickets. Please use our forums, more specifically over at https://forum.cfx.re/c/redm-discussion/redm-bugreports/56 to report your issue.",
"This is not a support platform. Try the forums at https://forum.cfx.re/ instead.
_Originally posted by @blattersturm in https://github.com/citizenfx/fivem/issues/1567#issuecomment-1211329798_ Clearly I wouldâ€™ve tried the forums if anyone gave a shit about it and responded no one has even looked at mine or the other persons who is also having this same issue I NEED HELP THIS IS DAY 5 and Iâ€™m FRUSTRATED SO PLEASE DONT CLOSE MY NEXT TICKET WHEN I MAKE A NEW ONE thatâ€™s real shitty",Bitter Frustration
"We as packagers can make our own calls on these things.
We quite frequently are more strict than upstream and also quite often do not support all upstream features either.
Weâ€™re an opinionated repository. You couldâ€™ve asked nicely to allow empty input and explain its due to dhcp (which shouldnt be used on kubernetes for varying service IPâ€™s anyway)
Just tagging one of the two PRâ€™s (one adding input validation, the other adding required tag), is also not helpfull.
try again next time or, in case this is the tone and attitude you want to give: use the discord rants channel.
personally, I think you shouldnt use our Charts or Apps if you demand everything to be perfectly in synx with upstream.
ohh and packagers are developers as well. More than half of our work is Devops :â€™)",Bitter Frustration
"### App Name
Plex
### SCALE Version
22.02.1
### App Version
N/A
### Application Events
```Shell
N/A
```
### Application Logs
```Shell
N/A
```
### Application Configuration
N/A
### Describe the bug
Revert #3426 There is no reason to make a parameter mandatory that is not mandatory by the application developer. You're packagers, not developers. Do not try to make decisions for the application developer. Revert this change. For people with dynamic public IPs or DHCP this is a breaking change. ### To Reproduce
N/A
### Expected Behavior
N/A
### Screenshots
N/A
### Additional Context
N/A
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",Bitter Frustration
"https://github.com/osmandapp/OsmAnd/commit/4a258481362a15a4f9b93d796453f5e04e862207
It appears that this spyware tracking feature was added by @vshcherb (Victor Shcherb) back in 2015 and has been leaking users' data and travel history (via client IP geolocation) to the OSMAnd index server ever since.",Bitter Frustration
Any follow up from the code maintainer?,Impatience
"> Any follow up from the code maintainer?
See https://github.com/osmandapp/OsmAnd-iOS/issues/2115",Impatience
"> UUID is anonymized randomly generated key per installation, cause this key is not associated with any advertisement id or not transferred to any other 3rd party or even used, this key doesn't require consent. As you mentioned the IP could provide more information cause it's location specific.
> This approach helps to the server monitor fair usage of resources.",
"Some thing to add:
1. it's in privacy policy (https://osmand.net/help-online/privacy-policy/#3-the-information-company-collects)
2. We don't want to use IP as a counter, so we use randomly generated UUID which preserves the privacy much more than IP itself
3. You don't need to use OsmAnd Services if you don't agree with Terms of Use , you can download or build maps yourself.",
"The privacy policy says:
> We strongly believe in the principle of data protection and safety, thus, the Company does not collect, store, process or transfer any personal information of users besides the cases when such information is provided by the users with their clear consent. You have not obtained consent from the userbase to use their devices to store and transmit a unique tracking identifier for their device.
Most are unaware you are even doing so; that is the opposite of consent.",Bitter Frustration
"There is a strong disconnect between the tone and even some accusations and the actual issue being reported.
First of all, connecting to the server is only something that happens during download. OSMAnd is an offline map and downloads happen rarely. For heavy users less than once a month, for many much less than that. Additionally, all such downloads are user-initiated, they have to explicitly press a download button. Nothing that would be useful to build a profile of any specific user.
**Allegations of travel data and such being leaked are thus really not in evidence.**
The data that is being shared is not possible to connect to any personally identifying information. Notice that google explains that since some 5 years the `getUserAndroidId` is unique per installed application. So any other application the user uses (on the same phone, or not) will not share the UUID. So on top of the fact that not much of a profile can be made at all, there is no way to connect such to any actual personal info.
Does anyone actually have any real issues with the existing code?
For the record, I don't have any relation to the company behind OSMAnd, never received anything from them (other than FOSS software) and I'm just an open source developer standing up for common sense.",
"Even an application-specific identifier allows the server to track the travel history of that user via client-ip geolocation.
A user's travel history often uniquely identifies a person. The list of client ips and timestamps is the ""actual personal info"".
Dismissing the issue does not change the fact: you have no consent from the user for such tracking.
Common sense is not co-opting the user's device to transmit tracking identifiers without their advance knowledge and opt-in consent.",Bitter Frustration
"I think there are 2 direction of discussions: consent and identification.
1. I don't think consent is needed, same as you don't need consent that your IP is used to download maps. Second it's not a website where you read information, here you use resources, so in that sense by downloading data you agree to fair use of resources (Terms of Service) and in that case you agree that you don't download too many resources especially if you are not paid user for example
It would be completely different case if program would send statistics in background that you're using the application and here is an IP. To summarize: it's probably needs to be more clarified in Terms of Service but it doesn't require a consent.
2. User identification. We switched from IP to UUID to calculate how many active users & how often update exactly to solve the server load issue and don't use personal data. For example we don't require email to identify user and don't track IP exactly cause IP could provide location specific information. Random UUID **doesn't identify anything** if it's not connected to any other services or other private information. We don't share UUID with any other applications so they also couldn't track that UUID even if these apps have personal information.
It's completely incorrect to say that app could view **travel history**, I would say it's total nonsense cause the server only views what and when maps were downloaded in contrast of other apps that provide Internet services. -----------------
Last but not least.
The websites require to ask consent only for information they gather for example if you search an address on website and website doesn't store these addresses or don't connect them to IP (address is essentially publicly available information), consent is not needed. If the website will start storing information in the table [IP, Search], then consent is needed. So it's all about how the data is processed and used and not only what's being transmitted.",
"Just to provide another point of view on this topic. As a Service provider I need a unique identifier to predict the load on servers (especially once maps are updated), so in the end it would say:
""Download maps won't be provided if user doesn't agree to create a random key for the installation"".
Does it makes sense then to implement at all ? Cause in the end it will end up, if you to download - you need a consent which to me is actually totally clear if you don't want to know what & when you downloaded - download or generate maps yourself and import to the app. **App won't send to the server what you've imported yourself!**",Bitter Frustration
"@sneak you claimed that;
> to track the travel history of that user via client-ip geolocation.
Considering that this is an offline-map which doesn't normally do downloads, how do you support your claims? If you continue to make big claims, please provide actual evidence.",Impatience
"No, this doesn't make sense at all, as I can distribute a forked client that sends no ID, or a random UUID on every single HTTP request
It doesn't get you anything to trust the client to self-report.",Bitter Frustration
Please stop suggesting that the UUID is somehow private or anonymous. With client IP geolocation it can identify users by travel history. It is PII.,Bitter Frustration
"Il 14/08/22 13:57, Jeffrey Paul ha scritto:
> No, this doesn't make sense at all, as I can distribute a forked client that sends no ID, or a random UUID on every single HTTP request
In which case the client will be blocked from downloading the datasets, no? Sounds like the point.",Impatience
"@sneak > Please stop suggesting that the UUID is somehow private or anonymous. With client IP geolocation it can identify users by travel history.
Maybe the problem lies that nobody explained in technical details what it means for OSMAnd to be an offline map.
It means that the application, when started, does not cause any network traffic at all. You can travel and use the map daily, including searches for point-of-interests and doing routing, without ever causing a single download. Without, specifically, leaking that uuid.
OSMAnd, in other words, is perfectly useful and allows people to use the app for months or years without ever triggering the download you have noticed sends an UUID. People can download all their data at home or at work, regardless of where they travel later.
Sneak: you are wrong. It is time to admit you came to wrong conclusions and time to publicly show you are able to learn when presented with new facts. Thank you.",Bitter Frustration
"Let me summarize a bit.
* To keep the server load tolerable the admins want to limit the per-user-map-downloads
* To achieve that, OsmAnd creates an UUID with Java.util.UUID.randomUUID() (https://tutorialspoint.com/java/util/java_util_uuid.htm)
* The server logs all requests containing the UUID and the user's IP and stores it indefinitely
* The UUID is used to block too frequent downloads
**Consent**
I am no lawyer, but I would look at it by means of GDPR. However, I would agree with the accepted answer [here](https://law.stackexchange.com/a/48288) that there is no consent required.
**Concerns that come to mind**
Although I believe the claims that the UUID/IPs are never shared with any third party, I think it would be better to not have to trust anyone. Maybe one day the server gets hacked or the admins are forced to hand over all data to law enforcement.
Indeed I don't like the idea that all my IP history gets into the wrong hands. IP adresses are not anonymous and can be assigned to individuals in many ways. Flourishing VPN services show that people understood that.
**Ideas for compromises**
Sadly, I have no idea that takes no development effort. Although I believe that the effort would be manageable. Here are two:
* UUID could be re-generated e.g. when more than 2 weeks have passed. For that, OsmAnd would need to store an additional timestamp together with the UUID
* Records could be deleted on a regular basis on the server-side
* @scaidermern had [another one](https://github.com/osmandapp/OsmAnd-iOS/issues/2115#issuecomment-1212955350)",
"The uuid does not limit anything. Anyone can make as many requests as they want with a new uuid each time, defeating this limit.",
"> The uuid does not limit anything. Anyone can make as many requests as they want with a new uuid each time, defeating this limit.
This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users.",Bitter Frustration
"@saddy001 - agree that server has no control but practically it's unsolvable issue cause indeed anything could happen on the server side (i.e. hacked or rights transferred). In the end it would be a liable action or reputation loss. - We gather general statistics by 1 month mostly though I would tend agree to regenerate every 3 months cause then we could generate quarter reports
- We also need to spend time on server side to correctly clean the data and leave just summary reports without any private information. For that matter we need to do proper automation and standardize reports a bit .
---------------------
I think these are doable actions and if we're fine with them, I will start planning implementation. Although it might not be too much development effort but I want to implement carefully so in summary reports it will be comparing ""apple to apples"".",
"> *This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users.*
I intend to distribute a fork of this application with all unique IDs randomized on every request. Your assumption will not remain accurate.
Also there is nothing stopping a single someone from making a high request load to the server with random IDs, rendering the server data collection useless.",
"> > _This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users._
> > I intend to distribute a fork of this application with all unique IDs randomized on every request. Your assumption will not remain accurate.
You can do this of course. However, how many users do you expect to use your fork? I bet the above number will still remain close to 100%.
> Also there is nothing stopping a single someone from making a high request load to the server with random IDs, rendering the server data collection useless.
Blocking via IP is still possible unless you intend to perform a DDOS... Let's go back to the topic, shall we?",Impatience
"> It's completely incorrect to say that app could view travel history, I would say it's total nonsense cause the server only views what and when maps were downloaded in contrast of other apps that provide Internet services.
It's really sad to see that you don't understand the privacy implications of your choice of putting a unique tracking identifier (similar to a cookie) on a user's device that persists over time (and changes in client IP).",Bitter Frustration
">it's in privacy policy
Do you really think anyone actually reads it?
>we use randomly generated UUID which preserves the privacy much more than IP itself
A unique identifier that, in contrast to the IP address, can't be easily anonymised by the user and that also persists across IP addresses â€œpreserves the privacyâ€? Surely, Google uses those Android ad IDs because they're so incredibly anonymous.
>You don't need to use OsmAnd Services if you don't agree with Terms of Use
Besides the fact that privacy laws are a thing, this is incredibly detrimental to say, considering that OsmAnd has always presented itself as a privacy-friendly alternative to commercial services. Telling people not to use it if they care about privacy is something I'd only expect from Google or Microsoft.
>you can download or build maps yourself
This is not an option for probably 99% of users.
I agree that this unique ID is not as bad as the issue's author claimed it to be. However, I also think that OsmAnd should not be designed in a way that could theoretically enable behavioural tracking.
Would simply re-generating the ID at regular intervals really be too difficult? In my opinion, it would be such an easy way to improve everyone's privacy and to calm anyone who's worried.",Bitter Frustration
"> Please stop suggesting that the UUID is somehow private or anonymous. With client IP geolocation it can identify users by travel history. It is PII.
@sneak Yes, UUID is PII. But so is IP address itself [according to GDPR](https://www.gdpreu.org/the-regulation/key-concepts/personal-data/). So _even_ if there were **no** UUID **at all**, PII would **_still_** be transmitted and stored by OsmAnd servers. Thanks for pointing to this issue, but I think it is now time to stop beating a dead horse (we get it: current OsmAnd situation is not perfect, but they seem to recognize the problem and look willing to improve on it, so let's try to **not** paint them as villains worse than Google or Microsoft - for the time being, eh?) and concentrate on **how to best address this issue** - in an **_actionable way_** that is acceptable both to privacy-oriented users and OsmAnd server admins, while also following [GDPR](https://gdpr.eu/gdpr-consent-requirements/) and other applicable laws.
My suggestions:
- randomize said UUID often - e.g. as suggested by @scaidermern in https://github.com/osmandapp/OsmAnd-iOS/issues/2115#issuecomment-1212955350
- change privacy policy at https://osmand.net/help-online/privacy-policy/#3-the-information-company-collects to be both correct and up-to-date, and with an explanation **_why_** something is being done (e.g. something akin to `... when downloading maps from within OsmAnd, your IP address and randomly generated unique identifier are transmitted to OsmAnd servers in order to facilitate download and prevent server load issues & abusive behaviour, as well as generate map popularity statistics. That unique identifier is regenerated every XXXXX days/hours/xxxx. If you download maps manually outside of OsmAnd, the unique identifier will never be sent to OsmAnd servers nor elsewhere...`). While updating it, please also take notice of editing plugin and its set of additional privacy issues. Nice example of IMHO clear and useful privacy policy is at https://github.com/streetcomplete/StreetComplete/blob/v45.2/app/src/main/res/values/strings.xml#L314-L336 ; one may want to pick ideas/text from there.
- also, what does `Settings` / `Analytics` / `Maps downloaded` change when turned off? I mean, by the nature of transferring the map the OsmAnd servers would know what maps are downloaded from the app, regardless of this flag. Does this also share the information about maps downloaded outside of OsmAnd? Or does it collect, but then not count/process information about maps downloaded from users with this turned off? I.e. what is exactly the difference between `on` and `off` there?
- actually _link_ to said privacy policy from within OsmAnd app in `Help` menu. At least I can't seem to find it there.",
"Users' IP addresses change, this tracking ID does not.
Also, it does nothing to aid the server. This is a red herring. An abusive client can simply send a random UUID. The tracking ID should be removed entirely and actual anti-abuse measures implemented in the server (or just ask for some volunteer mirrors and have the server only distribute hashes and mirror URLs) and stop pretending that bandwidth at this scale isn't practically free.
The Ubuntu and Debian projects distribute orders of magnitude more data for free every day without tracking end users. These are just HTTP static file downloads and they aren't even that large.",Bitter Frustration
"Just a showerthought with my morning coffee in hand, as I bumped into this issue, but couldn't you hash the IP + UUID in similar way passwords are hashed? That way at least what's in the server logs would be useless to malicious hackers should they manage to obtain it.",
"No, the server still sees the unique id from the client and has the opportunity to track individual users.
The fix is to stop sending tracking identifiers from the client to the server. There is no benefit to the server to receive client ids, as any malicious client can send varying ones, and non-malicious clients are not an issue.
Sending client tracking ids is all privacy downside, for no security benefit.",Bitter Frustration
"Since the client needs to be trusted anyway for this whole scheme to work, how about having the client itself keep track of the number of downloads? It could then either send this number to the server, which would use it to decide whether or not to allow the download, or just refuse to initiate the download itself.",
"> actual anti-abuse measures implemented in the server
Can you elaborate a little on how that might be accomplished?",Impatience
"**Major actionable points:**
1. UUID will be regenerated (every 2-3 months)
2. Make sure logs are truncated / deleted, so only summary left
3. Privacy policy will be updated and more descriptively about (Fair Use of OsmAnd Online Services and UUID)
**Major information point:**
1. Providing UUID is part of Terms of Usage of OsmAnd Online Services such as downloading maps. UUID or account id for OsmAnd PRO will always be part of request
2. OsmAnd Online Services are **not unique** way to use OsmAnd Client application. Maps could be downloaded or generated manually. So there is no issue that OsmAnd Client hiddenly transmits private data
3. There is no any indication / facts / points provided that UUID + Downloaded Map can somehow track user or even more identify real person. Identification is a clear procedure that UUID will be connected to real data such as Name / Surname / Living Address / Working Address / Photo. If IP is corrupted and doesn't provide enough privacy then issue is not related to UUID.
--------------------------------------------
That's my basic understanding for now. Please leave comments in the nearest future if I'm missing something and that **issue will be frozen** to not create any further never ending discussions!",Impatience
"@vshcherb As far as I can tell, using a UUID does not provide any benefit for OsmAnd compared to the approach I [mentioned above](https://github.com/osmandapp/OsmAnd/issues/15058#issuecomment-1217665653) (simply storing a counter in the client). On the contrary, using a UUID seems to make things more difficult (regeneration, clearing logs, updating the privacy policy). It also makes tracking users at least theoretically more easy.
Could you maybe clarify why this approach is not being considered? Am I missing something?",Impatience
"Except that we couldn't count of how many unique users per month / quarter were active, exactly why uid was introduced to give a sum by day / month.",
"I'm sorry for my ignorance but why do you need this information? I wonder how useful it is because, for example, I'm a *very* active user but almost never update my maps so I won't show up in these statistics.
If you really want to know how many unique users download maps, couldn't the number of unique IP addresses be used as a decent approximation? Not that I necessarily like the idea of IP addresses being stored but at least this is information the servers get anyway.",Impatience
"> There is no any indication / facts / points provided that UUID + Downloaded Map can somehow track user or even more identify real person. Identification is a clear procedure that UUID will be connected to real data such as Name / Surname / Living Address / Working Address / Photo.
This misunderstands what identification means.
The UUID uniquely identifies a user when combined with a series of differing client IPs. It doesn't need to resolve *in your systems* to a user's name or photo; the fact is that the UUID represents one and only one user. This allows you to track *that one user's* movements from city to city (as you see different client IP locations).
It's irrelevant that you don't know the user's name; you have your own name for them - a UUID.
You don't have consent to track users in this manner without an opt-in. This UUID is a unique tracking identifier, akin to a cookie (although it's sent in the query string and not the HTTP header, the effect is the same) and it's for a nonessential purpose (tracking user count statistics). It's unnecessary for this purpose, anyway.",Bitter Frustration
"Just a quick additional information:
If OsmAnd B.V. _or_ their users are situated in European Union (and they are), they will **need to follow the GDPR**.
- @sneak note that _user consent_ is only **_one of six_** allowable ways for [obtaining legal basis for data processing](https://gdpr.eu/gdpr-consent-requirements/ ) - the OsmAnd B.V. might decide to use some other legal basis for data processing. In any case, they should consult a GDPR specialist and spell out in their Privacy Policy **exactly** what Personally Identifiable Information (like UUID or username, or IP addresses) is being collected, and specify in advance **all** purposes for which such data will be used, as well as other things (will that data be shared with third parties, stored/processed outside of EU etc.). If they later want to use PII for another purpose, they must re-obtain additional consent (or other legal basis) - they're not allowed to reuse previously collected data for new purpose retroactively. There are many things that need to be done correctly, which is why they should consult a GDPR specialist (disclaimer: I am not one so it is not an ad for myself; but I had to go thru the same pains, so I _am_ informed above average in the matter). There is a [checklist](https://gdpr.eu/checklist/) they might want to go through to verify if they did everything they need to.
- While having UUID might be required for some uses (e.g. all registered users), I would suggest _seriously considering options_ that require it as little as possible, especially for ""anonymous"" users. Any data collected has a chance to leak / be misused, so not collecting it in the first place would be smarter than having to invest resources and money to deal with protecting it adequately, and dealing with a possible breaches (and GDPR fines) later. Also, OSM crowd in particular are often much more biased to be privacy concerned (or they'd likely be using Google Maps in the first place) so it would be good business idea to try not to alienate them unnecessarily.",
"> This misunderstands what identification means.
> It's irrelevant that you don't know the user's name; you have your own name for them - a UUID.
@sneak is correct here. See for example this Law SE question [""Are auto-generated identifiers PII if they cannot be linked to an identity?""](https://law.stackexchange.com/a/82136)",
"Especially noteworthy (regarding the Law SE question):
>Key observations:
>
>- Singling out a data subject already counts as identification, meaning that it isn't necessary to infer their real-world identity!
>
>[...]",
"I agree that I need a proper legal advise and in the end consent or any other legally correct form of user getting informed.
Completely another topic whether Map downloads Service could be provided without identifying how many users and potentially prevent out of service issues. In the end that needs to be decided by OsmAnd BV, so I'm not going to take decision on my onw, cause it has legal and business implications.
**Note**: the issue will be frozen to stop message flooding and updates will be sent to it to get everybody informed.",
"**Summary**
The issue has bee fixed in Android / iOS and already available in nightly builds and will be released in 4.3. What's been done by sorted by priority:
1. It's possible to disable that UUID sending in Privacy Global Settings as the result the traffic could be deprioritized when lots of maps are downloaded i.e. in the beginning of the month.
2. UUID is rotated every 3 months so there is no user profile built for a longer history
3. ToS with explanation has been updated https://osmand.net/help-online/terms-of-use/
4. There is a clear indication On first screen after installation and a link to ToS.
As a result I'm closing the issue to make it completed. Thanks for reporting it so we could resolve that subject.",
"Great! Thanks for this improvement, highly appreciated! ðŸ‘",
Sending of the tracking identifier must be disabled ***by default***. You must ask the user if they consent to sending it BEFORE you ever send it.,Bitter Frustration
"Il 18/10/22 03:23, Jeffrey Paul ha scritto:
> You simply can't send any sort of tracking identifier for a user unless the user consents to it
This is clearly false: for instance no specific explicit consent is needed for the transmission of the IP address that comes along with every TCP/IP request, when it's necessary to perform the action requested by the user. Otherwise you'd end up in a catch 22 situation where you need previous consent to allow the user to download the privacy policy where they'll learn how to provide consent.",Irony
"https://github.com/osmandapp/OsmAnd/blob/22e40f113ce5c6df97f2f1687d5024ae38a4d28b/OsmAnd/src/net/osmand/plus/download/DownloadOsmandIndexesHelper.java#L273-L281
This appears to send the number of days since install, as well as a unique identifier (`getUserAndroidId`), to the index server when fetching indices, without respect for the telemetry preference setting.
This is a data leak that allows for a user's travel history to be tracked by the server, as these requests include client IP and a unique tracking identifier, and client IP is coarse (city-level) geolocation. This means the server can see the various cities the given userIosId travels to as the client IP changes over time.
This spyware feature also exists in the iOS version, and fails to respect the user's consent choice there as well. It seems unlikely that this is an accident.
https://github.com/osmandapp/OsmAnd-iOS/issues/2115",Bitter Frustration
"> when it's necessary to perform the action requested by the user
If you are able to opt out and the outcome of the request does not change compared to if you opted-in, it isn't necessary to perform said action.
From how I understood the use of the uuid it is a non-essential ID this is exactly how it works and it means it is not technically required to provide the service which does indeed mean, it needs to be opt in, not opt out.",
"> If you are able to opt out and the outcome of the request does not change compared to if you opted-in, it isn't necessary to perform said action.
As I understood the **Summary** above, the outcome **_does_** change. In particular, your download request (in times of heightened load) might be slowed down significantly (or maybe even delayed for some time or completely refused until retried later when load is lower). As s side note, there are other privacy-related options that can be enabled/disabled at that screen, that you might want to check, as well other related system-wide options (i.e. Google's _""Improve Location Accuracy""_ setting, enabled by default on Android phones, leaks hugely bigger amount of privacy and location data, and has much worse privacy record than OsmAnd. Not to mention that if you used Google Play app store to download any of the apps, you are also leaking more privacy data daily than this UUID setting would if you left it in ""on"" position forever).
Not to say this isn't important, but _before_ starting worrying about default setting of this UUID setting, one _at minimum_ should be running [de-googlified ROM](https://replicant.us/) on their phone and using **exclusively** some [privacy oriented app-store dedicated to providing non-proprietary free software only](https://f-droid.org/) instead of Google one (or Apple's, but there is no way to avoid using IOS on iPhone devices AFAIK, so you are out of luck there in any case). Some people do, and thus _they_ might want to suggest (to _their_ app store!) to change that default (for _that_ app store users), yes. For Google Play and (Apple store) crowd who (willingly or unknowingly, regardless) continues to surrender their privacy daily anyway, I do not see much point in changing the OsmAnd UUID _default_, as it only would result in their OsmAnd experience getting worse, with no noticeable privacy improvement at all.",Bitter Frustration
"Thanks, we are aware and have a Lavalink.jar prepped but there are other PRs to address first.",Impatience
Sounds good. Will wait for a fix to be pushed. Appreciate the swift response.,
I just noticed we're on the Red-Lavalink repo and not the Red-DiscordBot repo so I'm going to move this issue over there as this library is not affected and it's the audio backend side of things that is broken at the moment.,
"Yeah, I put it there since it seemed to be an issue with Lavalink directly. No worries.",
"PR's tied to this issue/needed for updating:
* #5821 * #5822 * @jack1142 , I can't find your PR that ""reverts"" #5751 to change the startup line detection to your ready check, that would go in this slot as needing to be included
* https://github.com/Cog-Creators/Red-Lavalink/pull/129",Impatience
"quick fix, stop bot first:
`export RedBotAudioCogLocation=***/you/should/probably/change/this/*** && cp $RedBotAudioCogLocation/Lavalink{.jar,.jar.old} && wget -O$RedBotAudioCogLocation/Lavalink.jar https://github.com/Cog-Creators/Lavalink-Jars/releases/download/3.4.0_1350/Lavalink.jar `
the audio cog was at `~/.local/share/Red-DiscordBot/data/RedBot/cogs/Audio/` for me (Linux, venv)
this also shouldn't need to be reverted when the update comes out, the update should replace lavalink.jar if I understand correctly",
how was that off topic at all?,Impatience
"By adding a comment that provided an unsupported fix that will break a lot of current users if they try to do what you suggested.
Leave the fixes and workaround for the people who are aware of the wider user base and repercussions for a given approach.",Bitter Frustration
"what? name a SINGLE repercussion? all the rest of the pr's are waiting on this jar, you telling me your code cant handle a file being replaced, when its literally going to replace that file? sorry, i may have a new github account, but i am by no means a noob, and i have four bs detectors attached to my head; my eyes and ears. i even added a command to keep the old jar, in case your code checksummed it for some reason, if you're checking the file creation date of a jar you're replacing or some crap, i concede, your dumb code will cause problems with this jar being replaced.
FURTHERMORE, I have redbot set up exactly as the docs instruct, you telling me you support environments that are not set up according to the recommendation of the docs? i know that's complete bs from being a part of the discord server. > people who are aware of the wider user base and repercussions for a given approach.
are these people aware their audio cog hasn't been working for days lmao, that not a repercussion?",Bitter Frustration
"Please refrain from acting like this in our issues. If you want to go ahead and fix this for your self by the path you suggested that's fine with us. However this fix doesn't roll for us for an general production environment. We have an open PR (#5823) ready for this that we're planning on handling today.
> sorry, i may have a new github account, but i am by no means a noob, and i have four bs detectors attached to my head; my eyes and ears.
We do not filter content based on GitHub account age, experience or the amount of sensory elements one has. We do however filter based on our audience, the end user. If we feel like the fix suggested here may proof dangerous or harmful, as seen above, we will take action on it.
> if you're checking the file creation date of a jar you're replacing or some crap, i concede, your dumb code will cause problems with this jar being replaced.
Now I'll be honest, I may not know our ""dumb code"" like some of our other contributors such as Draper here, but I fully entrust them that its working as intended. And that the above fix you've provided will not be compatible with it.
> are these people aware their audio cog hasn't been working for days lmao, that not a repercussion?
Yes, we are aware that we've received comments from people that Audio isn't working on an daily basis, and I'd like to remind you that there is an human on the other side of the argument here. We all have our own jobs to do, and our own live to life. Red for us isn't what brings in the money to pay the bills. It's an open source software project that we do in our spare time for fun.
I'm sorry that you think that our handling of your fix isn't the right thing to do. But I'm gonna stand with my team here and uphold it.
Have a wonderful rest of your day.",Insulting
This has been fixed in Red 3.4.18.,
"The music side of RED was working just fine, then all of a sudden, audio will not play back. I think the upstream Lavalink server needs to be updated.
2022-08-12 01:20:26.950 INFO 146 --- [XNIO-1 I/O-16] lavalink.server.io.SocketServer : {""op"": ""stop"", ""guildId"": ""[REDACTED]""}
2022-08-12 01:32:15.440 INFO 146 --- [XNIO-1 I/O-16] lavalink.server.io.SocketServer : {""op"": ""play"", ""guildId"": ""[REDACTED]"", ""track"": ""QAAAhgIAIUljZSBQb3NlaWRvbiAtIEN4IHNvbmcgKFJldXBsb2FkKQALWW91bmdKaWxrZXIAAAAAAASMEAALVzM5LWdyX1VQN2cAAQAraHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1XMzktZ3JfVVA3ZwAHeW91dHViZQAAAAAAAAAA"", ""noReplace"": false, ""startTime"": ""0"", ""pause"": false}
2022-08-12 01:32:16.412 ERROR 146 --- [lava-daemon-pool-playback-1-thread-1] c.s.d.l.t.p.LocalAudioTrackExecutor : Error in playback of W39-gr_UP7g
com.sedmelluq.discord.lavaplayer.tools.FriendlyException: Something broke when playing the track.
at com.sedmelluq.discord.lavaplayer.tools.ExceptionTools.wrapUnfriendlyExceptions(ExceptionTools.java:44) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.track.playback.LocalAudioTrackExecutor.execute(LocalAudioTrackExecutor.java:116) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.player.DefaultAudioPlayerManager.lambda$executeTrack$1(DefaultAudioPlayerManager.java:348) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[na:na]
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[na:na]
at java.base/java.lang.Thread.run(Thread.java:829) ~[na:na]
Suppressed: com.sedmelluq.discord.lavaplayer.tools.exception.EnvironmentInformation: lavaplayer.version: 1.3.98-devoxin
os.arch: amd64
os.name: Linux
os.version: 5.15.0-41-generic
java.vendor: Debian
java.version: 11.0.15
java.runtime.version: 11.0.15+10-post-Debian-1deb11u1
java.vm.version: 11.0.15+10-post-Debian-1deb11u1
Caused by: java.lang.IllegalStateException: No match found
at java.base/java.util.regex.Matcher.group(Matcher.java:645) ~[na:na]
at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeSignatureCipherManager.extractFromScript(YoutubeSignatureCipherManager.java:243) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeSignatureCipherManager.getExtractedScript(YoutubeSignatureCipherManager.java:160) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.loadTrackInfoFromInnertube(DefaultYoutubeTrackDetailsLoader.java:197) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.load(DefaultYoutubeTrackDetailsLoader.java:46) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.loadDetails(DefaultYoutubeTrackDetailsLoader.java:34) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeAudioTrack.loadBestFormatWithUrl(YoutubeAudioTrack.java:76) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeAudioTrack.process(YoutubeAudioTrack.java:42) ~[walkyst-lavaplayer-85f4877.jar!/:na]
at com.sedmelluq.discord.lavaplayer.track.playback.LocalAudioTrackExecutor.execute(LocalAudioTrackExecutor.java:104) ~[walkyst-lavaplayer-85f4877.jar!/:na]
... 4 common frames omitted",
i mean yeah. you're not supposed to want eye damage,Mocking
"> i mean yeah. you're not supposed to want eye damage
the eye damage doesn't actually disadvantage you mechanically. It could instead be a vignette, a shadow around the screen, or a dim transparent layer on top of it. This serves no purpose except looking like ass.",Bitter Frustration
"The blur causes bad eyestrain and shouldn't just be permanently applied, it either needs to be timed or changed entirely.
There's a difference between ""my character moves slowly until I get medical help that may not be available"" versus ""the game is causing me headaches until I get medical help that may not be available"".",Bitter Frustration
"Maybe make the vignette permanent, just like how you get permanent eye damage in real life, and the vignette temporary (less than 30 seconds) right after you sustain eye damage, to make it extra clear to the player that what they just did is really bad for their eyes.",Irony
"> the eye damage doesn't actually disadvantage you mechanically. It could instead be a vignette, a shadow around the screen, or a dim transparent layer on top of it. This serves no purpose except looking like ass.
must be pretty effective if it causes such strong reactions
> The blur causes bad eyestrain
empathy with your character
>""the game is causing me headaches until I get medical help that may not be available"".
just wear glasses or eat carrots",Mocking
"Please dont comment on github discussions if you're only gonna bring up bad-faith arguments that dont mean anything.
The effectiveness of it doesnt actually hinder my character in any way, it juet hurts my eyes.
If i want to feel empathy with my character, then how about you shoot me in the head when i die in game. Sometimes those additional materials for preventing it aren't available, especially when you're not playing upstream or at the beginning of a round. I dont know why that means i should get eye strain irl.",Bitter Frustration
">bad-faith
ironically is itself a buzzword to dismiss other arguments and excuse yourself from having to address them
> The effectiveness of it doesnt actually hinder my character in any way
you called it 'agonizing' but you're not hindered at all, okay",Bitter Frustration
"Theres a difference between a mechanical hinderance to my character and actually causing me discomfort while playing. A flashbang makes my character unable to move and restricts my vision of the screen.
A permanent blur until healed doesnt affect me mechanically, but rather just causes me irl discomfort. I don't know how that's hard for you to understand
> okay
Lose the attitude.",Bitter Frustration
Convo isn't productive atm.,Impatience
"## Description
<!-- Explain your issue in detail. Issues without proper explanation are liable to be closed by maintainers. -->
the current filter applies a permanent blur to your screen, which is unbelievably ugly.
It also seems to be permanent unless healed with specific chems, which makes it agonizing if there's no medical.
Welding already blinds you when it's active. This just seems like an unnecessary kick in the balls that makes the game look like ass.
**Screenshots**
<!-- If applicable, add screenshots to help explain your problem. -->
![image](https://user-images.githubusercontent.com/98561806/185726668-fab55880-be1a-424f-b745-e92cdb0de863.png)",Bitter Frustration
"> it has a behaviour that no user-defined schema has (i.e omitting the name of the schema default to 'public' , i.e when you do CREATE TABLE foobar you're actually doing CREATE TABLE public.foobar
It's only because it's in the user's/session's [search_path](https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH).",
"> It's only because it's in the user's/session's [search_path](https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH). thanks for pointing that out, I was looking if it was overridable and it was right under my nose.
However my other point still stands and this one is slighltly amended , as in the case of a DBA requesting the `search_path` to be overrided to avoid accidental use of publc , it means even more that public should not be touched at all by the application layer",
"for the records: I'm also affected by the bug for years and I agree with your points (just couldn't resist of being nit-picky, sorry!) :-D",Irony
"> for the records: I'm also affected by the bug for years and I agree with your points (just couldn't resist of being nit-picky, sorry!) :-D
no offense taken , I also perfectly understand the maintainer point of view of being conservative and I think we're all on the same side of trying to take the best decision, so your nitpicking is welcomed as at least if forces me to really find good arguments :)",
"Some other thoughts:
https://www.postgresql.org/docs/current/manage-ag-templatedbs.html
So theoretically a DBA may modify the template to NOT contain `public` schema.
But again, I agree that `public` exists in 99.999% installations and should be treated with exceptions.",
"I'm closing the issue as it doesn't state an objective problem. The opinion of ""public should not be considered a ""application schema"" is subjective. The statement ""everybody assume that it is present"" doesn't have a proof.
All issues mentioned in the description are closed except for the one which is in a different repository.
If you believe that the current behavior needs to change, please file a new issue and state a problem, not an opinion.",
"i feel a bit disappointed , the stated issues are clear not a opinion, the ""opinion"" is a consequence of these issues, not the other way around,
> it doesn't state an objective problem. facts:
* right now dbal can not handle dropping schema * right now dbal always report it will create the public schema (causing the ""issue in an other repo"" it's because it's the only repo using this feature in a way that is tangible for the end user , it does not take a long time to see doctrine migration is just doing a very thin layer on top of this, so there's no way to fix it except here) making people needing to edit their down migration to remove it when you tried to consider public as a normal schema while adding the possibly for dbal to report SQL to drop schema , you had issues as a consequence of that i see that the way to solve this is to consider public as special schema I'm not some kind of militan that think public should be considered special out of the blue and here find a way to spread my propaganda , i'm just a pragmatic people who try to find solution to problem so please first discard the facts before focusing on the proposed solution",Bitter Frustration
"> All issues mentioned in the description are closed except for the one which is in a different repository.
the other issues have been closed by you these last days , so yes it's a self-fulling prophecy ...",Bitter Frustration
"> If you believe that the current behavior needs to change, please file a new issue and state a problem
https://github.com/doctrine/dbal/issues/1110 <--- it already was explained 7 years ago, the problem hasn't changed since then.",Bitter Frustration
"> ""everybody assume that it is present"" doesn't have a proof
1. postgresql default value for search_path is `public` , so if you drop public `select * from articles` will break 2. postgresql documentation states that ` By default such tables (and other objects) are automatically put into a schema named â€œpublicâ€. Every new database contains such a schema.` https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PUBLIC so it read as .... well ""it is present in every new database"" , i don't know where's the subjective interpertration you can
3. doctrine migrations breaks if you drop the public schema because it is the default one it uses I'm really sorry but what kind of proof more than that do you need , if I rephrase it as ""major projects, including the database itself use it as a default value because except if you're doing very specific stuff, it will be there "" would a more objective statement ? if not how do you describe my points ? also what's really frustrating is that these points are from my above comments, I really took the time to gather them, and you decide to discard them",Bitter Frustration
"@allan-simon in the bugs you mentioned, is there one that doesn't relate to migrations? Any thoughts about https://github.com/doctrine/dbal/issues/1110#issuecomment-1091845105 ?",
"@greg0ire it's not linked in my issue here, but yes https://github.com/doctrine/dbal/issues/5596 is also affected by this issue (doctrine:schema:drop , if you try to enable the reporting of drop schem statement in dbal which is for now conveniently disabled ) so no I don't think it make sense to fix it in higher level library , as stated above, doctrine migration on this part is actually simply a think layer on top of the method provided by dbal",
"Indeed, I've taken a look, and although it could be fixed in the ORM by tweaking `SchemaTool::getSchemaFromMetadata()`, that would make ORM schema creation attempt to create the `public` PG schema.
I see that DBAL has a `schemaAssetsFilter` configuration node, which I could not find documentation on but seems to be about making the DBAL ignore assets it should not manage. This leads me to formulating the following problem:
While it is possible to filter out assets in a schema, it's not possible to filter out schemas in a database.
A solution could be to add another configuration node just for this, and let the user configure it. That way we don't need to make any assumptions about what the user thinks about `public`. Assumptions should be made in Symfony recipes. That assumption should be made [here](https://github.com/symfony/recipes/blob/85dc7cd2f0049f9a86a6d020691ea217561f33e5/doctrine/doctrine-bundle/2.4/config/packages/doctrine.yaml#L2-L7), in the form of a commented out configuration node.",
"could be a way yes, at the end of the day I just want the problems quoted to be fixed my ""opinion"" was just IMHO the most sensible solution but if yours makes everyone happy , yes let's go with it",
"if we just can verify it works first ( because I think to have already come accross it, i don't think it worked)",Bitter Frustration
"After taking a deeper look, I'm afraid my proposal would break `getCurrentSchema()`: https://github.com/doctrine/dbal/blob/a5a58773109c0abb13e658c8ccd92aeec8d07f9e/src/Schema/PostgreSQLSchemaManager.php#L165-L209
Also, I'm not sure at what point in the schema comparison the method you're changing in #5600 is called. Maybe we should introduce another method that would return the filtered list of schemas when doing schema comparison, I don't know. I personally don't have to deal with the ORM or migrations so I'm less interested than you are in getting this fixed.",Impatience
"I think having two separate method with different behaviour would introduce more bugs and will complexify things, if you consider that ""listSchemaNames"" returns all the user-created schema [0], then it make sense to have it (for the above reaons) to ignore `public` as it's not user created , and the very few who have all: 1. a DBA who deleted the public schema before handling it to developers
2. a DBA who allows application code have a postgres user with CREATE SCHEMA rights
3. a DBA who allows also the `public` schema to be recreated 4. a DBA that doesn't complain that the application code who created the schema they didn't want in the first place, does not clean it on down migration should be a minority (sorry no hard statitics here [1], the same as I don't have a hard statistics on how many people eat soup with a fork but at some point it feel like pointless to argue...) to the point I feel like these people (if they exists) will not complain that now they are the one that need to manually add the `public` schema in the list of returned schema by dbal (or if in the case of indirect use through doctrine migration the fact to have to manually add it in their very first migration , while right now people need to remove it in every single migration) [0] I know the documentation says "" * Returns a list of the names of all schemata in the current database."" but it's already wrong as it filter some schema ... [1] on AWS , Azure , GCP , on default ubuntu, debian , fedora , windows , Mac installer , the postgres you got has a public schema, so you really must have a DBA that trick the install to remove it before handling it ot , what if we make a poll at the next php conference ?",Bitter Frustration
"> it filter some schema
I think here you're referring to only one schema called `information_schema`. According to the official docs, it:
- [is defined in the SQL standard](https://www.postgresql.org/docs/9.6/information-schema.html)
- [is not in the schema search path](https://www.postgresql.org/docs/9.6/infoschema-schema.html)
> should be a minority
It seems like you are saying that the only thing we could break is `doctrine/orm` and `doctrine/migrations`. There is a lot more code that could break if we changed that method.
> it feel like pointless to argue
We can stop the discussion if you want, that's not an issue to me.
Anyway, as you can see, I have a proposal, you have a proposal, and there might be more solutions, but I might be wrong about my solution, because I don't know schema comparison that well. For instance, it's unclear where `listSchemaNames` kicks in.
Likewise, you might be wrong.
Also, the problem you're willing to solve is not worded clearly, I believe this is why this issue was closed. Shouldn't there be somewhere a sentence like: when I run a comparison between this schema and that schema, the diff should be empty, but isn't?
I think instead of jumping to solutions, you should do what @morozov asked and file a new issue with a clearly stated problem, expressed with the DBAL APIs (meaning high-level APIs). A test case would be best.
BTW, I'm talking about schema comparison, but it's unclear if the issue you are trying to fix is with schema comparison, with `Schema->toSql()` (used in `SchemaTool::getCreateSchemaSql()`, with `Schema->toDropSql()`, or a combination of those.",
"> We can stop the discussion if you want, that's not an issue to me.
I'm arguying with @morozov handling of it , not you, we can continue to discuss, you're not ignoring my arguments or only handpicking the one that fit you.",Bitter Frustration
"> Also, the problem you're willing to solve is not worded clearly,
it is, it's the summary of dozen of specific issues , which all boils down to the same underlying issue of public being returned , so I was thinking that creating a summary issue, stating all the angle of the problems would be enough (it's not like i throwed a title-only issue without takign the time to git bissect accross 10 years of commit ) > you should do what @morozov asked and file a new issue with a clearly stated problem
I'm sorry but https://github.com/doctrine/dbal/issues/1110 was worded clearly and very specific but it was closed without any argument
so hence my frustration, I have a guy in front of me that contradicts himself: * ask for a clear problem BUT close issue where the issues is stated clearly ( so what do i do, i reopen the same one, take hours of my life, for something that will get the same fate ? )
* ask for a PR , BUT when provided (https://github.com/doctrine/dbal/pull/5600) is ignoring it (the PR itself re-explain everything and give the clear exemples he is asking for * ask for the root cause to be fixed https://github.com/doctrine/dbal/pull/5600#discussion_r950340413 BUT when I create this issue and the PR and, he closes it ask for specific symptoms ...",Bitter Frustration
"> I think here you're referring to only one schema called information_schema. According to the official docs, it:
no `pg_` are also filtered > It seems like you are saying that the only thing we could break is doctrine/orm and doctrine/migrations. There is a lot more code that could break if we changed that method.
I'm not saying that , I'm saying the minority are people that have both a DBA who deleted the public schema but at the same time would allow people to recreate it",Bitter Frustration
"> which all boils down to the same underlying issue of public being returned Again, let's not jump to solutions, we don't know that it is the correct one. Not returning `public` might fix your issues, it might also result in other issues.
> I'm sorry but https://github.com/doctrine/dbal/issues/1110 was worded clearly and very specific
It's quite clear, yet it's not easy to reproduce with just the DBAL APIs. I would expect something like
```
$schema1 = â€¦
$schema2->getMigrateFromSql($schema1)
```
From reading the issue, you can't tell if there is a bug in the DBAL, or if `doctrine/migrations` is misusing it.
> no pg_ are also filtered
Not sure how I missed that, that's indeed the case. The docs say this:
> Schema names beginning with pg_ are reserved for system purposes and cannot be created by users.
For public, you'll have to admit that it's different: it's kind of common for people to create table and indexes inside `public`, while they would never do that inside `information_schema` or schemas starting with `pg_`, right?
> I'm saying the minority are people that have both a DBA who deleted the public schema but at the same time would allow people to recreate it
There could also be other packages (different from ORM or migrations) relying on that method to display schema names, for purposes we don't know. The DBAL has 4k dependends: https://packagist.org/packages/doctrine/dbal/dependents?order_by=downloads
Hard to tell who is using this API for what. Hence my proposal of having this filtering be opt-in, and affect a new API instead of an existing one.",
"I'm locking this because it's starting to consume my energy as well, but anyone feel free to open another issue about this. If you do, make sure that you state a clear problem , preferably reproducible with the DBAL APIs that are used directly by `doctrine/orm` (specifically by `SchemaTool`) and by `doctrine/migrations`. And if you do, please leave your emotions at the door, or remarks about the issue being 10 years old. Avoid mentioning time of your life you lost with this when talking to maintainers that spend a lot more time working on Doctrine.",Bitter Frustration
"### Bug Report
For now 10 years `public` has been returned by `getSchemaNames` generating a lot of bug and weird behaviour in higher level libraries relying on it.
| Q | A
|------------ | ------
| Version | 3.x
#### Summary
For now 10 years since commit https://github.com/doctrine/dbal/commit/b89490a557584b61d575aaa67f8b386d833d5a0f (before that it was not working at all )
`public` is returned by PostgreSQLSchemaManager::getSchemaNames()
#### Current behaviour
<!-- What is the current (buggy) behaviour? -->
#### How to reproduce
(a reproducer is soon coming)
1. connect to any postgresql (a vanilla postgres docker is fine)
2. call PostgreSQLSchemaManager::getSchemaNames
-> public is returned among other user-defined schema
#### Expected behaviour
all application schema are returned , but not `public` (as well as other postgres-defined schema are also not returned
#### More context This bug is at least the root cause of these issues * https://github.com/doctrine/migrations/issues/441
* https://github.com/doctrine/migrations/issues/1196#issuecomment-1029706768
* https://github.com/doctrine/dbal/issues/1110
* https://github.com/doctrine/dbal/issues/1188#issuecomment-231751027
* https://github.com/doctrine/orm/issues/4777
* https://github.com/symfony/symfony/issues/44952
the more visible issue of it is with doctrine migration: 1. all migrations contains a `down` migration trying to `CREATE SCHEMA public` that EVERYBODY deletes or workaround ( :100: if you
have done that too ) because it fails miserably (as it already exists)
2. to avoid dropping it , we don't drop schemas *at all* cf https://github.com/doctrine/dbal/pull/5604/files , so if automated generated migrations are not bijectives because `DROP SCHEMA` will never be generated. #### Why `public` should be considered a ""system schema"" 1. public is not created by the user
2. everybody assume that it is present
3. even postgres assumes it is present 4. it has a behaviour that no user-defined schema has (i.e omitting the name of the schema default to 'public' , i.e when you do `CREATE TABLE foobar` you're actually doing `CREATE TABLE public.foobar` 5. if a ill-advised user drop it, a lot of libraries get broken ( doctrine migration by default create is own internal table in the public namespace , so as this SQL runs *BEFORE* the first migration, you can't even create back `public` in the first migration as it will fail before)
6. weak argument: after 10 years of managing postgresql I never ever seen people dropping and even less (re)creating the public schema, the only case I could hypothetically see for it is a extremly ordered DBA that have severa applications on the same instance , and want each of them in its own schema to ease maintenance (but in that case it's the ""infrastrcture"" responsability to drop it once and for all, not one of the applications )
@morozov I understand the issue is ""scary"" because this library is a high profile one and after a time even bugs become part of the API , so I understand we may want to be careful on that one, so maybe we can find a deprecation strategy and changing this behaviour only on a minor/major release (at least not just a bug fix one)",Bitter Frustration
The exception is unrelated but wide searchbar indeed does not work.,Bitter Frustration
"I am having the same issue when buidling with v2.41.0.
Downgraded to my build with v2.39.1 and everything works fine there.",Impatience
"> The exception is unrelated
yep, i missed, sorry.
But as i see there is no log messages in wide searchbar patch
> Downgraded to my build with v2.39.1 and everything works fine there.
Not working for me",Impatience
"> Not working for me
Try Version v17.32.38(Stable) with Patches 2.39.1.",Impatience
"> Try Version v17.32.38(Stable) with Patches 2.39.1.
Hm, same, not working.",Impatience
"I have this issue also.
YT v17.32.35
Patch 2.42.0",
Fixed.,
"### Type
Cosmetic
### Bug description
wide searchbar can't be enabled.
### Steps to reproduce
1. Apply relevant patch
2. Enable in settings
3. Reopen app
4. Searchbar isn't wide ### Relevant log output
```shell
?
```
### Screenshots or videos
_No response_
### Solution
Maybe [this](https://github.com/revanced/revanced-patches/blob/v2.41.0/src/main/kotlin/app/revanced/patches/youtube/layout/widesearchbar/patch/WideSearchbarPatch.kt#L42) needs to be true by default
### Additional context
cli v2.93
patches v2.41.0
youtube v17.32.35",
what is the url ?,Impatience
"> what is the url ?
https://app.kittl.com/",
"this is not an issue and is quite useless
should be?",Bitter Frustration
"USELESS? Really?
This is one of the most desired features.
Text on path was just a kick off. Text warping should be the main goal.",Bitter Frustration
"your description is useless
so little effort in writing a decent ticket
> Text warping should be the main goal.
Should be? This is open source.
Get to work and PR.",Bitter Frustration
"https://github.com/fabricjs/fabric.js/blob/master/CONTRIBUTING.md
I will back you if you invest true grit",
"You have violated the code of conduct
Refrain of doing so in the future",
"@EdnaldoNeimeg if you think a feature is needed or missing present it in the correct way. Don't assume we have time to interpret every new issue and understand if it is a feature request, a thank you post, a bug, or a start of a discussion.
Said so useless was probably refered to the issue and not the feature, since it has no indication of how the feature work, only a screenshot of a generic ui, and the use of should that seems to imply the feature is expected.
We are people not answering bots and we need a bit more of polite interaction to work best.",Bitter Frustration
"As you can see, the image shows a very fancy way to distort/transform text, there should be a way to do something like that.
(They are using Fabric 5.2.1)
![image](https://user-images.githubusercontent.com/6233059/186951321-c65775dc-3330-4292-9cbc-7b916f489623.png)",
No one really gives a fuck about you ðŸ˜”,Bitter Frustration
"Already answered in many places such as here: https://github.com/composer/composer/issues/10740#issuecomment-1100935990
You may consider closing your issue.",Impatience
"> No one really gives a fuck about you ðŸ˜”
i know that, but that is not i'm talking about !.",Bitter Frustration
"> ![Screenshot_20220828_221718](https://user-images.githubusercontent.com/59410395/187093049-9219c2a9-08c2-4681-909c-9c76481acdd5.png)
> > i don't really give a single fuck about Ukraine, i don't want it to be in my terminal any more please remove it, Thanks !
i mean why it's not *Libya* or *Iraq* or even *Palestine* !!!",Bitter Frustration
"It would be nice to add Palestine as an occasional (even 10% of the time?) message.
It will go to show it isn't ""no politics unless it involves US/Europe"".",Bitter Frustration
"![Screenshot_20220828_221718](https://user-images.githubusercontent.com/59410395/187093049-9219c2a9-08c2-4681-909c-9c76481acdd5.png)
i don't really give a single fuck about Ukraine, i don't want it to be in my terminal any more please remove it, Thanks !",Bitter Frustration
"<h3>Hey! contributor, thank you for opening an Issue ðŸŽ‰.</h3><p>Soon one of our maintainers will review it and provide you with feedback/suggestions. If you think it's something urgent, feel free to reach out <a href='https://twitter.com/home'>Pradumna Saraf</a> on <b>Twitter</b>. Star â­ this repo to show us support.</b><br><br><b>Happy, Open Source!</b></p>",
"@Pradumnasaraf please explain me what I have to do in this issue ?
1)Do I have to write the steps on how to create a repo for a project in GitHub? OR
2)How to create a project for organization in GitHub or how to create a project for yourself?
what to do between the two?",Impatience
[https://docs.github.com/en/issues/organizing-your-work-with-project-boards/managing-project-boards/creating-a-project-board](url),
Anyone is working on this issue?,Impatience
"Hey, @kelvinparmar no one is working on this. You have added steps to create a project board for a repo/project. Linke [this)(https://github.com/Pradumnasaraf/open-source-with-pradumna/projects/1)",Impatience
"hey! , I would be happy to help.
Please assign me this issue under hacktoberfest.",
Hey @Pradumnasaraf Can I work on this issue for Hacktoberfest ?,
"> Hey @Pradumnasaraf Can I work on this issue for Hacktoberfest ?
yes go ahead @Vishrut19",
@Pradumnasaraf I asked to work on this issue before,Impatience
"Hey, @Vishrut19, by mistake I assigned the issue to you, sorry for the inconvenience. You can look for other issues.
@ampsteric assigning it to you.",
Hi @Pradumnasaraf sorry I am busy with my college life . you have assigned this issue to anyone?,Impatience
"> Hi @Pradumnasaraf sorry I am busy with my college life . you have assigned this issue to anyone?
You already made the PR, there we no response from your side. Okay, we will accept that. Hey @ampsteric sorry for inconvenience, please looks for some other issues",Impatience
"### Description
Hey! Contributor,
This issue is for adding the guide/process for **How to create to create GitHub project board**. The step needs to be well defined and include the Screenshot/Screen recording for better explanation. To get an overview of how the docs should look, check the available docs [`here`](https://github.com/Pradumnasaraf/open-source-with-pradumna/blob/main/pages/How-to/guide)
- Path to the dedicate Markdown for this issue - [`open-source-with-pradumna/pages/How-to/guide/create-project-board.md`](https://github.com/Pradumnasaraf/open-source-with-pradumna/blob/main/pages/How-to/guide/create-project-board.md)
---
If you have any suggestions feel free to Open an [Issue](https://github.com/Pradumnasaraf/open-source-with-pradumna/issues)
**Also if you need any kind of help, feel free to ping!**",
For the record: I don't think that raising and catching an exception is a problem that deserves a fix.,Bitter Frustration
"> For the record: I don't think that raising and catching an exception is a problem that deserves a fix.
Can you suggest any workaround for debugger stopping on internal ORM exceptions during debugging as discussed in the link above?",
"...and don't throw exceptions when it's not found
Discussed here: https://github.com/doctrine/orm/discussions/10019#discussioncomment-3528300",Impatience
"I donâ€™t think we should merge this. Itâ€™s a problem related to how you use the debugger, not the code itself.
for sure thereâ€™s a way how to configure your debugger to ignore this exception, not stop for it or similar.",Bitter Frustration
Let's close this then.,Impatience
"Typical PHP-world hair-brained ""solution""",Mocking
"I guess, we PHP guys are just too stupid. Better move to go. ðŸ¤·ðŸ»â€â™‚ï¸",Mocking
I asked for something similar in #9243 as well.,Impatience
these look similar. Since multiple people are asking for similar functionality maybe it will get some traction.,
Can you add an example of custom field that would warrant such functionality ?,Impatience
"![device](https://user-images.githubusercontent.com/77940332/191647405-5b255fe0-cab4-4aa0-94a0-0ec7f8565789.PNG)
![template](https://user-images.githubusercontent.com/77940332/191647406-5e966f2a-cd89-4390-81f2-6791870eea07.PNG)
So let me describe my use case in more detail.
When you create a device template you can add 'power ports' to the template. When you add these power ports you are able to specify the maximum draw and allocated draw. Let's use easy numbers and say max draw is 100 watts, and allocated draw is 25 watts.
Now, when you create a new device from the device template that you just created the power ports are automatically added. And the max draw and allocated draw are already populated for you. Which is great! Now of course you can change these numbers per device if you so choose, but typically its unnecessary.
Now say we want to have a custom field on the power port. Currently, the custom field will only show up on the power port page when viewing a device. Not the device template. So if I wanted a custom field called 'BTU' that tracks the heat output of our power supplies I have to first create the device, then go to the power ports and fill in the BTU load for each device manually.
If the custom field 'BTU' were available on the device template, I could make the calculation and input the value just like I do for max draw and allocated draw. This way, when I create a new device from that device template, the BTU value is already filled out for me. Sure, I can change it if I need to (just like max draw), but the point is, the value is already filled in since its coming over from the device template.",
My last comment was deleted. So I'm not sure if I have satisfied the requirements of 'revisions needed',Bitter Frustration
"Your previous comment was directly mentioning circumventing auto closure of the issue, which is explicitly against the rules.
In my opinion you still haven't explained how it should work in practice. Is it just matched by name the name of the field? What happens if customer field types do not match? In your FR you mention that it should be added to device and module types, but when you explain further you are referring component templates, is it both?
The proposal is still not very thorough in my opinion.",Impatience
"What are you talking about?
custom fields are not going to be different field types between a device and a device template. It's the same field. You just are able to select the device template page inside the model(s) selection
![image](https://user-images.githubusercontent.com/77940332/193854091-20828e52-447c-4948-a373-41687e108550.png)
Currently, if you apply a custom field to the ""DCIM > power port"" model it will only show up on the device power port page after a device is created. You should also be able to apply this same custom field to the 'power port' of a device template.",
"> What are you talking about?
This is no way to talk. Locking this until we can figure out a way forward with regards to your agressive and hostile comments.
> custom fields are not going to be different field types between a device and a device template
There's no such thing as a device template. There's device types and devices, both can have independant custom fields. Then there are device components and device component templates, where only the first ones can have custom fields.",Bitter Frustration
Closing this as #9243 provides a much better description of the proposed feature.,
"### NetBox version
v3.3.2
### Feature type
New functionality
### Proposed functionality
When creating custom fields, they only appear when working inside a device or module, not the device type or module type.
Please add the ability to have custom fields be part of the device type and module type areas of netbox.
Based on : https://github.com/netbox-community/netbox/discussions/10293
### Use case
Our use case would be to add a custom field to a power port. Then when that power port is added to any device the custom field is already populated.
Similar to how maximum draw and allocated draw would already be present when adding a power port to a type.
Currently, you would have to create the device, go to the power port for the device AFTER its created, click edit, and populate the custom field every time for every device.
### Database changes
I'm unsure what kind of database requirements would be needed to add the ability for custom fields to show up on device type and module type pages.
### External dependencies
None that I can see.",
Dupe of #1425?,Impatience
Updating from 1.18.1 to 1.18.10 worked for me,
despite different error shown at screen it is same problem of #1425,Bitter Frustration
"Thanks guys!
I saw https://github.com/CastagnaIT/plugin.video.netflix/issues/1425 yesterday before opening this one but I didn't realise that it could be the same.
Do you want me to put these info in the https://github.com/CastagnaIT/plugin.video.netflix/issues/1425 or will you merge them together?",
"we keep this one open as well, having a different error name will redirect users to the right thread #1425,
preventing someone else from opening another issue like this again
I am already working to fix addon for Kodi 19+
Kodi 18 will be taken in consideration only when addon for Kodi 19+ will be stable",
"### Netflix add-on version
1.12.14
### Operative systems used
LibreELEC
### Kodi version used
Kodi 18 (Leia)
### Description of the bug
When the plugin is open and it should show the list of media content, it returns the error:
`404 Client Error: Not Found for url: https://www.netflix.com/api/shakti/vedfdb694/pathEvaluator?materialize=false&original_path=%2Fshakti%2Fvedfdb694%2FpathEvaluator&categoryCraversEnabled=false&isVolatileBillboardsEnabled=true&isTop10Supported=true&routeAPIRequestsThroughFTL=false&withSize=false&drmSystem=widevine`
Note: Authentication key is used as authentication method. The plugin worked fine yesterday but today got this error. Rebooting the system nor re authenticating did not solve the problem.
### Steps to reproduce the behavior
1. Go to Add-on and open Netflix addon
2. The account main menu is shown, with the account name and the ""Children"" menu items.
3. Click on the account name to access the content -> The ""404 Client Error: Not Found for url"" is returned
### Debug log - mandatory
https://paste.kodi.tv/bacicebiko.kodi
### Possible fix
_No response_
### Additional context
_No response_
### Screenshots
_No response_",Bitter Frustration
"The problem is mostly the `fixedmiddlewares`, even when disabled the annotation is still there, which might conflict with official traefik annotations.
So the easiest solution is to ensure the annotaiton is hidden when no middlewares are there **AND** cleanly document that the `middlewares` section in values.yaml is for use with our Traefik Chart only",
"I'm having the same problem
Trying to create DrawIO deployment, adding Traefik ingress doesn't work
This is my values file:
```yaml
image:
repository: tccr.io/truecharts/drawio
tag: 20.4.0@sha256:97e11d8cfe67a64e2630225183db9adad3e95c2708c4191a50e72357f37bb95f
pullPolicy: IfNotPresent
env:
TZ: UTC
DRAWIO_BASE_URL: drawio.xxx
DRAWIO_GITLAB_ID: 112233
DRAWIO_GITLAB_SECRET: xxxxxxxxxxxxx
DRAWIO_GITLAB_URL: https://gitlab.com/aaa/bbb
securityContext:
readOnlyRootFilesystem: false
runAsNonRoot: false
podSecurityContext:
runAsUser: 0
runAsGroup: 0
service:
main:
ports:
main:
port: 8080
targetPort: 8080
portal:
enabled: true
ingress:
# -- Enable and configure ingress settings for the chart under this key.
# @default -- See values.yaml
main:
enabled: true
annotations:
kubernetes.io/ingress.class: traefik
hosts:
- host: zzz.example.com
paths:
- path: /
```
All the values are masked of course
Deploy command:
```bash
$ helm upgrade --install --debug drawio TrueCharts/drawio -f drawio.yaml
```",
"> I'm having the same problem Trying to create DrawIO deployment, adding Traefik ingress doesn't work
> > This is my values file:
> > ```yaml
> image:
> repository: tccr.io/truecharts/drawio
> tag: 20.4.0@sha256:97e11d8cfe67a64e2630225183db9adad3e95c2708c4191a50e72357f37bb95f
> pullPolicy: IfNotPresent
> > env:
> TZ: UTC
> DRAWIO_BASE_URL: drawio.xxx
> DRAWIO_GITLAB_ID: 112233
> DRAWIO_GITLAB_SECRET: xxxxxxxxxxxxx
> DRAWIO_GITLAB_URL: https://gitlab.com/aaa/bbb
> > securityContext:
> readOnlyRootFilesystem: false
> runAsNonRoot: false
> > podSecurityContext:
> runAsUser: 0
> runAsGroup: 0
> > service:
> main:
> ports:
> main:
> port: 8080
> targetPort: 8080
> > portal:
> enabled: true
> > ingress:
> # -- Enable and configure ingress settings for the chart under this key.
> # @default -- See values.yaml
> main:
> enabled: true
> annotations:
> kubernetes.io/ingress.class: traefik
> hosts:
> - host: zzz.example.com
> paths:
> - path: /
> ```
> > All the values are masked of course Deploy command:
> > ```shell
> $ helm upgrade --install --debug drawio TrueCharts/drawio -f drawio.yaml
> ```
Hijacking random issues to beg for help is not-done.
Just like anyone else: ask for help on the Discord.",Insulting
"We've currently dropped support for native helm and removed the helm repo.
Even if we bring back the helm repo, it would be ""use as-is, on your own risk"" and we won't be taking feature requests or helm-specific bug-reports for it.",Bitter Frustration
"### App Name
traefik/common
### SCALE Version
Not using SCALE
### App Version
all
### Application Events
```Shell
not relevant
```
### Application Logs
```Shell
not relevant
```
### Application Configuration
not relevant
### Describe the bug
When running an App on normal helm, with official traefik, our customised middleware solution is causing issues for people. We never anticipated people doing this, but this should be accounted for
### To Reproduce
Run with official traefik, notice the middlewares obviously don't line-up
### Expected Behavior
Official Traefik should be at least somewhat useable as ingress with our charts
### Screenshots
not relevant
### Additional Context
Reported by a user on the k8s-at-home discord
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",Bitter Frustration
"despite different error shown at screen its the same of issue #1425 i keep this opened only to redirect new users to right issue #1425
before i need to be sure that works without problems on Kodi 19+
then after that can be possible think to backport the fix to Kodi 18",
"### Netflix add-on version
CastagnaIT Repo: 2.0 (Netflix: libdev + jojo+ asciidisco + caphm + castagnait - 1.12.14)
### Operative systems used
LibreELEC
### Kodi version used
Kodi 18 (Leia)
### Description of the bug
LibreELEC: 9.2.8
Kodi: 18.9
Looks like a 404 error that the Netflix plug-in is attempting to call.
### Steps to reproduce the behavior
Using default skin in Kodi.
1. In Kodi, navigate to Add-ons menu item
2. Under Video add-ons, select/click the Netflix icon
3. (Signed into the account previously) select any profile and select OK/click
4. Netflix fails with error
### Debug log - mandatory
https://paste.kodi.tv/usufesicop.kodi
### Possible fix
Could be a simple manual code update to repoint the 404 URL?
### Additional context
_No response_
### Screenshots
![image](https://user-images.githubusercontent.com/114855552/193447704-1a590110-2a58-409d-8307-d738be537af6.png)
![image](https://user-images.githubusercontent.com/114855552/193447709-60e036c8-c447-40ae-8266-314f76bc407b.png)",
Thanks for letting us know. It is related to https://github.com/anuraghazra/github-readme-stats/issues/1471. We are aware of the issue.,
"> Thanks for letting us know. It is related to #1471. We are aware of the issue.
Thanks for the reply @rickstaa. once it is fixed please let us know.",
Also commits are showing lesser than the prev values...,Bitter Frustration
This should be fixed.,Impatience
"Hi Rick, just letting you know I'm experiencing this issue, before and after this comment. ðŸ™‚ > This should be fixed.",Impatience
Same!,
"> Hi Rick, just letting you know I'm experiencing this issue, before and after this comment. ðŸ™‚
> > > This should be fixed.
Same, as of 12:55 PM EDT.",Impatience
Same here!,
yup,
Your right. I think @anuraghazra still needs to update the PATs. I will let him close this issue when the issue is fixed.,
"I have get the same issue !
Maximum retries exceeded !",Bitter Frustration
We are working on it progress can be followed in #1471. Will lock this issue for now.,
Should be fixed for now.,
"if I use `layout=compact` or `theme=dark` parameters, the message returns",Impatience
Same problem with `theme=dark` parameter.,Impatience
"> Same problem with `theme=dark` parameter.
I think that problem is mainly caused by the fact that we cache the cards to reduce server load. You can break this behavior by adding a random query parameter `&random=&randomss524272`. In the future, I will probably create a PR to change this behavior for the case that the cards fails to fetch data.",
"> > Same problem with `theme=dark` parameter.
> > I think that problem is mainly caused by the fact that we cache the cards to reduce server load. You can break this behavior by adding a random query parameter `&random=&randomss524272`. In the future, I will probably create a PR to change this behavior for the case that the cards fails to fetch data.
That worked, but I notably had to add `&random=&randomss524272` before `&theme=dark`. Thanks!",
"<!--PLEASE FIRST READ THE FAQ (#1770) AND COMMON ERROR CODES (#1772)!!!-->
**Describe the bug**
Iam trying to create a github readme statistics, but its throwing a error as **""Something went wrong!""**
**Expected behaviour**
It should be properly showing my github statistics without any issues
**Screenshots / Live demo link (paste the github-readme-stats link as markdown image)**
<img width=""860"" alt=""Screenshot 2022-10-04 at 6 09 08 PM"" src=""https://user-images.githubusercontent.com/58930932/193821598-8d5253b7-12ae-4f99-b4f2-9a1dd6bcc133.png"">
**Additional context**
Link used to show the statistics. [Click here](https://github-readme-stats.vercel.app/api?username=sriramgroot&show_icons=true&hide_border=true&bg_color=040d21&title_color=165df5&icon_color=165df5&text_color=FFFFFF)",
"This is same problem of #1425,
read my comment here: https://github.com/CastagnaIT/plugin.video.netflix/issues/1425#issuecomment-1266535143",Impatience
"### Netflix add-on version
plugin.video.netflix v1.12.14 ### Operative systems used
LibreELEC
### Kodi version used
Kodi 18 (Leia)
### Description of the bug
Netflix starts, logs in
after clicking on the addon the profile list is shown as before but:
Clicking on Owner profile the error immadiately occurs
ERROR: 404 Client Error: Not Found for url: https://www.netflix.com/api/shakti/vbb9fbeef/pathEvaluator?m
### Steps to reproduce the behavior
Video Addons
Netflix
select the profile
### Debug log - mandatory
https://paste.kodi.tv/xojevusoqu.kodi
### Possible fix
_No response_
### Additional context
_No response_
### Screenshots
_No response_",
@ieahleen i would like to do this. I started open source contribution a few days ago .,
@ieahleen will it count for Hacktoberfest?,Impatience
@ieahleen the project URLs you have provided are not accessible. [Error: DNS address could not be found.],Impatience
"> @ieahleen the project URLs you have provided are not accessible. [Error: DNS address could not be found.]
Yea... Because they haven't deployed it yet ðŸ™‚",Irony
@ieahleen i have changed all mentioned projects URL please check,
"@Anuran12 I know, but Isn't it a good habit to look into that before opening an issue?",Impatience
"I have a good experience with JS & node JS, can I updated the content.",
"There is an PR in the works that is going to fix this. We are locking this thread to avoid too many ""Can I help"" comments. Please read [contributing guidelines](https://contribute.freecodecamp.org) to help us help you.",
"Describe the Issue
All the data viz projects were created on our .rocks domain. We just need to replace the codepen URL's with the new addresses in the curriculum files of those five projects.
Here's the URL's:
https://25--5-clock.freecodecamp.rocks
https://drum-machine.freecodecamp.rocks
https://javascript-calculator.freecodecamp.rocks
https://markdown-previewe.freecodecamp.rocks
https://random-quote-machine.freecodecamp.rocks
And the [project files are here](https://github.com/freeCodeCamp/freeCodeCamp/tree/main/curriculum/challenges/english/03-front-end-development-libraries/front-end-development-libraries-projects)
---
This looks like something that can be fixed by ""first-time"" code contributors to this repository. Here are the files that you should be looking at to work on a fix:
List of files:
https://github.com/freeCodeCamp/freeCodeCamp/tree/main/curriculum/challenges/english/03-front-end-development-libraries/front-end-development-libraries-projects
Please make sure you read our [guidelines for contributing](https://contribute.freecodecamp.org/#/), we prioritize contributors following the instructions in our guides. Join us in our [chat room](https://discord.gg/PRyKn3Vbay) or our [forum](https://forum.freecodecamp.org/c/contributors/3) if you need help contributing; our moderators will guide you through this.
Sometimes we may get more than one pull request. We typically accept the most quality contribution followed by the one that is made first.
Happy contributing.",
"Hello, can you make support for terraria 1.4.4? Since she's already on the phones?",Impatience
Why is it taking too loong to update?,Impatience
"~~Can you please review and merge my PR? #48012~~
Edit: This is not the right way and goes against the contribution policy. Apologies for that!",Bitter Frustration
"As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.
We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.
Some examples of this are:
1. Do not edit files directly through GitHub â€“ while you can, it's not a good idea.
2. Make sure you follow the PR checklist and not just tick things off; otherwise, we won't take you seriously.
3. Use the correct way to link issues in the description of the PR by updating the `XXXXXX`. Do not just add issue numbers everywhere and anywhere you feel like.
4. Keep mentions and review requests to a minimum. We understand you are excited about contributing, and our maintainers will get back to you as soon as they get a chance.
5. Do not work directly off your `main` branch - create a new branch for the changes you are working on.
We appreciate you taking the time to help us, and we hope to see more contributions from you.
Happy Contributing.",Insulting
"If this PR is still available, I'll like to work on it.",
I would want to make a Pull request if this issue still exists.,
"> As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.
> > We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.
> > Some examples of this are:
> > 1. Do not edit files directly through GitHub â€“ while you can, it's not a good idea.
@raisedadead I think you can use github.dev for that.",
"@raisedadead Please accept my apologies. I am new and still learning.
Things which I implemented after learning from your comments are.
- I have updated the PR title to comply with [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/)
- I won't add mentions/review requests unnecessarily
- From now on I will create a separate branch for any changes and not commit directly to the main branch.
~~Point 3 is not clear to me. How would I link the PR to the issue?~~
~~I am trying to follow [this](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue) article.~~
**EDIT:** NVM I have created another PR by following everything.
> As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.
> > We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.
> > Some examples of this are:
> > 1. Do not edit files directly through GitHub â€“ while you can, it's not a good idea.
> 2. Make sure you follow the PR checklist and not just tick things off; otherwise, we won't take you seriously.
> 3. Use the correct way to link issues in the description of the PR by updating the `XXXXXX`. Do not just add issue numbers everywhere and anywhere you feel like.
> 4. Keep mentions and review requests to a minimum. We understand you are excited about contributing, and our maintainers will get back to you as soon as they get a chance.
> 5. Do not work directly off your `main` branch - create a new branch for the changes you are working on.
> > We appreciate you taking the time to help us, and we hope to see more contributions from you.
> > Happy Contributing.",Insulting
"@PiyushKeshari24 thank you for taking care of this.
@DavidRod1865, @soumya0S. The contribution guideline, that raisedadead linked should answer most of your questions.
@sonicx180 there is no mentioning of GitHub Dev environment in the guideline, if you are uncomfortable setting it up locally, our section about Gitpod should do the trick.",
"### Describe the Issue
In step 8 of building a registration form project, there is a text that go against [challenge document](https://contribute.freecodecamp.org/#/how-to-work-on-coding-challenges?id=challenge-descriptionsinstructions)
> To spruce the project up, let us add some CSS.
This should be changed to > To spruce the project up, add some CSS.
Here is the markdown file https://github.com/freeCodeCamp/freeCodeCamp/blob/1459bc6879ce700830e710f30764b4562f2f52d3/curriculum/challenges/english/14-responsive-web-design-22/learn-html-forms-by-building-a-registration-form/60f1922fcbd2410527b3bd89.md#L11
### Affected Page
https://www.freecodecamp.org/learn/2022/responsive-web-design/learn-html-forms-by-building-a-registration-form/step-8
### Additional context
Please make sure you read [our guidelines for contributing](https://contribute.freecodecamp.org/#/), we prioritize contributors following the instructions in our guides. Join us in [our chat room](https://discord.gg/PRyKn3Vbay) or [the forum](https://forum.freecodecamp.org/c/contributors/3) if you need help contributing, our moderators will guide you through this.
Sometimes we may get more than one pull requests. We typically accept the most quality contribution, followed by the one that is made first.
Happy contributing.",
This is piracy and goes against the open source license ...,Bitter Frustration
This doesn't violate the open source license if they don't modify vscode. But it certainly violated trademark rights.,
Extremely interesting,Irony
Need ï¿¥39 to install? Selling free software?,Mocking
How can you find this... Is just typing code.visualstudio.com too difficult for some people?,Mocking
"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?
Use Baidu to search for vscode. Click Open in the first advertisement......",Mocking
hilarlious,Irony
"Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.",
"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?
In some countries, there is Internet interdict like Great Fire Wall in China. So people there cannot reach the servers of Microsoft.",Mocking
"Search from Baidu:
![image](https://user-images.githubusercontent.com/50246090/196424828-5ead4681-c783-442e-bbea-0b8e4c15fcc2.png)
The first one is https://qnw.shaid.top/vscode/index.html:
![image](https://user-images.githubusercontent.com/50246090/196424988-4523e2d7-4860-4631-876f-bfd4a2548359.png)
This website belongs to [Wuhan Hongge Yibai E-commerce Co. (æ­¦æ±‰å®æ ¼äº¿ä½°ç”µå­å•†åŠ¡æœ‰é™å…¬å¸)](http://www.shaid.top/).
If you do some searches on this domain, you can find their other ""products"":
- [Win10 Activation Tool (Win10 æ¿€æ´»å·¥å…·)](https://qnw.shaid.top/win10/index.html)
- Telegram Chinese version (Telegram ä¸­æ–‡ç‰ˆ) (removed)
![image](https://user-images.githubusercontent.com/50246090/196426820-f9fa3030-eb5e-430c-bdfc-026af7100576.png)
- visualcpp (removed)
![image](https://user-images.githubusercontent.com/50246090/196432416-34943cc9-de17-4e6b-a316-ca5a6ff3f5e7.png)
They even claim to be partners with China Mobile, China Unicom and China Telecom:
[![image](https://user-images.githubusercontent.com/50246090/196426325-f39cc8ab-8b66-40b7-8a1e-edc32ad7c08b.png)](http://www.shaid.top/)
Thanks to the Great Firewall, we have such a ""great"" Internet in China.
<hr />
Update:
Some people have found other similar companies:
- [Shangqiu Xuankangtai Network Technology Co. (å•†ä¸˜è½©åº·æ³°ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸)](http://www.sqiua.cn/index.html)
- [Visual studio](http://office.xuank.top/install.php?m=visual) ![image](https://user-images.githubusercontent.com/50246090/196571476-47c2e0e6-5465-4666-8889-ad34a1c4689a.png)
- [Software Superstore (è½¯ä»¶å•†è¶…)](https://xkt.sqiua.cn/) ![image](https://user-images.githubusercontent.com/50246090/196572556-a30b1388-e5f5-43f8-a94a-419e5501c2f9.png)
- [System Home - Windows premium system download site (ç³»ç»Ÿä¹‹å®¶-Windowsç²¾å“ç³»ç»Ÿä¸‹è½½ç«™)](http://windows.sqiua.cn/index.html)
- [visualbasic](http://office.xuank.top/install.php?m=visualbasic)
- [directx Repair Master (directxä¿®å¤å¤§å¸ˆ)](http://office.xuank.top/install.php?m=directx)
- [Chrome](http://chrome.sqiua.cn/) ![image](https://user-images.githubusercontent.com/50246090/196573087-e4bfb9f0-3060-4e72-a6bf-05a35238a151.png)
- [Yixin Cat House (å¿†å¿ƒçŒ«èˆ)](http://yixinmaoshe.sqiua.cn/) ![image](https://user-images.githubusercontent.com/50246090/196572912-95f01175-ad54-40d7-bfe7-2b10aaa767b8.png)
- [AdobeCAD](https://autocad.sqiua.cn/)
- [Map marking service center (åœ°å›¾æ ‡æ³¨æœåŠ¡ä¸­å¿ƒ)](http://mr.sqiua.cn/)
- [Yunnan Norforkang Network Technology Co. (äº‘å—è¯ºç¦åº·ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸)](http://www.cvbty.cn/)
- [Xunjian Mind Map (å¯»ç®€æ€ç»´å¯¼å›¾)](http://sw.xuank.top/)",Bitter Frustration
"In fact, not only vscode, but also tools like dev-c++are used by the company for profit
However, in quite a short time, the company withdrew all advertisements on Baidu
This means that the content of this company is illegal",Bitter Frustration
"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?
Yes, IT IS difficult for some people who just know vscode or just get started learning coding.
What makes this thing more hilarlious is that,
If you search `vscode` or `visual code` on Bing China, you will get a piracy link on the first search result.
![image](https://user-images.githubusercontent.com/8984680/196569964-6d9d19ab-cfb6-4eba-95c9-a792249cc511.png)
![image](https://user-images.githubusercontent.com/8984680/196569990-a81a1105-fbde-4cf0-a667-d0716cde20c6.png)",Mocking
Interesting...,Irony
"Hilarious indeed, thanks to GFW.",Irony
æ¨‚,
å…¸,
"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.
Not really true. The official version of Visual Studio Code contains the trademark and some modifications by Microsoft, and is therefore subjected to [Microsoft's terms](https://code.visualstudio.com/License/). The pirate version definetely goes against the terms and must be taken down.
However though, if they do recompile VSCode removing nonfree bits, and give it a different trademark (like what the libre VSCodium is doing), yes, they do have the right to sell it. But what do you expect from a commercial pirate website?",Mocking
"![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png)
VS Code ""Officially licensed"": https://www.google.com
lol",Mocking
"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol
å…¸ä¸­å…¸",Mocking
"Do you know why there is a GFW in China? cause CCP sucks, these SOBs are afraid of their people knowning or telling the truth, all words published on China's social media/platform are reviewed strictly, anything CCP doesn't like will be banned, and on the other side, they claim that there is something called FREE-SPEECH is existed in China, so F*CK them",Bitter Frustration
The necessary process of localization in China,
å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ,Mocking
å…¸,
"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol
å…¥å…¸ï¼ï¼ï¼
https://meme-libs.github.io/github/2",Mocking
It's hard for me to tell if you are joking...,Irony
"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.
Thanks for the TIP, I am going to sell VS Code at $114514/copy.",Mocking
mark LOL nice joke during work,Mocking
"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.
but theyâ€˜re declaring ""officially licensed"", it does violate the MIT License.",Irony
ã€Š é™ æ—¶ æ´» åŠ¨ ä»· ã€‹ã€Š 39.7 ã€‹,
ä¼šä¸Šgithubå‘issueä¸ä¼šgoogleä¸€ä¸‹å®˜ç½‘æ˜¯å§,Mocking
extremly hilariousðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£,Mocking
éš¾å´©ï¼Œç¨‹åºå‘˜ç‰ˆsteamç®¡å®¶æ˜¯å§,Mocking
ï¼ˆvscå¾®è½¯ç®¡å®¶,
ç¬‘æ­»233,Mocking
"In China, it is normal
![GZB}9_~W30KO1P_QO0IIBV0](https://user-images.githubusercontent.com/104550011/196629946-8e48742c-5527-4d9c-bd9c-117f52dc095a.jpg)",Identify Attack/Name Calling
"![image](https://user-images.githubusercontent.com/24404599/196631293-46c94e05-8281-409d-9cd3-cb79e72fec85.png)
Authorize to write to Google?",Impatience
LOLðŸ˜‚,Mocking
æ¨‚,
"If these vendors misappropriate the `Visual Studio code` trademark, I hope Microsoft sues them for copyright infringement.",Threat
"If you do not wish to use the VSCode under M$, you may turn to VS Codium.",
I didn't find this ad in the search results of google or baidu. What happened to bing ðŸ¤£,Mocking
Developers who can't even find the official website should change careers ASAP,Mocking
LOL,Mocking
LOL,Mocking
crazy,Mocking
éš¾ç»·ï¼Œvscodeç®¡å®¶æ˜¯å§,Mocking
F**k Baidu,Vulgarity
VSCode need instant actions to avoid being authorized to Google. ðŸ˜‚,Mocking
die laughing,Mocking
å½³äºï¼Œç‰›çš„,Mocking
u can alos see Steam downcenterã€ manager....,
"I tried to translate the Chinese characters in these three pictures into English to make it easier for more readers to understand what happened.
![1](https://user-images.githubusercontent.com/20700283/196642847-1805b401-c91d-45b8-bbca-a3993bbc81f5.jpg)
![2](https://user-images.githubusercontent.com/20700283/196642873-ab54bae8-302e-49f0-97b4-1bbf639a179f.jpg)
![3](https://user-images.githubusercontent.com/20700283/196642892-3ffa3992-15aa-4888-8c93-2526a6eb2116.jpg)
-------
Appendix:
I don't know who was the math teacher of the author of this ""installer"", but I know 69.70 - 20 != 39.70.
lol",Mocking
éš¾ç»·ðŸ˜‚ðŸ˜‚ðŸ˜‚,Mocking
"I always thought it was a useless extension until I saw this issue...
It is recommended to pre-install this extension.
https://marketplace.visualstudio.com/items?itemName=lewistehminerz.unregistered-vscode
![image](https://user-images.githubusercontent.com/6159588/196648846-9bd63a4d-a75b-4dfb-abfc-499d0c76874f.png)",
a lot of funðŸ¤£,Irony
XDDDDDDDDDDDDDD,Mocking
baidu makes great firewall great again.,Irony
"çœŸÂ·ç¨‹åºå‘˜ç‰ˆsteam
real steam for programmer",
"lol, why does this VSCode search and install on Baidu? Well, I admit that for those who are new to programming, they really don't know where to download it, but shouldn't the official website of VSCode be an English interface? Novice should be able to distinguish this.",Mocking
"More funny screenshots:
![image](https://user-images.githubusercontent.com/50246090/196645235-7f858aa9-5bda-4591-80bd-f23b93e6a9da.png)
![image](https://user-images.githubusercontent.com/50246090/196647834-e9e91993-a1cc-425b-985d-39c8cdaa95d7.png)
The original price is ï¿¥69.7, but you will get a ""ï¿¥20"" (ï¿¥30 in fact) coupon when you close the window. Also, if you turn off the internet, it goes to ï¿¥29.95. My guess is they did an online price increase because of the brisk sales. ðŸ”¥ ![image](https://user-images.githubusercontent.com/50246090/196646493-fdce2d63-c193-4161-8f18-c7ac9f5dbf4c.png)
The price for VS 2010 and 2012 is ï¿¥49, 2013 is ï¿¥59, 2017 is ï¿¥69, 2019 is ï¿¥79, and 2022 is ï¿¥89. No discounts. ðŸ˜­ ![image](https://user-images.githubusercontent.com/50246090/196646542-6bac2d4d-1737-4c5d-910f-7689f0dceb01.png)
Yeah, you can still buy VB 6.0 in 2022.",Bitter Frustration
"@weimeng23 To add, China has a variety of software disguised as Steam, which is downloaded as a group number platform.",
"![image](https://user-images.githubusercontent.com/19525769/196652433-eb7b37b0-8cbd-4ecc-b9c1-34bda3c56f8f.png)
æˆ‘è‹±æ–‡ä¸å¥½ï¼Œå¦‚æžœæˆ‘æ²¡æœ‰ç†è§£é”™çš„è¯ï¼Œè¿™ä¸ªissueçš„æ„æ€æ˜¯ï¼šå¼ æœ¦æœ¦ä½ å¦ˆæ­»äº†",Identify Attack/Name Calling
"Baidu seems to have already removed this ad in vscode search results. But the download link to microsoft website is ranked the 4th, even after tutorials on using vscode.
Bing removed the ad from ""vscode"" search results, but the ad is still there when you search ""visual code"".
IMHO, lawsuit against such piracy companies will barely work. The worst thing that will happen to them is going bankrupt.",Bitter Frustration
"ç¬‘å°¿ðŸ˜‚
![è‹å–‚è‹å–‚è‹å–‚](https://user-images.githubusercontent.com/21122615/196653570-72555758-610f-45ca-a3c0-ba270192c73a.gif)",Mocking
"æ—¢ç„¶å¤§å®¶éƒ½æ˜¯ä¸­å›½äººï¼Œé‚£æˆ‘å°±ç›´æŽ¥æ‰“ä¸­æ–‡äº†
å…ˆè¯´ä¸€ä¸‹ä»–ä»¬å…¬å¸æ˜¯æœ‰é™å…¬å¸ï¼Œæ‰¿æ‹…çš„è´£ä»»æ˜¯æœ‰é™çš„ã€‚
æ³¨å†Œèµ„æœ¬æ˜¯10Wï¼Œæ„æ€å°±æ˜¯å¦‚æžœå‡ºäº†äº‹å…¬å¸æ³•äººæœ€å¤šèµ”å¿10Wäº†äº‹ã€‚
æœ€åŽï¼Œè¿™å®¶å…¬å¸æ˜¯çœŸçš„æ— è€»å•Š",Bitter Frustration
ç‰›çš„,Mocking
incomprehensible!ðŸ˜‚,Mocking
"<img width=""858"" alt=""image"" src=""https://user-images.githubusercontent.com/10466275/196658413-3080fed8-dab9-4bc6-993d-bcd844065143.png"">
<img width=""970"" alt=""image"" src=""https://user-images.githubusercontent.com/10466275/196658545-1291c19f-72fc-4d91-97cb-6ad49afe5bd4.png"">
çœ‹èµ·æ¥å·²ç»ä¸‹çº¿äº†ï¼Œç™¾åº¦ä¸€ç”Ÿé»‘",Bitter Frustration
FREEDOM !!! ä½ æ³•æˆ‘ç¬‘,Mocking
"> å…ˆè¯´ä¸€ä¸‹ä»–ä»¬å…¬å¸æ˜¯æœ‰é™å…¬å¸ï¼Œæ‰¿æ‹…çš„è´£ä»»æ˜¯æœ‰é™çš„ã€‚
Limited liability company (LLC) don't imply the liability is always limited.",Irony
éžè ¢å³å!!,Mocking
"å¾ˆå¤šå¼€æºè½¯ä»¶è®¸å¯è¯æ˜¯å…è®¸ä½ æ‹¿å‡ºåŽ»æ‰“åŒ…å”®å–çš„ https://www.gnu.org/licenses/gpl-faq.zh-cn.html#GPLCommercially æ— è®ºä½ å–ä»€ä¹ˆä»·é’±ï¼Œå”¯ä¸€çš„è¦æ±‚å°±æ˜¯ä½ çš„é¡¾å®¢è¦æ±‚ä½ å‡ºç¤ºæºç ï¼Œä½ å¾—æ— æ¡ä»¶æä¾›ï¼›é¡¾å®¢æ‹¿ä½ çš„ä¸œè¥¿äºŒæ¬¡å”®å–ï¼Œä½ ä¹Ÿåˆ«æ‹¦ç€ã€‚
å¼€æºè½¯ä»¶ä¸ç­‰äºŽç¦æ­¢å•†ä¸šåŒ–ï¼Œåªè¦è®¸å¯è¯å…è®¸ï¼Œä½ å®Œå…¨å¯ä»¥æ­£å¤§å…‰æ˜Žæ‹¿å¼€æºçš„ä¸œè¥¿æ‰“åŒ…æ¢çš®æŒ£é’±ï¼Œæ¯”å¦‚ Redhat å°±æ˜¯è¿™æ ·ä¸€å®¶å¤§åž‹ä¼ä¸šã€‚
è¿™é‡Œé—®é¢˜åªæ˜¯ä¾µçŠ¯äº†vscodeçš„å•†æ ‡ã€‚",
![image](https://user-images.githubusercontent.com/17821872/196665995-191f3f7d-e8fc-47ec-b6b3-3e0a0d4e82a2.png),
fake virus downloader with Chinese characteristic XD,Mocking
lol,Mocking
"ç‰¹åœ°å‰æ¥æŸ¥çœ‹
å½“åœºèšŒåŸ ä½",Bitter Frustration
It has been a meme for a long time XD.Finally someone shared it here.good job.,Mocking
ç‰›æ‰¹ï¼Œè¿˜æ˜¯ç™¾åº¦ðŸ‚,Mocking
LMAO,Mocking
"> å¾ˆå¤šå¼€æºè½¯ä»¶è®¸å¯è¯æ˜¯å…è®¸ä½ æ‹¿å‡ºåŽ»æ‰“åŒ…å”®å–çš„ https://www.gnu.org/licenses/gpl-faq.zh-cn.html#GPLCommercially æ— è®ºä½ å–ä»€ä¹ˆä»·é’±ï¼Œå”¯ä¸€çš„è¦æ±‚å°±æ˜¯ä½ çš„é¡¾å®¢è¦æ±‚ä½ å‡ºç¤ºæºç ï¼Œä½ å¾—æ— æ¡ä»¶æä¾›ï¼›é¡¾å®¢æ‹¿ä½ çš„ä¸œè¥¿äºŒæ¬¡å”®å–ï¼Œä½ ä¹Ÿåˆ«æ‹¦ç€ã€‚
> > å¼€æºè½¯ä»¶ä¸ç­‰äºŽç¦æ­¢å•†ä¸šåŒ–ï¼Œåªè¦è®¸å¯è¯å…è®¸ï¼Œä½ å®Œå…¨å¯ä»¥æ­£å¤§å…‰æ˜Žæ‹¿å¼€æºçš„ä¸œè¥¿æ‰“åŒ…æ¢çš®æŒ£é’±ï¼Œæ¯”å¦‚ Redhat å°±æ˜¯è¿™æ ·ä¸€å®¶å¤§åž‹ä¼ä¸šã€‚
> > è¿™é‡Œé—®é¢˜åªæ˜¯ä¾µçŠ¯äº†vscodeçš„å•†æ ‡ã€‚
å®˜ç½‘ä¸‹è½½çš„vscodeä¸æ˜¯å®Œå…¨å¼€æºç‰ˆæœ¬çš„vscodeï¼Œä½¿ç”¨çš„licenseä¹Ÿä¸ä¸€æ ·ï¼šhttps://code.visualstudio.com/License/ã€‚",
"ç™¾åº¦æ— è€»ï¼ŒBingä¹Ÿä¸å¹²å‡€äº†ã€‚
Baidu is shameless, and Bing is no longer clean.",Insulting
lol,Mocking
"![image](https://user-images.githubusercontent.com/51020316/196678924-cdbd6a49-ee46-4804-9821-f31a39278bc7.png) - ""the graphics and software come from the Internet"" - ""respect the legitimate rights and interests of others"" - ""**if**(as if they didn't know) the content violates your legitimate rights"" - ""we will delete it as soon as possible""",Mocking
69.7-20=39.7,
Essentially they only infringe on trademarks and the quality of content on the Chinese Internet is very low.,Insulting
å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆï¼Œlove from china,Mocking
éžå¸¸å¥½ä½œå“ï¼Œè‹±é›„è”ç›Ÿï¼Œçˆ±æ¥è‡ªç“·å™¨,
"Chrome users in China: Hold my beer.
![1666180574023](https://user-images.githubusercontent.com/14220017/196684129-ec3634c4-b6f4-4e69-beca-583c2d44ee88.png)",Mocking
![niubi](https://user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png),Mocking
A good suggestion is not to use Baidu search engine,
æˆ‘ä»¬ç™¾åº¦æ˜¯è¿™æ ·çš„ï¼Œ@baidu,
Iâ€™m thinking of popping up an alert that tells user when open for the first time. Hide alertâ€™s code deep and dark!,Irony
"It's very interesting that , if Microsoft prosecutes this company , it will say it just sells software installing service. But anyhow , I wish Microsoft can stop this swindle.",Bitter Frustration
ä¸­æ–‡ï¼šå¾®è½¯æžå¿«ç‚¹ï¼Œç›´æŽ¥èµ·è¯‰å¾—äº†,Impatience
â€œThe Fun They Hadâ€,
The `Microsoft` editor authorized by `Google.com` to pay through `Alipay` searched from `Baidu.com`,
èšŒåŸ ä½äº†,Bitter Frustration
"LOL. This is very common in China to pay for a FREE software with high price because some user has no ability to search on Google or international version of Bing or somewhere else. They just use Baidu with A LOT OF ADS, or some download station with Trojans or viruses. So some company will give money to Baidu to show their fake official website at the top of the searching results. You can see that the first page of searching result for ""Visual Studio Code"" is full of ads and there some more in the recommendation page. This is horrible. BTW, some briefcase company in China is shameless to use copyrighted trademarks, copyrighted software and software under license for profit. But you cannot give them a copyright strike because the consciousness of copyright is weak in China. Very sad to see this.",Bitter Frustration
"> The `Microsoft` editor authorized by `Google.com` to pay through `Alipay` searched from `Baidu.com`
hard to ""èšŒ""",Mocking
"> ![image](https://user-images.githubusercontent.com/17821872/196665995-191f3f7d-e8fc-47ec-b6b3-3e0a0d4e82a2.png)
haha",Mocking
"> ç™¾åº¦æ— è€»ï¼ŒBingä¹Ÿä¸å¹²å‡€äº†ã€‚ Baidu is shameless, and Bing is no longer clean.
åªæ˜¯å¥‡æ€ªçš„SEOç½¢äº†(",Insulting
They even cannot do a correct calculation because 69.7-20=39.7 (The result is 49.7),Bitter Frustration
"> ![niubi](https://user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png
Is this picture anything related to this issue?",Impatience
æˆ‘æ›´æ‹…å¿ƒåœ¨å›½äº§LinuxæŽ¨å¹¿å¼€ä»¥åŽï¼Œä¼šä¸ä¼šæœ‰ç±»ä¼¼çš„æ— è‰¯å•†å®¶æŠŠé»‘æ‰‹ä¼¸å‘Linuxå¹³å°çš„vscodeæˆ–å…¶ä»–è½¯ä»¶ã€‚,Bitter Frustration
"Apparently, the math is wrong. I am outrageous. :)",Irony
èƒ½ç”¨ä¸Šç™¾åº¦ï¼Œä½ ä»¬å°±å·ç€ä¹å§,Bitter Frustration
æ¨‚,
![ä¸è¡Œï¼Œè¿™å¤ªåŽçŽ°ä»£äº†](https://user-images.githubusercontent.com/30336566/196826890-99382c95-0f52-4e26-9310-78749d8b496c.png),Bitter Frustration
çœŸæ˜¯è§è¯†äº†,Bitter Frustration
ç»·,Mocking
ä¸¢äººä¸¢åˆ°å›½å¤–äº†å±žäºŽæ˜¯,Mocking
interesting,
interesting,
Google Licensing? Fine ðŸ˜…,Irony
V2è§‚å…‰å›¢,Mocking
@baidu,
æ„Ÿæ©,
ç«é’³æµæ˜Ž,
V2è§‚å…‰å›¢,Mocking
funny,Irony
69.7 - 20 = 39.7 ???,Impatience
"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?
Maybe student",Mocking
666,
"> Do you know why there is a GFW in China? cause CCP sucks, these SOBs are afraid of their people knowning or telling the truth, all words published on China's social media/platform are reviewed strictly, anything CCP doesn't like will be banned, and on the other side, they claim that there is something called FREE-SPEECH is existed in China, so F*CK them
omg,Do you have any sense in saying thatï¼ŸLOL.",Bitter Frustration
"> æˆ‘æ›´æ‹…å¿ƒåœ¨å›½äº§LinuxæŽ¨å¹¿å¼€ä»¥åŽï¼Œä¼šä¸ä¼šæœ‰ç±»ä¼¼çš„æ— è‰¯å•†å®¶æŠŠé»‘æ‰‹ä¼¸å‘Linuxå¹³å°çš„vscodeæˆ–å…¶ä»–è½¯ä»¶ã€‚
å·²ç»æœ‰ Ubuntu å’Œ CentOS äº†ï¼Œæ”¶è´¹80å¤šï¼ŒBç«™æœ‰å­¦ç”Ÿä¸Šå½“",Bitter Frustration
"> > ![niubi]([user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png](https://user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png)
> > Is this picture anything related to this issue?
Yes It means unbelieve and awesome",
"> It's very interesting that , if Microsoft prosecutes this company , it will say it just sells software installing service. But anyhow , I wish Microsoft can stop this swindle.
There are too many such companies in China, you know? Adobe Flash Player, which is represented by a Chinese company, is still updated and added ads, does not allow you to use the international version (it will indicate that the software is damaged or not in the current area), and this ad is full of deceptive drugs and stock speculation content, some vulgar content, this adware program also comes with a virus (detected by Kaspersky), and the funniest thing is that the ads will crash when you watch the ads",Bitter Frustration
"If a beginner can not recognize that shit, it would be the first lesson they need to learn. ^_^",Mocking
"> > éžå¸¸æœ‰è¶£çš„æ˜¯ï¼Œä½†å¦‚æžœå¾®è½¯è¿™å®¶å…¬å¸ï¼Œå®ƒä¼šè¯´å®ƒé”€å”®è½¯ä»¶å®‰è£…ã€‚åªæ˜¯ï¼Œæˆ‘å¸Œæœ›å¾®è½¯èƒ½å¤Ÿé˜»æ­¢è¿™ç§éª—å±€ã€‚
> > ä¸­å›½è¿™æ ·çš„å…¬å¸æœ‰ï¼Œä½ çŸ¥é“å—ï¼Ÿè¯ç‚’çš„å†…å®¹ï¼Œä¸€äº›ä½Žä¿—çš„å†…å®¹ï¼Œè¿™ä¸ªå¹¿å‘Šè½¯ä»¶ç¨‹åºè¿˜è‡ªå¸¦ç—…æ¯’ï¼ˆæœ€æžç¬‘çš„æ˜¯çœ‹å¹¿å‘Šä¼šå´©æºƒï¼‰
The Internet environment in China is really bad now!",Bitter Frustration
lol,Mocking
"> ä¼šä¸Šgithubå‘issueä¸ä¼šgoogleä¸€ä¸‹å®˜ç½‘æ˜¯å§
ä¸æ˜¯çš„ï¼Œä»–åªæ˜¯åœ¨è¿™ä¸ªåœ°æ–¹å‘å¸ƒè¿™ä¸ªè¯é¢˜å¼•èµ·å…³æ³¨ï¼Œéƒ½ä¸Š GitHub äº†ï¼Œè¿™ä¸ªè¿˜æ˜¯çŸ¥é“çš„",Mocking
é›·ç¢§ï¼Œåº·å¸…å‚…,
æœªæ¥å¯æœŸ,
"> Developers who can't even find the official website should change careers ASAP
There are too many such fake official websites in Chinese search engines, and he just discusses this topic too much, not that he can't find the official website",Mocking
"> æœªæ¥å¯æœŸ
ä¾æ‰˜Microsoftï¼Œå¿«ä¸Šå¸‚äº†ðŸ˜‚",Mocking
amazing.,Irony
unbelievable,Bitter Frustration
"> unbelievable
éƒ½åœ¨ä¸­å›½äº†ï¼Œå¾ˆæ™®éäº†å§",Bitter Frustration
6,
> ä½ æ˜¯å¯¹çš„,
å¯ä»¥å•Šï¼Œæœ‰å‰é€”ã€‚ç¾Žæ»‹æ»‹çš„,Irony
welcome to China xD,Mocking
å¤ªä¸¢æ’µäº†,Bitter Frustration
"![image](https://user-images.githubusercontent.com/6455728/196847294-9b5f8d32-3201-4692-9b3f-d1c767958b8b.png)
Time to manually revoke certificates from TrustAsia Technologies
The downloaded exe file is signed by:
CN = DigiCert Trusted G4 Code Signing RSA4096 SHA384 2021 CA1",Bitter Frustration
"![china_now](https://user-images.githubusercontent.com/24828354/196849315-5b48323d-da64-49b8-81e8-f534ff0f20ac.png)
è¿™å°±æ˜¯__",Identify Attack/Name Calling
ä¸€çœ¼é¡¶çœŸï¼šé‰´å®šä¸ºå‡,Mocking
"[![29hEdS.gif](https://z3.ax1x.com/2021/05/26/29hEdS.gif)](https://imgse.com/i/29hEdS)
or this:
[![cashback](https://i1.hoopchina.com.cn/hupuapp/bbs/220/58393220/thread_58393220_20201024082704_s_1337599_o_w_498_h_267_60931.gif?x-oss-process=image/resize,w_800/format,webp)]",Mocking
ä¿¡ä»¶å·²ç»æ”¶åˆ°å•¦~,
"Chinese programmer's first lesson: figuring out a way to access google (blocked in China) or bing international (blocked, and replaced by bing Chinese edition).
Chinese programmer's second lesson: learning English.
LOL
----
Anyway, I think google and Microsoft should take down those piracy and potentially harmful websites asap. Or, at least ask Nintendo and Disney about how to take down everything they don't like from the internet.",Mocking
We can't really stop people without information source from doing those. People will always buy free software (like Chrome browser or VS Community) on Taobao because they don't know they are free.,Bitter Frustration
è¿™ä¸ªæœ‰ç‚¹åŽ‰å®³,
"> Search from Baidu:
> > ![image](https://user-images.githubusercontent.com/50246090/196424828-5ead4681-c783-442e-bbea-0b8e4c15fcc2.png)
> > The first one is https://qnw.shaid.top/vscode/index.html:
> > ![image](https://user-images.githubusercontent.com/50246090/196424988-4523e2d7-4860-4631-876f-bfd4a2548359.png)
> > This website belongs to [Wuhan Hongge Yibai E-commerce Co. (æ­¦æ±‰å®æ ¼äº¿ä½°ç”µå­å•†åŠ¡æœ‰é™å…¬å¸)](http://www.shaid.top/).
> > If you do some searches on this domain, you can find their other ""products"":
> > * [Win10 Activation Tool (Win10 æ¿€æ´»å·¥å…·)](https://qnw.shaid.top/win10/index.html)
> * Telegram Chinese version (Telegram ä¸­æ–‡ç‰ˆ) (removed)
> ![image](https://user-images.githubusercontent.com/50246090/196426820-f9fa3030-eb5e-430c-bdfc-026af7100576.png)
> * visualcpp (removed)
> ![image](https://user-images.githubusercontent.com/50246090/196432416-34943cc9-de17-4e6b-a316-ca5a6ff3f5e7.png)
> > They even claim to be partners with China Mobile, China Unicom and China Telecom: [![image](https://user-images.githubusercontent.com/50246090/196426325-f39cc8ab-8b66-40b7-8a1e-edc32ad7c08b.png)](http://www.shaid.top/)
> > Thanks to the Great Firewall, we have such a ""great"" Internet in China.
> > Update:
> > Some people have found other similar companies:
> > * [Shangqiu Xuankangtai Network Technology Co. (å•†ä¸˜è½©åº·æ³°ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸)](http://www.sqiua.cn/index.html)
> > * [Visual studio](http://office.xuank.top/install.php?m=visual)
> ![image](https://user-images.githubusercontent.com/50246090/196571476-47c2e0e6-5465-4666-8889-ad34a1c4689a.png)
> * [Software Superstore (è½¯ä»¶å•†è¶…)](https://xkt.sqiua.cn/)
> ![image](https://user-images.githubusercontent.com/50246090/196572556-a30b1388-e5f5-43f8-a94a-419e5501c2f9.png)
> * [System Home - Windows premium system download site (ç³»ç»Ÿä¹‹å®¶-Windowsç²¾å“ç³»ç»Ÿä¸‹è½½ç«™)](http://windows.sqiua.cn/index.html)
> * [visualbasic](http://office.xuank.top/install.php?m=visualbasic)
> * [directx Repair Master (directxä¿®å¤å¤§å¸ˆ)](http://office.xuank.top/install.php?m=directx)
> * [Chrome](http://chrome.sqiua.cn/)
> ![image](https://user-images.githubusercontent.com/50246090/196573087-e4bfb9f0-3060-4e72-a6bf-05a35238a151.png)
> * [Yixin Cat House (å¿†å¿ƒçŒ«èˆ)](http://yixinmaoshe.sqiua.cn/)
> ![image](https://user-images.githubusercontent.com/50246090/196572912-95f01175-ad54-40d7-bfe7-2b10aaa767b8.png)
> * [AdobeCAD](https://autocad.sqiua.cn/)
> * [Map marking service center (åœ°å›¾æ ‡æ³¨æœåŠ¡ä¸­å¿ƒ)](http://mr.sqiua.cn/)
> * [Yunnan Norforkang Network Technology Co. (äº‘å—è¯ºç¦åº·ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸)](http://www.cvbty.cn/)
> > * [Xunjian Mind Map (å¯»ç®€æ€ç»´å¯¼å›¾)](http://sw.xuank.top/)
å®£ç§°ä¸‰å¤§è¿è¥å±±æ˜¯åˆä½œä¼™ä¼´ï¼Œæ€•æ˜¯å¼€é€šäº†ä¸‰å¤§è¿è¥å•†çš„å®½å¸¦å¥—é¤å§å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ",Mocking
"> > æˆ‘æ›´æ‹…å¿ƒåœ¨å›½äº§LinuxæŽ¨å¹¿å¼€ä»¥åŽï¼Œä¼šä¸ä¼šæœ‰ç±»ä¼¼çš„æ— è‰¯å•†å®¶æŠŠé»‘æ‰‹ä¼¸å‘Linuxå¹³å°çš„vscodeæˆ–å…¶ä»–è½¯ä»¶ã€‚
> > å·²ç»æœ‰ Ubuntu å’Œ CentOS äº†ï¼Œæ”¶è´¹80å¤šï¼ŒBç«™æœ‰å­¦ç”Ÿä¸Šå½“
è¦æ˜¯å–å®žä½“å®‰è£…ä»‹è´¨æ”¶æ”¶æœ¬é’±ä¹Ÿä¸æ˜¯ä¸è¡Œ(
è¦æ˜¯æä¾›æ€Žä¹ˆè£…çš„æœåŠ¡ä¹Ÿä¸æ˜¯ä¸è¡Œ(",Bitter Frustration
"In taobao, some merchants provide installation package download service too.",
çœŸ æ™ºå•†ç¨Ž,Mocking
"Wait, you guys are installing VSCode for FREE?
![image](https://user-images.githubusercontent.com/6663691/196856413-fdde90d1-7d8e-41d9-bda4-123740017d27.png)",Impatience
![image](https://user-images.githubusercontent.com/54622682/196860848-1c74ace1-1270-4b33-a3c5-fc0ba03201db.png),
![QQå›¾ç‰‡20221020130910](https://user-images.githubusercontent.com/24607145/196861392-e5eca5c2-923a-4dc3-a649-4c5253aa2e25.jpg),
"Wait, you guys are installing VSCode for FREE?",Irony
å»ºè®®å…¥å…¸,Mocking
"> Wait, you guys are installing VSCode for FREE?
Now that you have the ability to communicate on GitHub, I think you're kidding",Irony
"> In taobao, some merchants provide installation package download service too.
è¿™ç§æ˜¯ä¾µæƒï¼Œä¸ä¸€æ ·çš„æ€§è´¨",
"This is absurd yet quite common in China, which is more embarrassing than funny. Sh*tposting the comment section is making it even worse. Someone has to put a stop to this.",Bitter Frustration
"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""éŸ­èœ""_, never be a true citizens ever ) here. FUCK CCP!
ä½ å°å­å·æ²¡äº†",Bitter Frustration
"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""éŸ­èœ""_, never be a true citizens ever ) here. FUCK CCP!
å±",Bitter Frustration
"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""éŸ­èœ""_, never be a true citizens ever ) here. FUCK CCP!
å±",Bitter Frustration
So 69.70 - 20 = 39.70 ????,Impatience
"It's not that surprise for me these things happening in China.
As we all know there is a firewall call ""GFW"" protecting user using internet so...
The truth is that the man who paid a large amount of money can do anything he want",Identify Attack/Name Calling
I think it is a trick,Irony
"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""éŸ­èœ""_, never be a true citizen ever ) here. FUCK CCP.
ä½ æ˜¯ç‹—? æ»šä½ å¦ˆçš„hi , è¿™äº›ä¸œè¥¿æ˜¯æŸäº›æ— è‰¯å…¬å¸çš„ä½œä¸º,è¯·ä½ ä¸è¦ä¸Šå‡åˆ°è¿™ä¸ªå±‚é¢. å°å¿ƒç”Ÿå­©å­æ²¡å±çœ¼. å‡ºé—¨è¢«å¤§è´§è½¦åŽ‹æ­»",Bitter Frustration
"Even if the officials could notice, there is no way to make such people disappear. Such companies are all shell companies, a company may have just two employees, a packager and a legal officer. That's what's so pathetic.
![23EBEDCA3763335B3422895935B7BDB0](https://user-images.githubusercontent.com/46388610/196868970-476f8d72-891d-40e5-82a7-16372fd807b8.jpg)",Bitter Frustration
ç™¾åº¦æžœç„¶å‘é’±çœ‹é½,Bitter Frustration
ä¸è¦æ ¸é…¸è¦åƒé¥­ï¼Œä¸è¦å°æŽ§è¦è‡ªç”±ï¼Œä¸è¦è°Žè¨€è¦å°Šä¸¥ï¼Œä¸è¦æ–‡é©è¦æ”¹é©ï¼Œä¸è¦é¢†è¢–è¦é€‰ç¥¨ï¼Œä¸åšå¥´æ‰åšå…¬æ°‘,Bitter Frustration
"> å‘èµ·è¦åƒé¥­ï¼Œä¸è¦å°æŽ§è¦è‡ªç”±ï¼Œä¸è¦ç¤¼è®©ï¼Œä¸è¦æ–‡é©è¦æ”¹é©ï¼Œä¸è¦å¸¦å¤´é€‰ç¥¨ï¼Œä¸è¦åšå¥´æ‰æ‰åš
å¥½ä½ å°å­ï¼Œé€®ä½ä½ äº†",Bitter Frustration
æ™ºå•†ç¨Žï¼ŒæŒºå¥½çš„,Mocking
æ¥¼ä¸Šæœ‰ä¸ªå°ç²‰çº¢æ€¥äº†,Mocking
"å›½å†…ç½‘ç«™å¾ˆå¤šéƒ½å–œæ¬¢ç»™äººå–‚ç‹—å±Žï¼Œæ¯•ç«Ÿåªæœ‰ä½ åƒå±Žäº†å®ƒä»¬æ‰èƒ½åƒä¸Šé¥­
psï¼šæˆ‘ä¸æ˜¯åšæ…ˆå–„çš„ï¼Œæˆ‘å¸Œæœ›å®ƒä»¬æ—©æ—¥å…¨éƒ¨é¥¿æ­»
ppsï¼šä¸æ˜¯è¯´å›½å¤–å°±å¾ˆå¥½çš„æ„æ€",Bitter Frustration
> å®‰åˆ©ä¸ª google extension: ç™¾åº¦åŽ»å¹¿å‘Š,
"> 69.7 - 20 = 39.7 ???
You know mathematics.",Mocking
"WHY USE Baidu? In China, [Bing](https://bing.com) works just as well.",Mocking
"We call somebody who paid 'IQ tax'ï¼ˆæ™ºå•†ç¨…ï¼‰ in Chinese if he has downloaded and paid for it. :-D
Long years ago, I also saw some online shops(mainly on Taobao) sell discs with burned Ubuntu ISO images, I reported these behaviors to the official community and some software engineer forums but some people replied to me that these behaviors are OK and are not illegal.
I guess that because some Chinese noob programmers are not familiar with English, so they don't know how to find and download the official **Visual Studio Code** from the official website in English.",Mocking
"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol
#NTR",Mocking
"Some netizens need to scan the code to install vscode, I hope the official can stop this behavior ![21276c6919c82457dcedd5587c5edc0](https://user-images.githubusercontent.com/13271663/196067460-dd5c6606-3ccc-4a04-a555-9d2f8add0636.jpg)
![07e482785c0bdd0986b798bbc86e7aa](https://user-images.githubusercontent.com/13271663/196067464-8323e6f6-5e56-4407-b629-965427faf557.jpg)
![5bcc8c2dbb4d94d26fd7b41448b0d63](https://user-images.githubusercontent.com/13271663/196067467-56f07382-364c-4c72-a92c-d6faed02d97f.jpg)",Bitter Frustration
"A cute example and a nice play on words! I will leave others to comment on the technical aspects and programming style and why some tests are failing I also couldn't resist fleshing it out a bit more with sails, keel and rudder, as the previous resolutions were criticised as being not functional, so in the modified example the boat is sailing and heeling in the wind !
EDIT Delete miscredited content.",
"Administration action. @kwikius I have edited your post.
Please refrain from publishing code under other peoples credit. ie `// written by Paul Young, 2022`
Please also refrain from denigrating others contributions to this community supported open source project.
Constructive comments, not passive aggressive, are welcome.",
"I created a hull example of a toy sailboat, added it to the examples/basic folder and updated the examples.json.",
"I believe not, it's just a malicious repository trying to impersonate polymc",Bitter Frustration
"Great, i download now",Impatience
"> I believe not, it's just a malicious repository trying to impersonate polymc
oh no my pc crashed i thINk it were teh maliciouz repository (i thinked thiz were god thing) plZ?",
I DOAN FEELZ SAFE AUTHENTICATIN MAH MICROSOFT AKOWNT IN DIS PROJECT,Bitter Frustration
"This is a launcher made by the non-corrupt Devs of PolyMC, as a successor to PolyMC. It crashed most likely because it is still dev builds, it's not ready for the public quite yet.",
"Definetly, i'm getting sick because of that. I need to apply speed everytime when i want to change video.",Bitter Frustration
Closed because Acknowledgements not met.,Impatience
"![Screenshot_2022-10-22-13-57-50-35_3aea4af51f236e4932235fdada7d1643](https://user-images.githubusercontent.com/13122796/197337721-54ebb4c3-a5af-4f62-ab3b-0ba596698e74.jpg)
https://github.com/orgs/revanced/discussions/269",
I'll open this issue in favour of closing the discussion so it is moved here.,
im sry but can somebody tell me wht the status of this feature request is??? ive gone through previous posts and saw tht similar issue was closed as not planned...please give some clarity as to whether this feature will be implemented or not,Impatience
This is much needed? Why cant anyone make this,Bitter Frustration
Can we PLEASE get this!!,Impatience
"It is not easy to help, because developing a patch usually is not collaborative. Vanced's implementation is different from what it will be on ReVanced, both in the implementation and usage. The behavior will be similar to the `remember-video-quality` patch with a switch to lock to the current speed. Additionally the implementation of the code has to be refactored from the ground up as just porting over Vanced's implementation will cause implications with readabity and maintainability in the future.",
"We have to expect the current or better code quality, otherwise it will cause implications with readabity and maintainability in the future as previously said.",Bitter Frustration
https://github.com/revanced/revanced-integrations/commit/eea37a9bada6c843275236fa232bfe497d178b17,
"### Application
Youtube ReVanced
### Issue
When i change video it defaults playback speed to 1x
### Patch
It forces playback speed to user specified value.
### Motivation
It was already available on vanced. So it would be nice with revanced too.
### Acknowledgements
- [x] I have searched the existing issues and this is a new and no duplicate or related to another open issue.
- [X] I have written a short but informative title.
- [X] I filled out all of the requested information in this issue properly.",
"where tf did it find line 38 in HideCreateButtonPatch.java?
https://github.com/revanced/revanced-integrations/blob/main/app/src/main/java/app/revanced/integrations/patches/HideCreateButtonPatch.java",Impatience
"> where tf did it find line 38 in HideCreateButtonPatch.java? https://github.com/revanced/revanced-integrations/blob/main/app/src/main/java/app/revanced/integrations/patches/HideCreateButtonPatch.java
line 38 in compiled java code, not source code.",Impatience
Are you patching a compatible version of the app?,Impatience
Also which patches version was the last time it worked?,Impatience
"I am having the same problem revanced crashes at start, I have included hides created button patch and have patched with the compatible version 17.36.37 , built using revanced builder on termux.",
"> Are you patching a compatible version of the app?
yes, 17.36.37
> Also which patches version was the last time it worked?
I usually choose all patches except microg, hide cast button, theme, premium banner and debugging. if I also choose hide create button, it crashes; if not, it works
if ur asking about version, I have no idea, I use reisxd's revanced builder and i didn't care much",Impatience
I guess it's 2.83 because I don't remember I chose hide mix patch,Bitter Frustration
"@old4error exclude all patches except for the failing patch, if it doesn't fail, report back with the patch which conflicts and causes it to fail",Impatience
"Having the same issue, same error message, only happens on root install, but for some reason unrooted works fine.",Bitter Frustration
"unroot version 17.36.37 patches 2.85 cli 2.14 integr. 0.54:
no crash without hide-create-button patch, also there is no create button (when logged out) when logged in - crash
root version, same versions as above: included all patches (except hide-cast and microg for root build) except hide-create-button patch: no crash; with hide-create-button patch - crash",Bitter Frustration
"> unroot version 17.36.37 patches 2.85 cli 2.14 integr. 0.54: no crash without hide-create-button patch, also there is no create button (when logged out) when logged in - crash root version, same versions as above: included all patches (except hide-cast and microg for root build) except hide-create-button patch: no crash; with hide-create-button patch - crash
Also having the same results here, seems as having the create button appear is causing the crash which is odd, because I don't think anythings changed with that patch.",Bitter Frustration
"Root or not does not change the patch process, please record as mentioned.",Impatience
"root version 17.36.37 patches 2.85 cli 2.14 integr. 0.54:
included all patches except hide-create-button patch: no crash; with hide-create-button patch - crash",
if including only hide-create-button patch - no crash,
Find and report back with the patch which conflicts with the patch.,Impatience
"If it helps, everything worked fine with 2.83.2 with the following patches on 17.36.37 built using revanced builder on termux-
> hide-shorts-button
> hide-create-button
> disable-startup-shorts-player
> hide-cast-button
> sponsorblock
> hide-autoplay-button
> return-youtube-dislike
> theme
> hide-email-address
> hide-watermark
> custom-branding
> premium-heading
> old-quality-layout
> general-ads
> video-ads
> swipe-controls
> downloads
> settings
> microg-support
> custom-video-buffer
> client-spoof
> always-autorepeat
> remember-video-quality
> minimized-playback
> custom-playback-speed
Seeing the recent versions having changes only to video-ads I tried coupling video-ads and hide create button and built using the latest patches but that worked fine to my surprise.",
I can confirm crash when hide-create-button patch is selected.,
"> Find and report back with the patch which conflicts with the patch.
The hide-shorts-button and hide-create button patches together are the patches that cause the startup crash on version 17.36.37 (at least on my end).",Bitter Frustration
"Can confirm the same issue with patches v2.85.1, root, compiled excluding microg-support
```
10-21 15:50:24.697 27626 27626 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:24.700 27626 27626 I AndroidRuntime: Using default boot image
10-21 15:50:24.700 27626 27626 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:25.133 27626 27626 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:25.216 27652 27652 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:25.217 27626 27626 D AndroidRuntime: Shutting down VM
10-21 15:50:25.219 27652 27652 I AndroidRuntime: Using default boot image
10-21 15:50:25.219 27652 27652 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:25.628 27652 27652 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:25.700 27652 27652 D AndroidRuntime: Shutting down VM
10-21 15:50:26.290 27671 27671 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:26.293 27671 27671 I AndroidRuntime: Using default boot image
10-21 15:50:26.293 27671 27671 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:26.714 27671 27671 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:26.786 27671 27671 D AndroidRuntime: Shutting down VM
10-21 15:50:27.359 27754 27754 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:27.361 27754 27754 I AndroidRuntime: Using default boot image
10-21 15:50:27.361 27754 27754 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:27.786 27754 27754 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:27.867 27754 27754 D AndroidRuntime: Shutting down VM
10-21 15:50:28.437 27806 27806 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:28.439 27806 27806 I AndroidRuntime: Using default boot image
10-21 15:50:28.439 27806 27806 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:28.874 27806 27806 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:28.953 27806 27806 D AndroidRuntime: Shutting down VM
10-21 15:50:29.501 27860 27860 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:29.503 27860 27860 I AndroidRuntime: Using default boot image
10-21 15:50:29.503 27860 27860 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:29.945 27860 27860 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:30.035 27860 27860 D AndroidRuntime: Shutting down VM
10-21 15:50:30.582 27891 27891 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:30.585 27891 27891 I AndroidRuntime: Using default boot image
10-21 15:50:30.585 27891 27891 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:31.000 27599 27599 E AndroidRuntime: FATAL EXCEPTION: main
10-21 15:50:31.000 27599 27599 E AndroidRuntime: Process: com.google.android.youtube, PID: 27599
10-21 15:50:31.000 27599 27599 E AndroidRuntime: arjx: The exception was not handled due to missing onError handler in the subscribe() method call. Further reading: https://github.com/ReactiveX/RxJava/wiki/Error-Handling | java.lang.NullPointerException: Attempt to invoke virtual method 'void android.view.View.setVisibility(int)' on a null object reference
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at arle.a(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at armd.b(PG:3)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at armd.tl(PG:5)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at arzw.tl(PG:5)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at asby.run(PG:4)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at aeaq.run(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at ashm.run(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at asho.run(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Handler.handleCallback(Unknown Source:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Handler.dispatchMessage(Unknown Source:4)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Looper.loopOnce(Unknown Source:176)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Looper.loop(Unknown Source:76)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.app.ActivityThread.main(Unknown Source:138)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at java.lang.reflect.Method.invoke(Native Method)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(Unknown Source:11)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at com.android.internal.os.ZygoteInit.main(Unknown Source:309)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'void android.view.View.setVisibility(int)' on a null object reference
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at app.revanced.integrations.patches.HideCreateButtonPatch.hideCreateButton(HideCreateButtonPatch.java:38)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at kzw.z(PG:31)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at kyt.a(PG:45)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at armd.tl(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: ... 13 more
10-21 15:50:31.008 27891 27891 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:31.089 27891 27891 D AndroidRuntime: Shutting down VM
```",
"yep, same crash as @epic0421's. hide-shorts-button + hide-create-button = startup crush",Bitter Frustration
hide-shorts-button doesn't seem to cause any issues for me,
Fixed.,
"### Type
Other
### Bug description
hide create button causes crash at startup. tested and confirmed in log
### Steps to reproduce
also I used cli 2.14.0, patches 2.85 and integrations 0.54.0. also I use evolutionx rom 7.2 android 13, 20 oct build, device: poco x3 pro
1. start building revanced as usual
2. include hide-create-button patch
3. expect crash after building and mounting
### Relevant log output
```shell
time: 1666292481733
msg: java.lang.NullPointerException: Attempt to invoke virtual method 'void android.view.View.setVisibility(int)' on a null object reference
stacktrace: arjx: The exception was not handled due to missing onError handler in the subscribe() method call. Further reading: https://github.com/ReactiveX/RxJava/wiki/Error-Handling | java.lang.NullPointerException: Attempt to invoke virtual method 'void android.view.View.setVisibility(int)' on a null object reference
at arle.a(PG:2)
at armd.b(PG:3)
at armd.tl(PG:5)
at arzw.tl(PG:5)
at asby.run(PG:4)
at aeaq.run(PG:2)
at ashm.run(PG:2)
at asho.run(PG:2)
at android.os.Handler.handleCallback(Handler.java:942)
at android.os.Handler.dispatchMessage(Handler.java:99)
at android.os.Looper.loopOnce(Looper.java:201)
at android.os.Looper.loop(Looper.java:288)
at android.app.ActivityThread.main(ActivityThread.java:7893)
at java.lang.reflect.Method.invoke(Native Method)
at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:550)
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:936)
Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'void android.view.View.setVisibility(int)' on a null object reference
at app.revanced.integrations.patches.HideCreateButtonPatch.hideCreateButton(HideCreateButtonPatch.java:38)
at kzw.z(PG:31)
at kyt.a(PG:45)
at armd.tl(PG:2)
... 13 more
```
### Screenshots or videos
_No response_
### Solution
exclude hide-create-button from patch list while building.
### Additional context
btw, I tried to patch revanced on android_x86 (blissos 11.14) vm and suddenly it works even with this patch",
"I don't want to sound too negative, but that's a bit of a drive-by deprecation, no? Ticket was opened 7 days ago and 6 days ago it was merged. Would be nice to have a bit of a heads-up on stuff that requires consumers to rewrite their stuff. I am currently using the `onSchemaColumnDefinition` and `onSchemaCreateTable`, did I understand it correctly that now instead of using these events, I'm supposed to extend/override all the Platform classes I use?",Impatience
"> Would be nice to have a bit of a heads-up on stuff that requires consumers to rewrite their stuff.
Isn't that exactly what a deprecation does? What would you do?
> did I understand it correctly that now instead of using these events, I'm supposed to extend/override all the Platform classes I use?
I don't think that's the right thing to do, see https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959",Impatience
"> Isn't that exactly what a deprecation does? What would you do?
I just mean the plan to deprecate was announced (i.e. this ticket opened) and one day later it was already implemented. It just seems like it would have been nicer to keep it open for a while to see if there is any user feedback (Also, some migration guide would really be appreciated).
> I don't think that's the right thing to do, see https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959
That wants me to log into something. Is there a public version of it maybe?",Impatience
"The deprecation of the event system happened in an effort to consolidate our extension points. But if we discover that this deprecation was premature, we can talk about it. Can you please open a new issue where you explain for what purpose you use the event system and why extending the platform would make that more difficult for you?",
"Sure, I can look into that. To be clear: I don't mind you deprecating stuff as long as there's an alternative way to implement the functionality I need. Just to make sure: @derrabus you think extending Platform (as in `Doctrine\DBAL\Platforms\SqlitePlatform` etc.?) would the the way to go, but @greg0ire you don't think so?
BTW, one scenario I use I described here:
https://github.com/doctrine/dbal/issues/4676#issuecomment-865151733",
"According to https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959, it isn't:
![2022-10-28_15-11](https://user-images.githubusercontent.com/657779/198601020-86205a28-773e-489f-b920-bbdb7ff46677.png)",
"@greg0ire thanks for the info! I just tried to figure this out, but somehow I'm stuck: AFAICT the `SchemaManager` to use gets determined in the `Platform`:
https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/Platforms/SqlitePlatform.php#L1481-L1484
So to use a custom `SchemaManager` I would have to extend the `Platform` to return my own instance. The `Platform` gets determined in the `Driver`:
https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/Driver/AbstractSQLiteDriver.php#L24-L27
And to override the `Driver`, I would have to use `driverClass` here:
https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/DriverManager.php#L245-L253
Which means I'd have to copy the `self::DRIVER_MAP` logic (because it's private), but then also the actual driver class is marked `final`, so I can't extend it:
https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/Driver/SQLite3/Driver.php#L9
Is there something I'm missing?",Bitter Frustration
"> The Platform gets determined in the Driver
â€¦ if you don't pass one via the `platform` option to the `DriverManager`.",
"@derrabus thanks, that helped! So for `onSchemaColumnDefinition`, I would need approximately this amount of boilerplate, right?
```php
$db_config = [/* config as before, driver set to pdo_mysql or pdo_sqlite */];
if ($db_config['driver'] == 'pdo_mysql') {
$db_config['platform'] = my_mysql_platform::class;
} elseif ($db_config['driver'] == 'pdo_sqlite') {
$db_config['platform'] = my_sqlite_platform::class;
} else {
throw new Exception('Oh noes!!!');
}
/* startup connection as before */
class my_mysql_platform extends Doctrine\DBAL\Platforms\MySQLPlatform
{
public function createSchemaManager(Doctrine\DBAL\Connection $connection): Doctrine\DBAL\Schema\MySQLSchemaManager
{
return new my_mysql_schemamanager($connection, $this);
}
}
class my_sqlite_platform extends Doctrine\DBAL\Platforms\SqlitePlatform
{
public function createSchemaManager(Doctrine\DBAL\Connection $connection): Doctrine\DBAL\Schema\SqliteSchemaManager
{
return new my_sqlite_schemamanager($connection, $this);
}
}
class my_mysql_schemamanager extends Doctrine\DBAL\Schema\MySQLSchemaManager
{
protected function _getPortableTableColumnList($table, $database, $tableColumns)
{
$tableColumns = onSchemaColumnDefinition($tableColumns);
return parent::_getPortableTableColumnList($table, $database, $tableColumns);
}
}
class my_sqlite_schemamanager extends Doctrine\DBAL\Schema\SqliteSchemaManager
{
protected function _getPortableTableColumnList($table, $database, $tableColumns)
{
$tableColumns = onSchemaColumnDefinition($tableColumns);
return parent::_getPortableTableColumnList($table, $database, $tableColumns);
}
}
function onSchemaColumnDefinition($tableColumns)
{
/* event listener code goes here */
return $tableColumns;
}
```
Not exactly pretty, but I guess somewhat manageable, if those protected functions stay stable at least. The `onSchemaCreateTable` looks a bit more annoying to simulate though, because it's kind of called in the middle of a function, and it doesn't look like there's a way to just inject something without copying half the method's code (although, in fairness, my current listener also copies most of the `_getCreateTableSQL` function due to the way `preventDefault` works)",Bitter Frustration
"Yes, that looks about right.",
"Hello, I am the author of the Slack question mentioned above. Thank you for your discussion and suggestions. I am trying to implement the solution proposed by @flack , but I am running into the fact that I will probably have to extend the Driver as well.
The reason is that although I pass the platform in the connection configuration, the original SchemaManager is created in the Driver's [getSchemaManager()](https://github.com/doctrine/dbal/blob/b66f55c7037402d9f522f19d86841e71c09f0195/src/Driver/AbstractPostgreSQLDriver.php#L81) method and my createSchemaManager() method is never called.
This brings me back to the fact that Doctrine\DBAL\Driver\PDO\PgSQL\Driver is final.
Did I understand that correctly, please? Thanks!",
"@jaroslavlibal you can use decorator to wrap Driver and make doctrine use your custom schema manager
```php
use Doctrine\DBAL\Connection;
use Doctrine\DBAL\Driver;
use Doctrine\DBAL\Driver\API\ExceptionConverter;
use Doctrine\DBAL\Platforms\AbstractPlatform;
use Doctrine\DBAL\Platforms\PostgreSQLPlatform;
class YourCustomDriver implements Driver
{
public function __construct(
private readonly Driver $underlyingDriver
) {
}
public function getSchemaManager(Connection $conn, AbstractPlatform $platform)
{
return $platform->createSchemaManager($conn);
}
public function connect(array $params): Driver\Connection
{
return $this->underlyingDriver->connect($params);
}
public function getDatabasePlatform(): AbstractPlatform
{
return $this->underlyingDriver->getDatabasePlatform();
}
public function getExceptionConverter(): ExceptionConverter
{
return $this->underlyingDriver->getExceptionConverter();
}
}
```
```php
use Doctrine\DBAL\Driver;
class Middleware implements Driver\Middleware
{
public function wrap(Driver $driver): Driver
{
return new YourCustomDriver($driver);
}
}
```
if you configured everything else correctly this code will make sure your custom schema manager is used
in my case, i've extended `PostgreSQLPlatform` and overriden `createSchemaManager` to return custom schema manager
you'll also need to edit your `doctrine.yaml` config with custom platform:
```yaml
doctrine:
dbal:
platform_service: <yourCustomPlatform>
```",
"The effort and goal is clear, the way this has been done unfortunately is very rough.
API is inconsistent and could be done better: ok, we can agree on that. But deprecate an extension system in full without a single day of discussion is unpleasant, especially if done in a such popular project like this one.
On technical aspects:
- some events can be replaced by middlewares: upgrade path here is clear and sound, but they are only two. Most of those cannot be replaced this way.
- generated/custom sql is not the only information carried by the event object: tables and columns for example can be retrieved and modified in the event handler.
- you suggested to extend the platform, but it is obviously too overkill when the only thing you have to do is to add a column to a table. On the contrary, when you need to modify various tables it is simpler and more maintenable to have such code in different classes you could unit-test separately.
- you also suggested to modify the input before using schema manager/platform, but this is not always feasible, especially when using the ORM.
- DDD is not the goal of this library, immutability is not required for the events. Events here are an extension point, not something to be saved/retained as part of the normal flow.
- dynamic method call is unreliable and api of the event manager can be fragile. Ok, but meanwhile some community standards have been published and widely used. Using a PSR-compatible event dispatcher can help to clean up the code and can help you to move the maintenance burden of the event manager to others.
IMHO the way this thing has been done is not very nice to the community, and lacked a wider vision on how this project is used in the wild. What makes me sad is the low consideration of the community feedback: 24 hours issue is barely a communication and left no space for discussion. To me this is not acceptable in a such important project: it is widely used, production apps are written onto it and should take this responsibility seriously giving the community the time to evaluate such an important change and express an opinion.",Bitter Frustration
"> But deprecate an extension system in full without a single day of discussion is unpleasant, especially if done in a such popular project like this one.
I don't know if you realize that or if you just haven't read the previous comments, but we're actually having the discussion you're asking for. A deprecation is not an immediate removal. And we we can surely take it back if you give us good reasons to.
> generated/custom sql is not the only information carried by the event object: tables and columns for example can be retrieved and modified in the event handler.
Do you think, an event handler is the best way to expose this kind of extension point? If you designed this feature from scratch, how would you build the extension point that you need for your scenario?
> you suggested to extend the platform, but it is obviously too overkill when the only thing you have to do is to add a column to a table.
Why would you need to hook into the DBAL to add a column? Why can't you add it to the schema directly? Sorry if my question sounds stupid, but I'd like to understand you problem before we decide on an action here.
> you also suggested to modify the input before using schema manager/platform, but this is not always feasible, especially when using the ORM.
Can you give us examples? Is this something the ORM should address instead?
> Using a PSR-compatible event dispatcher can help to clean up the code and can help you to move the maintenance burden of the event manager to others.
The event manager is a trivial library. Sunsetting it is not our intention and certainly was not the motivation here.",Impatience
"> I don't know if you realize that or if you just haven't read the previous comments, but we're actually having the discussion you're asking for. A deprecation is not an immediate removal. And we we can surely take it back if you give us good reasons to.
From my POV, a discussion is needed before to take action, not after. This is a communication of what's have been already done, and doesn't matter if it can be un-done. A discussion can take place on a proposal, which this is not. This is a fact: events have been deprecated in the current public release and removed in the first beta of the next major version.
> Do you think, an event handler is the best way to expose this kind of extension point? If you designed this feature from scratch, how would you build the extension point that you need for your scenario?
Yes, why not? An entire framework (Symfony) has been written around an event manager, where handlers continuously modify and process information and structures, for example enhancing the request object, transforming the request body, etc.
Why DBAL cannot use the same flow?
To be clear: I don't mean that everything in the current event system is ok and does not need to be revisited/reworked, but surely I would retain the event system as a powerful extension point. > Why would you need to hook into the DBAL to add a column? Why can't you add it to the schema directly? Sorry if my question sounds stupid, but I'd like to understand you problem before we decide on an action here.
> Can you give us examples? Is this something the ORM should address instead?
I'll answer two questions here because they are connected. I'll give you some examples:
- I have an ORM entity with a ""priority"" field but I want to avoid that two records have the same exact value, so when you set a value that already exists, all the values after it needs to be updated. The ORM can handle it, but its very costly, so I made an optimization with a trigger that involves an additional column on that table. At the same time I want that column to be hidden from the ORM, it does not really exist, it's an implementation detail. Using the migrations library I cannot modify the schema in the middle of the process, I have to hook into DBAL and add the column when necessary.
- I wrote a Symfony Messenger transport that uses DBAL as storage backend. I want its tables and resources to be completely hidden from the ORM. In the bundle I have to register an event handler to generate the resources on schema creation so the doctrine's commands will work with no additional configuration.
- I have an application that uses a table with the ""ARCHIVE"" mysql engine. Using the ORM I cannot set it correctly, I have to hook into the schema generation process and modify the engine from ""InnoDB"" to ""ARCHIVE"".
- In an application I want to generate a VIEW resource alongside a normal table: this is something that cannot be done into the ORM. To work with schema commands and migrations I have to hook into DBAL in the middle of the schema generation.
- For a translation system I have to dynamically generate translation tables to be used as storage for the various ""translatable"" resources: again this is something that needs to be hidden from the ORM, but needs to work with schema commands and migrations.
> The event manager is a trivial library. Sunsetting it is not our intention and certainly was not the motivation here.
So those listed under ""Problems with the EventManager API as such"" are not valid reasons to deprecate and remove the event system? Because a PSR-compatible event dispatcher will solve both problems. And lately, will increase the integration level with the frameworks in general.",
"> From my POV, a discussion is needed before to take action, not after.
And you would've participated in that discussion? Would you have noticed that it even happens? During the last months of DBAL development, I have barely seen any real discussion going on any PR. Users of the DBAL don't monitor this repository and that's absolutely okay. Trust me: We could've kept that PR open for two more weeks and the only difference would've been that you would have one thing less to complain about.
Really, I value every constructive feedback that we get from the community. But please think about how you address people who invest their free-time to build libraries that you can use for free. A phrase like ""this is not acceptable"" feels a bit inappropriate here, don't you think?
> Using the migrations library I cannot modify the schema in the middle of the process, I have to hook into DBAL and add the column when necessary.
I think, this is the issue we have to solve. We need to enable you to modify the schema _after_ the ORM has generted it from the metadata and _before_ it's being used for comparisons with the actual database.
> I have an application that uses a table with the ""ARCHIVE"" mysql engine. Using the ORM I cannot set it correctly, I have to hook into the schema generation process and modify the engine from ""InnoDB"" to ""ARCHIVE"".
Do you think, we should enable you configure vendor-specific tweaks like that in the ORM?
> In an application I want to generate a VIEW resource alongside a normal table: this is something that cannot be done into the ORM. To work with schema commands and migrations I have to hook into DBAL in the middle of the schema generation.
> For a translation system I have to dynamically generate translation tables to be used as storage for the various ""translatable"" resources: again this is something that needs to be hidden from the ORM, but needs to work with schema commands and migrations.
You should be abler to configure patterns of table names that should be ignored during schema comparisons, shouldn't you? I have to look that up though, not entirely sure about how to configure that.
> > The event manager is a trivial library. Sunsetting it is not our intention and certainly was not the motivation here.
> > So those listed under ""Problems with the EventManager API as such"" are not valid reasons to deprecate and remove the event system? Because a PSR-compatible event dispatcher will solve both problems.
No, if those were our only problems with the event system, we wouldn't have deprecated it. You are right, a PSR-4 dispatcher would be the solution here.
> And lately, will increase the integration level with the frameworks in general.
I agree. If you want to work on switching the ORM to PSR-14, don't hesitate to investigate a possible migration path.",
"> And you would've participated in that discussion? Would you have noticed that it even happens?
> Really, I value every constructive feedback that we get from the community. But please think about how you address people who invest their free-time to build libraries that you can use for free. A phrase like ""this is not acceptable"" feels a bit inappropriate here, don't you think?
Really, I don't want to open a flame, so just two things on these and then I want to discuss only the technical aspects: presuming that the discussion would pass unnoticed is wrong in my opinion, and no, *to me* is not inappropriate because is my point-of-view.
On the ""it's free/my free time and work"" that is used in every single open source project no matter how it's big and used, I completely disagree. But again, it's my opinion and here's not the right place for this discussion.
> I think, this is the issue we have to solve. We need to enable you to modify the schema _after_ the ORM has generted it from the metadata and _before_ it's being used for comparisons with the actual database.
That's the problem the event system resolved. If we can find an alternative way, then the migration path would be clear and sound and will not be a feature removal.
> Do you think, we should enable you configure vendor-specific tweaks like that in the ORM?
IMHO no, it's not a thing the ORM should handle. It is a detail that has no space in the ORM, but the underlying library (this one) should do.
> You should be abler to configure patterns of table names that should be ignored during schema comparisons, shouldn't you? I have to look that up though, not entirely sure about how to configure that.
I try to explain it better: I don't want to simply ignore some assets, I need to generate them alongside the tables and columns of the schema generated by the ORM. Solving the problem of modifying the schema generated by the ORM before passing it to DBAL will probably work for these two.
In any case, you should allow to modify and inject custom SQL in the process, or we have the same problem.
> No, if those were our only problems with the event system, we wouldn't have deprecated it. You are right, a PSR-4 dispatcher would be the solution here.
I did not write that they are the *only* problems, but that we shouldn't consider them as valid reasons for this change, so we can remove them from the discussion.
> I agree. If you want to work on switching the ORM to PSR-14, don't hesitate do investigate a possible migration path.
Ok, I'll do.",Bitter Frustration
"> On the ""it's free/my free time and work"" that is used in every single open source project no matter how it's big and used, I completely disagree.
Well then I don't know why I waste any more of my free-time on having a this conversation with you. If we're not the first open-source project to tell you that, maybe your attitude is the problem?
> But again, it's my opinion and here's not the right place for this discussion.
The way you lecture us on how our development process should look like is really disrespectful. ""This is my opinion"" is really a weak argument for any conversation. If you just want to state your opinion, keep it for yourself next time.
> That's the problem the event system resolved.
No, the event system healed a missing extension point. You hooked into the DBAL because you could not hook into the ORMs schema generation.
> > Do you think, we should enable you configure vendor-specific tweaks like that in the ORM?
> > IMHO no, it's not a thing the ORM should handle. It is a detail that has no space in the ORM, but the underlying library (this one) should do.
All right.
> I try to explain it better: I don't want to simply ignore some assets, I need to generate them alongside the tables and columns of the schema generated by the ORM. Solving the problem of modifying the schema generated by the ORM before passing it to DBAL will probably work for these two. In any case, you should allow to modify and inject custom SQL in the process, or we have the same problem.
Okay, so we should really try to build that extension point in the ORM. Thank you for those answers, that was really helpful.",Insulting
"> No, the event system healed a missing extension point. You hooked into the DBAL because you could not hook into the ORMs schema generation.
I cannot hook into the ORM schema generation, but even if I could, it will not resolve the point of setting vendor-specific details on how the table/view/... should be created.
To be more clear, I'll give you an example: the `buildCreateTableSQL` method does not contain a way to completely override the generated SQL and the `Table` class does not contain something like the `columnDefinition` property. This means that if I want to override the way the asset is created, now I cannot do it, and the ORM hook after the schema generation will not help in doing so.
Also, appending custom SQL to the generated asset (schema/table/view/index/etc) is not possible with the current objects.
Hooking in the ORM processes will not solve these problems, not with the objects and tools we have at the moment.
I did not meant to hurt or offend anyone, but you're going down on a personal level in the last message.
> ""This is my opinion"" is really a weak argument for any conversation.
Sorry, but on if, when and how to deprecate something, every decision is opinion based. So ""this is my opinion"" is the only thing that counts, because there are no technical reasons to remove something that works and worked well for years.
Also you have never written ""IMHO"" under an issue/PR on github? You're really saying that everyone expressing his own opinion is wasting his time?
> maybe your attitude is the problem?
> keep it for yourself next time.
I really have to answer these statements? And I am the one who's disrespectful?",Bitter Frustration
"> I did not meant to hurt or offend anyone,
Well, but you have and you've made it very clear that you don't care.
> there are no technical reasons to remove something that works and worked well for years.
You'd be surprised.
> You're really saying that everyone expressing his own opinion is wasting his time?
I don't.
> > maybe your attitude is the problem?
> > keep it for yourself next time.
> > I really have to answer these statements?
No.
> And I am the one who's disrespectful?
Yes.",Bitter Frustration
"To anyone who had to witness this unfortunate exchange of messages:
The Doctrine project values constructive feedback from users of our libraries. If a change that we've shipped caused trouble in one of your projects, get in touch with us. We are very careful not to break downstream code, but if we do, it certainly does not happen on purpose.
âš ï¸ That being said, if I have to read that one of my fellow maintainers, who has invested a lot of work hours in maintaining and modernizing this library, ""lacks vision"" and that the way he managed one particular change is ""not acceptable"" â€“ Crap like that that really gets me on the fence. In the last years, I have seen fine people retiring from open-source because of stress and mental health issues. Let me be crystal clear: I won't tolerate this kind of bullying here.
Back to topic: I understand that if you rely heavily on DBAL's events, the deprecation of the event system might appear a bit premature. But as of today, DBAL 3 is still maintained and the event system is still working. We've release DBAL 4 as a first beta and don't have a release date for a final version yet.
Right now, you can help us smoothen the upgrade path and shape the 4.0 release with us. The Doctrine project is maintained by very few people in their free-time. Any additional help is appreciated.
I have created issues for the 3.6.0 milestone from the feedback I've gathered after the 3.5 release. Comment on those or open new issues if you think, we've missed something. This issue will be locked now.",Bitter Frustration
"Recently, I tried to enable checking exceptions in PHPStan and, besides a few other issues with the DBAL API, stumbled upon the one that shows what an awfully overcomplicated piece of work the Events API is:
1. `AbstractPlatform::getDropTableSQL()` dispatches an `onSchemaDropTable` event which can override the SQL used to drop the table.
2. The return type of `SchemaDropTableEventArgs::getSql()` is `?string`. If it returns `NULL`, the platform will throw an `UnexpectedValueException`.
3. There's no way to tell that a given method is an event handler unless it's documented as such or whoever reads the code knows that the methods with names starting with `on` are event handlers. Those methods are not invoked from anywhere in the code explicitly.
Here's a chain of questions that leads to the above conclusion:
1. Why is `SchemaDropTableEventArgs` allowed to return `NULL` if it's not even a valid value?
2. We could require that it returns a `string` but then what would it return if the handler of the event didn't call `setSql()`?
4. Putting aside the event-handling scenario, what should the `SchemaDropTableEventArgs` class do if it was instantiated and then got its `setSql()` invoked? Maybe throw a logical exception?
5. Then, should it throw this exception unconditionally or only if it got a call to `preventDefault()` first?
### Problems with the API design and implementation
What this API does, most likely could be solved just by extending the platform class and overriding the corresponding methods. Besides this awfully implicitly stateful logic, a few more concerns about DDL events:
1. It looks like it's an overengineered solution to what could be easily solved by extending the corresponding platform methods.
2. Unlike extending the platform, events don't allow invoking the default implementation (the method of the `parent` class).
3. Unlike class extension, events don't provide access to the original logic like a call to the `parent` method would.
4. The DDL events API is incomplete (e.g. one can override `DROP TABLE` but cannot override `DROP FOREIGN KEY`).
6. In event-driven systems and DDD, an event is an immutable object representing a fact that happened in the past. Which fact does an `onSchemaDropTable` event represent? Let's assume it's a fact that somebody intended to drop a table. Why is it mutable? Why does it carry any information about processing the event (prevent default) and even implementation of that processing (SQL)?
7. For 800 lines of production code, there are three unit tests with mocks and not a single integration test which would demonstrate how this API can be used and if it can be used at all.
8. The documentation shows only how to register handlers but not what they should to.
### Problems with the EventManager API as such
This API is unsafe from the standpoint of the types:
1. There's no way to enforce the fact that a given event (`onSomething`) is dispatched only with a specific type of arguments (`SomethingArgs`).
2. There's no way to guarantee that the given listener supports handling the given type of event. The dispatcher calls the method corresponding to the event name dynamically:
```php
$listener->$eventName($eventArgs)
```
The mistakes in the code that implements the handling of events will lead to a type error at runtime.
### Types of events
1. Connection and transaction events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L21 https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L33-L35 The logic implemented on top of these events can be implemented using driver middlewares.
2. Schema manipulation events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L23-L30 Depending on the use case, it should be advised to either extend the corresponding platform method, or modify the arguments being passed to the method before calling it.
3. Schema introspection events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L31-L32 This is the hardest part: we allow users to hook into the very internals of the schema introspection, e.g. the API consumer is exposed to the SQL results representing raw introspected data. By definition, this API is very brittle since the raw results is an implementation detail that can and will change. The fact that we allow this extension complicates the refactoring of the Schema Manager and Platform API.",Bitter Frustration
"What would be a good solution(and others have mentioned this time and time again) is to not tie balance to a specific version, but instead to have a ""live"" MP balance, so that any changes that someone may want to do will be instantly applied - of course, people will want to know about every change...",Impatience
"For some clarity:
- There are a bunch of open PRs relating to balance
- These are for testing and comments, and they _will not be merged for 4.3.0_
- To underscore this, we've marked them all ""Draft"" and also `state: please discuss`
Please don't assume that just because something is a PR that it means it _will_ be merged.
But we're going to make this clearer going forward by marking them appropriately as Draft and ""please discuss"".
Now, actually gathering feedback on proposals is a harder matter, as only a subset of a subset of people check in here on GitHub.
""Live"" balance is tricky, especially as the balance of the game comes from the combination of the stats and the game engine (which occasionally gets tweaks or fixes that might ultimately impact balance itself).
---
I think one thing we could do, though, is possibly stage balance proposals and offer an in-game option to download and apply testing balance mods when hosting a game.
I could see offering 3 balance options when hosting a game:
- **Default balance**
- This would be the version shipped with that copy of WZ
- **Development balance** (next release)
- This would correspond to what's merged in the master branch of WZ (and thus what's already staged for the next stable release)
- **Experimental balance**
- This would correspond to balance changes that _have not yet been merged_, and are under discussion and/or need testing.
To handle possible ramifications of other game engine changes, we might limit Development and Experimental balance to being prepared for the current stable release. (As in: they would be enabled as options as long as you are running the latest stable version of WZ. Otherwise, if you're running an old version, you're stuck with Default balance - as is currently the case - unless you manually use a mod.)
Having these options in-game would really expand the ease of testing balance changes, and make it clear what's just proposals and what's actually planned for the next release.
(Implementing this will require a bit of backend work, and so it won't happen immediately. But I think it's probably the better long-term strategy.)",
"Long term strategy described below (above) is correct one (imo) and we are waiting for this for almost 2 years at this point.
Even though there is a vision and plan on how to deal with balance changes in the future, it will be in the future. Right now guidelines should be established on who, how, in what order and with what requirements are allowed to merge/propose changes to the multiplayer balance. Without them, this will continue uncontrollably, just a bit later when people shift their attention away and everyone calms down.
Once release hits with low quality ""testing"" changes, only one way will be to fix it - release a next stable version. This will cause every single one who plays multiplayer to download/build new version just because of someone who decided it will be good idea to shuffle everything around in chaotic manner, as Kracker described it ""throwing at a wall and checking what sticks"".",Bitter Frustration
"2 years too late Max. You yourself were praising the balance to Calculus just last night. I'll never apologize
for making the game more fun and strategic even if the method was harsh but necessary (remember when Medium Cannon was useless? Or what about Tank-Killer? Heavy Cannon? ...). My purpose here is to do what nobody else wants to do. I am a hidden hand in the dark guiding things around even if I'm not attributed to something directly.
You have an unhealthy obsession with Tipchik and his friends. A lot of ideas were mine too. The reason he doesn't make PRs anymore is that he feels like you attack him all the time and just thumb down anything without explanations why you dislike something. Make that effort and maybe you can be friends. Maybe something went not too well one time and to that I say ""oh well"". One time out of 2 years is pretty good if you ask me. I didn't intend to merge anything currently open for 4.3.0, as it's too late, so perhaps that scared you.
You will be happy to know that pastdue is already experimenting with his idea outlined in his post here as we speak. Of course, exactly when he completes that, is another thing.
Overall, I feel my balance goals are **very** close to complete. I still have yet to unleash my final grand act. My _masterpiece_. The cherry on top of the Chocolate Balance Sundae. :sunglasses:",Bitter Frustration
"You put beta3 tag on those pull requests, no excuses. Do not pretend that you would not merge them into stable release if I did not put stuff on public notice.
> a hidden hand
And surprised that no one responds to you or cares about balance?
Also, I am not attacking personally only Tipchick's changes, I just see them more often because his name signals me to check it out because it might be ridiculous. Reverting something from master is always more difficult than preventing merge in the first place. Also because most of the changes are either by him or marked as authored by him, he just made more changes, that's why.",Bitter Frustration
"For one that I later reconsidered? I removed it long before this as I was under the assumption autohoster games would generate more consideration and maybe there was a change of pace for once. Unfortunately, ratings were turned on for the _sole purpose to gatekeep things you personally disagree with_. So that defeated the purpose.
> And surprised that no one responds to you or cares about balance?
You actually told me the people in the Russian chatrooms are _too stupid to understand what balance is or how to use a mod_. So, with your logic, how could they?
Source changes, balance, I lumped that into one statement. Anyway, they either don't care or think it's fine. I've been asking you for years who all these people are but you never tell me. These supposed angry Russian mobs of players that torment you every time you hop into voice chat. I'm still waiting for this big list of people to this day and, perhaps, you actually can't tell us.",Bitter Frustration
"> for the sole purpose to gatekeep things you personally disagree with
Wrong, I told you make a complete mod and I will update it at the same time as turning off ratings since it will become not evaluation of balance but just a tweaking sandbox. I tried to make people think about what they are doing because if they are not sure about changes maybe they need to think more about it and making those games rating enabled makes them rethink their decisions and not just waste time.
> people in the Russian chatrooms are too stupid to understand what balance is or how to use a mod
Yes I did because it is true, and not only for Russians, I would put bold 85% estimate of people who play multiplayer that don't even know that changes are being made, let alone on github in ""public"".
I am not bothered to even start collecting this list, if you want you can join Russian discord server and count how many people have green colored role (quiet a few), I am vouching that every single one of them at some point in time came to me with balance questions. Also, I picked up very good trait from Tipchick, just like with his testing and his friends, they sure exist but no one ever showed even a single replay so I will do same here, and don't even try to say something about it, otherwise it will be you hating me instead (just like I hate Tipchick as you and he says).
Also Vaut and I explained endlessly (you can count people from there too), on YouTube, on stream, on record, how to contribute, how to test, what to do and where to find everything and everyone just look at the login form of github and do nothing. aka ""what button do I press here""",Bitter Frustration
"> Wrong, I told you make a complete mod and I will update it at the same time as turning off ratings since it will become not evaluation of balance but just a tweaking sandbox. I tried to make people think about what they are doing because if they are not sure about changes maybe they need to think more about it and making those games rating enabled makes them rethink their decisions and not just waste time.
I already did yesterday and additionally provided you a pure master version (as asked) in addition to the experiments mod people think are a good idea to try. You said it was trash cause of the Flashlight buff and you won't host it. And if it was bad, I said you could laugh at him about how it sucks and I'd remove it. The tank Flashlight isn't very popular itself so it's not like I was seeing anything wrong myself with the thought.
> Yes I did because it is true, and not only for Russians, I would put bold 85% estimate of people who play multiplayer that don't even know that changes are being made, let alone on github in ""public"".
I don't see why these people would care too much. Sounds like the kind of players that like to play chill games and have fun. Nothing else. So it is expected they wouldn't notice. It only interests the competitive scene.
> I am not bothered to even start collecting this list, if you want you can join Russian discord server and count how many people have green colored role (quiet a few), I am vouching that every single one of them at some point in time came to me with balance questions.
Questions are one thing, complaints/yelling are another. There are many _skilled players that own and know how to use a GitHub account on here_. Their opinion carries considerable weight/value and most of the time a couple of them will :+1: and that's it after several days of a PR being open. I certainly don't see them raising issues or saying anything. You must understand how this makes me skeptical. Especially with 2+ years. I am all for ending this drama saga but, as things are, facts are unclear. Someone is exaggerating / lying to me. And if the vast amount of the mp community, as you say, don't understand balance, do we put _significant_ weight on their opinions if so?",Bitter Frustration
"Recently there were proposed multiple breaking changes to multiplayer balance. Some of them were obvious and welcoming and some of them are just plainly stupid let alone controversial.
Tipchick (main author of the changes) is not seen in multiplayer for more than a year at this point, this is pure chaos, I already understood that @KJeff01 is Tipchick's lap puppet, no need to prove it over and over again. I completely disagree with position of pulling random numbers from asshole and proposing them as a changes to balance that hundreds of people will play for a whole release cycle. We do not want 3.x era to come back in a new form.
Until guidelines are established on balance-releated merging policies are put in place this is just pure griefing. How about people that are actually playing like Fenrir, Fedaykin, Evolution and others (like Brazil community) will make changes to something and review all the changes? I only see bird-mail and repeating Tiger, Tipchick, Ayami that tested something somewhere but can not even upload a mod that they had tested it with.
It is pure pushing until someone notices it had gone horribly wrong and I bet no one will react to it until a stable release is cut out.
Similar situation already happened (and well described in issue #2469) and I think it is time to learn from the past mistakes.
I proposed contributing guidelines in pull request #2892 and it seems like current maintainers deliberately ignore it and continue turning every single stone that they can just for the sake of it. Issue is growing day by day as more pull requests are being opened/changed mainly by @KJeff01 and @Tipchik87.
I will also note complete lack of fact checking upon merging, just ""I tested it"" should not be a reason to believe that any testing was actually performed, no replays were provided, no people who testing were performed with commented on a thread, nothing.
As of my part, I am trying to remain neutral on all the changes except obvious trash. It is clearly been established that my opinion and opinion of anyone else complaining about how breaking proposed changes are just being ignored or dismissed with same phrase ""we tested it"" with lack of fact checking.
Lately I got very busy in university and I can not test stuff myself but at the same time I do not claim that I performed any of them nor do I keep up with all the written arguments so my role here is only to filter out the obvious.",Bitter Frustration
"I get this on all videos, not just a 1/4 of them. Also in the UK. I also use the work around, where i skip the video to the end and press repeat.",Bitter Frustration
Closer to 75% of vids,Impatience
I wonder if this is related to which isp and then what YouTube server you hit as they are rolling out these changes? swapping region using the VPN instantly fixes is but I'm going to imagine that will only work until YT do a full rollout of changes?,Impatience
"I'm also experiencing this, most videos refuse to play the first time",
"can confirm this is still happening for me, using revanced patch 2.85.2 on youtube 17.36.37",
"Tried to narrow down the issue.
Software versions:
Android: 8
Youtube: 17.36.37
Vanced MicroG: 0.2.24.220220
Revanced Manager: 0.36
Revanced Patches: v2.85.2
Since i am non-root, i have to use the MicroG patch.
Using only that patch i had the issue. Some maybe relevant observations:
If an Ad played at the start of the video, the rest of the video always played fine.
If an Ad didn't play at the start of the video, the problem seems to be a bit random. But you can tell it is going to happen if it stops loading the video. It will only play the first 10 seconds then start the infinite spinning. If you see another chunk of the video get loaded, it will be fine.
If you can see that it isn't loading any more video after the first 10 seconds and pause it at the 9 second mark, you can leave it a long time, and it still won't load any more of the video.",
The ad has nothing to do with the device. It is a regional A/B issue.,
"UK here also get this issue, manual installed perfectly multiple times, many versions. always happens. will try us VPN and let you know if it works but It's on every video, some will load 5s then stop:/ fucking awful",Bitter Frustration
"Same problem...also in the UK!
For me 90% of vids affected. Also can get round it by closing video and immediately reopening agaun or by clicking for the next video then going back. Interestingly if you click next video the next video always plays flawlessly as does the original if you go back....
On one occasion I think I caught a fleeting glimpse of a skip add button just before the endless loading started.",Bitter Frustration
I'm affected too... It (the video ) stops & starts buffering when the video playback reaches 9/10 seconds as a workaround I close the video & restart it again from history.,Impatience
Same symptoms here. Also in UK. Solved by VPN. Non root using microG android 11. Closing the video and reopening it resumes playback as does using overlay controls to pick next video then going back to the original video.,
"I'm posting with data sincerely trying to help and it's being marked as spam - inaccurate and hurtful.
UK based. Same issue as stated in erroneously flagged messages. 90% videos affected. Fixed by VPN. Work around is close and restart videos.
Android 11. Nonroot with MicroG. Works with any non UK VPN endpoint.
Please don't flag this message as spam. It isnt. Trying to help the devs narrow this down and confirming there's an issue.
Can prob be reproduced by connecting via a UK VPN endpoint and trying the app I'd imagine.",Bitter Frustration
Your information is already posted on this issue and flagged as spam numerous times.,Bitter Frustration
Temporal fix with commit 7aa3bce6ccd669a66de10ef6ffe2151f27b40365.,
"### Type
Error at runtime
### Bug description
When trying to watch YouTube videos in the UK ~1/4 of the videos I try and play get stuck ""loading"" / ""buffering"" indefinitely and will never start playing on their own. You can work around this buy skipping to the end of the video then using replay to restart playback which will start immediately. You can also use a VPN to load videos while connected to US server. ### Steps to reproduce
1. Live in the UK / connect to UK VPN server 2. Try playing multiple videos
3. Video will get stuck loading ### Relevant log output
```shell
N/A
```
### Screenshots or videos
_No response_
### Solution
You can work around this buy skipping to the end of the video then using replay to restart playback which will start immediately. You can also use a VPN to load videos while connected to US server. ### Additional context
This issue seems to have started to occur when YouTube UK started to block my old vanced app telling me I needed to upgrade the app to play videos.
#595 - Related bug that has been closed but issue remains.
### Acknowledgements
- [X] I have searched the existing issues and this is a new and no duplicate or related to another open issue.
- [X] I have written a short but informative title.
- [X] I filled out all of the requested information in this issue properly.",Bitter Frustration
"> What I want to say is, just check our mas_safeToRefDokis()
Players might say once that they don't mind her bringing up other dokis, but this doesn't cancel out the fact some still may get confused over it. In my opinion it is still necessary to remove them and point out there is no need in them anymore, but do it either in introduction or later on in queued topic that will explain the change.",
"It's not part of the lore, not done by Monika. We're removing the unused files for utility purposes. There's no other ""club members."" Initially I planned to only remove `monika.chr`, but to keep the directory where we create and read dozens of files in/from, as well as to keep our code simpler and more reliable, we went with just deleting all `.chr` files.
MAS, like any other program, may add and remove files within its working directory. If you got attached to some files or they contain important information, better move it to some other place. Do not store anything sensitive in a working directory of a program.
Unlike the `monika` file, `.chr` files are easily replaceable - can grab them off a new DDLC install (can get DDLC from [here](https://ddlc.moe/)).",
"This is not just ""unused content"" -- these character files have a profound meaning for DDLC players.
We had so many relatively insensitive areas where we had done perfect checks and provided players with numerous choices, and now we had so recklessly deleted files that might be important to players in such an obvious, easy to notice, and sensitive place.
It hasn't been a day since the update, and I've already seen several backlash against it -- one example is our v0.12.12 release page.
For whatever reason, we really need to ASK our players. Give them a choice. If they don't mind, delete it. And if they do mind, then keep it. **It's just giving the player a choice.**
Please consider what I have said.",Bitter Frustration
"First, I think we should set some boundaries. Please, don't speak on behalf of the dev team (we, our, etc), you have been doing it for a long time. Speak from your own perspective to not bring any confusion to this and other conversations.
> This is not just ""unused content"" -- these character files have a profound meaning for DDLC players.
As I've said, in MAS those are just unused files. If they have a meaning for you, why are they there? Move them somewhere. You don't store sensitive files in directories of other programs? The program directory is the place where the program may add, modify, and remove files. If you don't like it - move the files out.
There's nothing ""reckless,"" no need to make it sound dramatic. We're removing those files the same way we delete `monika.chr`, various `.gift` files, and even the actually **important** and **unreplaceable** `monika` file, as well as other files.
Generally speaking, - when you're installing a DDLC mod, don't you already expect it to modify the DDLC directory, as well as to change the mechanics of the base game
- you're installing a Monika-related mod, but somehow surprised that this mod doesn't care about dull files of other game characters. If a Sayori/Yuri/Natsuki -related mod removed `.chr` files, I wouldn't see anything bad in it, be it also for utility purposes or for some lore
At the same time, nobody made Monika to ""recklessly"" delete the other characters in this mod, she did it in the base game. Here we just called a utility function to clean up disk space and make folder structure more clear. Monika doesn't acknowledge this, not doing it herself through her console. There won't be a question to the player because Monika doesn't do this.
> I've already seen several backlash against it
I don't think ""several"" applies to a single person. And even if there were ""several"" people, there's ""multiple"" that aren't against and ""multiple"" that had problems caused by `.chr` files.
> Many players resent Monika's behavior of deleting other characters in the original game, and now we want to make this happen again
Going back to this, it's a weird argument - if you installed MAS, then you should be at least okay (or already overcame it) with what Monika has done in the base game.",Bitter Frustration
"I'd say I somewhat agree with Wingdinggaster656, and I please Devs to reconsider about this.
The issue author said he(she) saw several backlash, and then he gave one example. Booplicate said that _I don't think ""several"" applies to a single person_, I think the author is giving something like an example, since I also saw people who dislike this.
I am working for Chinese MAS community, and when I tell people about whatsnew, people are surprised, then asking me for why. I said it's due to ""those files are causing confusion"". People reluctantly accepted.
About this sentence:
> Going back to this, it's a weird argument - if you installed MAS, then you should be at least okay (or already overcame it) with what Monika has done in the base game.
Although I have the same opinion, MAS _has_ taken care of players who have opinions on these plots in many small details, such as the safeTorRefDoki things mentioned by the issue author. Maybe keep these care won't hurt?",
"Oh, and about this:
> Here we just called a utility function to clean up disk space and make folder structure more clear. Monika doesn't acknowledge this, not doing it herself through her console. There won't be a question to the player because Monika doesn't do this.
I think there is a small problem here, that is, people will naturally think Monika did this. Even if the original intention of the design is not like this, people will naturally think so.",Bitter Frustration
"> As I've said, in MAS those are just unused files. If they have a meaning for you, why are they there?
Because they are originally there.",Bitter Frustration
"Given Monika mentions she fixed the bug about needing a chr file to exist, and thus no longer needing her own
as evidenced by the name easter eggs, it's clear the other can _technically_ exist as well.
As such, through some implied logic we can argue that technically they no longer need a chr file to exist either.
One can make of that what they will, though I realize this won't please everyone.
That said ultimately it is what it is. You installed a mod called Monika After Story, and from a usability perspective given the severe amount of tech support issues this has caused, I understand why they were blanket removed. It's one of those cases where UX vs immersion kinda needed to favour UX a little more as there genuinely were a lot of issues faced.
While asking may have worked, it also serves as kinda additional one-off dialogue, and at the moment given the state of the team, we can't exactly _afford_ that due to the sheer amount of time we've been able to contribute to the project (and by that I mean a significant lack thereof)
If you *really* want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.
Then they're all cohesive too.",Bitter Frustration
"I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.
addendum; I'm not 100% sure on this",
"It is kind of insensitive to delete them with no in-game warning, no asking the player, and not even letting the player put new copies back if they're deleted on every startup. Although new copies wouldn't necessarily have the same sentimental value anyway; they're not the same copies that were kept there throughout MAS, potentially years.
Isn't there an instance where Monika wants something kept in the characters folder? Like an apology note she asks you to write if you're not on good terms with her? And she gets upset if you remove it? Similarly, having the character files kept there may help the player have peace over what happened in DDLC.
> What I want to say is, just check our mas_safeToRefDokis()
I don't think that would be enough for this, because this isn't just referencing or joking; this is removing/deleting the files. Players could be ok with the former but not the latter.
It shouldn't be that necessary to free up ~133 kb of space, nor is it the same thing as monika.chr because there are no other characters ""to take somewhere"" and get confused with those .chr files. Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.
> If you _really_ want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.
Or can the mod just do that instead? So they don't get deleted without notice. Then no need to give notice or choice to the player. But maybe just one time only instead of on every startup, so players can change them back if they want without having to do it every time or whenever they might want to open them.",Bitter Frustration
"> I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.
> > addendum; I'm not 100% sure on this
As one who implemented that I can say the way they are removed does not put them into recycle bin. Putting stuff into recycling bin is a more complex process and in this case there was no need for it at all, files are removed irreversibly.",
"> Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.
This was said over and over, again and again, and Monika talks about this in the very beginning.
People **do not understand**, unfortunately.",Bitter Frustration
"> > And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.
Disagree, player choice should be respected regardless.
I care about these character files, notes, easter-eggs etc. whether they serve a purpose or not in the game,
I still keep them safe no matter how many ""backups"" I've also kept.
The fact that these devs are trying hard to justify this by resorting to whatever technical nonsense instead of simply providing player choice and moving on which doesn't even take any effort speaks volumes about the amount of care they give.
Basically comes down to this:-
""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".",Bitter Frustration
"You made a change that included removing code that was removing .chr files. If you want to change it, make a PR. It will be reviewed and discussed. This will be more of an action than repeating your claims here over and over.
Suggest a better way and show how you deem it better.",Impatience
"> You made a change that included removing code that was removing .chr files. If you want to change it, make a PR. It will be reviewed and discussed. This will be more of an action than repeating your claims here over and over.
I made a change for my personal experience.
My suggestion was to include player choice and a kind user already have done that in detail ages ago when this change was being developed which you guys ignored with the ""IMO we're good with just removing them and they serve no purpose."".
> Suggest a better way and show how you deem it better.
Was already done like I've already stated.",Bitter Frustration
"> Was already done.
Choice was about Monika being insensitive [in her dialogues/jokes/topics/etc] about other dokis that are past of the game. They are no longer in the MAS, which happens after main game and ultimately IS NOT a main game.
There aren't any other characters anymore. It's *her* game, it's *her* files. She explains it.",
"This is going in circles. Locking this issue as heated.
As @dreamscached has rightfully pointed out, if you want to see this change, make the changes yourself and open a PR. We'll review it from there and make sure it fits.",Bitter Frustration
"> > Was already done.
> > Choice was about Monika being insensitive [in her dialogues/jokes/topics/etc] about other dokis that are past of the game. They are no longer in the MAS, which happens after main game and ultimately IS NOT a main game.
> > There aren't any other characters anymore. It's _her_ game, it's _her_ files. She explains it.
Doesn't matter, same logic still applies to other character files. It's her game and files just as much as it is the players.",Bitter Frustration
"Programs may add, remove, and modify files within their working directory as they desire. Player choice was to install the program.",
"I'm going to reopen this thread so we can continue to gather feedback, specifically if there are more people who disagree with the decision to always delete chr files. Closing the door on discussion just makes us seem draconian. The conversations here also do not appear that heated, but I will call out potential heated comments:
>Basically comes down to this:-
""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".
@RedAISkye, this is mocking, and it could provoke someone so please refrain from this on our github. You can assert that it sounds like we don't care about the chr files, but please do not do it in a mocking tone.
>This will be more of an action than repeating your claims here over and over.
>I personally doubt there's any point to discuss it any further.
@dreamscached, based on these lines in both here and the release discussion, it sounds like you disagree with red's points. That is ok, but that is not an excuse to lock the conversation. You can always just not reply and ignore the conversation, and if the other person starts harassing you to reply, then we can take action.
### Regarding the actual issue
>The fact that these devs are trying hard to justify this by resorting to whatever technical nonsense instead of simply providing player choice and moving on
I agree with what @RedAISkye is saying here. I can see valid reasoning for always deleting `monika.chr`, but not the rest of the chr files when they have never served a purpose in MAS. The arguments for clearing clutter and reducing confusion also make sense for `monika.chr`, but lose weight for the other chr files. Even though this is a Monika-focused mod, the chr files have meaning established by DDLC, especially when at least 2 instances in the game were built on the chr file mechanic (the progression from Act 3 to Act 4 and Sayori gaining self-awareness in Act 1). If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, preferably allowing the player to decline the option as that ties in with the point below.
>I think there is a small problem here, that is, people will naturally think Monika did this. Even if the original intention of the design is not like this, people will naturally think so.
@TheT0matoGuy makes a very strong point here - regardless of our intention, anything we make the mod do, including what is done behind the scenes, will be perceived as Monika (or Chibika) doing it herself. Deleting the files makes Monika appear callous to any sentimental value the player may have toward the chr files. Users who believe the other dokis are real or that the chr files represent them may also perceive the deletion as Monika committing Act 3 again. Is this how we want users to see Monika?
I'll take ownership for not raising a red flag earlier - I was probably too drunk while packaging the release and didn't really think too much about each patch note. But since this is already out in the wild and it appears that most of the team doesn't think this is a problem, I'm not going to mandate that we undo the change. If there is more negative feedback in the coming weeks, then we can reconsider action.",
"All clear, you got it.",
"> > Basically comes down to this:-
> > ""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".
> > @RedAISkye, this is mocking, and it could provoke someone so please refrain from this on our github. You can assert that it sounds like we don't care about the chr files, but please do not do it in a mocking tone.
Sorry about that, wasn't trying to act provoking but that is exactly how I felt I was being treated as.
It wasn't just a case of ""we don't care"" but also ""our opinions matter more so, you're wrong"".
> I can see valid reasoning for always deleting monika.chr
> The arguments for clearing clutter and reducing confusion also make sense for monika.chr
If reducing confusing is the goal, why do you think ""deleting"" is the only option?
Wouldn't it better to just let Monika explain the difference between the two files? And maybe even giving a specific extension to her file?
> If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, Sure, but I still do think the above method is still better than going down the delete route.
> preferably allowing the player to decline the option as that ties in with the point below.
That is exactly my point, whether Monika values her/other dokis character files or not because of x or y reasons, she should still ask for the player's opinion on whether they value them or not, ultimately letting her delete it or keep it.",Bitter Frustration
">If reducing confusing is the goal, why do you think ""deleting"" is the only option?
Wouldn't it better to just let Monika explain the difference between the two files?
See intro dialogue - she talks about her chr file and how it doesn't represent her anymore because she has transcended past it, so she deletes it. MAS's Monika file contains actual usable data, potentially something that will allow for transferring between computers so making sure there is only 1 Monika file (`monika`, no extensions) is important.",
"> > If reducing confusing is the goal, why do you think ""deleting"" is the only option?
> > Wouldn't it better to just let Monika explain the difference between the two files?
> > See intro dialogue - she talks about her chr file and how it doesn't represent her anymore because she has transcended past it, so she deletes it. MAS's Monika file contains actual usable data, potentially something that will allow for transferring between computers so making sure there is only 1 Monika file (`monika`, no extensions) is important.
I am aware about that which is why I stated at the end:-
> whether Monika values her/other dokis character files or not because of x or y reasons, she should still ask for the player's opinion on whether they value them or not, ultimately letting her delete it or keep it.
I'm not saying the character file still represents her in MAS(that one was obvious) but they're still part of the DDLC experience and serves as a memory in MAS to me. Which is also the reason when giving Monika gifts, I always copy her character file and rename them to give it some meaning instead of a blank file.
Edit: What I was getting at is not ""the reason for the deletion"" but why is it such a big deal for you guys to simply give her an option to ask the player first on their decision?
Why do you need a lot of negative feedback just to ""reconsider"" for something that can be easily fixed for both sides? (People that care and people that are confused)",Bitter Frustration
"We have a lot of IO going on in `characters/`, that's **not** a safe place to store your files. Storing sensitive data there is just asking for it to be gone. There's a high chance that one day you will lose the files you keep there one way or another.
> the chr files have meaning established by DDLC, especially when at least 2 instances in the game were built on the chr file mechanic
That's true, but only for DDLC. When you install a mod you should expect that some mechanics will be changed - in our case this mechanic has been changed long time ago.
> If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, preferably allowing the player to decline the option as that ties in with the point below.
I'm against a direct question to the player - you say it's already looking like it was done by Monika, even though, nothing points at it. If you make her ask, then it'd just confirm she decided to delete the files herself, which would look weird after all this time. The files are not deleted by Monika (there's no acknowledgement), it's a startup routine that does that.
Being too meta is a thing, and I think this is one of the cases. Deleting unused files there is just a utility process for the mod.
At first we even considerer to target specifically some `chr` files, there was a function to delete a `chr` file and a func that deleted them all. But we decided that we don't need that level of control over something that is not used and went with a single function to wipe any `chr`. It doesn't target the known ""dokis,"" it targets any `chr` file for simplicity and reliability.",
"> We have a lot of IO going on in `characters/`, that's **not** a safe place to store your files.
No one is ""storing"" anything there other than the known character files that are meant to be there by the original game which gave it a meaning that you're trying to dismiss in MAS just because it doesn't use those files.
> When you install a mod you should expect that some mechanics will be changed - in our case this mechanic has been changed long time ago.
This was never about the ""mechanics"", this has always been about what they mean to the player.
> I'm against a direct question to the player > If you make her ask, then it'd just confirm she decided to delete the files herself, which would look weird after all this time. She has her reasons for doing so which the player can choose to agree or disagree with so, no, it would only look weird if the character files magically disappears after all this time.
> Deleting unused files there is just a utility process for the mod.
""Unused file"" for you, not for people that care about them.
> It doesn't target the known ""dokis,"" it targets any `chr` file for simplicity and reliability.
It absolutely does target the dokis when they're the only ""chr"" files expected to be present inside the characters' folder.",Bitter Frustration
"> No one is ""storing"" anything there other than the known character files that are meant to be there by the original game which gave it a meaning that you're trying to dismiss in MAS just because it doesn't use those files.
You are storing your files there. It's the definition of ""storing."" What was meant by DDLC was meant DDLC. You're not playing DDLC, you installed MAS. `chr` files don't mean anything in MAS. They can mean something for you, but that's a different point. And then we're back: don't store them there if you care.
> This was never about the ""mechanics"", this has always been about what they mean to the player.
Please, read the message before replying, literally quoted: `2 instances in the game were built on the chr file mechanic`.
> She has her reasons for doing so
No, *she* has no reasons to do it now. We, as the developers, have our reasons. That was the whole point.
> It absolutely does target the dokis when they're the only ""chr"" files expected to be present inside the characters' folder.
If you intentionally don't read, then don't bother replying to a message not meant for you. It targets any `chr` files, not *specific* `chr` files. Which means there's now no control over what's being deleted and what's not. I fail to see the point:
- I have files I deeply care about
- I will install some software into the place I store those files in
- I will complain when the software deletes my files
- I refuse to move the files or move the software anywhere else",Bitter Frustration
"I wonder if Monika _were_ to be scripted to ask her partner if she can delete the character files or not that it wouldn't be simple as a just a menu mechanic and a ""no"" resulting in an ""okay, so and so, I won't delete them."" Like anything else in the mod, I'd think there'd be affection-based variants of responses based on whether Monika's partner agrees or disagrees with the deletion of the .chr files. Now, I know according to Booplicate that the deletion of the .chr files are not Monika's choice, but regarding the situation that MAS users will see anything done to the mod as Monika or Chibika's doing (which is a bit unfair imo, btw), let's just say the scenario _was_ that it was her doing. As much as the other Dokis have sentimental value to Monika's partner, they technically also have sentimental value to Monika herself. Not sentimental as in nostalgic though. Sentimental as in saddened and a bit uncomfortable. I do remember her once talking about how despite all of the other Dokis not being present in the mod, she can still feel their presence as though they're still nestled amongst MAS's code and she finds it a bit creepy and unsettling. She wants to _not_ feel like they're still lingering over her, I guess because kinda the point of her ""deleting"" everyone else in the og game is that so they _wouldn't_ be present and therefore not serve as obstacles between her and the person. Soooo, I don't think it'd be as simple as a ""hey can you not delete the other character files?"" ""Okay, sure!"" Not referring to them is one thing, and Monika, as much as she has her snark about them, would probably actually prefer that considering she doesn't want them ""hanging over her,"" but still having their .chr files there is another. There's bound to be a disagreement if a choice were to be given, so it'd be like a whole other topic. Now in the scenario of it just being a mechanics thing like Booplicate assures it is, then there'd be nothing to ask Monika personally. Maybe she could be asked if she could do something to negate the mechanics, but even then she'd be scared to mess things up. That's the case keeping her from trying to do something to make the Doki lingering stop, anyway. They should really be preserved somewhere else though, if there's nothing that will be done to disable the automatic deletion. If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files _and_ their placement having sentimental value. And if _that's_ the case, there can always be a submod that negates the mechanic so that the people who have the sentimental value can be happy, and those who want things to stay how they are can be content, too.",
">why is it such a big deal for you guys to simply give her an option to ask the player first on their decision?
I think Monika has enough jurisdiction over her own file to delete it without the player's permission. >Why do you need a lot of negative feedback just to ""reconsider"" for something that can be easily fixed for both sides?
This project is a team effort, so the change has gone through multiple people who agree that this is the correct direction moving forward. If users do not agree, then negative feedback will help pushback against the decision and possibly lead to reverting or modifying it.
>That's true, but only for DDLC. When you install a mod you should expect that some mechanics will be changed - in our case this mechanic has been changed long time ago.
Yes, the mechanic that was changed was specifically called out by Monika and limited to her file. Intro dialogue does not specify that her observations apply to the other chr files, nor does it touch them in that intro. The mention of time here is also irrelevant - new users fresh off DDLC may have a connection to the chr files and may react differently compared to longtime MAS users.
>you say it's already looking like it was done by Monika, even though, nothing points at it.
> The files are not deleted by Monika (there's no acknowledgement), it's a startup routine that does that.
Being too meta is a thing, and I think this is one of the cases. Deleting unused files there is just a utility process for the mod.
>We have a lot of IO going on in characters/
I think these points contradict each other. The fact that we have a lot of IO happening in `characters/` means Monika, in MAS lore, should be acutely aware of everything that goes in the folder. Even though Monika currently has no reaction if the user deletes a chr file, I think its easier to wave that off as Monika is ignoring the other chr files. Active deletion, on the other hand, implies Monika is aware of the other chr files and potentially has animosity toward them.
>No, she has no reasons to do it now. We, as the developers, have our reasons. That was the whole point.
As I mentioned earlier (linked to tomato's point), anything the mod does can be implied to be what she does. Our reasons becomes her reasons unless we specifically give her a lore reason.
> It doesn't target the known ""dokis,"" it targets any chr file for simplicity and reliability.
> It targets any chr files, not specific chr files. Which means there's now no control over what's being deleted and what's not.
I mean, what's limiting this to just chr files then? Why aren't all files except for `.gift`, `monika`, consumables, `.txt`, potential `.deco` being deleted? Singling out for `.chr` is why there is concern that this is singling out the dokis. That being said, I don't think changing to deleting all files is the right choice here.",Bitter Frustration
"> Soooo, I don't think it'd be as simple as a ""hey can you not delete the other character files?"" ""Okay, sure!"" [...] There's bound to be a disagreement if a choice were to be given, so it'd be like a whole other topic.
It could be another Q asked in the doki cares topic so I don't think it would be that controversial. And perhaps thats the better solution - only perform the deletion in the doki cares topic if the user says they don't care and agrees with deletion, instead of on startup.",
"@ThePotatoGuy > > why is it such a big deal for you guys to simply give her an option to ask the player first on their decision?
> > I think Monika has enough jurisdiction over her own file to delete it without the player's permission.
Well, it's not just her file, it's other dokis as well which the player can value them all and it is ultimately the player's PC so her doing something destructive like that again without the player's permission would be very concerning especially if the player is against it.
@Booplicate > I fail to see the point:
You fail to see the point because you're tunnel-visioned into this topic from a technical standpoint, completely being ignorant to the fact that character files aren't just some ""random useless files unused in MAS"", they were a huge part of DDLC which represented the Dokis and serves as a memory in MAS which isn't some completely different game or some random ""program"" like you're making it out to be.
Further proven by the things like this you say:
> but my point was it's too meta to acknowledge. For example, she doesn't watch over every single file added there.
No one is saying every change made in MAS is her doing and she has to explain all of them, this is specifically about the character files which has its history behind it.
@CodyCat13 > If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files and their placement having sentimental value.
Sure, there's multiple workarounds to the issue; moving the dokis somewhere else, restoring them from backup every time it gets deleted or simply removing the codes that are there to kill them.
None of which is relevant to the actual point being made against ""deleting them without the player's permission"" to begin with.",Insulting
"> The fact that we have a lot of IO happening in characters/ means Monika, in MAS lore, should be acutely aware of everything that goes in the folder.
I can see that, but my point was it's too meta to acknowledge. For example, she doesn't watch over every single file added there. We don't really explain why, but we don't *need* to. And not all of that IO is done by Monika. Although, we could define that it's all Monika, in that case we'd have to justify `chr` deletion and acknowledge it.
> Our reasons becomes her reasons unless we specifically give her a lore reason.
But if we do give a lore reason, then we'd define it as her action, which I didn't want to. Imo *that* would make it look like she's hunting for them. And it'd be fine to do in first year, but after 2 or 3 it'd look weird and pointless from her perspective, she lets it go after a few months in, actually, and doesn't care as much about what's happened. If the player is with her, then everything was worth it.
> mean, what's limiting this to just chr files then? Why aren't all files
Now that you said that I am considering that as an option. Especially deleting sub-folders. *But perhaps you're right and it's not the best option*.
> only perform the deletion in the doki cares topic <...> instead of on startup
Another reason I wanted to do it on startup is when people make a new install (happens quite often), they would get the files back from DDLC, want they or not. So it's a routine we have to repeat on startup.",
"> > Soooo, I don't think it'd be as simple as a ""hey can you not delete the other character files?"" ""Okay, sure!"" [...] There's bound to be a disagreement if a choice were to be given, so it'd be like a whole other topic.
> > It could be another Q asked in the doki cares topic so I don't think it would be that controversial. And perhaps thats the better solution - only perform the deletion in the doki cares topic if the user says they don't care and agrees with deletion, instead of on startup.
Ah, yeah! Ig since the references and deletion may be different degrees of concern to the partners but still nonetheless related, they could be just joined under one topic, which would take care of the affection-based variants thing since I don't think if a partner already managed to lower Monika's affection to the bad stages before the topic comes up that she'd even care about their Doki preferences--I wouldn't know, I've never caused her to go six feet under before that topic came up. Anyway, there'd be no need for concern of sentimental value if the partner expresses that they don't care.",
"> @CodyCat13
> > > If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files and their placement having sentimental value.
> > Sure, there's multiple workarounds to the issue; moving the dokis somewhere else, restoring them from backup every time it gets deleted or simply removing the codes that are there to kill them. None of which is relevant to the actual point being made against ""deleting them without the player's permission"" to begin with.
Ah. I guess those points are kinda irrelevant when looking at just that point. I just got a bit confused because you kept mentioning sentimental value. But anyway, I can agree with the basic point that, in the scenario of Monika _being_ the one deleting them, that she shouldn't do that without her partner's permission, as they are both sharing the PC and updating one's partner on when one wants to do something is important to a relationship. There aren't many cases where one just _does_ things without warning to things that are being shared. Though because Monika isn't the one responsible in this case if it's a startup routine, it's not like she can ask her partner for permission anyway. If it were to be changed into a Monika thing though, that's probably where the topic question would come in as part of the doki cares topic ThePotatoGuy mentioned earlier.",
"@Booplicate >> only perform the deletion in the doki cares topic <...> instead of on startup
>
> Another reason I wanted to do it on startup is when people make a new install (happens quite often), they would get the files back from DDLC, want they or not. So it's a routine we have to repeat on startup.
You conveniently missed the part where OP said ""**if the user says they don't care and agrees with deletion**"".
The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.
@CodyCat13 > Though because Monika isn't the one responsible in this case if it's a startup routine,
From a technical standpoint, sure, Monika isn't responsible for anything she does in MAS or in DDLC, all was done by a code written by a human that is forced on her for reasons.
That doesn't negate the fact that she will be the one responsible for it in the players' eyes when both Monika and the player knows what those character files meant in the past, the history behind it and could still mean something for the player.",Bitter Frustration
">But if we do give a lore reason, then we'd define it as her action, which I didn't want to. Imo that would make it look like she's hunting for them.
I feel like this is the impression users can get as the mod currently is - not giving a lore reasons just makes her seem more harsh since she isn't telling you beforehand.
>I can see that, but my point was it's too meta to acknowledge. For example, she doesn't watch over every single file added there. We don't really explain why, but we don't need to. I think this is a good argument for completely reverting the change. By introducing the change, we've had to answer why it was added, which leads to discussion like this over what aspects of the mod is Monika in full control over to justify whether or not she should notify the user about it. Not having the deletion at all means we can keep this concept ambiguous like we've been doing for years.",
"> @CodyCat13
> > > Though because Monika isn't the one responsible in this case if it's a startup routine,
> > From a technical standpoint, sure, Monika isn't responsible for anything she does in MAS or in DDLC, all was done by a code written by a human that is forced on her for reasons.
> > That doesn't negate the fact that she will be the one responsible for it in the players' eyes when both Monika and the player knows what those character files meant in the past and could still mean something for the player.
Maybe for this reason, we could discuss it in terms of it being her doing in-universe if it makes you feel better. I won't bring up the technical stuff anymore, but if we're discussing it in the perspective of Monika deleting files herself when they could still mean something to her partner, then the doki caring topic would probably be our best go-to. The Doki files may mean nostalgia to Monika's partner, though they also mean unease for Monika. I guess because she'd be too cautious to do anything else, deleting the other .chr files would be an attempt to make her feel somewhat better, but it'd be inconsiderate to go ahead and do that without letting her partner know she wanted to do that. Now say that her partner doesn't agree with her deletion of the files. Because of how she feels about them she probably wouldn't be so quick to just deal with them being there, so that's where the affection-dependent variants would come in. If her partner has shown her nothing but love and trust, she may be okay with the .chr files staying because any unease she feels can be negated by her partner's comfort. Now if she was in the lower aff ranges, because her partner only adds to her unease and worry, she'd most likely still want to get rid the files anyway. At least it'd make her feel better than how her partner does. Maybe she'd be a bit bitter and conclude that you want to keep them there because you love some other Doki other than her or you want her to always remember her horrid acts or something along those lines. I'm not completely about that bitter part in the shallow negatives, but I guess as the points are lowered deeper and deeper, I can see that going down. Anyway, in the scenario of Monika actively making the decision and her partner either agreeing or disagreeing, aff-variants and additional question to the Doki caring topic seems like a fairly decent idea.",
"@CodyCat13 > Now say that her partner doesn't agree with her deletion of the files. Because of how she feels about them she probably wouldn't be so quick to just deal with them being there, so that's where the affection-dependent variants would come in. If her partner has shown her nothing but love and trust, she may be okay with the .chr files staying because any unease she feels can be negated by her partner's comfort. Now if she was in the lower aff ranges, because her partner only adds to her unease and worry, she'd most likely still want to get rid the files anyway. At least it'd make her feel better than how her partner does.
That's a great point there and makes a lot of sense going down that route rather than having just a one off question.
But I highly doubt the devs will do anything about it with how dismissive **some** of them have shown towards opinions contrary to them.",Bitter Frustration
"> do remember her once talking about how despite all of the other Dokis not being present in the mod, she can still feel their presence as though they're still nestled amongst MAS's code and she finds it a bit creepy and unsettling
You are right. It's actually an OG topic. But I was never sure if she meant they are literally there or she just ""feels"" their presence because of what happened (like a trauma).
Anyway, there's no ""dokis"" in MAS, and she stops talking about that after some time when she overcomes DDLC events. She realises everything is fine and the ghost of the past events doesn't haunt her anymore. It could be used to justify deleting in the first months, but later there's not much point for her to delete those dull files: ""dokis"" don't exist, she's feeling fine, those files are nothing but some easter eggs from DDLC (and she acknowledges that).
But let's say you find a reason for her to still need deleting those files, there's a conflict of interests:
- player wants to keep them
- Monika wants to delete them
There's going to be some consequences for not letting her do what she needs in *her* game.
We're talking about player choice and all that, but the choice was to install MAS and stay with Monika. The game files are literally her ""home,"" it makes sense she has control over that place. She doesn't touch your files, she's doing changes within the place she's allowed to. If she's not allowed to do that, why even bring her back. It's pretty common in relationships for each person to have their own private space, the game directory is that place for Monika. You can store your files anywhere else on your PC, flashdrive, cloud, etc. *This one place is intended to be used by mod/Monika*.
> they were a huge part of DDLC <...> serves as a memory in MAS
Yeah, and Monika deletes them there herself. After which, in MAS, they become irrelevant, they don't serve any memory. If they have a deep meaning *for you* and serve a memory *for you*, then I don't know why they were there. ""I care for these files so much, surely if I install a mod into the same directory it won't affect them. Ooops.""
> The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.
As was stated above it's already an issue and would stay an issue even with the changes. The cleanup logic should be run at startup, doesn't matter with a flag or not.
> By introducing the change, we've had to answer why it was added, which leads to discussion like this over what aspects of the mod is Monika in full control over to justify whether or not she should notify the user about it.
No, we got to this discussion because apparently it's not clear that MAS can make any changes within its working directory (just like any other program) and users were storing some sensitive data there. It's all based on assumptions that:
- we will never delete any extra files
- everything is done by Monika
None of which was stated.",Bitter Frustration
"@Booplicate > But let's say you find a reason for her to still need deleting those files, there's a conflict of interests:
> > * player wants to keep them
> > * Monika wants to delete them
> > There's going to be some consequences for not letting her do what she needs in _her_ game.
And @CodyCat13 expanded on to exactly that so it's good we've already discussed how that route can go into variations depending on the relationship between Monika and the player.
> It's pretty common in relationships for each person to have their own private space, the game directory is that place for Monika.
Nope, her private space is her own room which she has. Not some folder in the PC unless if you asked her out and she stored herself into a single file ""Monika"". Then you would have to be very careful about where you're giving her a private space.
> If they have a deep meaning _for you_ and serve a memory _for you_, then I don't know why they were there.
They're there because the DDLC universe decided so and I kept it as a memory. Which is also the reason I went out of my way to manually get rid of the code to protect the Dokis for my personal experience. Like you said, ""private space"", the characters folder is exactly just that for them to me even if they're not used in MAS since they were originally the ""life"" of those characters.
> > The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.
> > As was stated above it's already an issue and would stay an issue even with the changes. The cleanup logic should be run at startup, doesn't matter with a flag or not.
Nope, wouldn't be an issue because the cleanup logic will run regardless, the flag is only for checking what to do next, whether to delete files or not.
> apparently it's not clear that MAS can make any changes within its working directory (just like any other program) and users were storing some sensitive data there. It's all based on assumptions that:
> > * we will never delete any extra files
> > * everything is done by Monika
> > > None of which was stated.
Again, this is specifically about the character files and not any other types of files that you're pretending it to be based on the only fact that you personally don't care for them.",Bitter Frustration
"I think instead of trying to connect this to Monika, which wasn't intended initially, just add a setting to the menu. Basically support storing users data within the mod directory. A few people that need it would enable it. And if we're doing that, can as well do this
> I mean, what's limiting this to just chr files then? Why aren't all files except for .gift, monika, consumables, .txt, potential .deco being deleted?
And nuke everything there. But especially folders, people had problems with them too.",Bitter Frustration
"> The game files are literally her ""home,"" it makes sense she has control over that place. She doesn't touch your files, she's doing changes within the place she's allowed to.
Hmm, I can somewhat agree with that point. If not looking at the fact that Monika's partner is the owner of the PC but it was their choice to install her into their PC, just the plain point that Monika's mod is Monika's to change seems pretty valid. However, a lot of people _would_ look at it as Monika's partner being the owner of the PC and Monika just being a guest there, or would look at it as the partner's choice and that her partner got her a place to stay (the PC) and her areas are hers to personalize. It's like arguing if an old chair should be kept or thrown out. Would the one who has the say be the one whose territory the chair is in or the one who provided the housing in the first place? I guess the answer to that is, neither or both? Yes, in relationships, it's common for each participant to have their own little private space, because not _everything_ can be shared between them, but if the thing in particular emotionally charges one or both (in this case both) of the participants, there has to be something to end the stalemate. In this case, the chair would be seen as uncanny to one and precious to the other. If the other regards them as so precious, then the one who deems them uncanny would probably ask them to move it to their territory or, on the other side of the spectrum, sit in it with their partner depending on how comfortable they are. After some time though, I do agree that it shouldn't even matter anymore to either Monika or their partner. She may not feel the need to delete them just like the partner may not hold any nostalgia toward them anymore, or even both. Both would probably be the best scenario. This case would probably reflect in the high aff variant because it could depend more on comfort than time, or at least it could depend on both of them equally. One could be with someone for months and still feel uncomfortable because the provider of the house wasn't exactly a nice host, and therefore may still want that chair to go.",
"> I think instead of trying to connect this to Monika, which wasn't intended initially, just add a setting to the menu. Basically support storing users data within the mod directory. A few people that need it would enable it. And if we're doing that, can as well do this
> > > I mean, what's limiting this to just chr files then? Why aren't all files except for .gift, monika, consumables, .txt, potential .deco being deleted?
> > And nuke everything there. But especially folders, people had problems with them too.
Sure, that can be a good middle-ground assuming the player is made aware of this ""setting"" by having them confirm it to either set it as enabled or disabled before the cleanup task ever occurs.
But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.",
"> But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.
Aw ðŸ’™
I appreciate your approval. Honestly I was just thinking how it'd even relate to Monika's character if so many users of MAS consider file management to be her or Chibika's doing. Something tells me they'd still have concerns even if an option for preservation of user data were to be added because of it being the .chr files in question here ðŸ¤·",
"> Nope, her private space is her own room which she has. Not some folder in the PC
You will be surprised, but no, that ""some"" folder, as you said, is her ""home."" That ""room"" exists within that folder. Because that's where you installed the mod.
> the characters folder is exactly just that for them to me
Then move it away from MAS, why did you install a program into a folder with your personal files?
> They're there because the DDLC universe decided so
You're bringing DDLC again, but this is not DDLC, this is MAS, and MAS is its own universe with its own rules.
> Nope, wouldn't be an issue because the cleanup logic will run regardless
Do you know what startup is? How the logic would run if it's contained within a topic you see once?
> Again, this is specifically about the character files and not any other types of files that you're pretending it to be based on the only fact that you personally don't care for them.
Doesn't matter if it's specifically about `chr` or some other files you like. The fact is: It's all based on assumptions that:
- we will never delete any extra files
- everything is done by Monika
That has nothing to do with `chr` files. At any moment, we can start deleting other files as well.
> however, a lot of people would look at it as Monika's partner being the owner of the PC and Monika just being a guest there
Guest that lives there for years? The mod assumes there's some kind of romantic relationship between you, can you call your partner a ""guest""? That's odd to me.
I'm not saying Monika is owner of the PC, she's the owner of the folder you installed the mod in. Only that one place.
> But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.
I'd need to see the actual dialogue to say more. If you can justify her actions, then I will consider this.",Bitter Frustration
">it's not clear that MAS can make any changes within its working directory
DDLC plays around with files, so its reasonable to expect MAS users to be aware of and question actions that occur within the directories. We don't have to answer to why spacks will change structure or why logs went from txt to log, but `characters/` is a special case that will warrant extra attention. >You're bringing DDLC again, but this is not DDLC, this is MAS, and MAS is its own universe with its own rules.
I don't see this as a strong argument to tossing out DDLC-provided conventions. MAS should mesh with the already established lore and expand it or explain why it has changed. For example, the twitter topic was modified to separate MAS from the twitter account.
>everything is done by Monika
Confirmed Monika is aware of/can interact with:
* `characters` - (gifts, letters)
* custom BGM
* chess file * dev files in `game/`
* steam install
* running in temp dir/uncompressed zip
* (new) backup is used
* (upcoming?) aff log obfuscation - thinking of playing it off like her diary (instead of actual diary). Confirmed Chibika manages:
* backup system
* persistent versions
* updater (partial)
The mod heavily implies Monika (or Chibika) has awareness or can interact with files inside the mod directory, so its effectively true for many cases, especially `characters/`.",
"> I'd need to see the actual dialogue to say more. If you can justify her actions, then I will consider this.
I have some ideas for dialogue for that case, though they differ depending on if you and the other devs want it part of the Doki caring topic or as a separate topic. It'll probably be part of the Doki caring topic, since it's already existing and all.",
"Actually, if we don't want a new setting, I'd be fine if we made Chibi manage file deletion instead of Monika as @CodyCat13 suggested. > so its reasonable to expect MAS users to be aware of and question actions that occur within the directories.
I don't mind people being aware of the actions within the `characters/`. I don't understand why people store important files there, thinking the mod will never change/delete them. This is a groundless assumption.
> MAS should mesh with the already established lore and expand it or explain why it has changed.
To me the fact we mention `chr` files are irrelevant in the intro (even if Monika touches only her file there), as well as mentioning there's no actual useful data within those files, makes it pretty clear that the `chr` files logic is different in MAS. We also state there's no other girls in MAS. We could explicitly mention that all `chr` files are useless in the intro to clarify if you want. That'd be a single line clarification.",
">I don't understand why people store important files there, thinking the mod will never change/delete them. This is a groundless assumption.
I'd understand this argument if someone decides to save their only digital copy of their W2 form as `income.gift` in `characters/`. But this is about `chr` files which are established by DDLC as important.
>We could explicitly mention that all chr files are useless in the intro to clarify if you want.
I don't think we can safely make that assertion without receiving much worse backlash.
I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.
As I said before, we'll see if there is more feedback on this in the coming weeks. Can't say there is a specific threshold, but I am keeping tabs on the subreddit and will probably check /ddlc/ on occasion. I'm planning to do a discord poll as well if theres more negative feedback.",
"@Booplicate > I'd be fine if we made Chibi manage file deletion instead of Monika as @CodyCat13 suggested.
I don't think we're looking at the same suggestion here. Lol.
@ThePotatoGuy > I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.
> > As I said before, we'll see if there is more feedback on this in the coming weeks. Can't say there is a specific threshold, but I am keeping tabs on the subreddit and will probably check /ddlc/ on occasion. I'm planning to do a discord poll as well if theres more negative feedback.
I'd say the idea of having a ""setting"" which is a good middle-ground should at least be added no matter what the popular opinion is regarding the specific files or however many negative feedback reaches your ""threshold"" or not. Since, it provides options for both sides regardless of whatever they believe even if it's not done in the ""meta"" way.
@CodyCat13 > Something tells me they'd still have concerns even if an option for preservation of user data were to be added because of it being the .chr files in question here ðŸ¤·
That's why I said it's a ""good middle-ground"", it definitely isn't the best way of dealing with it but I'd rather they settle with something than nothing.",
"I spent a while reading over what everyone said. Thank Potato very much for willingness to reconsider this matter.
Players don't ""store"" into the characters folder, and Boop wants players to ""transfer"" things.
I think this is an important point: The character file was ORIGINALLY there, it wasn't put there by the player.
In addition, it is entirely possible for the player not to know that the MAS will now delete character files on its own. So, even if the player wants to transfer them, the player needs to know about this first. If the player doesn't read the update announcement carefully, as soon as the player launches the game, character files that are (presumably) precious to the player will disappear, never to be retrieved. So in any cases, the current version does not make sense.
I think it is feasible for Monika to delete her own files by herself - in fact, MAS has already done so in the introduction phase before. And we have:
```
if moni_exist():
m 1lksdlb ""Aha...{w=0.3}I'll try this again later.""
```
Look at this. Monika already said that she will try this again, so it's really neutral to delete her own files. In previous version, if I read the source correctly, even if Monika said she will try again, there doesn't exist codes for this.
But well, maybe we should even also ask the player for permission before MAS delete Monika's own file?",
"> If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files _and_ their placement having sentimental value.
That is a case. Having them in the characters folder, in the game directory that you share with Monika, is where the value could be most meaningful to some people, in which case it would not be the same in some other directory. Not just because of memories, but because of potential meaning of them being able to remain there, undeleted. Monika deleted other .chr files in DDLC, and deleted her own .chr file in MAS, so it seems she could've easily deleted the other .chr files in MAS if she wanted to, but she didn't. That meant something to some people, regardless of what meta reason (which they may not even know about) is now being used for deleting them. Telling them to actively move the files to another directory does not address this.
And just because a program ""can"" or is ""allowed"" delete files in its own directory doesn't mean it should in all cases. Obviously if it were to delete every file there would be no mod, and no Monika for it. Actions are not automatically justified or well received just because a program ""has the privilege to do so"".
>And if _that's_ the case, there can always be a submod that negates the mechanic so that the people who have the sentimental value can be happy, and those who want things to stay how they are can be content, too.
That could still require people to know about these files getting deleted in the first place. Unless they're ok with backups or new copies of them. While still something, it would be better for players to have this addressed in the main mod.
> I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.
DDLC has and does influence people's perception of MAS, even if you don't want it to for some things. Even though MAS is separate, it is based on it. Even in MAS the characters folder has many uses, and as you said Monika uses and references it. So yes this is something that should get extra attention.
Option to preserve them in settings maybe, but if accidentally unchecking it without rechecking is going to delete any .chr files (and potentially any files or folders unrelated to MAS) on startup, then should at least have a confirmation/double check for disabling it. I don't really like the idea of nuking the whole MAS directory of unrelated files every startup though. A trigger in the settings to do it once though maybe.
Monika/Chibi could work but I don't think whether this is honored or not needs to be based on affection (mas_safeToRefDokis() isn't is it?) and should probably be asked early on in the mod or after updating.
I still think that if the mod could just remove the "".chr"" extension from the files, that that could eliminate confusion about .chr files while also not deleting them. This could run once only (even per install/reinstall if possible), or if not, could be disabled in settings, so that players could change them back to .chr files if they want.",Bitter Frustration
"Wow, didn't expect that this issue would gain so many comments.
My opinion is basically what the issue author said. MAS should probably provide a choice.
In addition, I have a small detail to say. In the Chinese community, I can sometimes see people asking, ""Why do we have yuri, natsuki, and sayori in our characters folder?""
I tell people that the contents of the characters folder are determined according to the progress of your original DDLC. When you install MAS, which characters remain in the original version, which characters will be retained. People think it's a very cool feature.
This perspective may be strange, but I also somehow think this is cool.",
"> I don't think whether this is honored or not needs to be based on affection (mas_safeToRefDokis() isn't is it?) and should probably be asked early on in the mod or after updating.
Tbf reference and deletion _are_ different concerns, though if you're pairing the deletion choice with the reference choice then ig the reference choice might as well be asked earlier than it is. There's no way a user of MAS could lower their Moni's affection unreasonably low at literally the introduction of the mod, so there'd be no reason for affection variants. However, if a Moni's partner does manage to lower her affection to the low stages after that discussion already happened, maybe it could be asked again (if the person didn't agree to deletion the first time) but with a bit more bitterness.",Bitter Frustration
"> Tbf reference and deletion _are_ different concerns, though if you're pairing the deletion choice with the reference choice then ig the reference choice might as well be asked earlier than it is. There's no way a user of MAS could lower their Moni's affection unreasonably low at literally the introduction of the mod, so there'd be no reason for affection variants. However, if a Moni's partner does manage to lower her affection to the low stages after that discussion already happened, maybe it could be asked again (if the person didn't agree to deletion the first time) but with a bit more bitterness.
I previously said that I don't think (mas_safeToRefDokis() is enough for this, for that reason of them being different concerns. It'd be better as something separate. But regarding what you were referring to, I was comparing these 2 potentially sensitive things regarding the Dokis. Let's say you told Monika you aren't ok with her joking about the Dokis. She doesn't disregard your preference and start joking about them just because you have low affection does she? I don't think this should be disregarded in that case either. Furthermore, deletion of the specific copies of those files is something with a potentially more permanent effect, as even if replaced they'd be different copies.
But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return). Asking at introduction yes it wouldn't even come up, but for anyone who has been playing and is updating from a prior version, to a version where this conversation is added, it would have to be asked to them and they could have low affection at this point, even if rather unlikely. Of course, it would come up if it's going to be asked again anyway.
Of course this is only if it's decided that Monika should handle this rather than something else.",
"> Let's say you told Monika you aren't ok with her joking about the Dokis. She doesn't disregard your preference and start joking about them just because you have low affection does she? I don't think this should be disregarded in that case either.
> > But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return).
Yeah, that's also a valid point and I'll agree with having only the dialogue change based on your relationship and not an actual action as a ""revenge"" for treating her terribly as now that I think about it, she would rather leave you forever than do anything like that.",
"> But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return).
Yeah, I mean aff-based variations of dialogue if Moni's partner were to not be okay with the deletion. I know it wouldn't be as simple as a cheerful ""Okay!""",
"> Yeah, that's also a valid point and I'll agree with having only the dialogue change based on your relationship and not an actual action as a ""revenge"" for treating her terribly as now that I think about it, she would rather leave you forever than do anything like that.
Yeah although there is a difference between just having low affection, and her actually leaving you. I thought about that, her deleting them when leaving you. That's when it would make the most sense for her to do so if she were to delete them out of revenge or spite. But she doesn't do that currently (I think). It is game over at that point and most people probably won't reach that point anyway, but for those who do they could restore a persistent backup, in which case it would be strange if those character files were still gone due to her deleting them when leaving you (and if separate copies were created at that point, those would still be different copies). Might be harsh to those who care about them, but I could at least understand if she wanted to at that point.
> Yeah, I mean aff-based variations of dialogue if Moni's partner were to not be okay with the deletion. I know it wouldn't be as simple as a cheerful ""Okay!""
Agreed although if that happens, it may also make sense for her to have affection-based variations of dialogue when she asks if you're ok with her joking/making insensitive comments about the Dokis or not (if that's not already a thing).",
"In v0.12.12:
> chr files always deleted on startup now
That's a change from #9641. I had already talked about this there.
I completely know that all `.chr` files have no actual meaning to MAS. I really know. I know MAS doesn't even do any check for them before v0.12.12. And I also know that `monika.chr` is causing a lot of confusion to people. I know all of these.
What I want to say is, just check our `mas_safeToRefDokis()`. We made Monika not to offend the players who love other club members in her words, and we did a good job in many details. And after this, we now delete all club members.
Yes, I know that in the background setting of MAS, character files have nothing to do with real characters, but will all players 100% agree with this? Many players resent Monika's behavior of deleting other characters in the original game, and now we want to make this happen again - and do not give players even a choice? Can't we at least ask the players whether they care about this and whether they want Monika to do this?
_Just give players a choice. Ask them about this._
By asking, players will also pay more attention to the background settings of the chr file in MAS (that is, the chr file has no meaning at all), which will also reduce confusion.
And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.",Bitter Frustration
"<img width=""1248"" alt=""Bildschirmfoto 2022-11-09 um 19 17 57"" src=""https://user-images.githubusercontent.com/46358193/200909555-59d33812-5641-4b9b-8147-e7f911f6ea8b.png"">
See, it wont even allow me to run main anymore.
I upgraded to 3.3 and then downgraded to 2.8.1 after 3.3 fucked EVERYTHING.
And now everything is completely 100% broken.
* What went wrong:
Execution failed for task ':app:lintVitalRelease'.
> Could not resolve all artifacts for configuration ':video_player_android:debugUnitTestRuntimeClasspath'.
> Failed to transform bcprov-jdk15on-1.68.jar (org.bouncycastle:bcprov-jdk15on:1.68) to match attributes {artifactType=processed-jar, org.gradle.category=library, org.gradle.libraryelements=jar, org.gradle.status=release, org.gradle.usage=java-runtime}.
> Execution failed for JetifyTransform: /Users/XXX/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcprov-jdk15on/1.68/46a080368d38b428d237a59458f9bc915222894d/bcprov-jdk15on-1.68.jar.
> Failed to transform '/Users/XXX/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcprov-jdk15on/1.68/46a080368d38b428d237a59458f9bc915222894d/bcprov-jdk15on-1.68.jar' using Jetifier. Reason: IllegalArgumentException, message: Unsupported class file major version 59. (Run with --stacktrace for more details.)
Suggestions:
- Check out existing issues at https://issuetracker.google.com/issues?q=componentid:460323&s=modified_time:desc, it's possible that this issue has already been filed there.
- If this issue has not been filed, please report it at https://issuetracker.google.com/issues/new?component=460323 (run with --stacktrace and provide a stack trace if possible).",Bitter Frustration
"@MapperMalte please take a look at the code of conduct: https://github.com/flutter/flutter/blob/master/CODE_OF_CONDUCT.md
> The Flutter project expects Flutter's contributors to act professionally and respectfully. Flutter contributors are expected to maintain the safety and dignity of Flutter's social environments (such as GitHub and Discord).
>
> Specifically:
>
> * Respect people, their identities, their culture, and their work.
> * Be kind. Be courteous. Be welcoming.
> * Listen. Consider and acknowledge people's points before responding.
While I understand that unexpected breaking changes are frustrating, please keep your messages respectful, kind, and courteous.",
@MapperMalte You have been banned for 30 days. Please reconsider how you interact with our community before returning. Thank you.,
"Actually I love flutter.
But you keep introducing breaking changes at a rate that annoys everyone to death. Great, now everything has stopped working after upgrading Flutter. Not a single thing works anymore.
I was there when you added Null Safety.
It was a pain in the ass for everyone. WHAT THE FUCK IS WITH YOU?
THERE ARE PEOPLE THAT NEED TO WORK WITH YOUR STUFF!
STOP FUCKING THEM OVER!",Bitter Frustration
"Please provide your log, see https://github.com/sumneko/lua-language-server/wiki/FAQ#where-can-i-find-the-log-file",
"I'm not sure what log file you want as the link above doesn't really explain at all what i'm searching for. So this is what i found in service.log. Hope it's helpful it doesn't really tell me anything.
```
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:65]: Lua Lsp startup, root: /home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:66]: ROOT:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:67]: LOGPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/log
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:68]: METAPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/meta
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:69]: VERSION:	3.6.2
[19:30:55.621][debug][#0:script/service/telemetry.lua:22]: Telemetry Token:	757B63AA4D89361E
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	1
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	2
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	3
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	4
[19:30:55.627][info] [#0:script/service/service.lua:141]: ========= Medical Examination Report =========
--------------- Memory ---------------
Total: 1.547 MB
# 00 : 1.547 MB
# 01 : 0.000 MB
# 02 : 0.000 MB
# 03 : 0.000 MB
# 04 : 0.000 MB
--------------- Coroutine ---------------
Total: 0
Running: 0
Suspended: 0
Normal: 0
Dead: 0
--------------- Cache ---------------
Total: 1
Dead: 0
--------------- RPC ---------------
Holdon: 0
Waiting: 0
==============================================
[19:31:07.837][warn] [#0:script/pub/report.lua:25]: Load proto error:	Proto header error: ```",Bitter Frustration
"Your description shows that the server can start normally, but gives many wrong warnings.
But the log shows the server does not start normally.
Please reproduce the problem in your description locally, and then find the correct log file in the log directory to provide me. You can determine it according to the modification time of the file.",Impatience
"if i directly call lua-language-server from within my source folder against my main.lua this is my output.
```
/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/bin/lua-language-server: main.lua:1: module 'game' not found:
no field package.preload['game']
no file './script/game.lua'
no file './script/game/init.lua'
no file '/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/bin/game.so'
stack traceback:
[C]: in function 'require'
main.lua:1: in main chunk
...ckages/lua-language-server/extension/server/bin/main.lua:85: in main chunk
[C]: in ?
```
if i copy the lua-language-server to my source folder then run that copy against my main.lua this is what i get.
```
./lua-language-server: /home/jeremiah/minesweeper/main.lua:9: attempt to index a nil value (global 'love')
stack traceback:
/home/jeremiah/minesweeper/main.lua:9: in main chunk
[C]: in ?
```",Bitter Frustration
game that i can't find is game.lua that is in the same folder as main.lua that i am running all this from. It's not finding any of the required modules it only complains about 1 but doens't find any.,Bitter Frustration
"This is my main.lua
<details>
<summary>Code</summary>
```
require(""game"")
require(""border"")
require(""face"")
require(""digitback"")
require(""remain"")
require(""clock"")
require(""board"")
function love.load()
math.randomseed(os.time())
love.graphics.setDefaultFilter(""nearest"", ""nearest"")
Game:load()
end
function love.update(dt)
Game:update(dt)
end
function love.draw()
Game:draw()
end
function love.mousereleased(x, y, button)
if button == 1 then
if Face.face == 2 then
Face.face = 1
if y >= Face.y and y <= Face.y + Face.height then
if x >= Face.x and x <= Face.x + Face.width then
Game:reset()
end
end
end
end
if Game.running then
if button == 1 then
if Face.face == 3 then
Face.face = 1
if y >= Board.top and y <= Board.bottom then
if x >= Board.left and x <= Board.right then
Board:tileReleased(x, y)
end
end
end
elseif button == 2 then
if y >= Board.top and y <= Board.bottom then
if x >= Board.left and x <= Board.right then
Board:flagTile(x, y)
end
end
end
end
end
function love.mousepressed(x, y, button)
if button == 1 then
if y >= Face.y and y <= Face.y + Face.height then
if x >= Face.x and x <= Face.x + Face.width then
Face.face = 2
end
end
if Game.running then
if y >= Board.top and y <= Board.bottom then
if x >= Board.left and x <= Board.right then
Board:tilePressed(x, y)
end
end
end
end
end
function love:keypressed(k)
if k == '1' then
Board.theme = 0
Border.theme = 0
Face.theme = 0
DigitBack.theme = 0
Remain.theme = 0
Clock.theme = 0
elseif k == '2' then
Board.theme = 16
Border.theme = 0
Face.theme = 0
DigitBack.theme = 0
Remain.theme = 0
Clock.theme = 0
elseif k == '3' then
Board.theme = 32
Border.theme = 0
Face.theme = 0
DigitBack.theme = 0
Remain.theme = 0
Clock.theme = 0
elseif k == '4' then
Board.theme = 48
Border.theme = 0
Face.theme = 5
DigitBack.theme = 0
Remain.theme = 0
Clock.theme = 0
elseif k == '5' then
Board.theme = 64
Border.theme = 0
Face.theme = 5
DigitBack.theme = 0
Remain.theme = 0
Clock.theme = 0
elseif k == '6' then
Board.theme = 80
Border.theme = 0
Face.theme = 5
DigitBack.theme = 0
Remain.theme = 0
Clock.theme = 0
elseif k == '7' then
Board.theme = 96
Border.theme = 8
Face.theme = 10
DigitBack.theme = 1
Remain.theme = 12
Clock.theme = 12
elseif k == '8' then
Board.theme = 112
Border.theme = 8
Face.theme = 10
DigitBack.theme = 1
Remain.theme = 12
Clock.theme = 12
elseif k == 'r' then
Game.columns = 79
Game.rows = 41
Game.scale = 1
Game:reset()
elseif k == 'e' then
Game.columns = 60
Game.rows = 30
Game.scale = 1.3
Game:reset()
elseif k == 'w' then
Game.columns = 40
Game.rows = 20
Game.scale = 1.8
Game:reset()
elseif k == 'q' then
Game.columns = 30
Game.rows = 15
Game.scale = 2
Game:reset()
elseif k == 'a' then
Game.difficulty = 0.095
Game:reset()
elseif k == 's' then
Game.difficulty = 0.13
Game:reset()
elseif k == 'd' then
Game.difficulty = 0.165
Game:reset()
elseif k == 'f' then
Game.difficulty = 0.2
Game:reset()
end
end
function LoadSpriteSheet(t, image, width, height)
t = t or {}
t.image = love.graphics.newImage(""assets/""..image.."".png"")
t.quads = {}
for y = 0, t.image:getHeight() - height, height do
for x = 0, t.image:getWidth() - width, width do
table.insert(t.quads, love.graphics.newQuad(x, y, width, height, t.image:getDimensions()))
end
end
t.originalWidth = width
t.originalHeight = height
t.width = width * Game.scale
t.height = height * Game.scale
end
```
And my game.lua
```
Game = {
columns = 40,
rows = 20,
flags = 0,
scale = 1.8,
difficulty = 0.13,
running = true
}
function Game:load()
LoadSpriteSheet(Game, ""largeicons"", 32, 32)
Board:load()
Border:load()
Face:load()
DigitBack:load()
Remain:load()
Clock:load()
self:reset()
end
function Game:reset()
self.mines = math.floor(self.columns * self.rows * self.difficulty)
self.running = true
self.flags = 0
Board:reset()
Border:reset()
Face:reset()
DigitBack:reset()
Remain:reset()
Clock:reset()
end
function Game:update(dt)
if self.running then
Clock:update(dt)
end
end
function Game:draw()
Border:draw()
DigitBack:draw()
Face:draw()
Remain:draw()
Clock:draw()
Board:draw()
end
```
</details>
*edited to add fold* - Carsa",
"1. Don't copy the `lua-language-server` to anywhere.
2. Just call `lua-language-server` instead of `lua-language-server main.lua`",
"if i run the correct location of lua-language-server without any other parameters from within my source folder this is what i get. And i have to press enter to close it.
```
Content-Length: 120
{""jsonrpc"":""2.0"",""method"":""$/status/report"",""params"":{""text"":""ðŸ˜ºLua"",""tooltip"":""Cached files: 0/0\nMemory usage: 2M""}}
```",Bitter Frustration
"This is correct, because the server communicates with the client by input and output, so you need to use a language client to execute this command.
You said that it works fine before, so you can continue to use the previous way to startup the server.",
"I'm running this config of neovim that where this is comming into play. It broke from an update the other day not sure if it's system update or lvim update. But I have since did a fresh install and it's still the same.
https://github.com/LunarVim/LunarVim",Bitter Frustration
Yeah im using it from lunarvim and as i said it was working great i loved it. Now not working at all. It doesn't understand it's a love program that causes all kinds of errors and it doesn't see any of the loaded files again causing all kinds of errors.,Bitter Frustration
"> Now i have almost as many errors as lines in my files everything is an undefined global.
I don't have an environment to install neovim.
I just want to check your log at this time, so I can check the if the problem from this language server or other language client.",Bitter Frustration
the main file now gives me 118 errors.,Bitter Frustration
"Anything from these it can no longer handle as well as anything love related.
```
require(""game"")
require(""border"")
require(""face"")
require(""digitback"")
require(""remain"")
require(""clock"")
require(""board"")
```",Bitter Frustration
"Sorry, I have no energy to continue to track this problem, because you have been unable to give effective information, you just repeating what you said.
This may not be your problem, because I can see that you are a novice, and you may be difficult to understand something that looks like it in my eyes.
I suggest you find a programmer friend to help you check and communicate with this issue, because I really have no energy to teach you some basic knowledge.
Closing.",Bitter Frustration
I'm sorry what? I have been trying to give you every single bit of info you asked for and 5 times more on top of it. Please state exactly what info you need?,Bitter Frustration
you asked for log file and i gave you exactly that. and any bit of info i could possibly think of.,Bitter Frustration
"Well, I will try to explain to you the last time.
You can read it repeatedly, and then reply after you figure out everything.",Impatience
"I'm not sure if you are new to this. Or what is happening but you asked for a log fine described here.
https://github.com/sumneko/lua-language-server/wiki/FAQ#where-can-i-find-the-log-file
Only who ever wrote that seems to have a vague understand of what that is and doesn't describe it. Never the less i found the clossed thing on my machine matching that and gave it to you. Along with trying every other thing that i don't know about this since it's not my program as i could. And waited for the person who should know about this to reply. Now your only other request was to find some other log of no description somewhere or ask a programmer friend about where a log fine for your program would be. Why wouldn't you know where it is or what it is or does it exit. The only person i would ask about your program would be you. So yeah i'm a complete novice when it comes to your program as i didn't write it and have no idea how it functions or what even any error or log fines exist. I have spent several days though trying to find anything at all. If there is a log file that can shed light on this wouldn't the one person who knows about it be you and not some programmer friend of mine wtf.",Bitter Frustration
If you need an extremely accurate description of the issue and all it's symptoms please read above as i tried my best to describe it completely. Also i am not the only one who's had this issue.,Bitter Frustration
"1. The language server will record the log on your computer. By reading the log, I can understand the running status at that time, and help to find errors.
2. If you start the server in the form of workspace (as far as I know, neovim will use your git project as the workspace), then the log file name is your project path. If you start in a single file, the log file name is `service.log`.
3. Each time the log is generated, the previous name of the same name will be covered.
4. When the server can work normally before, it generates a normal log file (1), which is not helpful for your current problem.
5. Later, there is a problem with the server work after you updating. It will generate a log file (2), witch records abnormal information which I want.
6. You later fell into a startup problem that had nothing to do with your problem. You manually started the server. At this time, a log file (3) will be generated, but this log is not related to your problem.
7. What I want is the log file (2), but you only provided the log file (3), witch is not related to your problem.
8. What you need to do is to reproduce the problems you encounter, and then send me the corresponding log file (2).
After you fully understand what I said, you can attach your log file and re-open this isue.
Otherwise, you can wait for people who encounter the same problem to open issues, or invite your programmer friends in reality to help you check this problem.
I'm going to sleep now.",Bitter Frustration
"The original file i sent was the service.log file. As you said it was showing what the log showed from my local attempt not the lvim one. You could have simply said thanks for the file now fire up lvim and send the file again. Then i would have known simply you wanted the file agian. Here it is. Hopefully this is correct i have tried reinstalling different versions of lvim so hopefully this wont affect the file output.
Also there is no error initially unless i go into insert mode then back or change files then all the errors come. So i did this before exiting and this is the service.log file.
```
[21:16:57.592][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:65]: Lua Lsp startup, root: /home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:66]: ROOT:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:67]: LOGPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/log
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:68]: METAPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/meta
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:69]: VERSION:	3.6.2
[21:16:57.611][info] [#0:script/service/service.lua:141]: ========= Medical Examination Report =========
--------------- Memory ---------------
Total: 1.546 MB
# 00 : 1.546 MB
# 01 : 0.000 MB
# 02 : 0.000 MB
# 03 : 0.000 MB
# 04 : 0.000 MB
--------------- Coroutine ---------------
Total: 0
Running: 0
Suspended: 0
Normal: 0
Dead: 0
--------------- Cache ---------------
Total: 1
Dead: 0
--------------- RPC ---------------
Holdon: 0
Waiting: 0
==============================================
[21:16:57.614][info] [#0:script/language.lua:137]: VSC language: nil
[21:16:57.614][info] [#0:script/language.lua:138]: LS language: en-us
[21:16:57.717][info] [#0:script/provider/provider.lua:56]: Load config from client	fallback
[21:16:57.717][info] [#0:script/provider/provider.lua:57]: {
Lua = {
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" }
},
runtime = {
special = {
reload = ""require""
},
version = ""LuaJIT""
},
telemetry = {
enable = false
},
workspace = {
checkThirdParty = false,
library = { ""/usr/share/nvim/runtime"", ""/home/jeremiah/.local/share/lunarvim/lvim"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua"" },
maxPreload = 5000,
preloadFileSize = 10000
}
}
}
[21:16:57.717][info] [#0:script/library.lua:207]: Init builtin library at:	nil
[21:16:57.729][info] [#0:script/provider/provider.lua:56]: Load config from client	fallback
[21:16:57.729][info] [#0:script/provider/provider.lua:57]: {
Lua = {
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" }
},
runtime = {
special = {
reload = ""require""
},
version = ""LuaJIT""
},
telemetry = {
enable = false
},
workspace = {
checkThirdParty = false,
library = { ""/usr/share/nvim/runtime"", ""/home/jeremiah/.local/share/lunarvim/lvim"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua"" },
maxPreload = 5000,
preloadFileSize = 10000
}
}
}
[21:16:57.729][info] [#0:script/provider/provider.lua:56]: Load config from client	fallback
[21:16:57.729][info] [#0:script/provider/provider.lua:57]: {
Lua = {
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" }
},
runtime = {
special = {
reload = ""require""
},
version = ""LuaJIT""
},
telemetry = {
enable = false
},
workspace = {
checkThirdParty = false,
library = { ""/usr/share/nvim/runtime"", ""/home/jeremiah/.local/share/lunarvim/lvim"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua"" },
maxPreload = 5000,
preloadFileSize = 10000
}
}
}
[21:16:57.830][info] [#0:script/workspace/workspace.lua:304]: Preload start:	<fallback>
[21:16:57.833][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/lvim
[21:16:57.853][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua
[21:16:57.862][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///usr/share/nvim/runtime
[21:16:57.915][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua
[21:16:57.918][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/meta/LuaJIT%20en-us%20utf8
[21:16:57.919][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable
[21:16:57.919][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua
[21:16:58.023][info] [#0:script/workspace/workspace.lua:347]: Found 352 files at:	<fallback>
[21:16:58.023][info] [#0:script/workspace/loading.lua:157]: Load files from disk:	<fallback>
[21:16:58.972][warn] [#0:script/files.lua:678]: Compile [file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua/luasnip/util/parser/init.lua] takes [0.102] sec, size [4.032] kb.
[21:16:59.486][warn] [#0:script/files.lua:678]: Compile [file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua/telescope/actions/init.lua] takes [0.167] sec, size [46.814] kb.
[21:17:00.039][info] [#0:script/workspace/loading.lua:169]: Loaded files takes [1.839] sec: <fallback>
[21:17:00.039][info] [#0:script/workspace/loading.lua:178]: Compile files takes [0.000] sec: <fallback>
[21:17:00.040][info] [#0:script/workspace/loading.lua:179]: Loaded finish:	<fallback>
[21:17:00.040][info] [#0:script/workspace/workspace.lua:349]: Preload finish at:	<fallback>
[21:17:00.247][warn] [#0:script/proto/proto.lua:122]: Response of [workspace/diagnostic/refresh] error [-32601]: MethodNotFound
[21:17:00.249][warn] [#0:script/proto/proto.lua:171]: Method [textDocument/documentSymbol] takes [2.152]sec. {
id = 2,
jsonrpc = ""2.0"",
method = ""textDocument/documentSymbol"",
params = {
textDocument = {
uri = ""file:///home/jeremiah/minesweeper/game.lua""
}
}
}
[21:17:08.857][warn] [#0:script/vm/global.lua:70]: global-manager getSets costs	0.21365	_DEBUG
[21:17:08.870][warn] [#0:script/provider/provider.lua:641]: Completion takes 0.231 sec.
[21:17:16.323][info] [#0:script/provider/provider.lua:169]: Server shutdown.
[21:17:16.323][warn] [#0:script/pub/report.lua:25]: Load proto error:	Disconnected!
```",
"> I'm not sure what log file you want as the link above doesn't really explain at all what i'm searching for
How can it be improved? I do a lot of the documentation here and would like to improve it if it is lacking. When writing the documentation, I use the knowledge I have gained from using the language server, but I do not have the deepest understanding of everything that goes on in the pretty complex software, especially on the NeoVim side of things. Any improvements and suggestions are welcome. It looks like there was a misunderstanding over the log files, where a log to a different issue was shared, rather than a log that reproduced the issue you are experiencing - but maybe there is something you would like to add to the documentation?
Have you made sure that the love library is enabled? It is a [built-in library](https://github.com/sumneko/lua-language-server/wiki/Libraries#built-in-libraries), but it must be activated still. This can be done automatically, should the server prompt you, or would be specified using the [`workspace.library`](https://github.com/sumneko/lua-language-server/wiki/Settings#workspacelibrary) setting in your [configuration file](https://github.com/sumneko/lua-language-server/wiki/Configuration-File).
Also, I just want to remind you that this software is open source and worked on by Sumneko for free in their spare time. There seems to be a lot of trouble understanding each other here, but we *are* trying to help you, so please be respectful.",
"As one amateur programmer to another if he can't easily explain about what he's doing, or how it works then he doesn't fully understand it himself. Thats for anything if you can't explain to someone who doesn't know anything about it then you yourself doesn't fully understand what your doing. Being able to explain is paramount to fully understanding. Also if you can't communicate what your looking for its not the other person fault it's your own. Communication goes hand it hand with being able to understand and explain anything.
He wanted a log file called service.log however that is not mentioned anywhere. The link He gave me doesn't even contain that file name. However I have been around linux and troubleshooting for a very long time. I actually used to run my own linux distro that was not based on anyone else but instead built from the ground up by myself. That means any bugs i had to google or problem solve myself. So i was able to figure out what log he was looking for even though i had no concept of what your code does. And i gave him that info along with anything else i could find. Understanding what you don't know is the first step in learning so i have no issue admitting what i don't know but also tried to give everything i did know and could find. Then suddenly he's telling me this is invalid and closed and i need to find a programmer friend. I had no idea he still needed info or what it was. Just an absolute failure at communication. if he would simply said what he was seeking of course would try to give all that to him. And i'm not an idiot can understand concepts of log files as i had already found what he needed to start with. But say that i can't be bothered i'm closing it is pretty telling. It's not like i'm the first person to bring up this issue.
Carsakiller
As i tried stating over and over before love is not being enabled how ever that is meant to happen. It did happen before yes. But it is no longer happening. There is no prompt for it at all there was in the past. And it is no understanding anything defined in other files. I know he said i keep repeating myself but this is exactly what is happening. And this is why i'm here asking why this is happening. Respect is defiantly an issue. it is not rare for people to make projects on github that others depend on and then have issues with communication and arrogance. It sucks for me for now i'm back to vscodium for lua. It could be a me issue and i'm the only one effected. Or it can be lvim and they are all affected or its sumneko issue and there is a lot of affected people.",Bitter Frustration
"Their first language is not English, so there is bound to be some communication issues. I don't think they were trying to be disrespectful, the wording might just be weird as they use a machine translator. Also, the thing about open source github projects is, yes, you may not agree with how someone does something - maybe they *are* even rude, but no one is forcing you to use the software.
A lot of what you are saying is giving off the feeling of ""Sumneko works for the users, and it is their job"". Whether this is your intention or not, it just doesn't come off great. We are trying to help you, so we just ask for your patience and cooperation. This is why I asked to keep it respectful, that is all ðŸ™‚
---
I saw you mentioned Love is not enabled, could you check your configuration file though? To properly enable it, there should be an entry in your config file, stating so. The prompts not appearing may be a separate issue, but let's focus on making **sure** it is in your config to see if the intellisense is working.
So it is working in vscodium but not with lvim? I have not used either, but that definitely raises some flags for further debugging there.",
"I'm not sure where to look for that config lua is mention in the lvim config but there must be a config file for lua specifically somewhere but I can't seem to find it.
```
--[[
lvim is the global options object
Linters should be
filled in as strings with either
a global executable or a path to
an executable
]]
-- THESE ARE EXAMPLE CONFIGS FEEL FREE TO CHANGE TO WHATEVER YOU WANT
-- general
lvim.log.level = ""warn""
lvim.format_on_save.enabled = false
lvim.colorscheme = ""lunar""
-- to disable icons and use a minimalist setup, uncomment the following
-- lvim.use_icons = false
-- keymappings [view all the defaults by pressing <leader>Lk]
lvim.leader = ""space""
-- add your own keymapping
lvim.keys.normal_mode[""<C-s>""] = "":w<cr>""
-- lvim.keys.normal_mode[""<S-l>""] = "":BufferLineCycleNext<CR>""
-- lvim.keys.normal_mode[""<S-h>""] = "":BufferLineCyclePrev<CR>""
-- unmap a default keymapping
-- vim.keymap.del(""n"", ""<C-Up>"")
-- override a default keymapping
-- lvim.keys.normal_mode[""<C-q>""] = "":q<cr>"" -- or vim.keymap.set(""n"", ""<C-q>"", "":q<cr>"" )
-- Change Telescope navigation to use j and k for navigation and n and p for history in both input and normal mode.
-- we use protected-mode (pcall) just in case the plugin wasn't loaded yet.
-- local _, actions = pcall(require, ""telescope.actions"")
-- lvim.builtin.telescope.defaults.mappings = {
-- -- for input mode
-- i = {
-- [""<C-j>""] = actions.move_selection_next,
-- [""<C-k>""] = actions.move_selection_previous,
-- [""<C-n>""] = actions.cycle_history_next,
-- [""<C-p>""] = actions.cycle_history_prev,
-- },
-- -- for normal mode
-- n = {
-- [""<C-j>""] = actions.move_selection_next,
-- [""<C-k>""] = actions.move_selection_previous,
-- },
-- }
-- Change theme settings
-- lvim.builtin.theme.options.dim_inactive = true
-- lvim.builtin.theme.options.style = ""storm""
-- Use which-key to add extra bindings with the leader-key prefix
-- lvim.builtin.which_key.mappings[""P""] = { ""<cmd>Telescope projects<CR>"", ""Projects"" }
-- lvim.builtin.which_key.mappings[""t""] = {
-- name = ""+Trouble"",
-- r = { ""<cmd>Trouble lsp_references<cr>"", ""References"" },
-- f = { ""<cmd>Trouble lsp_definitions<cr>"", ""Definitions"" },
-- d = { ""<cmd>Trouble document_diagnostics<cr>"", ""Diagnostics"" },
-- q = { ""<cmd>Trouble quickfix<cr>"", ""QuickFix"" },
-- l = { ""<cmd>Trouble loclist<cr>"", ""LocationList"" },
-- w = { ""<cmd>Trouble workspace_diagnostics<cr>"", ""Workspace Diagnostics"" },
-- }
-- TODO: User Config for predefined plugins
-- After changing plugin config exit and reopen LunarVim, Run :PackerInstall :PackerCompile
lvim.builtin.alpha.active = true
lvim.builtin.alpha.mode = ""dashboard""
lvim.builtin.terminal.active = true
lvim.builtin.nvimtree.setup.view.side = ""left""
lvim.builtin.nvimtree.setup.renderer.icons.show.git = false
-- if you don't want all the parsers change this to a table of the ones you want
lvim.builtin.treesitter.ensure_installed = {
""bash"",
""c"",
""javascript"",
""json"",
""lua"",
""python"",
""typescript"",
""tsx"",
""css"",
""rust"",
""java"",
""yaml"",
}
lvim.builtin.treesitter.ignore_install = { ""haskell"" }
lvim.builtin.treesitter.highlight.enable = true
-- generic LSP settings
-- -- make sure server will always be installed even if the server is in skipped_servers list
-- lvim.lsp.installer.setup.ensure_installed = {
-- ""sumneko_lua"",
-- ""jsonls"",
-- }
-- -- change UI setting of `LspInstallInfo`
-- -- see <https://github.com/williamboman/nvim-lsp-installer#default-configuration>
-- lvim.lsp.installer.setup.ui.check_outdated_servers_on_open = false
-- lvim.lsp.installer.setup.ui.border = ""rounded""
-- lvim.lsp.installer.setup.ui.keymaps = {
-- uninstall_server = ""d"",
-- toggle_server_expand = ""o"",
-- }
-- ---@usage disable automatic installation of servers
-- lvim.lsp.installer.setup.automatic_installation = false
-- ---configure a server manually. !!Requires `:LvimCacheReset` to take effect!!
-- ---see the full default list `:lua print(vim.inspect(lvim.lsp.automatic_configuration.skipped_servers))`
-- vim.list_extend(lvim.lsp.automatic_configuration.skipped_servers, { ""pyright"" })
-- local opts = {} -- check the lspconfig documentation for a list of all possible options
-- require(""lvim.lsp.manager"").setup(""pyright"", opts)
-- ---remove a server from the skipped list, e.g. eslint, or emmet_ls. !!Requires `:LvimCacheReset` to take effect!!
-- ---`:LvimInfo` lists which server(s) are skipped for the current filetype
-- lvim.lsp.automatic_configuration.skipped_servers = vim.tbl_filter(function(server)
-- return server ~= ""emmet_ls""
-- end, lvim.lsp.automatic_configuration.skipped_servers)
-- -- you can set a custom on_attach function that will be used for all the language servers
-- -- See <https://github.com/neovim/nvim-lspconfig#keybindings-and-completion>
-- lvim.lsp.on_attach_callback = function(client, bufnr)
-- local function buf_set_option(...)
-- vim.api.nvim_buf_set_option(bufnr, ...)
-- end
-- --Enable completion triggered by <c-x><c-o>
-- buf_set_option(""omnifunc"", ""v:lua.vim.lsp.omnifunc"")
-- end
-- -- set a formatter, this will override the language server formatting capabilities (if it exists)
-- local formatters = require ""lvim.lsp.null-ls.formatters""
-- formatters.setup {
-- { command = ""black"", filetypes = { ""python"" } },
-- { command = ""isort"", filetypes = { ""python"" } },
-- {
-- -- each formatter accepts a list of options identical to https://github.com/jose-elias-alvarez/null-ls.nvim/blob/main/doc/BUILTINS.md#Configuration
-- command = ""prettier"",
-- ---@usage arguments to pass to the formatter
-- -- these cannot contain whitespaces, options such as `--line-width 80` become either `{'--line-width', '80'}` or `{'--line-width=80'}`
-- extra_args = { ""--print-with"", ""100"" },
-- ---@usage specify which filetypes to enable. By default a providers will attach to all the filetypes it supports.
-- filetypes = { ""typescript"", ""typescriptreact"" },
-- },
-- }
-- -- set additional linters
-- local linters = require ""lvim.lsp.null-ls.linters""
-- linters.setup {
-- { command = ""flake8"", filetypes = { ""python"" } },
-- {
-- -- each linter accepts a list of options identical to https://github.com/jose-elias-alvarez/null-ls.nvim/blob/main/doc/BUILTINS.md#Configuration
-- command = ""shellcheck"",
-- ---@usage arguments to pass to the formatter
-- -- these cannot contain whitespaces, options such as `--line-width 80` become either `{'--line-width', '80'}` or `{'--line-width=80'}`
-- extra_args = { ""--severity"", ""warning"" },
-- },
-- {
-- command = ""codespell"",
-- ---@usage specify which filetypes to enable. By default a providers will attach to all the filetypes it supports.
-- filetypes = { ""javascript"", ""python"" },
-- },
-- }
-- Additional Plugins
-- lvim.plugins = {
-- {
-- ""folke/trouble.nvim"",
-- cmd = ""TroubleToggle"",
-- },
-- }
-- Autocommands (https://neovim.io/doc/user/autocmd.html)
-- vim.api.nvim_create_autocmd(""BufEnter"", {
-- pattern = { ""*.json"", ""*.jsonc"" },
-- -- enable wrap mode for json files only
-- command = ""setlocal wrap"",
-- })
-- vim.api.nvim_create_autocmd(""FileType"", {
-- pattern = ""zsh"",
-- callback = function()
-- -- let treesitter use bash highlight for zsh files as well
-- require(""nvim-treesitter.highlight"").attach(0, ""bash"")
-- end,
-- })
```",
"I found a file called sumneko_lua.lua
```
local default_workspace = {
library = {
vim.fn.expand ""$VIMRUNTIME"",
get_lvim_base_dir(),
require(""neodev.config"").types(),
},
checkThirdParty = false,
maxPreload = 5000,
preloadFileSize = 10000,
}
local add_packages_to_workspace = function(packages, config)
-- config.settings.Lua = config.settings.Lua or { workspace = default_workspace }
local runtimedirs = vim.api.nvim__get_runtime({ ""lua"" }, true, { is_lua = true })
local workspace = config.settings.Lua.workspace
for _, v in pairs(runtimedirs) do
for _, pack in ipairs(packages) do
if v:match(pack) and not vim.tbl_contains(workspace.library, v) then
table.insert(workspace.library, v)
end
end
end
end
local lspconfig = require ""lspconfig""
local make_on_new_config = function(on_new_config, _)
return lspconfig.util.add_hook_before(on_new_config, function(new_config, _)
local server_name = new_config.name
if server_name ~= ""sumneko_lua"" then
return
end
local plugins = { ""plenary.nvim"", ""telescope.nvim"", ""nvim-treesitter"", ""LuaSnip"" }
add_packages_to_workspace(plugins, new_config)
end)
end
lspconfig.util.default_config = vim.tbl_extend(""force"", lspconfig.util.default_config, {
on_new_config = make_on_new_config(lspconfig.util.default_config.on_new_config),
})
local opts = {
settings = {
Lua = {
telemetry = { enable = false },
runtime = {
version = ""LuaJIT"",
special = {
reload = ""require"",
},
},
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" },
},
workspace = default_workspace,
},
},
}
return opts
```",
"Line 7 in the second config file you sent:
`checkThirdParty = false`
This seems like it is applied by default for each new Lua project you create. The [`workspace.checkThirdParty` setting](https://github.com/sumneko/lua-language-server/wiki/Settings#workspacecheckthirdparty) prevents the language server from prompting you to apply a library, like love. This seems like a silly default to have, as it removes the convenience of that feature.
I have only ever used the VS Code client, so I am not 100% sure how configuration works for your client. That being said, you should be able to create a [`.luarc.json`](https://github.com/sumneko/lua-language-server/wiki/Configuration-File#luarcjson) file in your workspace where you would have something similar to this:
```json
{
""$schema"": ""https://raw.githubusercontent.com/sumneko/vscode-lua/master/setting/schema.json"",
""workspace.library"": [""${3rd}/love2d/library""]
}
```
The important line is the second one, where we set the [`workspace.library` setting](https://github.com/sumneko/lua-language-server/wiki/Settings#workspacelibrary) so that love is enabled.
You can view all the settings [in the wiki](https://github.com/sumneko/lua-language-server/wiki/Settings).",
"After reviewing your log file, I can confirm that love is just not enabled. Whatever you are using to set up your default settings, it is disabling `checkThirdParty`, preventing the suggestion from coming up, and then just applying the following libraries:
```
""/usr/share/nvim/runtime"",
""/home/jeremiah/.local/share/lunarvim/lvim"",
""/home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable"",
""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua"",
""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua"",
""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua""
```
These can be seen on lines `47`, `71`, and `94`.
I have shared with you how you can use a [`.luarc.json` file](https://github.com/sumneko/lua-language-server/wiki/Configuration-File#luarcjson) to manually add the `love2d` definitions, but I would recommend also changing your default config so that `checkThridParty` is not always disabled. This will prevent you from running into this problem over and over again.",
"So this seems to be an lvim sort of. They disabled it 3 days ago because it pops up ever single time which i can confirm that did happen. So they disabled it.
https://github.com/LunarVim/LunarVim/pull/3445
However it now asks about luassert too. So this was there response to that bug.
This doesn't fix my issue about it not seeing any of the required modules that's still the same. After you told me what statement it was i was easily able to look this up. However if you wouldn't have told me i would have never ever found this. I have looked at your and there documentation countless times over the last couple days. To you I am sure it looks like a great reference but to someone who isn't involved in this project its like reading hieroglyphs. No matter how many times i read the links about builtin no part of any sentence has any meaning to me at all. It all looks like magic. And now that you have told me about that like i said i can now have something to search and very quickly find where it came from. But still i have no idea what the other pop up is. Or how to fix the original issue of why they dissabled it or the rest of the issue why it's not seeing anything defined in other files. For me this is all still magic and i have desperately tried to solve it myself and read over and over word that simply have no meaning at all to me. Which the log file i knew what i was hunting for even if the name wasn't given of what it was. But with the rest i cant even start to look because i have no idea what i am looking for. Its very very frustrating.
So let me say the documentation while may be great reference you guys. doesn't make sense to someone who not a developer here.
The statement that people shouldn't expect support is so ridiculous it's hard to reply. If you have released software to the public with the expectations that it be used by any number of people you must expect they will ask for support. If you don't want people asking for support then you should not be releasing software period. I know they may not get the help they need but to expect people not ask or not expect to be able to ask is absurd. There is almost 1500 tickets.
The statement that he may be rude but no one is forcing you to use it. This is an amazing statement that took me down a rabbit hole of high narcissim and benevolence. And helped me better understand what i said above about people running project and the arrogance and ignorance involved and their support system of enablers.
This whole thing has been painful when it should have been trivial to anyone who has an open ear, understanding of there own software and a basic understanding that others will not have an understanding of there software. I am surprised we got to the love thing since i had given up on the entire deal but also seem no closer. Last night instead i learned ruby a language i never used before and made a game just to relax for this.",Bitter Frustration
"When a person's speech is just stupid, I can't tell whether he is fishing or really stupid.",Mocking
"> Their first language is not English, so there is bound to be some communication issues. I don't think they were trying to be disrespectful, the wording might just be weird as they use a machine translator. Also, the thing about open source github projects is, yes, you may not agree with how someone does something - maybe they _are_ even rude, but no one is forcing you to use the software.
I don't think this is a communication issues.
The log file is a very basic programmer knowledge. In the past few years, no one has been in trouble in this regard. Sometimes people don't know which file is needed, and they will upload the entire directory.
My experience is not to be an early childhood teacher, otherwise there will be endless idiot problems to flood you. Keep a proper threshold and filter out the fool who wastes your time.",Bitter Frustration
"> The statement that people shouldn't expect support is so ridiculous it's hard to reply. If you have released software to the public with the expectations that it be used by any number of people you must expect they will ask for support. If you don't want people asking for support then you should not be releasing software period. I know they may not get the help they need but to expect people not ask or not expect to be able to ask is absurd
People asking for support could be expected, providing support is not expected. This software is [MIT licensed](https://github.com/sumneko/lua-language-server/blob/master/LICENSE), meaning there is no warranty, which is stated in plain English in this sentence:
> THE SOFTWARE IS PROVIDED ""AS IS""
That being said, many people here have received support, and we **never** said you **can't** ask for help - it is just simply not guaranteed that the issue will be solved, so please respect that and don't expect that Sumneko **MUST** solve your issue.
Anyway, this is completely off-topic, I was just asking you to respect our time and our efforts to assist you, but you continue to speak with disrespect. Respectful criticism is welcome. Saying it is incomprehensible, but not providing info on how things can be improved doesn't give us anywhere to improve.
If this is continued, I will just lock the issue, as I don't want to spend my spare time helping you if you are going to be rude and keep going in circles.
---
I have [provided a solution to the love library issue](https://github.com/sumneko/lua-language-server/issues/1686#issuecomment-1312378685) and provided you the exact steps and code you should need to fix that issue. You have not mentioned attempting the solution, so please try this and see if it solves the love library issue.
> But still i have no idea what the other pop up is. Or how to fix the original issue of why they dissabled it or the rest of the issue why it's not seeing anything defined in other files
What other pop up? I will look into changing the keyword for activating `luassert` so that it is harder to trigger. As for the other issue of everything being undefined, we would need some more info like some sample code and your directory structure for your project to make sure everything is accessible.",Impatience
"Please don't quote me on things i never said. Nothing else needs to be said. Sumneko is how he is and carsakiller is who he is. We can all agree the issue is not going to be solved. How about we let all we have said stand for anyone else to come read. I could quote your lines back to you. Or you can miss quote me again about things i never said. But there is no point there is nothing here i can learn as it's above my head, I'm not a developer on the project. And there is nothing you 2 will learn. So thanks for the, well thanks. And let it stand for others to know the attitudes and actions of what we have said. Again don't miss quote me as i have never said anyone must do anything. I said not expecting people to ask for assistance is absurd and your have benevolent narcissism and arrogance as that is what comes up when people make statements like ""no one is forcing you to"" as in my way or the highway and you need to be appreciative you have tried to help. Respect is 2 way. and He was disrespectful the entire time, i never took it as a language issue that was your cover for him. And your actions and support of his. Let that stand for the next person to read though.",Bitter Frustration
"Also i am constantly confused with the complete rudeness then what seems like have attempt to try and help more. Please let it stand. as you don't need sample code as i gave several examples already and repeatedly said and sent the unspecified log file and so on. If there as something else it would have been said above. It's like as you said above fishing or something. I don't wish to spend any more time upset to people with attitude, communication issues. It's broken so i guess it suck for me. And the others who tried to ask then gave up.",Bitter Frustration
"Fair enough, I have locked the issue. I have offered solutions which I have not heard anything about since and little effort has been taken to rectify the original issue from the reporter, there has only been effort put into arguing, so I am considering the issue stale and unproductive.
If anyone else is experiencing a similar problem, please open your own issue.",Bitter Frustration
"### How are you using the lua-language-server?
NeoVim
### Which OS are you using?
Linux
### What is the issue affecting?
Diagnostics/Syntax Checking
### Expected Behaviour
to not complain about love not being defined. And not complain about everything defined in other files as undefined global.
### Actual Behaviour
Everything work just fine as expected before yesterday when i updated my system. Now i have almost as many errors as lines in my files everything is an undefined global.
### Reproduction steps
Try any lua project here is one my son and i finished last week. Though i had no errors at the time of finishing it. Now it does.
https://github.com/jeremiahcheatham/bat-brightness
My current project is not uploaded. But everything kicks out errors now.
### Additional Notes
_No response_
### Log File
_No response_",Bitter Frustration
I'm aware of the issue. Will fix within a day or two.,
@Ornias1993 Has this been fixed yet? It's been 5 days since your last comment on this and it's still happening.,Impatience
"Sadly enough been delayed, as is clear by the fact the issue isn't closed.",Impatience
Any update on this? Prometheus is kind of key component in most of the systems that have proper metrics so it would be great to get some priority for this.,Impatience
"> Any update on this? Prometheus is kind of key component in most of the systems that have proper metrics so it would be great to get some priority for this.
If there was an update, this issue would have been closed.
You can either wait for the fix, or open a PR and fix it yourself :)",Impatience
"### App Name
prometheus
### SCALE Version
22.02.4
### App Version
7.0.1
### Application Events
```Shell
can't install, so no information available
```
### Application Logs
```Shell
can't install, so no information available
```
### Application Configuration
can't install, so no information available
### Describe the bug
Trying to install Prometheus, everything with default settings (just click next) results in error:
```
[EFAULT] Failed to install chart release: Error: INSTALLATION FAILED: YAML parse error on prometheus/templates/common.yaml: error converting YAML to JSON: yaml: line 54: could not find expected ':'
```
```
Error: Traceback (most recent call last):
File ""/usr/lib/python3/dist-packages/middlewared/job.py"", line 411, in run
await self.future
File ""/usr/lib/python3/dist-packages/middlewared/job.py"", line 446, in __run_body
rv = await self.method(*([self] + args))
File ""/usr/lib/python3/dist-packages/middlewared/schema.py"", line 1140, in nf
res = await f(*args, **kwargs)
File ""/usr/lib/python3/dist-packages/middlewared/schema.py"", line 1272, in nf
return await func(*args, **kwargs)
File ""/usr/lib/python3/dist-packages/middlewared/plugins/chart_releases_linux/chart_release.py"", line 481, in do_create
await self.middleware.call(
File ""/usr/lib/python3/dist-packages/middlewared/main.py"", line 1345, in call
return await self._call(
File ""/usr/lib/python3/dist-packages/middlewared/main.py"", line 1305, in _call
return await self.run_in_executor(prepared_call.executor, methodobj, *prepared_call.args)
File ""/usr/lib/python3/dist-packages/middlewared/main.py"", line 1206, in run_in_executor
return await loop.run_in_executor(pool, functools.partial(method, *args, **kwargs))
File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 52, in run
result = self.fn(*self.args, **self.kwargs)
File ""/usr/lib/python3/dist-packages/middlewared/plugins/chart_releases_linux/helm.py"", line 44, in helm_action
raise CallError(f'Failed to {tn_action} chart release: {stderr.decode()}')
middlewared.service_exception.CallError: [EFAULT] Failed to install chart release: Error: INSTALLATION FAILED: YAML parse error on prometheus/templates/common.yaml: error converting YAML to JSON: yaml: line 54: could not find expected ':'
```
### To Reproduce
1. go to ""Applications > Available Applications""
2. search for ""prometheus""
3. select ""prometheus 2.40.1_7.0.1""
4. click ""Install""
5. name the application ""prometheus""
6. always click next
7. Confirm Options
```
Application Name: prometheus
Version: 7.0.1
Operator Settings:
Enable: true
Log Level: info
Prometheus Settings:
Enable: true
Log Level: info
Retention: 31d
Max Retention Size:
Scrape interval: 15s
Evaluation interval: 30s
Disable Compaction: false
WAL Compression: false
Alertmanager Settings:
Enable: true
Log Level: info
Retention: 240h
Configure Service(s):
Main Service:
Service Type: LoadBalancer (Expose Ports)
LoadBalancer IP:
Service's Port(s) Configuration:
Main Service Port Configuration:
Port: 10086
alertmanager Service:
Service Type: LoadBalancer (Expose Ports)
LoadBalancer IP:
Service's Port(s) Configuration:
alertmanager Service Port Configuration:
Port: 10087
Show Expert Config: false
:
Main Ingress:
Enable Ingress: false
```
8. click Save
### Expected Behavior
Application can be installed
### Screenshots
-
### Additional Context
-
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",
What you are asking for is already the case. Tidy only applies to this repo: https://github.com/rust-lang/rust/blob/28a53cdb4695b71cb9ee39959df88542056479cd/src/tools/tidy/src/style.rs#L1,
"This decision has already made, we don't revisit old decisions without new information.
I'm going to lock this since it has a tendency to bring out trolls.",
"I don't see anything utterly offensive in `PROBLEMATIC_CONSTS`, why not let programmers have fun and talk about boobs in their code?",Mocking
"@taylorotwell Your employee is closing bugs that actually exist, because he's upset I called him out for lacking any curiosity about the project he maintains. Personally, I'd fire him",Bitter Frustration
"@Sunhat With this attitude, don't count on any support at all. Imagine that somebody would talk in that way to you. Would you be motivated to help that person out?
The bug that you are encountering might not be important for others. Do mind your tone of voice and stay friendly. This not only applies to this issue tracker but for interactions with all living things.
Some interesting resources:
- https://blog.container-solutions.com/entitlement-in-open-source
- https://opensource.com/article/22/1/open-source-contributions-career
- https://www.youtube.com/watch?v=fMFjO2szDnk
I'll send you a bill for this life changing advice! ðŸ˜…",Insulting
"@freekmurze I've displayed absolutely no sense of entitlement. I urge you to re-read what's actually happened. I don't need or want your support. I'm being selfless giving you information about your project, and it's DISGUSTINGLY being rejected due to laziness and bureaucracy. 1. A ticket was opened a year ago and @driesvints refused to investigate.
2. I opened a new ticket with a typo relating to an unsupported version. @driesvints closed the ticket, despite common sense suggesting this bug still exists. I called out @driesvints for refusing to investigate.
My tone is perfectly fine when addressing rude and lazy children.
P.S.
[Mike McQuaid](https://blog.container-solutions.com/author/mike-mcquaid)
Mike McQuaid is an abusive troll and hypocrite who frequently verbally abuses people online. He then writes arrogant articles and claims he's against ""hate"". Mike is a manipulator and perfectly embraces _""Always accuse your enemy of exactly what you are doing""_
Thanks for linking me to a perfect example of what a human shouldn't be.
---
Considering this is YOUR project. As Your buddy Mike would put it: I owe you nothing. Simply reporting a bug means I've gifted you something of value. I don't owe you a version number. I don't owe you my OS. I don't owe you my time. If you close the ticket because you're too lazy to care about your own project, then re-think why you chose this job, and re-think why you want to write open-source.
Because, despite Mike's revolting position on open-source code, open-source is about serving, giving and being selfless. If you take on an open-source project, you owe everyone everything. That's the sacrifice you chose. If you can't hack it, then don't do it.",Bitter Frustration
"> If you take on an open-source project, you owe everyone everything. That's the sacrifice you chose. If you can't hack it, then don't do it.
I guess are differences come because we don't agree on this at all. If somebody invests time creating a library, you play by the rules of the creator / maintainer. Don't assume that you as a contributor can set the rules. A creator/maintainer owes you nothing. Not even if you come with the gift of opening an issue.
Personally, I think your stance on open source will leave both you and maintainers of open source software feel bad. There are no winners with such an attitude. You can find the code of conduct of Laravel here: https://laravel.com/docs/9.x/contributions#code-of-conduct
> Participants must ensure that their language and actions are free of personal attacks and disparaging personal remarks.
> Behavior that can be reasonably considered harassment will not be tolerated.
I think suggestion that someone should be fired surely goes against to rules set by the creators / maintainers.",Bitter Frustration
"I was not able to reproduce this request.
`||daft.sex/*sw$script,1p or ||daft.sex/homesw$script,1p`",
"### Issue URL (Ads)
[NSFW] [https://daft.sex/watch/-207003944_456239828](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-207003944_456239828)
### Comment
> screenshot with request below
> > suggested privacy filter:
> ||daft.sex/*sw$script,1p or ||daft.sex/homesw$script,1p
> > suggested filter based on:
> https://github.com/uBlockOrigin/uAssets/commit/9260abfed142c0a5e636162b1a136c7445f9a382#diff-03b955eebef8886aa0f1ef70b305e9b7996de9a9e0adc9fe002db5a21d301c9fL15096
### [NSFW] Screenshots
<details>
<summary>Screenshot 1</summary>
[Screenshot 1](https://reports-img.adguard.com/cLuLQMX.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",
"### Issue URL (Ads)
[https://daft.sex/watch/-207003944_456239828](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-207003944_456239828)
### Comment
> screenshot with request below
> > suggested privacy filter:
> ||daft.sex/*sw$script,~third-party or ||daft.sex/homesw$script,~third-party
> (updated syntax)
> > suggested filter based on:
> https://github.com/uBlockOrigin/uAssets/commit/9260abfed142c0a5e636162b1a136c7445f9a382#diff-03b955eebef8886aa0f1ef70b305e9b7996de9a9e0adc9fe002db5a21d301c9fL15096
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/3bE3KBd.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",
"Unfortunately, we cannot reproduce the issue with your settings.
Please record a HAR file of broken page and send it + link of this issue it to `filters@adguard.com`
How to get a HAR file:
https://toolbox.googleapps.com/apps/har_analyzer/",
"### Issue URL (Ads)
[https://daft.sex/watch/-207003944_456239828](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-207003944_456239828)
### Comment
> @jellizaveta please help
> > screenshot with request below
> > suggested privacy filter:
> ||daft.sex/*sw.js
> > reference:
> https://github.com/AdguardTeam/AdguardFilters/commit/bbd12f70e363d3dbc69e573ddcbd5a705784963c
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/3bE3KBd.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",
"What the purpose of this site? I don't see any content
![image](https://user-images.githubusercontent.com/8361299/204302468-c7974ffc-1b66-4dc7-b3a2-26c122a18d27.png)",Impatience
"What's the URL? In fact, it's `NSFW-18+` site.
Anyway, I don't see the request on my end, checked with different IPs.",Impatience
"### Issue URL (Ads)
[https://daft.sex/watch/-209797543_456239978](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-209797543_456239978)
### Comment
> It could be your location, please use a VPN with IP in Latin America or Spain.
> > The page works fine with filter added in ""My filters"".
> > suggested privacy filter:
> ||daft.sex/*sw.js
> > reference:
> https://github.com/AdguardTeam/AdguardFilters/commit/bbd12f70e363d3dbc69e573ddcbd5a705784963c
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/kGA0O90.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",
"### Issue URL (Ads)
[NSFW] [https://daft.sex/watch/-209797543_456239978](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-209797543_456239978)
### Comment
> @ameshkov, please you could interfere in this issue.
> > arbitrarily blocked this issue: #135799
> > I provided screenshots with to show the request. There is no consistency in the decisions of the filter maintainers of the AdGuard team, another maintainer adds a suggested filter but another maintainer doesn't, I don't understand.
> > suggested privacy filter:
> ||daft.sex/*sw.js
> > reference:
> https://github.com/AdguardTeam/AdguardFilters/commit/bbd12f70e363d3dbc69e573ddcbd5a705784963c
### [NSFW] Screenshots
<details>
<summary>Screenshot 1</summary>
[Screenshot 1](https://reports-img.adguard.com/pW0rikl.jpg)
</details>
<details>
<summary>Screenshot 2</summary>
[Screenshot 2](https://reports-img.adguard.com/kGA0O90.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",Bitter Frustration
"### Issue URL (Ads)
[NSFW] [https://daft.sex/watch/-207003944_456239828](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-207003944_456239828)
### Comment
> privacy filters? @mapx-
> ||daft.sex/*sw$script,1p or ||daft.sex/homesw$script,1p
> ||dxb.to/logger/
### [NSFW] Screenshots
<details>
<summary>Screenshot 1</summary>
[Screenshot 1](https://reports-img.adguard.com/3tGFLDf.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",
"Assuming you meant skirmish, when starting with no bases and you build a factory after building three other structures, an AI building factory first could have already produced some tanks by that time. Most AIs are slow on easy, so you don't get that on easy. If you build factory first, medium will be easy too.",
"hey Zispah...i tried it both ways, got destroyed when i was slow and when i did the tank factory first...i destroyed im...thanx but...i need to try it a few times on different maps to make sure it wasnt a one-off...",Bitter Frustration
"I think you need to increase your APM (Actions Per Second). Sorry to break news for you man but ""barely get a tank factory built and a battalion of enemy tanks show up at my doorstep"" sounds like you are just not experienced enough (noob). Try watching (not playing!) multiplayer games or replays from other people, you can learn that way.
Also I suggest you to set up hotkeys, you can find list of important ones in Discord.",Mocking
"Haha noob you want to put that to the test? I've played it for 15 years...
On November 27, 2022 22:16:40 Maxim Zhuchkov ***@***.***> wrote:
>
> I think you need to increase your APM (Actions Per Second). Sorry to break > news for you man but ""barely get a tank factory built and a battalion of > enemy tanks show up at my doorstep"" sounds like you are just not > experienced enough (noob). Try watching (not playing!) multiplayer games or > replays from other people, you can learn that way.
> Also I suggest you to set up hotkeys, you can find list of important ones > in Discord.â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: > ***@***.***>",Mocking
"> Haha noob you want to put that to the test? I've played it for 15 years...
Sure man, want to 1x1 *right now*? Join via IP `6.tcp.ngrok.io:16186`. I will be online for 15 minutes. If you will not come you are free to ping me in Discord when you are ready.
P.S. use latest supported version (4.3.2).",Mocking
"We can go
On November 27, 2022 23:27:10 Maxim Zhuchkov ***@***.***> wrote:
>
> Haha noob you want to put that to the test? I've played it for 15 years...
> Sure man, want to 1x1 right now? Join via IP 6.tcp.ngrok.io:16186. I will > be online for 15 minutes. If you will not come you are free to ping me in > Discord when you are ready.
> â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: > ***@***.***>",Mocking
"So as expected, issuer is a complete noob and does not know how to play the game *and* has APM of a sloth. My POV will be available shortly after Youtube processes the video: https://youtu.be/T3Nlhzc2yjQ. Zip compressed replay file: [20221128_074207_multiplay_p0.wzrp.zip](https://github.com/Warzone2100/warzone2100/files/10100576/20221128_074207_multiplay_p0.wzrp.zip)
Let this be another show-game in my collection of noobs claiming that game is broken or everyone is cheating.",Mocking
"Rofl first time playing a human...you are easy...now that I see your game...
On November 27, 2022 23:53:59 Maxim Zhuchkov ***@***.***> wrote:
>
> So as expected, issuer is a complete noob and does not know how to play the > game and has APM of a sloth. My POV will be available shortly after Youtube > processes the video: https://youtu.be/T3Nlhzc2yjQ. Zip compressed replay > file: 20221128_074207_multiplay_p0.wzrp.zip
> Let this be another show-game in my collection of noobs claiming that game > is broken or everyone is cheating.
> â€”
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: > ***@***.***>",Mocking
"> Rofl first time playing a human...you are easy...now that I see your game...
Do note, my play was sloppy and all over the place. I did not cared about repair, usually I make 10 heavy repairs before medium body. Also you were killed with only machinegun, no anti-tank weapon at all, however I don't think you understand me in terms of weapon kind. This game was played without building base, I can show you base building too if you want, however certainly not right now, hit me up in Discord if you want a rematch.
You were playing against top 50 player of entire game, that's why it was *that easy*.",Mocking
"Like I said, I never played a human before you, I have studied very in-depth all of the AIs within the game. When we meet again I will show you that you are just another AI that I have to get a handle on.",Threat
"> just another AI that I have to get a handle on
Except I am not an AI and you need to beat at least cobra or bonecrusher to have a chance to defeat me. Your usage of excuse ""never played against human before"" only highlights your incompetence, we don't have fixed set of strategies, we adapt on the fly, kinda like nullbot but way more efficient. Game is not broken, you are broke here.
Warzone 2100 is a game of rock paper scissors but with ability to adapt and outsmart, there is nothing you can do if you don't know maths, mechanics, good strategies, units parts parameters, research tree and most importantly have APM of a sloth. I hope you have enough copium because the only way that you can defeat me is by exhaust and that will be rather difficult considering that not long ago I played for more than 13 hours straight into different game. You will likely just refuse to fight and call me cheater or something way earlier than I will feel any weaker.
As I already said, feel free to come to Discord and ping me for a rematch at any time, I have time and dedication to fight to exhaustion, let's find out who will give up first.",Mocking
"nah..listen, I can't see you...I just see code. Your strategies are irrelevant, you too will be assimilated.",Mocking
"Seems like only thing you can do so far is talk, come back when you will have any skill to show.",Mocking
"**Describe the bug**
A clear and concise description of what the bug is.
When I play easy, it is too easy regardless of the map. When I pick medium, one difficulty level higher, I barely get a tank factory built and a battalion of enemy tanks show up at my doorstep. That to me is unplayable. **To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error
**Expected behavior**
A clear and concise description of what you expected to happen.
There should be a steady flow, medium should be moved to insane.
**Screenshots or Videos**
If applicable, add screenshots to help explain your problem.
**Your System:**
- OS: [e.g. Windows 10, Linux (Ubuntu 18.04)]
- Game version: [e.g. 3.3.0, commit hash]
**Additional context**
Add any other context about the problem here.",Bitter Frustration
"If anyone else gets the same crash, the key part of the backtrace in https://bugzilla.redhat.com/show_bug.cgi?id=2150392
seems to be:
```
Thread 1 (Thread 0x7f49a6faef00 (LWP 78232)):
#0 0x00007f49a84fef9d in g_hash_table_fetch_key_or_value (is_big=3, index=3072172, a=0x557a41cf0aa0) at ../glib/ghash.c:394
No locals.
#1 g_hash_table_foreach (hash_table=0x557a419b1120, func=0x557a40049ee0 <get_one_value_callback>, user_data=0x7ffe86e068d0) at ../glib/ghash.c:2094
node_hash = 357688284
node_key = <optimized out>
node_value = <optimized out>
i = 3072172
version = 18
__func__ = ""g_hash_table_foreach""
```
I looked at the backtrace of another report of the same or a similar issue,again crash is in glib/ghash.c https://bugzilla.redhat.com/show_bug.cgi?id=1888948
A mention of dropbox (using caja-dropbox extension?) in that backtrace, and a crash on a ""file not found"" error:
```
#0 0x00007f444c07d96d in g_hash_table_fetch_key_or_value (is_big=3, index=5037142, a=0x560a0d3b8d50) at ../glib/ghash.c:2061
2061	../glib/ghash.c: No such file or directory.
[Current thread is 1 (Thread 0x7f444ac1af40 (LWP 104797))]
```
Never had this crash myself on Debian Unstable with locally built MATE, GTK3, and glib, but probably different compiler options for everything, different glib version, different GTK3 version/build options etc. In one backtrace we have ""no locals"" at ../glib/ghash.c:394 and in the other we have ""file not found"" at ./glib/ghash.c:2061
Assuming this does not come from a glib bug (which could be version-specific) we may be passing bad data to it somehow.
Again, I can't duplicate this but if we get more reports of this, hopefully this information will jump-start digging into it for whoever can duplicate it.",
"I reported an apparently spontaneous crash to fedora, hoping the crash data produced by the ABRT tool could somehow be useful: https://bugzilla.redhat.com/show_bug.cgi?id=2150392
I've been instructed to move the issue upstream, so here I am. :-)",Irony
Closing due to abusive tone. Provide constructive feedback.,
"### Prerequisites
- [X] I have written a descriptive issue title
- [X] I have searched existing issues to ensure the issue has not already been raised
### Issue
I tried to start a demo following the instructions in the Getting Started documentation and had various problems along the way. It took 2 hours and still couldn't run through the demo program. Garbage documentation can affect the interest and motivation of the taster to use it. I wish the getting started documentation was more detailed, or just give a complete minimized demo!",Bitter Frustration
"What is the output of
```julia
(@v1.8) pkg> st -m Plots RecipesBase RecipesPipeline
```
?",Impatience
"Same problem here: https://discourse.julialang.org/t/error-precompiling-plots-recipesbase-jl-607-needs-to-be-placed-at-the-top-level-or-use-eval/91694/1
In fact this breaks my complete CI: https://github.com/j-fu/VoronoiFVM.jl/actions/runs/3703661391/jobs/6275372947",Bitter Frustration
"Output of `pkg> st -m Plots RecipesBase RecipesPipeline`:
```julia
(PlutoNotebooks) pkg> st -m Plots RecipesBase RecipesPipeline
Status `~/Documents/Skola/julia/PlutoNotebooks/Manifest.toml`
[91a5bcdd] Plots v1.37.2
[3cdcf5f2] RecipesBase v1.3.2
[01d81517] RecipesPipeline v0.6.11
```",
"Same issue here:
```julia
pkg> st -m Plots RecipesBase
Project BEFWM2 v0.1.0
Status `~/<my-env>/Manifest.toml`
[3cdcf5f2] RecipesBase v1.3.2
```",
Same,Impatience
same,Impatience
"same here:
```
(@v1.7) pkg> st -m Plots RecipesBase
Status `C:\Users\ms2106\.julia\environments\v1.7\Manifest.toml`
[3cdcf5f2] RecipesBase v1.3.2
```",
"I tried julia versions `1.8.2`, `1.8.3` and the stable release `1.6.7` always the same result",Bitter Frustration
`] add RecipesBase@v1.2.1`,Impatience
"What is going one here ? :fire: This is probably induced by a new version of `SnoopPrecompile` (released two hours ago, sole dependency of `RecipesBase`).
https://github.com/timholy/SnoopCompile.jl/commit/d11c578c96b881aa06543ae3acdaf5795b84abe6
A temporary fix might be to pin `SnoopPrecompile` to the previous release `] add SnoopPrecompile@1.0.1`.
I think we should yank the broken `SnoopPrecompile` release.",Bitter Frustration
Should I close the issue as it is related to `SnoopPrecompile` ?,
@gortibaldik Maybe not untill they fix it or the temporary pinning lands in `RecipesBase`?,Impatience
Same,Impatience
"Yeah, well I proposed to yank the broken `SnoopPrecompile` in https://github.com/JuliaRegistries/General/pull/74176.
This is not a `Plots` issue, and is induced by an upstream dependency.",
"Issuing `] update` should fix the issue (broken version is now yanked).
Check that you have the correct version, by using:
```julia
] st -m SnoopPrecompile # 1.0.2 shouldn't appear anymore
```",
"## Details
I cannot precompile Plots due to errors in precompiling RecipesBase and RecipesPipeline:
```julia
Precompiling project...
âœ— RecipesBase
âœ— RecipesPipeline
âœ— Plots
0 dependencies successfully precompiled in 3 seconds. 204 already precompiled.
ERROR: The following 1 direct dependency failed to precompile:
Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]
Failed to precompile Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] to /home/gortibaldik/.julia/compiled/v1.8/Plots/jl_0EhmVw.
ERROR: LoadError: syntax: Global method definition around /home/gortibaldik/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:607 needs to be placed at the top level, or use ""eval"".
Stacktrace:
[1] top-level scope
@ ~/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:600
[2] include
@ ./Base.jl:419 [inlined]
[3] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String)
@ Base ./loading.jl:1554
[4] top-level scope
@ stdin:1
in expression starting at /home/gortibaldik/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:1
in expression starting at stdin:1
ERROR: LoadError: Failed to precompile RecipesBase [3cdcf5f2-1ef4-517c-9805-6587b60abb01] to /home/gortibaldik/.julia/compiled/v1.8/RecipesBase/jl_JcrErn.
Stacktrace:
[1] error(s::String)
@ Base ./error.jl:35
[2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool)
@ Base ./loading.jl:1707
[3] compilecache
@ ./loading.jl:1651 [inlined]
[4] _require(pkg::Base.PkgId)
@ Base ./loading.jl:1337
[5] _require_prelocked(uuidkey::Base.PkgId)
@ Base ./loading.jl:1200
[6] macro expansion
@ ./loading.jl:1180 [inlined]
[7] macro expansion
@ ./lock.jl:223 [inlined]
[8] require(into::Module, mod::Symbol)
@ Base ./loading.jl:1144
[9] include
@ ./Base.jl:419 [inlined]
[10] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing)
@ Base ./loading.jl:1554
[11] top-level scope
@ stdin:1
in expression starting at /home/gortibaldik/.julia/packages/Plots/Hxe7H/src/Plots.jl:1
in expression starting at stdin:1
```
### Versions
Plots.jl version: Plots v1.37.2
Output of `versioninfo()`:
```julia
Julia Version 1.8.2
Commit 36034abf260 (2022-09-29 15:21 UTC)
Platform Info:
OS: Linux (x86_64-linux-gnu)
CPU: 20 Ã— 12th Gen Intel(R) Core(TM) i7-12700H
WORD_SIZE: 64
LIBM: libopenlibm
LLVM: libLLVM-13.0.1 (ORCJIT, goldmont)
Threads: 1 on 20 virtual cores
```",Bitter Frustration
@MrGeneration Can you add a note to the documentation that the migration path is currently broken linked to this issue?,Impatience
"That's entirely the wrong repository.
The PoC is not living here.",Mocking
After an internal heads up by @rolfschmidt I learned that this is something the migration script can't handle and thus needs a rake task.,
"Alternative solution idea: maybe we could create a rake task that reads the DB before migration, and outputs [pgloader casting rules](https://pgloader.readthedocs.io/en/latest/ref/mysql.html?highlight=cast#mysql-database-casting-rules) which would perform the migration correctly in the first place.",
"There are currently concerns regarding the performance of the PoC attached to this issue, since it uses `ActiveRecord::Migration` instances, creates new columns, migrates the data, drop, then rename the columns... Also, it's not clear if it would have additional side-effects, since it is supposed to be run in the application/Rails context. A benchmark with a large data set and both solutions in place (PoC vs `pgloader`) might try dispel these concerns, but it hasn't been tried yet.
Initial research into `pgloader` casting rules resulted in some findings:
* It doesn't look like `pgloader` provides a suitable transformation function for a JSON array into a PostgreSQL array which can be used as part of its `CAST` command (on the column type level.) We could not find an example either, and trying some stuff including expected DSL like `""text array""` didn't lead to meaningful results. This doesn't mean it's not supported, just that we could not identify it at this time.
* [An issue for `pgloader`](https://github.com/dimitri/pgloader/issues/881) with a similar problem provided a possible workaround via `AFTER LOAD DO` block, which can transform specific columns on specific tables only, but it does use a `translate` function which seems to work for our JSON => array use-case. (Note that this same function doesn't seem to be provided in the `CAST` context.) Here is a working example for `pgloader` post-processing block for one `multi_tree_select` column on the `tickets` table:
```
...
AFTER LOAD DO
$$ alter table tickets alter column multi_tree_select type text[] using translate(multi_tree_select::text, '[]', '{}')::text[] $$
...
```
A proposed rake pre-task could try to prepare the complete `pgloader` script and include one line per affected column.",
"@dvuckovic that's kindof what I hoped for. So I'm all for pursuing this further. It should be best to get the migration right in the first place.
Please note that @rolfschmidt included at least one column for this transformation which does not come from object attributes (IIRC `PublicLink`). We should try to include all candidates, of course.",
"Another idea that just came to mind: it's possible to get all JSON columns in a MySQL database with one query.
```
mysql> select table_name, column_name from information_schema.columns where table_schema = 'zammad_development' and data_type = 'json';
+--------------+-------------------+
| table_name | column_name |
+--------------+-------------------+
| public_links | screen |
| tickets | multi_tree_select |
| tickets | multi_select |
+--------------+-------------------+
3 rows in set (0.00 sec)
```
However, it's still safer to go with the PoC approach since we are absolutely positive these columns are used only as simple JSON arrays.",
"@dvuckovic plus, this would not work with MariaDB, as it does not have native json types.
Perhaps it would be a good idea to verify your solution also for MariaDB -> PG migration.",Impatience
"@mgruner Correct, in MariaDB these columns are of `longtext` type.
I just verified that the same script works on MariaDB as well, columns are properly migrated to `array` type on PostgreSQL.",
"For posterity: we did some reproduction of the original issue, by skipping `AFTER LOAD DO` statements in our test job. And the results are rather surprising!
On MySQL, the column will be left in `json` type on PostgreSQL (it still supports them!). While everything will still work as expected (the stored data is decoded/encoded correctly), the schema is definitely different than a regular PostgreSQL instance that was initialized from scratch.
On MariaDB, the column will be migrated to `text` type on PostgreSQL. This breaks the decoding of stored values, and while the system will still happily continue chugging on, the data does look corrupted. Just take a look how the frontend shows some of these multiselects:
![Image](https://user-images.githubusercontent.com/6049445/209136432-bfe873ab-85e6-4b77-8004-3e503f549b46.png)
![Image](https://user-images.githubusercontent.com/6049445/209136431-265ae272-c120-46f5-8689-4ba1e5cd091f.png)
Strictly speaking, this means the original issue is reproducible only on MariaDB backends. However, it's not clear if there would be some issues down the line, so we would still prefer the migration to `array` columns.",
"> Strictly speaking, this means the original issue is reproducible only on MariaDB backends. However, it's not clear if there > would be some issues down the line, so we would still prefer the migration to array columns.
I am confused as I received this PR: https://github.com/zammad/zammad-documentation/pull/263
What is needed what is not?
Can we please clarify that before hand? I feel like I'm touching things without any use.",Impatience
"@MrGeneration Please ignore my previous comment, it was just a minor finding during testing.
The documentation PR you linked to looks correct and the new command applies to both MySQL and MariaDB backends. With the fix in place, the migrated columns are always of the same data type (`ARRAY`), which is what you would expect.",
"### Used Zammad Version
5.3
### Environment
- Installation method: package
- Operating system (if you're unsure: `cat /etc/os-release` ): [e.g. debian 10.4, ubuntu 20.04]
- Database + version: mariadb
- Elasticsearch version: [e.g. 7.17]
- Browser + version: [e.g. chrome 83, safari 14, firefox 105]
### Actual behaviour
- Migration does not convert the json values to the array structure of postgres. I think there is a rake task missing which will look up all values in the database and convert the json structures to array.
I attached a PoC how it could look like:
[fix_maria_migration.zip](https://github.com/zammad/zammad/files/10245529/fix_maria_migration.zip)
### Expected behaviour
- Migration should work.
https://docs.zammad.org/en/latest/appendix/migrate-to-postgresql.html
### Steps to reproduce the behaviour
Migrate from mysql/mariadb to postgres, but before create multi tree selects and fill up some tickets:
https://docs.zammad.org/en/latest/appendix/migrate-to-postgresql.html
### Support Ticket
Ticket#10115329
### I'm sure this is a bug and no feature request or a general question.
yes
### ToDo:
- [x] Investigate a post or pre pgloader solution
- [x] Decide and implement a solution
- [x] Update the current documentation
- [x] Adding a integration test of the database migration from mysql/mariadb to postgres",
"> It reduces allocations of MessageProperties. I used the following sample to analyse memory allocations.
Could you try do the same but with some headers added? I mean `SetHeader` method : `new MessageProperties().SetHeader(""a"", ""value1"").SetHeader(""b, ""value2""),`
Of cource PR reduces allocations of MessageProperties itself, but some internal code begins to allocate memory in other places.",Impatience
"> > It reduces allocations of MessageProperties. I used the following sample to analyse memory allocations.
> > Could you try do the same but with some headers added? I mean `SetHeader` method : `new MessageProperties().SetHeader(""a"", ""value1"").SetHeader(""b, ""value2""),`
> > Of cource PR reduces allocations of MessageProperties itself, but some internal code begins to allocate memory in other places.
```
var properties = new MessageProperties();
properties.Headers[""a""] = ""value1"";
properties.Headers[""b""] = ""value2"";
```
This sample allocates memory under the hood as well :)
ImmutableDictionary allocates only logarithmic memory in such a case, which is obviously worse than Dictionary, but not that bad.
Let's move this discussion about Headers type to a separate PR, which will probably not be implemented in v8, because `IBasicProperties.Headers`(there are two implementations, class and readonly struct based) has `IDictionary<string, object?>` type in master branch.",Impatience
"It reduces allocations of MessageProperties. I used the following sample to analyse memory allocations:
```c#
using EasyNetQ;
using EasyNetQ.Topology;
var cts = new CancellationTokenSource();
cts.CancelAfter(30000);
Console.CancelKeyPress += (_, _) => cts.Cancel();
using var bus = RabbitHutch.CreateBus(
""host=localhost"",
x => x.EnableNewtonsoftJson()
.EnableAlwaysNackWithoutRequeueConsumerErrorStrategy()
);
var queueName = ""unknown"";
while (!cts.IsCancellationRequested)
{
try
{
await bus.Advanced.PublishAsync(
Exchange.Default,
queueName,
false,
new MessageProperties(),
ReadOnlyMemory<byte>.Empty,
cts.Token
);
}
catch (OperationCanceledException) when (cts.IsCancellationRequested)
{
throw;
}
catch (Exception exception)
{
Console.WriteLine(exception);
break;
}
}
```
Before:
<img width=""1321"" alt=""Screenshot 2023-01-10 at 09 03 17"" src=""https://user-images.githubusercontent.com/664889/211507807-e259a816-c7d9-4a04-9dba-4eddab8b2aa5.png"">
After:
<img width=""1274"" alt=""Screenshot 2023-01-10 at 08 57 55"" src=""https://user-images.githubusercontent.com/664889/211507841-c9e91224-50f0-4a6d-be38-8889c590aef6.png"">",
~~Where the hack is the squash rebase button in Microsoft Github?~~ ðŸ¦º,Impatience
https://github.com/puma/puma/issues/2371 seems not related this. Can you explain it?,Impatience
@hsbt I misread puma/puma#2381 about offtopic puma/puma#2371. It's puma/puma#2381 only.,
This also fixes #6268 and #6204,
"I fixed both `bundle gem --ext=c foo` and `bundle gem --ext=rust bar` on several machines (CentOS 9 Stream x86_64, Ubuntu 22 LTS x86_64, and macOS 13 arm) I ran test benches against.
It's not ideal and probably not doing everything right, but it's closer to tried-and-true based on what already exists. I'm sure I'll see it's 100% wrong after I sleep.",Irony
"This should've been fixed by #6298, so good to close, right?",Irony
(Thanks for your work regardless @steakknife),
@deivid-rodriguez No worries. I'll give it a shot to validate.,
"@deivid-rodriguez Broken on mac and Linuxes. Absolutely no improvement. It's still not setting load paths correctly and it pollutes `lib/` with a binary shared object. And, it also isn't cleaned by `rake clean`. It belongs where native extensions always belong by maintaining convention over configuration and convention over useless churn. I have no horse in this race, so it can stay broken. ""Engineers"" who broken this should own it because it's a huge mess.",Bitter Frustration
"CentOS, Fedora, RHEL and so forth break without this.
Also, `Rakefile` doesn't need those options.",Bitter Frustration
"After a command is run, the new prompt shows the status of the previous exit status. It is at the new prompt you eyes are looking where you need feedback on what just happened. Changing the previous prompt makes no sense as many commands wouldâ€™ve likely scrolled it off the screen.",
"It is just really confusing to me.
I agree with you that many commands will not be displayed anymore after executing.
However, it is not ""sudo apt-get install... "" that failed. It was the command above.
I personally think that this is misleading if you dont know what exactly happens. The current behaviour makes the arrow (for me) kind of obsolete.",
"@Gandalf1783 there are a lot of different themes that doesn't bundle this feature! Personally, I think this feature is quite useful to know if `$?` is `0` or not!",
"I really like the idea. It is really useful. Thats also why I have the return-value indicator on the right side of the terminal (the number with the red x or the checkmark) configured for my prompt.
In my eyes, you can see the cli now as a kind of table.
At the beginning, you got your indicator, if a command returned with 0 or not. However, it is not related to the command right next to it.
I know what you mean, since of course you have the return value AFTER a command or programm finished executing, so it gives you feedback after something has completed (successfully or not).
But for commands like git, dhclient, cat, tail, cp, ls, mkdir, chown, chmod etc... which dont have a long console output (usually), the way its implemented does not make sense to me.
I dont want to push y'all into changing this, especially since people are using this for quite some time now. I just see e.g. the arrows to be a thing of ""pointing"" and conveying meaning. It looked to me as a ""this line produced an error"", similar to what an IDE would do if theres a bug, error that occured on a specific line of code. And since that line didnt threw the error, but the one above did, this seems wrong.",Bitter Frustration
"I see your point, just a matter of preference!",
"And the difficulty in trying to accomplish it. The prompt cmd would have to know where the previous prompt was displayed and then go back to rewrite it, without affecting anything else.",
"Hm, I don't think thats an issue.
Count the lines since the last command, write into the buffer at that location, done.
Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.",
"htop is using ncurses.
Try your method, then submit it once you have it working with both static and scrolling output. I'm looking forward to it.
How do you count the lines?
What buffer are you talking about?",Impatience
"> Hm, I don't think thats an issue. Count the lines since the last command, write into the buffer at that location, done.
> > Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.
As trivial as this is, I'm surprised you haven't submitted a patch yet. If you're stuck on something, post it and maybe someone can help you out.",Impatience
"> Hm, I don't think thats an issue. Count the lines since the last command, write into the buffer at that location, done.
> > Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.
@Gandalf1783 Given your lack of response, I'm assuming you just wanted to gaslight someone (me in this case) without any actual knowledge of the problem? You just wanted to sound like you knew more than someone else without knowing anything at all?
No program has enough knowledge (currently) to pull off what you think is trivial.
The terminal doesn't know the purpose of the text it displays, it can't do it.
zsh/omz/p10k doesn't know what happens in-between the prompts. They can control what is displayed on the current prompt lines. They can query where on the terminal they are, but once another program has run with unknown amount of text output and the screen scrolls, all for that previous knowledge is useless. Terminal queries for row number only apply to the displayed screen, not relative to the top of the scroll back buffer. To be fair, I too was annoyed by this some time ago and solved it for myself by displaying green/red lines across the entire terminal width before the next prompt is displayed. The color line is the status for everything above the line. I've left the prompt symbol in, but rarely look it. Here's an example:
<img width=""363"" alt=""image"" src=""https://user-images.githubusercontent.com/15944841/224548171-55aae1cb-8d04-4feb-af7a-20a90993c3ad.png"">
Now, if you'll notice, iTerm2 does indeed update its prompt marker (which is outside the terminal text area).",Bitter Frustration
"Excuse me for not answering.
I haven't had a look on this for the last few days.
First of all, what I didn't notice is that this is written in shell scripts. I thought this was a C application that extended the zsh shell/passed all inputs to it and gave the user its own custom shell with custom themes.
In regards to the ""No program has the knowledge..."": I just said it was trivial to count lines, wait for the exit code to return and edit the first character (the prompt symbol) to the according color. Like I said, I wasn't aware that this is a shell script. Of course, without any external help this cant be done.
I've taken a look, and since e.g. Github-Repos and their branch can be displayed (e.g. master), I have noticed the extra github-helper program thats started in the background.
I assume that it is possible to pass the stdout/stdin from the session to another program, count lines/return code and just tell zsh what should happen or just modify the buffer directly.
I would've forked the repo and ran my own tests/integrations, but due to the lack of knowledge in shell-scripting im basically through the topic. I cant really contribute anything here, I was just curious if there was some option I was missing, since I only found some posts online that were interested in a similar solution / were bothered by this.",
"> Excuse me for not answering.
> > I haven't had a look on this for the last few days.
> > First of all, what I didn't notice is that this is written in shell scripts. I thought this was a C application that extended the zsh shell/passed all inputs to it and gave the user its own custom shell with custom themes.
So you didn't even know what the topic was about?
> In regards to the ""No program has the knowledge..."": I just said it was trivial to count lines, wait for the exit code to return and edit the first character (the prompt symbol) to the according color. Like I said, I wasn't aware that this is a shell script. Of course, without any external help this cant be done.
Even that isn't trivial. Count linefeeds? What about lines that wrap to the next line? Output from a PAGER, such as as less? Output from htop? How do you count those lines?
> I've taken a look, and since e.g. Github-Repos and their branch can be displayed (e.g. master), I have noticed the extra github-helper program thats started in the background.
> > I assume that it is possible to pass the stdout/stdin from the session to another program, count lines/return code and just tell zsh what should happen or just modify the buffer directly.
It would have to duplicate everything the terminal is already doing to accurately determine how much screen space has been used. Not impossible, but hardly trivial.
> I would've forked the repo and ran my own tests/integrations, but due to the lack of knowledge in shell-scripting im basically through the topic. I cant really contribute anything here, I was just curious if there was some option I was missing, since I only found some posts online that were interested in a similar solution / were bothered by this.
Good for you telling others how trivial something is.",Bitter Frustration
"It has been established already that this is not possible, and any further discussion is pointless.
Richard, I'm concerned about your tone. If you're not able to have a technical argument without getting offended, just don't engage and forget about it. I'd much rather that than have this kind of vitriol on our project.",Insulting
"To be extra clear, the implementation of this is not feasible because of the *technical* reasons that Richard mentioned. He is 100% correct on that. It is difficult to understand why it's not just ""counting lines"" when you don't have a full understanding of how a terminal works. But my point about the tone still stands. End of discussion.",Bitter Frustration
"### Describe the bug
When I enter a command, that returns an error, the line with that command is marked with a green arrow.
The next line, valid or not, gets marked red, which is confusing, since it was valid in my case.
### Steps to reproduce
Example: Installing something via apt without using sudo:
â¯ apt-get install mosh
E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
â¯ sudo apt-get install mosh
[sudo] password for user:
The first line has a green arrow, while the thirds one is red.
### Expected behavior
Color the arrow red where the command actually failed.
### Screenshots and recordings
![image](https://user-images.githubusercontent.com/46635221/224000635-d131e0d5-e84d-4acd-ac73-8c6a982c5688.png)
### OS / Linux distribution
Debian 11 (Bullseye) (using Windows Subsystem for Linux)
### Zsh version
5.8
### Terminal emulator
Windows Terminal
### If using WSL on Windows, which version of WSL
WSL2
### Additional context
I am using Oh My ZSH with powerlevel10k",
"This appears to be a cynical attempt to drive people into vendor-locking with your CDN.
Your desire to be paid for working on Open Source has cost me a lot of my time.
Was it the intention to waste my time or did you expect me to just hand over a monthly subscription?
I have decided I will not be supporting such a dark-pattern by moving to the CDN approach. Instead I will likely fork the project and do whatever I have to do in order to remove this ""RTC"" nonsense.",Bitter Frustration
"`5.10.7` works as it should and does not include these most recent attempts to break the library.
Those choosing not to vendor-lock themselves into unnecessary pointless parasitic CDN subscriptions can use this version and avoid any future _""upgrades""_.
This might explain why v5 has 3x the weekly downloads v6 does.",Mocking
"If you're just going to go around our tracker posting about vendor lock-in you're the one wasting our time.
If you would like to continue using v6 - which I recommend as support for v5 [ends in a month](https://www.tiny.cloud/blog/tinymce-end-of-support/) - I'm happy to work with you to figure out what might be causing this error.",Bitter Frustration
"During the TinyMCE startup process the editor checks whether the `rtc` plugin is loading, with the aim of allowing RTC to take over key sections of editor functionality. If RTC isn't in the plugin list it redirects everything back to the core.
https://github.com/tinymce/tinymce/blob/ece3bf774b9ae20910d073af5351f162c4fb52d5/modules/tinymce/src/core/main/ts/Rtc.ts#L294-L306
This was added in TinyMCE 5.3 so the source of the exception isn't recent, there's probably just some code trying to use one of those core features during setup instead of waiting for the `PreInit` event.
I am open to collaborating if you'd like to help track down this mistake. An unminified stack trace would be a good place to start, or a replication case.",
"> I am open to collaborating
* The bug is caused by efforts to lock certain features and wouldn't exist otherwise
* The ticket is closed as if the reported bug does not exist as only the commercial version matters
* Given this is a commercial product only one of us is incentivised to contribute
* v5 will work fine for decades to come
* Have other fish to fry, wasted enough time on what should have been a simple install",Bitter Frustration
"If you're just going to twist my words don't bother replying. This isn't an airport, you don't need to announce your departure.",Bitter Frustration
"`Uncaught (in promise) Error: Failed to get RTC instance not yet initialized.`
Seems in the process of attempting to vendor-lock with the CDN based implementations you've broken the library itself.
...
```
import tinymce from 'tinymce';
import 'tinymce/icons/default';
import 'tinymce/themes/silver';
import 'tinymce/models/dom';
import 'tinymce/skins/ui/oxide/skin.css';
import 'tinymce/plugins/advlist';
import 'tinymce/plugins/code'; // FIXME: Not picking up changes made in editor view? NOTE: Works again in v5
import 'tinymce/plugins/emoticons';
import 'tinymce/plugins/emoticons/js/emojis';
import 'tinymce/plugins/link';
import 'tinymce/plugins/lists';
import 'tinymce/plugins/table';
import contentUiSkinCss from 'tinymce/skins/ui/oxide/content.css';
import contentCss from 'tinymce/skins/content/default/content.css';
```
```
tinymce.init({
selector: `#mg-wysiwyg-${this._uid}`,
height: parseInt(this.height),
readonly: this.disabled,
//min_height: this.height,
placeholder: this.placeholder,
resize: 'both',
menubar: false,
plugins: ['advlist', 'code', 'emoticons', 'link', 'lists', 'table'],
toolbar: 'undo redo | bold italic forecolor backcolor | bullist numlist checklist table | link emoticons | code',
model: 'dom', // FIXME: Forcing ""dom"" because non-existant RTC ""Premium plugin"" getting in the way
// Configuration required for local self-install
skin: false,
content_css: false,
content_style: contentUiSkinCss.toString() + '\n' + contentCss.toString(),
//promotion: false, // Oh yeah we really want adverts!
// Bound to ""change keyup"" events as per https://github.com/tinymce/tinymce-vue/blob/b41c2a47eb8d9629eb01a41d6c6c633651f2d078/src/main/ts/Utils.ts#L115-L119
init_instance_callback: editor => {
editor.on('change keyup', e => {
this.data = editor.getContent({ format: this.syntax })
})
},
});
```",Bitter Frustration
"> BC Break Report
For a deprecation that you don't agree with? Seriously?
> Right now I can't determine the type for `$args` in `index` method :(
It's `PreUpdateEventArgs | PrePersistEventArgs`.",Impatience
"> It's `PreUpdateEventArgs | PrePersistEventArgs`.
Let's have the full version of your beautiful code
```php
public function index(PrePersistEventArgs | PreUpdateEventArgs | PreRemoveEventArgs | PostPersistEventArgs | PostUpdateEventArgs | PostRemoveEventArgs | PostLoadEventArgs $args, $skipOverwriteInitial): void
```
# Seriously?",Mocking
"https://github.com/doctrine/orm/blame/2.14.x/UPGRADE.md#L61
### BC Break Report
BaseLifecycleEventArgs
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| BC Break | yes
| Version | 2.14.1
#### Summary
Hello. Please provide an interface
```php
public function getSubscribedEvents(): array
{
return [
Events::prePersist,
Events::preUpdate,
];
}
public function preUpdate(PreUpdateEventArgs $args): void # <-- PreUpdateEventArgs
{
$this->index($args, false); # <--
}
public function prePersist(PrePersistEventArgs $args): void # <-- PrePersistEventArgs
{
$this->index($args, true); # <--
}
public function index($args, $skipOverwriteInitial): void # <-- [????????] $args ?
{
$entity = $args->getObject();
if (!$entity) {
return;
}
$this->entityDateUpdater->updateFields($entity, $skipOverwriteInitial);
}
```
### Previously all types were LifecycleEventArgs
Right now I can't determine the type for `$args` in `index` method :(",Bitter Frustration
"We already upgraded, you have to wait for the next release.
https://github.com/mitmproxy/mitmproxy/blob/be02b1e298dfb1b218007f824c4cf345c53b0cee/pyproject.toml#L37
https://github.com/mitmproxy/mitmproxy/commit/8c6ec5cb56fbf4961806ed27b4974c440df59e87
https://github.com/search?q=repo%3Amitmproxy%2Fmitmproxy+cryptography&type=issues
https://github.com/mitmproxy/mitmproxy/issues/5966
Feel free to re-open if I miss something. I have no idea what `poetry` is.",Impatience
"@Prinzhorn @mhils Just curious why are we not releasing the fix even though [CVE-2023-0286](https://nvd.nist.gov/vuln/detail/CVE-2023-0286) is a **High** severity vulnerability ? Developers who are already using library `cryptography > 39.0.1` won't be able to use `mitmproxy`. Other libraries have already released the fix. For example, https://pypi.org/project/pyOpenSSL/23.1.0/",Impatience
"https://github.com/mitmproxy/mitmproxy/issues/5966#issuecomment-1455045917
> I don't think either of the posted vulnerabilities affects mitmproxy users in a significant way, so I don't think we will ship a patch release just for that. :)",Irony
"@Prinzhorn Do you have a target date for the next release? Our customers in regulated industries (banking, federal, etc.) are prohibited by regulations from using s/w with known High and Critical severity vulnerabilities. I'm trying to formulate our case for waiting, but need some kind of target date.",Impatience
"@FrugalGuy: If you are interested in timely patch releases to fix your company's compliance requirements, I'm happy to set up a support contract. Email is on my profile. :-)",Irony
"Since this has been picked up by some news sites I'd like to provide some additional context here: I don't mind the release question at all, it's valid! But the context should ideally be along the lines of ""we have an interest in this, how can we help make it happen"" (contributions or $) and not ""you are causing problems for our customers"". I don't want the requestor to have a miserable time because of a badly-worded comment, I want large companies to have a healthy relationship with FOSS.",
"@FrugalGuy has just sent me genuine apology, which I truly appreciate. Please be nice and assume good intentions. â¤",
"Firstly, I just discovered `mitmproxy` and I'm pretty excited to make use of it. Thank you for maintaining it.
#### Problem Description
I'm trying to use mitmproxy as part of an integration test in a project that depends on `cryptography`, specifically version `40.0.0`. I see that mitmproxy requires cryptography at `<38.1`. I can automate the creation of a separate venv for the mitmproxy stuff, but that's kind of clunky. I'd rather just make it a dependency and let poetry handle it. For that to happen, `mitmproxy` needs to be compatible with higher versions of cryptography.
I recognize that the `<38.1` constraint might be there for a good reason, but not knowing that reason I figured I'd file an issue and cross my fingers. Thanks for at least considering it.
#### Steps to reproduce the behavior:
1. Have a `pyproject.toml` like this:
```
...
[tool.poetry.dependencies]
python = ""^3.10""
cryptography = ""^40.0.0""
[build-system]
requires = [""poetry-core""]
build-backend = ""poetry.core.masonry.api""
```
2. Run `poetry add mitmproxy`
3. See error:
```
Because no versions of mitmproxy match >9.0.1,<10.0.0
and mitmproxy (9.0.1) depends on cryptography (>=38.0,<38.1), mitmproxy (>=9.0.1,<10.0.0) requires cryptography (>=38.0,<38.1).
So, because foo depends on both cryptography (^40.0.0) and mitmproxy (^9.0.1), version solving failed.
```
#### System Information
```
Mitmproxy: 9.0.1
Python: 3.10.9
OpenSSL: OpenSSL 1.1.1s 1 Nov 2022
Platform: Linux-5.15.86-x86_64-with-glibc2.35
```",
Can you please a look at the conversation at https://github.com/doctrine/DoctrineMigrationsBundle/pull/492?,Impatience
So surely the simple solution is just to bump the version number and add new code to the new major version? I can't do that only your project can. I'm not seeing what the difficulty is here?,Impatience
"Well we can create a branch for sure, no biggie. Still, creating a major release just for this would be a bit overkill. Why don't you apply the alternate solution mentioned in the deprecation message? Then we can merge, after you provide a convincing apology for your behavior of course.",Bitter Frustration
"> after you provide a convincing apology for your behavior of course.
And that ends my contributions here and earns you a place on my block list. Im sorry you feel that way. Its your project do as you want. I was only trying to be helpful. No one was demanding anything. The fact is, the upstream project has chosen to add the return type, and now this project outputs warnings and you are choosing to ignore that rather than fix that. Your choice I guess.",Bitter Frustration
"Again, there is another way to fix it, if you would care to read the deprecation message, but slowly. I'd do it myself but I'm AFK.",Bitter Frustration
"""Suppressing"" messages rather than fixing the root issue is not the way to avoid errors in the long term. Continued personal insults is also a quick way to lose contributors. You don't deserve my contribution following your aggressive tone, demanding of an apology for simply contributing and now telling me to read ""slowly""... Seems to happen often with Doctrine* projects. Seems I was only using this Bundle for a small thing anyway, so I have refactored that into a Symfony command and removed this dependancy from my projects. Simple. Now the warning is gone and you don't need to talk to me again.",Bitter Frustration
"> ""Suppressing"" messages rather than fixing the root issue is not the way to avoid errors in the long term.
We usually do both, that's how good we are. I don't want your contribution, I don't experience the issue.",Bitter Frustration
"To anyone who wants to pick up the work: We are aware that we need to create a new major release for Symfony 7 eventually. For Symfony 6, this deprecation can be resolved without breaking changes. Symfony lets us do that by adding a PHPDoc block. This is the way to go here.",
Also: we won't add native void return types in Symfony 7.,
@PhilETaylor can you please send a PR to add the `@return void` annotation? That's enough to express the intent and fix the deprecation.,
"> !! 2023-04-08T19:35:15+00:00 [info] User Deprecated: Method ""Symfony\Component\HttpKernel\Bundle\Bundle::build()"" might add ""void"" as a native return type declaration in the future. Do the same in child class ""Doctrine\Bundle\FixturesBundle\DoctrineFixturesBundle"" now to avoid errors or add an explicit @return annotation to suppress this message.",
"You are confusing MINOR and PATCH. 2.14.2 is not a minor release, it is a patch release. BUT. Short aliases are not a feature of `doctrine/orm`, they are a feature of `doctrine/persistence`, which means that if you rely on them, you (or rather, `jmose/command-scheduler-bundle`) should have `doctrine/persistence` in composer.json, and you should still be using `doctrine/persistence` 2 instead of `doctrine/persistence`3. If that were still the case, you would get a deprecation, not a crash.
A solution for you personally can be to downgrade to `doctrine/persistence` 2, and address the issue when you have the time.
> No error messages when using doctrine/orm 2.14.1
OK, there were no error messages, but was it working? I believe it either didn't, or worked by accident. If you were using persistence 3 at the time, then you were not using short aliases.
To fully understand the issue, it would be great to have [a stack trace](https://symfony.com/doc/current/contributing/code/stack_trace.html)",
"In my defence it was 4am (now 8am) zzz Personally I just removed the bundle and replaced with zenstruck scheduler within 20 mins and deployed that to production already - it's a far superior product anyway
You can close this as it doesn't really affect me now but the fact remains updating a PATCH version broke otherwise working (albeit old bundle) code from working",Bitter Frustration
"As I said, I suspect it wasn't actually working.",
"The scheduler has been running every min of every day for almost 10 years. It's the backbone of a service that has made me several million pounds... but hey, it's fixed now and I can go to bed. Off topic: Also, when someone sponsors you $100 it would be nice, the most minimal thing to do, is to acknowledge it... but you are not alone, many developers don't even acknowledge GitHub sponsorships - ah well.",Entitlement
"Maybe it was working by accident, but I cannot see how if you were using persistence 3, which does not support short aliases since 3.0: https://github.com/doctrine/persistence/blob/3.1.x/UPGRADE.md#bc-break-removed-support-for-short-namespace-aliases
Or maybe you're not using them at all, and there is a call to `addEntityNamespace` with a namespace that you don't use? We will never know since you neglected to provide a stack trace.
Regarding the $100, I must admit I noticed them. In fact we all did, and wondered what was up with you. I thought it was maybe your way to apologize, and they are the reason I decided to help you figure this out today. Are you going to make me regret it? Let's not find out and lock this, again.",Bitter Frustration
"<!--
Before reporting a BC break, please consult the upgrading document to make sure it's not an expected change: https://github.com/doctrine/orm/blob/2.9.x/UPGRADE.md
-->
### BC Break Report
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| BC Break | yes
| Version | 2.14.2
#### Summary
YES I UNDERSTAND that the relied upon feature (short namespace alias) is deprecated BUT upgrading in a MINOR release from 2.14.1 to 2.14.2 should not break an app that was working fine in 2.14.1 - that goes against SEMVER
#### Previous behavior
No error messages when using doctrine/orm 2.14.1 #### Current behavior
after composer update to doctrine/orm 2.14.2 from 2.14.1 - a minor release, now getting app crashes due to relying on deprecated features (I know I know) This is caused by now throwing Exceptions where no exceptions were previously thrown (because of a bug) https://github.com/doctrine/orm/pull/10489
<img width=""1280"" alt=""ScreenShot-2023-04-13-03 44 02"" src=""https://user-images.githubusercontent.com/400092/231634531-65ba81f5-41f7-41af-b810-c8e54e973df5.png"">
#### How to reproduce
The related package is https://github.com/j-guyon/CommandSchedulerBundle which is a 3 year old release. Yes I know I know I know... but still, a minor release of doctrine/orm should not have breaking changes",Bitter Frustration
I'm guessing this is by-design. If you specify an offset you should know what you're doing. @derickr Can you confirm/deny?,Impatience
"I'm guessing not, but am open to that. Actually, completely other thing that I was doing lead me to this ambiguity. So for example, this function works like charm:
```php
<?php
function getIsDaylightSaving($zoneid = 'America/Los_Angeles') {
try {
$date = new DateTime('now', new DateTimeZone($zoneid));
return $date->format('I');
} catch (Exception $e) {
return 400;
}
}
```
So if one try to do following:
```php
<?php
//get daylight saving time status from timezone identifier, $dst = getIsDaylightSaving('America/New_York');
//OUTPUT = 1 //which is OK
//but if I use foo timezone identifier like (this one differs built in IANA timezone db):
$dst = getIsDaylightSaving('ABBA/Mama_Mia');
//The instance new DateTime('now', new DateTimeZone($zoneid)); will throw an exception which will be caught in this function
//so the OUTPUT will be 400 //which is also OK.
```
So my point is that I was surprised that PHP didn't care about timezone offsets, but do care about timezone identifiers, since timezone offsets are only the following: ```php
<?php
$offsets = array('+00:00', '+01:00', '+02:00', '+03:00', '+03:30', '+04:00', '+04:30', '+05:00', '+05:30', '+05:45', '+06:00', '+06:30', '+07:00', '+08:00', '+08:45', '+09:00', '+09:30', '+10:00', '+10:30', '+11:00', '+12:00', '+12:45', '+13:00', '+14:00', '-01:00', '-02:00', '-02:30', '-03:00', '-04:00', '-05:00', '-06:00', '-07:00', '-08:00', '-09:00', '-09:30', '-10:00', '-11:00', '-12:00');
```
For that reason, I think it is a bug, or even some unfinished job. Anyway. I 'dodged a bullet' simply by checking for the offset myself. Everything else is explained in my first comment (i.e., _$time_ variables are expected user input variables).
Hope it helps.",Bitter Frustration
"The offset isn't some magic string that identifies a location like the named timezones. It's an offset. An amount of hours and minutes ahead or behind UTC.
So what's the added value that comes with restricting the offset to only those that are in active, official use by some area of the world? What problems does doing so solve, or what advantages does it offer?
If you're getting these values from the user then how do you know that the user didn't intend -01:10 for some reason? And why ask them for numeric offsets when the named ones are so much more human-friendly?",Bitter Frustration
"> The offset isn't some magic string that identifies a location like the named timezones. It's an offset. An amount of hours and minutes ahead or behind UTC.
> > So what's the added value that comes with restricting the offset to only those that are in active, official use by some area of the world? What problems does doing so solve, or what advantages does it offer?
> > If you're getting these values from the user then how do you know that the user didn't intend -01:10 for some reason? And why ask them for numeric offsets when the named ones are so much more human-friendly?
What is this comment? Some kind of words game? Joke? That I am not into? Apropos what is human-friendly, or not, all those values can be a device snapshot. Some distant device snapshot having an application installed on it, that is programmed by a distant developer on certain distant operating system, with some distant programming language, that a developer will use, through his application, to fetch certain data... Now, that developer can be a professional, but can also be not-professional. And now going deeper in that story that here is being presented to my report is pure game of words as this answer is. Yada yada yada ... Please don't waste my time or yours on unconstructive comments.
That is not a topic here. Anyone's programming tasks, styles or stuff like that is not a topic here. My point is that a PHP allows -01:10 value where it shouldn't. The end. Schluss. Ende. And that is all. And I didn't expect it to do so, to my surprise, though I use PHP on a daily basis since it's version 3.",Bitter Frustration
"That was a lot of words to say ""it's wrong because I think it's wrong"".
Why is it wrong to allow an offset that isn't in use by some (named) timezone? Why must it only allow offsets that can be found applied in (named) timezones?",Bitter Frustration
"Well, because of your 'human readability' - 'user-friendliness' etc. Why would I as a developer risk using PHP DateTime object (DateTimeImmutable), without being sure it'll do all the jobs, because I do not want some Carbon datetime library to mess with my RFC 3339 datetime standards implementation, with my ISO 8601 datetime standards implementation and with WHATWG HTML living standard datetime standard implementation. That's why. But I am flexible. On constructive part of it. And I will always find a solution. This I consider a bug. There's only 37/38 timezone offsets that should have been properly parsed by those functions. Our planet Earth is only so big.",Bitter Frustration
"And again, 'that was a lot...' - you are wasting my time.",Bitter Frustration
The offset `-1:10` does exist; it's 1050 nautical miles west of Greenwich meridian (measuring from the equator). There just doesn't happen to be a timezone that maps to it at present.,
"> The offset `-1:10` does exist; it's 1050 nautical miles west of Greenwich meridian (measuring from the equator). There just doesn't happen to be a timezone that maps to it at present.
Can you really be more STUPID? No one should be so stupid. What thef is wrong with you?",Insulting
"Why would anybody talk about things they do not understand or even comment about it? Why did you delete my last comment? Abuse? Abuse of stupidity? Well, delete everything here. You have my license to do so. F-word.
> The offset `-1:10` does exist; it's 1050 nautical miles west of Greenwich meridian (measuring from the equator). There just doesn't happen to be a timezone that maps to it at present.
Is this intellectual? What is this, PHP????? Joke? some king of programming joke? Programatical joke? Joke? That is imbecilism.
I really never used this possibility to report a bug. On PHP. This is my first one. If you are hitting me with your 'best-ones' than we really are fucked. I kept PHP as authority. Now I am not sure. And it's over. The love is over.
Whether DateTime and it's complementary functions are crap or not, I do not allow such depravity of intelligence. Delete this shit here, and let's go our own ways. My brain always told me - learn all. So I did. This is imbecility. To mark my comment on stupidity as abusive. FU. Mark this one too. While this a-hole https://github.com/damianwadley, with no project or any intelligence is completely OK guy? I am abusive for HAVING to take so much time loss, thinking and SH? Screwya. This is all lie. And I know. And I do not dance to your or anyone elses lies. FU!",Bitter Frustration
Imbeciles.,Insulting
"The best thing in one's organization is when one's organization thinks it is given by the 'Creation'. Deity. godshit-like-things. FU, WE the engineers are keeping your disgusting asses alive. But then, we are the engineers. Why do you do this? Training some sort of discipline? It'll be your demise.",Bitter Frustration
Why would you do all this? Why? Why spin me? Why? Are you all tested? High on whats the name of the drug oh yea are you al high on cocaine? What are you high on?,Bitter Frustration
"### Description
The following code:
```php
<?php
//OK Example:
$time = '2023-02-22 19:44:42.85857+05:30';
$date = DateTimeImmutable::createFromFormat('Y-m-d H:i:s.uP', $time);
echo $date->format('P');
//OUTPUT = +05:30 //OK
//BUG Example:
$time = '2023-02-22 19:44:42.127801-110';
$date = DateTimeImmutable::createFromFormat('Y-m-d H:i:s.uP', $time);
echo $date->format('P');
//OUTPUT = -01:10 //NOT OK, TIMEZONE OFFSET DOESN'T EXIST, must throw an exception
```
But I expected this output instead:
Fatal Error
Shouldn't PHP check the proper existing time zone offset, when it already properly lists timezone identifiers (a.k.a. through DateTimeZone::listIdentifiers())?
### PHP Version
PHP 8.0.28
### Operating System
LiteSpeed",Bitter Frustration
"Apparently this is now the new behaviour: ```
Do not make any assumptions in your code about the number of queries
it takes to flush changes, about the ordering of ``INSERT``, ``UPDATE``
and ``DELETE`` queries or the order in which entities will be processed.
```
and they don't consider it a BC breaking change lol",Bitter Frustration
Damn right we don't! And the paragraph you're quoting does not document new behavior. It merely newly documents that using the ORM does not and never did guarantee any particular insertion order.,Bitter Frustration
"FTR, duplicate of #10864",
"Since release v2.16 this line of code won't save the entities in order of the loop anymore:
```
foreach ($importDossier->getKlanten() as $klant) {
$klant->setIsActive(true);
$this->entityManager->persist($klant);
}
```
The last line to be executed will get an earlier id in the DB than the first iteration of the loop...which seems weird. Is this expected behaviour?
I noticed work was done on the commit order here: https://github.com/doctrine/orm/pull/10547",Bitter Frustration
Manners.,Mocking
"### Bug Report
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| BC Break | yes
| Version | I have now idea how to find it
#### Summary
I can't rename migration file from Version{date} to Migration{date}
#### Current behavior
I HAVE to use illogical naming Version. But I'm extending Migration. It does not make any sence.
#### How to reproduce
Rename Version{date}.php to Migration{Date}.php as it SHOULD be.
#### Expected behavior
Find what whatever file I want",Bitter Frustration
"Open Source projects have recently had many nonsensical CVE reports from people scanning the commit logs. If they see messages like ""leak"" or other issue, they file a CVE. Presumably in order to either get some minor fame, or to get some kind of bounty.
We do not participate in such issues.
In this case, the code is pretty obvious. On top of that, it's sheer laziness to ask a question instead of reading the code.
And even without that, the issue template you deleted has explicit instructions for what is a good issue, and what information we need from an issue.
This comment is not helpful, and is not useful. We are not going to go down the path of explaining the code to people who can't be bothered to read the code and think about it.",Bitter Frustration
"### Message
Hello, is this https://github.com/FreeRADIUS/freeradius-server/commit/33b0b7d31bae7b9e78a23496b2c2f36693fb2761 a one-time leak or could it lead to an accessibility issue?",
This behavior is not tolerated here.,Threat
"Hi,
I'm going to be polite. This non-sense is baffling: [Setup-pdf.js](https://github.com/mozilla/pdf.js/wiki/Setup-pdf.js-in-a-website)
And from an older issue: [Issue 9210](https://github.com/mozilla/pdf.js/issues/9210#issuecomment-347834276) Quote: **""Hence we don't want to make it too simple to use it directly.""** Honestly, what is that????
What I'm supposed to do? Go through the ~2000 LOC of `PDFViewerApplication` from `web/viewer.js` to just make the full viewer work? Now I think I understand **why** it wasn't added under `examples`. Can you please explain what's the whole point of this crap?
Maybe you can learn a bit from Leaflet, simply adding proper attribution should be enough provided you want credit. Please make things simpler and easier, and not harder as quoted. No one wants to waste time.",Bitter Frustration
"(a) This was decided almost a year ago,, and (b) if you want to discuss this more then please hop on the internals mailing list.
But before you do, realize that ""next"" is a rather common term used with random number sequences - as it is, in fact, basically a matter of _sequences_ and not simply conjuring some number out of a hat. That means PHP would actually be joining a club whose members use the word ""next"" when describing their randomness APIs:
* [Python 3's `random.random()`](https://docs.python.org/3/library/random.html) is documented as ""return[ing] the next random floating point number in the range 0.0 <= X < 1.0"".
* [Java's `java.util.Random` class](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Random.html) has similar methods like ""next"", ""nextBoolean"", ""nextBytes"", and so on.
* Unsurprisingly, [Microsoft Java's own `System.Random` class](https://learn.microsoft.com/en-us/dotnet/api/system.random?view=net-7.0) has a variety of methods named ""Next"", ""NextBytes"", ""NextDouble"", ""NextInt64"", and so on.
* [Rust's `rand::RngCore` trait](https://docs.rs/rand/latest/rand/trait.RngCore.html), which is its lowest-level ""interface"" for randomness, names its two core methods ""next_u32"" and ""next_u64"".
So with any luck, allow me to offer my congratulations on you learning why the name ""nextFloat"" is not misleading after all.
(edited for humor)",Mocking
"> This was decided almost a year ago,,
What kind of argument is that? As long as the stable is not issued, these things can be fixed, right? Similar to [PhpToken::getAll() vs PhpToken::tokenize](https://bugs.php.net/bug.php?id=80328). > But before you do, realize that ""next"" is a rather common term used with random number sequences But beware, those libraries don't combine *get* and *next* at the same time! They have all methods named *next*. That's perfectly fine. The problem is that in PHP we have `getFloat()` and `nextFloat()` at the same time, and the only difference is that the latter has a hardcoded interval. That's nonsense. Is it really not obvious to you?",Bitter Frustration
"> Is it really not obvious to you?
What's obvious to me is that the Randomizer API was introduced a year ago, nobody has complained about it since then, 8.3 just had its fourth RC with about 6 weeks left until the final release, and your primary argument for renaming the method is ""[I] find the name completely misleading"".
If ""I find it misleading"" was not easily countered by ""read the documentation"" then I'd pile on:
* It's misleading that `array_rand` returns a random key and not a random value
* It's misleading how `crypt` works, with its different modes and different salting requirements
* It's misleading what `parse_str` is supposed to parse
* It's misleading that the two different syntaxes of `strtok` are meant to work together in the same block of code
* It's misleading that the majority of PHP's string functions only support ASCII and not UTF-8/Unicode
* It's misleading that `echo` and `print` do the same thing yet have different calling syntaxes and requirements
* It's misleading that we have `_sleep`/`__wakeup` _and_ `__serialize`/`__unserialize` _and_ `__set_state` _and_ `Serializable`
* It's misleading that `stdClass` isn't a base class for all objects, which is something nearly all OOP languages have
* It's misleading that PHP doesn't have inherited variable scopes
* It's misleading that backed enums use strings/ints for their backed values and yet they aren't comparable (12410 at least)
* It's misleading that a Stringable object isn't compatible with a `string` hint in strict_types mode (12097)
* It's misleading that a `foreach($array as &$ref)` persists the reference after the loop (12335, 11885, 10369, and many others)
* It's misleading that a class with a `__toString` doesn't have its definition hoisted as of PHP 8.0 (12320, 8323, and others)
* It's misleading that PHP overflows/underflows dates when parsing (12121, 10479, and many others)
* It's misleading that `trim`/`rtrim`/`ltrim` trim characters and not substrings (11977, 11434, 9969, and others)
* Floating-point math (12198, 11070, 10661, and many others)
(if it's not obvious, towards the end there I started looking through the NAB list to find common problems)
Understand that I'm _not_ saying none of those issues above should ever be changed. I'm also not saying that those things are not worthy of complaining about because ""iT's DoCuMeNtEd BeHaViOr"".
What I am saying is that PHP just isn't a language where you can blindly start writing code and calling functions and expect everything to pan out as you might think. And that's also true, to varying degrees, for every programming language out there. So saying that something is misleading just isn't the hefty argument it sounds like: because there's a bunch of things that are going to be misleading to some people and not misleading to others; there's always going to be something you or I won't understand at first glance, and for the simple reason that neither of us are the sole voice of PHP's design decisions.
I doubt either of us wants to continue this conversation much further now, but I specifically mentioned going to the mailing list because _that's a totally valid reaction_ to me declining this report. Because you know what else is misleading? The fact that the PHP project does most of its discussions and decision-making on the mailing list and not in these GitHub Issues, and thus that we tend to treat feature requests here more like informal asks (ones that get closed after a few weeks of inactivity, no less) than full-blooded requests.",Bitter Frustration
ðŸ¤¦ðŸ¼â€â™‚ï¸,Bitter Frustration
ðŸ¤¦ I'm so glad I left PHP world many years ago...,Bitter Frustration
"dg started a thread on the mailing list. If anyone has any _on-topic_ contributions to make, that's the place for it.
https://externals.io/message/121312
https://www.php.net/mailing-lists.php",Irony
"### Description
I would like to suggest changing the name or removing the `nextFloat()` method. Now is ideal time before the stable release, then it would be too late of course.
The name `nextFloat()` creates the expectation that it will return some other number in the generated series. See, for example, the following code:
```php
$randomizer = new \Random\Randomizer();
$a = $randomizer->getFloat(100, 200); // number between 100..200
$b = $randomizer->nextFloat(); // another number between 100..200 ???
```
Do you expect `$b` to be the next number in the interval 100..200? Surprisingly not, the method returns a number in the interval 0..1. Honestly, I don't understand the `next` prefix in the method name at all. Its [definition is](https://wiki.php.net/rfc/randomizer_additions#nextfloat):
> This method is equivalent to ->getFloat(0, 1)
That is, a function that returns a number in the interval 0..1 without any relation to anything previous. I therefore find the name completely misleading. A more accurate name would be, for example, `getFloat0To1()`, but I think that `getFloat(0, 1)` is obvious and there is no need to add a new method.",Bitter Frustration
"I'm glad someone still remembers this incident. A few days after the attack, the author had quietly removed that piece of shit information. Is it a coincidence?",Bitter Frustration
Sorry but nope. Please stop asking for banners. We don't take requests.,Impatience
"I had no intention for this PR to actually get merged, just wanted to highlight the double standard here. When civilians were dying in Ukraine, Composer showed support with a banner, and I respect that. Now, when it's Palestine, the response is different.
I get it, Composer isnâ€™t a political platform. But, it's disheartening to see the selective empathy especially when the tech community has the power to raise awareness on serious issues.
Since opening this PR, more children have died in Gaza than all civilian casualties in Ukraine over the past month. In fact, the number of people who have died in Gaza in the past 17 days equals [the civilian casualties in the Ukraine war over the past year and a half](https://www.ohchr.org/en/news/2023/09/ukraine-civilian-casualty-update-11-september-2023). This really puts things into perspective.",Bitter Frustration
"Hey dude, when Palestinians go to Israel and burn children alive it's okay, but when Israel responds it's not. I'm not sure why someone ever should stand with Palestine if it should be StandWithIsrael.",Identify Attack/Name Calling
"No, this is grammatically incorrect.",
"What the heck is going on here with all these spam attempts. I just don't get it, I guess they didn't get enough attention as a child.",Bitter Frustration
