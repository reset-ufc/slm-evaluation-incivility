index,message,prediction,actual,source
4,"How does https://github.com/sidorares/nodejs-mysql-native handle this? Any reason not to just borrow parts of the way it's done over there?
I'm still somewhat struggling with the number of different MySQL drivers for Node.JS. I think Node makes it way too fun to write network protocol code. :P Maybe in a year or so the community will have coalesced around one or two really solid libraries.",0,1,msr
7,"@dresende the SQL method still winds up tampering with values to make them safe (escaping quotes, etc), whereas the protocol method explicitly separates query from values so tampering is not necessary. To be fair, as long as its impossible to smuggle a query in as a value, the driver is plenty secure enough. I suppose I'm just being a nitpicky ex-PHP developer who wants everything to be conceptually elegant. :P",0,1,msr
8,"I'm not sure if this is officially supported by Doctrine, since it is not documented, but it works - with an error notice.
For example, if I have documents A and B where document B has an embedded document, E, I can design the mapping as follows:
``` php
/** @Document */
class A {
/** @ReferenceMany(targetDocument=""B"", mappedBy=""foo.bar"") */
private $whatever = [];
}
/** @Document */
class B {
/** @EmbedMany(targetDocument=""E"") */
private $foo = [];
}
/** @EmbeddedDocument */
class E {
/** @ReferenceOne(targetDocument=""A"", inversedBy=""whatever"") */
private $bar;
}
```
However, reading A::$whatever generates the following notice:
> Notice: Undefined index: foo.bar in mongodb-odm/lib/Doctrine/ODM/MongoDB/Persisters/DocumentPersister.php on line 709
Despite this, the document collection is retrieved correctly.",0,0,msr
9,"This doesn't work if class `B` has a binary ID and `$bar` is a discriminator reference. I'm not sure what the culprit is here, but the symptomns are that `count(A::$whatever)` will never be > 1. That is, at most only one item is loaded into the collection on the field with an embedded document mapping.",0,0,msr
10,This issue has been automatically marked as stale because it has not had any recent activity. It will be closed in a week if no further activity occurs. Thank you for your contributions.,0,0,msr
11,Imagine thinking anyone cares about design flaws.,0,1,msr
12,Imagine thinking anyone cares about Open Source being a joint effort.,0,1,msr
13,Imagine thinking open source means other people work for you for free.,0,1,msr
16,Why do you close the issue?,0,0,msr
17,"Because you don't offer a patch, and profanity really pisses me off.",0,1,msr
18,"Mutable programming pisses me off even more.
Anyway, the point of github issues is to have a bug tracker.
The contributions comes in the pull-requests tab.",0,1,msr
20,"As for your second point, you may be onto something there, but it's too late to change it, it's unreasonable overhead to call back to the server to calculate that every time, the results are cached for a reason. If you populate the cache with bad data, then you can't really effectively clear it. I accept that it's confusing, but I won't change it.",0,0,msr
22,"I won't fix it, and it's been the way it is for five years without anyone running into problems. I can't afford the time to test it, fix it and make sure it's safe before releasing it. That's what it boils down to. Those variables are a source of a lot of confusion, and they are fragile, and have different meanings wherever you call them, anyway.
I'd like to be able to fix it, your profane issue report aside; but the reality is, I simply can't.",0,0,msr
23,Then leave this issue open until the end of times.,0,1,msr
24,"Otherwise it would be the first time in my entire life in which I see a bug closed as WONTFIX.
I mean, I've seen feature requests closed as WONTFIX status. But bugs? It's like denying to recognize that there is a bug.",0,0,msr
28,@knocte did you reproduce it on v3?,0,0,msr
30,"See rubygems, early september
On 2 Nov 2013 20:30, ""Andres G. Aragoneses"" notifications@github.com
wrote:
> Oh, when was that released?
> > —
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630493
> .",0,0,msr
32,"We have more than 100 obsolete issues to close, and past experience
suggests the OPs never reapond
On 2 Nov 2013 20:36, ""Andres G. Aragoneses"" notifications@github.com
wrote:
> Ok, then I have not tested that. And I'm afraid I will not be able to test
> it very soon.
> > Just one advice: if you want to close issue in order to request feedback
> from users about new versions, add a comment explaining why you are closing
> the issue.
> Thanks
> > —
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630609
> .",0,0,msr
33,That doesn't respect the Robust principle ;) http://en.wikipedia.org/wiki/Robustness_principle,0,1,msr
34,"People are still upvoting my stackoverflow answer linked above, which seems to hint that this bug is still present in 3.x.",0,0,msr
35,thanks for checking the so answer upvote rates @knocte,0,0,msr
36,"Running into this issue currently I think.
How does one call the current release path that is right now being built?
I have a task running at `after :updated, :build do` and I'm trying to change directories and then run composer install, but changing directories using release_path is giving me the previous release, not the one currently being built.
Is that this issue? If not I'm sorry but the stackoverflow question led me here and I'm going kind of crazy trying to figure out how to change directories and then run a command, without it doing it in the release that's about to be replaced.",0,0,msr
41,"The only query / issue I have is the big lines it draws blue (or themed borders) around countries.
There is a method to remove them?",0,0,msr
43,"It's a shame you couldn't provide an answer. Pointing to a documentation page that has no response to the original question isn't useful at all. Equally, it's pretty useless having something that draws a map with a big blue border around every country and no explanation as to how to remove it.",0,1,msr
44,"@abrice This kind of passive-aggressiveness is disrespectful towards maintainers.
I suggest reading [""How To Ask Questions The Smart Way""](http://www.catb.org/esr/faqs/smart-questions.html) and [""How to Report Bugs Effectively""](http://www.chiark.greenend.org.uk/~sgtatham/bugs.html) so you can make good bug reports in the future.",0,1,msr
48,"No tutorial really, just execute shell commands or something similar. http://php.net/shell_exec
`shell_exec('php /path/to/composer.phar install');`
Never really tried it however.",0,0,msr
49,"Unfortunately, `shell_exec` and similar functions are also disabled on most shared hosting that don't support ssh login.
The current solution is to run composer locally and upload the vendor dir with the rest of your files.
As a gift, you can use [composer as a service](http://composer.borreli.com/) by @pborreli to download a zip of all your dependencies. It must be possible to create a simple PHP page that upload your `composer.json`, download the zip and extract it.",0,0,msr
52,"If you don't have a CLI access on these systems, there is a high probably than other tools used by composer are not available either. In such case, the proper workflow would indeed be to run composer locally and to upload the code including the vendor folder, a suggested by @GromNaN",0,0,msr
53,WordPress can download and modify files from internet. I'm sure it is possible to make a similar installer (dependency-manager) with user interface.,0,0,msr
56,"We have created a client for the Contao CMS to use composer directly from the CMS Backend.
https://github.com/contao-community-alliance/composer-client
The client can run composer in 3 modes:
- inline: directly within the web servers php process
- process: start a process with `proc_open` within the web servers php process
- detached: start and fork composer into background process",0,0,msr
57,"This isn't really an issue with composer and should be closed. > Sadly Composer makes PHP projects for only rich people who can buy hostings with SSH access.
That's entirely untrue as Digital Ocean sells VPS for $5/month and Linode sells them for $10/month.",0,0,msr
58,"Hi, im running node-mysql latest on node-latest.
Somebody using the acunetix vulnerability scanner has triggered this error:
UNKNOWN COLUMN '$acunetix' IN WHERE CLAUSE.
The query: SELECT id, email FROM accounts WHERE username = ?
How is this possible? Its very dangerous to our application, please respond quickly.",0,1,msr
59,"The problem seems to be about params that are not strings. Although I'll continue to sanitize all my user inputs (to avoid username impersonation attacks like `admіn` posing as `admin`), I'd expect the query engine to convert any param to a string if it should have been one in the first place. If it already was, `String(param)` should be of low cost.",0,0,msr
60,@thekiur @mk-pmb can you post code samples?,0,0,msr
61,"We can confirm that the problem is caused by passing objects to the query call.
The objects come from the express bodyParser middleware.
We were simply passing req.body.username as the parameter for that query.
The acunetic vulnerability tester injected an object there.
We are not sure on the severity of this issue, but its unexpected to say atleast.
As we experienced, this can crash a live application in production mode if you dont expect any db errors.
There is no code to show: its as simple as passing a req.body.something to the .query call of node-mysql when using express with the bodyparser middleware. Running the vulnerability scanner against https://gist.github.com/ssafejava/9a2d77704712a8769322 causes the exception to be thrown.",0,0,msr
62,"This is not an issue with escaping with this library; this library is properly escaping all values and column names. The security issue is just with the way you are combining express and this library, such that you were expecting to get a string from express, so you were only expecting the `?` to expand according to string rules.
`req.body` properties can be anything with `bodyParser` and as such you need to at least verify what you are using is a string before passing to your query.",0,0,msr
63,"I consider prepared statements as intended to mitigate lack of input validation in the params in general. Therefor, limiting it to the case where input has already been validated as being a string, in my opinion misses the point.
Yours, MK",0,0,msr
65,"I see. Looks like an unlucky case of embrace and extend. I wish you had opted for something like `??` in that case. Probably too late to change the interface?
Edit: Not really embrace and extend, as you wrote they aren't prepared statements. Rather just a pitfall for people who learn from tutorials and conclude topical similarity from visual similarity.
Edit 2: I see, `??` is already used for column names.",0,0,msr
67,"@mk-pmb sure, though this module only has a small Readme, which has all the `?` stuff explained (https://github.com/felixge/node-mysql#escaping-query-values), so it's not even some weird hidden feature. Unfortunately if people on the Internet are writing tutorials about this module and giving incomplete or wrong information, it's hard for us to even try to police that.",0,0,msr
68,"@mk-pmb it's the programmers role to understand the libraries he/she is using at least to the extend they are documented before including them in any production environment. If the library isn't fully documented, that's on the creator, but since this is an open-source world you can't really blame somebody for dedicating their time towards creating something for free.
Inferring functionality from syntax is useful, but think rationally: if the `?` operator accepts strings, would it only accepts strings? What if it accepted other data types? Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.
Libraries and languages that make it easier to start developing are extremely useful, but I fear it gives a novice developer a misplaced sense of confidence. It's easy to build a small application, and when it ""just works"" assume nothing could possibly go wrong.",0,0,msr
69,"> Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.
I agree with that. And still, lots of people do it. So for all software that I manage, I'll try and have it be compatible with everyday flawed humans, in hopes to lessen the risk and impact of errors in software based on mine, written by fallible humans.
BOfH would ship a GNU/Linux distro where the default shell acts fully like bash, just that on every line starting with an uppercase letter, the meaning of `&&` and `||` is swapped. Might even document it properly. You'd read the manual and probably wouldn't use it. However, if the next day a toy drone crashes into your car because it's pilot didn't read the manual as thoroughly as you did, your expectations of how humans should act had much less impact than how they really do act. And I'd still partially blame that BOfH.
Update: Thanks for making it opt-in.",0,0,msr
70,"Please, this issue doesn't need any more comments. It is still open as a tracking issue for me. There are coming changes that will affects this module and even things like `express` which will make any kind of ""shoot yourself in the foot"" operations opt-in. As an example, for this module `?` really should strictly only result in a single entry in the SQL (i.e. numbers, strings, null, etc.). Anything over that should be opt-in (on the connection-level or one-off on the query level to reduce accidental exposure.
These are changes that are coming I listed, not speculation. Please just know that this issue is taken seriously.",0,0,msr
73,@SystemParadox yeah I just took a look at the formatting and escaping code. I don't see any way that passing unvalidated data to be interpolated into the query could result in an injection vulnerability. Without validation you can easily get a syntax error.,0,0,msr
74,"Could you please implement xBR shader or xBRZ filter or both in GSDX plugin. It would be very beneficial for both PS2 and PSX 2D and sprite-based games.
xBR and xBRZ are pixel art scaling algorithms ,they give best results in 2D/sprite based games with low resolution textures and games with pre-rendered backgrounds which dont upscale well with higher internal resolutions but they also give good results in 3D games. xBR/xBRZ are already used with good results in emulators like Retroarch , Higan, Desmume and PPSSPP.
Here is explanation:http://code.google.com/p/2dimagefilter/wiki/ImageScaling#xBR
http://www.vogons.org/viewtopic.php?t=34125
Here is newest xBR source code including hybrid variants: https://github.com/libretro/common-shaders
Source code for xBRZ is in source code of HqMAME : https://sourceforge.net/projects/hqmame/ ,http://sourceforge.net/projects/hqmame/files/xBRZ.zip/download, Spline36: http://code.google.com/p/remote-joy-lite-fix/source/browse/trunk/RemoteJoyLite_pc/spline36.psh
https://github.com/xbmc/xbmc/tree/master/xbmc/cores/VideoRenderers/VideoShaders
Here is comparison for 3D graphics:http://blog.metaclassofnil.com/?p=306
Here is official tutorial about xBR: http://www.libretro.com/forums/viewtopic.php?f=6&t=134
http://forum.zdoom.org/viewtopic.php?f=19&t=37373&sid=57269f5e32514a88a5d5252839c9ff6a&start=45
Some 2D graphics of old version of xBR: http://imgur.com/a/ZZiiH
I also found interesting algorithm Libdepixelize: http://bazaar.launchpad.net/~vinipsmaker/libdepixelize/trunk/revision/184
http://vinipsmaker.wordpress.com/tag/libdepixelize/
https://sourceforge.net/projects/inkscape/
There is also ''Ours'' but I cant find source code for it anywhere: http://research.microsoft.com/en-us/um/people/kopf/pixelart/supplementary/
But Libdepixelize and Ours both use Kopf-Lischinski algorithm so they should have similiar effects.
http://www.mediafire.com/download/22o6ahnchkbzhef/Shaders.rar
http://www.mediafire.com/download/86bo6bl66cnwv2j/chromaNEDI.rar
https://github.com/jpsdr/NNEDI3
http://forum.doom9.org/showthread.php?t=170727",0,0,msr
77,"Instead of asking, try submitting a patch with your desired changes.",0,0,msr
79,"This is something i would like to see in the future, the way its implemented in PPSSPP is great. Although really not very necessary for PS2 emulation just a nice extra.",0,0,msr
81,"It wouldn't actually... since its design/function is for sprites, not textures.",0,0,msr
82,Its possible to use it for textures as well.,0,0,msr
88,"xBR/xBRZ are ugly because computers are not artists, they ruin the artwork of games.",0,1,msr
90,yes that's true ratchet and clank games have this problem and are very ugly only works in software mode and it's freaking me out with a amd fx 8350 ¬¬ a points of fps of 20 or 30,0,1,msr
91,"You mean Native graphics, most of which created professionally by artist then you want to ruin it with an over-exaggerated interpolation.",0,1,msr
92,"Ratchet & Clank was my first PS2 game I ever owned, i have being waiting years for it to be fixed in PCSX2",0,0,msr
93,Developers wanted games to look like they look in high resolution but were limited by underpowered hardware.,0,0,msr
94,Developers certainly not want to apply a silly image interpolation like xBR to ruin all their artwork,0,1,msr
95,Prove they dont.,0,1,msr
97,Snes games have huge blocky pixels because hardware was primitive and not because developers intended it to look that way. xBR is advanced upscaling algorithm not ''silly gimmicky interpolation'' show little respect to shader/filter developers.,0,1,msr
100,xBR destroys Pixel art,0,1,msr
101,Pixel art is term invented by nostalgia fetishists no such thing exists chunky pixels are result of low resolution forced by weak hardware.,0,1,msr
105,Nearest-Neighbor is primitive garbage you may like what you want but I prefer games to not look like blurry blobs.,0,1,msr
109,"Arguing aside, PPSSPP has it i think it works well (that's IMO). Something to consider if there is enough of a demand for it. Also i believe PPSSPP De-posterizes the textures as well which helps with the colour banding.",0,0,msr
110,"Your argument about XBRZ being good or not is irrelevant. In the case of a github feature request, nobody cares about your points about XBRZ being wrong because it's not a forum discussion thread. The fact is that there is a huge amount of emulation scene fans that really enjoy XBRZ texture scalling... ...and the context ends here. With PPSSPP leading the way and lots of other emulators now following these steps (PS1, N64...) it's safe to say that everybody would turn to see if PCSX2 would follow on a such well recieved feature. It's not a leage of who likes it or not, making points etc, it's a polite-asking feature request and it's actually possible. Now it's up to somebody with the interest and skill to pick it up. If closed source plugins like Pete's OpenGL2 could be modified to achieve such effect then I bet PCSX2/GSDx can aswell. Let's just not argue if it's possible or not.",0,0,msr
111,nobody cares,0,1,msr
112,#800,0,0,msr
114,"It would be great for games like metal slug, megaman, etc.",0,0,msr
116,"I would like to have a deeper knowledge to implement it, unfortunately not.
However I have an idea how this goes, bilinear filtering for pixel art just like Xbrz, so if it can be implemented, it goes right here. We need an expert developer on the subject.
![image](https://user-images.githubusercontent.com/46450049/97360816-afce4e80-186c-11eb-9d76-8f4c1bc0f4fc.png)",0,0,msr
120,then I would like to add bicubic,0,0,msr
122,"![2021-04-06_17h16_08](https://user-images.githubusercontent.com/42075250/113734722-dafe7900-96fb-11eb-91a4-aeb572ca14c1.png)
Which is better ? Xbr or nearest neighbor ?",0,0,msr
125,"> xbr or nearest neighbor ?
nearest neighbor. your mistaking the smooth look of xbr with an HD upscale. what it is, is a smoothed over approximation of nearest neighbor. i don't think any game studio would have intentionally made a game that looks like the xbr. it some how looks blockier and chunkier than nearest neighbor.",0,0,msr
127,"> > xbr or nearest neighbor ?
> > nearest neighbor. your mistaking the smooth look of xbr with an HD upscale. what it is, is a smoothed over approximation of nearest neighbor. i don't think any game studio would have intentionally made a game that looks like the xbr. it some how looks blockier and chunkier than nearest neighbor.
It literally does not matter what game studios would or would not have their games look like.
xBR is not perfect, but it is the closest we are going to get in real time to high resolution 2D assests (short of texture replacement, wich not all games will have; and it fills disc space, wich might not be ideal to some people, depending on the amout of games they own).
If you prefer nearest neighbor, nice, luckily that option is already available to you.
This is not a discussion of wich is better (wich is highly subjective); this is about having the option to use a feature that is already available in other emulators, that a number of people enjoy.",0,0,msr
128,"Locking as the discussion here hasn't been very productive.
If you're interested in implementing these shaders please open a pull request.
If you're wondering about the status of this feature request, check if there is a linked pull request.",0,0,msr
130,"```
SPL: Loaded module v0.6.3-1
ZFS: Loaded module v0.6.3-1, ZFS pool version 5000, ZFS filesystem version 5
```
doing nothing but some basic rsync's with moderate sizes 4-10G result always in having approx 1MB/sec throughtput (very slow) on a up-date 16G RAM HP server - CentOS 6 with OpenVZ and selfcompiled modules for ZFS - dmesg:
```
INFO: task rsync:6517 blocked for more than 120 seconds.
Tainted: P --------------- 2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
rsync D ffff8804029bafb0 0 6517 6516 0 0x00000080
ffff88001e55da18 0000000000000086 0000000000000003 00000003d2dad320
000000000001ec80 0000000000000001 ffff88001e55dab8 0000000000000082
ffffc900105a3428 ffff8803fe837d60 ffff8804029bb578 000000000001ec80
Call Trace:
[<ffffffff810a1dfe>] ? prepare_to_wait_exclusive+0x4e/0x80
[<ffffffffa019fb35>] cv_wait_common+0x105/0x1c0 [spl]
[<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
[<ffffffffa019fc45>] __cv_wait+0x15/0x20 [spl]
[<ffffffffa02ae1fb>] txg_wait_open+0x8b/0x110 [zfs]
[<ffffffffa027194e>] dmu_tx_wait+0x29e/0x2b0 [zfs]
[<ffffffff81530bfe>] ? mutex_lock+0x1e/0x50
[<ffffffffa0271a41>] dmu_tx_assign+0x91/0x490 [zfs]
[<ffffffffa027fab7>] ? dsl_dataset_block_freeable+0x27/0x60 [zfs]
[<ffffffffa02e8d3e>] zfs_write+0x43e/0xcf0 [zfs]
[<ffffffff8100bc4e>] ? apic_timer_interrupt+0xe/0x20
[<ffffffff811c5e5c>] ? core_sys_select+0x1ec/0x2d0
[<ffffffffa02fd354>] zpl_write_common+0x54/0xd0 [zfs]
[<ffffffffa02fd438>] zpl_write+0x68/0xa0 [zfs]
[<ffffffff811ac798>] vfs_write+0xb8/0x1a0
[<ffffffff811ad091>] sys_write+0x51/0x90
[<ffffffff810f4dee>] ? __audit_syscall_exit+0x25e/0x290
[<ffffffff8100b102>] system_call_fastpath+0x16/0x1b
INFO: task txg_sync:876 blocked for more than 120 seconds.
Tainted: P --------------- 2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
txg_sync D ffff880405d28640 0 876 2 0 0x00000000
ffff8803fe839b70 0000000000000046 0000000000000001 ffff880405101430
0000000000000000 0000000000000000 ffff8803fe839af0 ffffffff81060032
ffff8803fe839b40 ffffffff81054939 ffff880405d28c08 000000000001ec80
Call Trace:
[<ffffffff81060032>] ? default_wake_function+0x12/0x20
[<ffffffff81054939>] ? __wake_up_common+0x59/0x90
[<ffffffff8152f833>] io_schedule+0x73/0xc0
[<ffffffffa019fadc>] cv_wait_common+0xac/0x1c0 [spl]
[<ffffffffa02f53e0>] ? zio_execute+0x0/0x140 [zfs]
[<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
[<ffffffffa019fc08>] __cv_wait_io+0x18/0x20 [spl]
[<ffffffffa02f561b>] zio_wait+0xfb/0x1b0 [zfs]
[<ffffffffa02867e3>] dsl_pool_sync+0xb3/0x440 [zfs]
[<ffffffffa029a67b>] spa_sync+0x40b/0xae0 [zfs]
[<ffffffffa02aebb4>] txg_sync_thread+0x384/0x5e0 [zfs]
[<ffffffff8105b309>] ? set_user_nice+0xc9/0x130
[<ffffffffa02ae830>] ? txg_sync_thread+0x0/0x5e0 [zfs]
[<ffffffffa01978e8>] thread_generic_wrapper+0x68/0x80 [spl]
[<ffffffffa0197880>] ? thread_generic_wrapper+0x0/0x80 [spl]
[<ffffffff810a1596>] kthread+0x96/0xa0
[<ffffffff8100c34a>] child_rip+0xa/0x20
[<ffffffff810a1500>] ? kthread+0x0/0xa0
[<ffffffff8100c340>] ? child_rip+0x0/0x20
```",1,0,msr
133,"not similar but related message:
Aug 29 05:37:06 morpheus kernel: [46185.239554] ata6.00: configured for UDMA/133
Aug 29 05:37:06 morpheus kernel: [46185.239562] ata6: EH complete
Aug 29 05:53:40 morpheus kernel: [47179.890587] INFO: task txg_sync:1462 blocked for more than 180 seconds.
Aug 29 05:53:40 morpheus kernel: [47179.890589] Tainted: P O 3.16.0_ck1-smtnice6_BFQ_integra_intel #1
Aug 29 05:53:40 morpheus kernel: [47179.890590] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Aug 29 05:53:40 morpheus kernel: [47179.890591] txg_sync D 0000000000000006 0 1462 2 0x00000000
Aug 29 05:53:40 morpheus kernel: [47179.890594] ffff8805fbda9e40 0000000000000046 ffff88020931a9b0 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890596] 0000000000000066 ffff8807fb2a0f20 000000000000b020 0000000000013100
Aug 29 05:53:40 morpheus kernel: [47179.890597] ffff88065b6103a0 00002992b81d47dd ffff880617373fd8 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890599] Call Trace:
Aug 29 05:53:40 morpheus kernel: [47179.890605] [<ffffffff82a3eaa8>] ? io_schedule+0x88/0xd0
Aug 29 05:53:40 morpheus kernel: [47179.890613] [<ffffffffc028b536>] ? __cv_timedwait+0x96/0x110 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890616] [<ffffffff820fa7b0>] ? finish_wait+0x90/0x90
Aug 29 05:53:40 morpheus kernel: [47179.890623] [<ffffffffc03c38cb>] ? zio_wait+0xeb/0x1a0 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890631] [<ffffffffc03553da>] ? dsl_pool_sync+0xaa/0x450 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890639] [<ffffffffc036d4c3>] ? spa_sync+0x483/0xb20 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890642] [<ffffffff820f7c3d>] ? default_wake_function+0xd/0x20
Aug 29 05:53:40 morpheus kernel: [47179.890644] [<ffffffff821168bd>] ? ktime_get_ts+0x3d/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890652] [<ffffffffc037ce8a>] ? txg_sync_start+0x6ea/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890655] [<ffffffff820013ca>] ? __switch_to+0x2a/0x560
Aug 29 05:53:40 morpheus kernel: [47179.890662] [<ffffffffc037cb60>] ? txg_sync_start+0x3c0/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890664] [<ffffffffc0286f75>] ? spl_kmem_fini+0xa5/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890667] [<ffffffffc0286f00>] ? spl_kmem_fini+0x30/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890669] [<ffffffff820ea5dc>] ? kthread+0xbc/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890670] [<ffffffff82a40000>] ? __ww_mutex_lock_slowpath+0x8c/0x2cc
Aug 29 05:53:40 morpheus kernel: [47179.890672] [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 05:53:40 morpheus kernel: [47179.890674] [<ffffffff82a4242c>] ? ret_from_fork+0x7c/0xb0
Aug 29 05:53:40 morpheus kernel: [47179.890675] [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 06:28:01 morpheus kernel: [49241.775276] usb 1-1.1: USB disconnect, device number 3
this seems to occur from time to time with a rather slow USB3.0 powered 4TB hdd (Touro Desk 3.0, HGST5K4000) in an external case during rsync & transferring of large files (several GiB)",0,0,msr
135,@wankdanker I think that your issue is separate. It might have been caused by the zvol processing occuring inside an interrupt context. Pull request #2484 might resolve it.,0,0,msr
136,"@freakout42 Would you tell us more about your pool configuration? Also, do you have data deduplication enabled on this pool?",0,0,msr
137,"```
[root@blood ~]# zpool status
pool: tank
state: ONLINE
scan: scrub repaired 0 in 0h53m with 0 errors on Tue Sep 9 12:45:13 2014
config:
NAME STATE READ WRITE CKSUM
tank ONLINE 0 0 0
mirror-0 ONLINE 0 0 0
sda4 ONLINE 0 0 0
sdb4 ONLINE 0 0 0
errors: No known data errors
[root@blood ~]# zpool get all
NAME PROPERTY VALUE SOURCE
tank size 824G -
tank capacity 34% -
tank altroot - default
tank health ONLINE -
tank guid 3198719639486948540 default
tank version - default
tank bootfs - default
tank delegation on default
tank autoreplace off default
tank cachefile - default
tank failmode wait default
tank listsnapshots off default
tank autoexpand off default
tank dedupditto 0 default
tank dedupratio 1.00x -
tank free 538G -
tank allocated 286G -
tank readonly off -
tank ashift 0 default
tank comment - default
tank expandsize 0 -
tank freeing 0 default
tank feature@async_destroy enabled local
tank feature@empty_bpobj active local
tank feature@lz4_compress enabled local
```",0,0,msr
141,"I'm systematically having this issue when trying to RSync when using the latest ZFS from Arch: zfs-git 0.6.3_r170_gd958324f_3.18.2_2-1
```
[46181.967521] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
[79468.027144] INFO: task txg_sync:583 blocked for more than 120 seconds.
[79468.027287] Tainted: P O 3.18.2-2-ARCH #1
[79468.027363] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[79468.027468] txg_sync D 0000000000000000 0 583 2 0x00000000
[79468.027476] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[79468.027483] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[79468.027489] ffff8801591955b8 ffffffff00000000 ffff880159195570 00000000025e92cf
[79468.027494] Call Trace:
[79468.027509] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[79468.027518] [<ffffffff81550b59>] schedule+0x29/0x70
[79468.027524] [<ffffffff81550e38>] io_schedule+0x98/0x100
[79468.027547] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[79468.027554] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[79468.027565] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[79468.027588] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[79468.027616] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[79468.027625] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[79468.027656] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[79468.027663] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[79468.027694] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[79468.027724] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[79468.027734] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[79468.027742] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[79468.027749] [<ffffffff81090e0a>] kthread+0xea/0x100
[79468.027755] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[79468.027761] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[79468.027767] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120097] INFO: task txg_sync:583 blocked for more than 120 seconds.
[82589.120252] Tainted: P O 3.18.2-2-ARCH #1
[82589.120347] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[82589.120479] txg_sync D 0000000000000001 0 583 2 0x00000000
[82589.120489] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[82589.120497] ffff88021ce03fd8 0000000000013640 ffff8802240ceeb0 ffff88021cd5eeb0
[82589.120505] ffff88022363f7b0 ffff88022363f798 0000000000000000 0000000000000003
[82589.120512] Call Trace:
[82589.120529] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[82589.120540] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[82589.120549] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[82589.120558] [<ffffffff81550b59>] schedule+0x29/0x70
[82589.120564] [<ffffffff81550e38>] io_schedule+0x98/0x100
[82589.120591] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[82589.120599] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[82589.120613] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[82589.120641] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[82589.120676] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[82589.120689] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[82589.120727] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[82589.120735] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[82589.120774] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[82589.120811] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[82589.120823] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[82589.120834] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[82589.120842] [<ffffffff81090e0a>] kthread+0xea/0x100
[82589.120849] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120858] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[82589.120864] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.468460] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89311.468644] Tainted: P O 3.18.2-2-ARCH #1
[89311.468741] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89311.468872] txg_sync D 0000000000000000 0 583 2 0x00000000
[89311.468882] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89311.468890] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89311.468897] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89311.468905] Call Trace:
[89311.468922] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89311.468933] [<ffffffff81550b59>] schedule+0x29/0x70
[89311.468940] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89311.468968] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89311.468976] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89311.468989] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89311.469017] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89311.469052] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89311.469065] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89311.469103] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89311.469112] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89311.469150] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89311.469187] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89311.469199] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89311.469211] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89311.469218] [<ffffffff81090e0a>] kthread+0xea/0x100
[89311.469226] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.469234] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89311.469241] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
```
```
hinoki% free -h
total used free shared buff/cache available
Mem: 7.8G 4.3G 533M 1.1M 3.0G 702M
Swap: 4.0G 0B 4.0G
```",1,0,msr
142,"Still trying to run rsync:
```
[89431.510429] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89431.510571] Tainted: P O 3.18.2-2-ARCH #1
[89431.510647] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89431.510753] txg_sync D 0000000000000000 0 583 2 0x00000000
[89431.510762] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89431.510769] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89431.510774] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89431.510780] Call Trace:
[89431.510796] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89431.510805] [<ffffffff81550b59>] schedule+0x29/0x70
[89431.510811] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89431.510836] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89431.510842] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89431.510853] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89431.510877] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89431.510906] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89431.510915] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89431.510946] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89431.510953] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89431.510984] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89431.511014] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89431.511024] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89431.511033] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89431.511039] [<ffffffff81090e0a>] kthread+0xea/0x100
[89431.511045] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89431.511052] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89431.511058] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.552404] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89551.552565] Tainted: P O 3.18.2-2-ARCH #1
[89551.552660] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89551.552792] txg_sync D 0000000000000000 0 583 2 0x00000000
[89551.552803] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89551.552812] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89551.552819] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89551.552826] Call Trace:
[89551.552844] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89551.552854] [<ffffffff81550b59>] schedule+0x29/0x70
[89551.552862] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89551.552890] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89551.552898] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89551.552911] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89551.552940] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89551.552975] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89551.552987] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89551.553025] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89551.553034] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89551.553073] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89551.553110] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89551.553122] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89551.553133] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89551.553140] [<ffffffff81090e0a>] kthread+0xea/0x100
[89551.553148] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.553156] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89551.553163] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.594371] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89671.594507] Tainted: P O 3.18.2-2-ARCH #1
[89671.594605] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89671.594711] txg_sync D 0000000000000000 0 583 2 0x00000000
[89671.594720] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89671.594727] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89671.594733] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89671.594738] Call Trace:
[89671.594753] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89671.594762] [<ffffffff81550b59>] schedule+0x29/0x70
[89671.594768] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89671.594792] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89671.594798] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89671.594809] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89671.594832] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89671.594860] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89671.594870] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89671.594901] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89671.594908] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89671.594939] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89671.594969] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89671.594979] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89671.594988] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89671.594994] [<ffffffff81090e0a>] kthread+0xea/0x100
[89671.595000] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.595007] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89671.595013] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636364] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89791.636507] Tainted: P O 3.18.2-2-ARCH #1
[89791.636583] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89791.636688] txg_sync D 0000000000000000 0 583 2 0x00000000
[89791.636697] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89791.636703] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89791.636709] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89791.636715] Call Trace:
[89791.636729] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89791.636738] [<ffffffff81550b59>] schedule+0x29/0x70
[89791.636744] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89791.636767] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89791.636774] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89791.636785] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89791.636808] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89791.636836] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89791.636852] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89791.636883] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89791.636890] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89791.636921] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89791.636951] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89791.636960] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89791.636969] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89791.636975] [<ffffffff81090e0a>] kthread+0xea/0x100
[89791.636981] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636989] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89791.636994] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90105.367103] systemd[1]: systemd-journald.service stop-sigabrt timed out. Terminating.
[90105.490377] systemd[1]: Listening on Journal Audit Socket.
[90105.490436] systemd[1]: Starting Journal Service...
[90151.762415] INFO: task kswapd0:31 blocked for more than 120 seconds.
[90151.762569] Tainted: P O 3.18.2-2-ARCH #1
[90151.762663] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.762795] kswapd0 D 0000000000000000 0 31 2 0x00000000
[90151.762805] ffff8802235a3908 0000000000000046 ffff880224225a90 0000000000013640
[90151.762814] ffff8802235a3fd8 0000000000013640 ffffffff81818540 ffff880224225a90
[90151.762821] ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.762828] Call Trace:
[90151.762845] [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.762856] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.762865] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.762893] [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.762902] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.762915] [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.762956] [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.762984] [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763011] [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763040] [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763049] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763075] [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763083] [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763090] [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763097] [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763106] [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763115] [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763123] [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763131] [<ffffffff8116e0e9>] kswapd_shrink_zone+0x129/0x1d0
[90151.763138] [<ffffffff8116eb7a>] kswapd+0x54a/0x8f0
[90151.763147] [<ffffffff8116e630>] ? mem_cgroup_shrink_node_zone+0x1c0/0x1c0
[90151.763155] [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.763162] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763171] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.763178] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763190] INFO: task systemd-journal:138 blocked for more than 120 seconds.
[90151.763340] Tainted: P O 3.18.2-2-ARCH #1
[90151.763433] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.763563] systemd-journal D 0000000000000000 0 138 1 0x00000004
[90151.763571] ffff88022310b128 0000000000000082 ffff880222c25080 0000000000013640
[90151.763577] ffff88022310bfd8 0000000000013640 ffff88021cd5e4a0 ffff880222c25080
[90151.763583] ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.763590] Call Trace:
[90151.763599] [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.763607] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.763614] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.763627] [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.763635] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.763647] [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.763683] [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.763710] [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763737] [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763765] [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763774] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763799] [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763805] [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763812] [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763819] [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763827] [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763834] [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763842] [<ffffffff811c1523>] ? mem_cgroup_iter+0x2f3/0x4d0
[90151.763850] [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763858] [<ffffffff8116da45>] do_try_to_free_pages+0x365/0x4e0
[90151.763866] [<ffffffff8116dc71>] try_to_free_pages+0xb1/0x1a0
[90151.763873] [<ffffffff81160ca7>] __alloc_pages_nodemask+0x697/0xb50
[90151.763884] [<ffffffff811a6e9c>] alloc_pages_current+0x9c/0x120
[90151.763891] [<ffffffff811afba5>] new_slab+0x305/0x370
[90151.763899] [<ffffffff811b2175>] __slab_alloc.isra.51+0x545/0x650
[90151.763907] [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763914] [<ffffffff814380f4>] ? raw_pci_write+0x24/0x50
[90151.763923] [<ffffffff812e8be6>] ? pci_bus_write_config_word+0x66/0x80
[90151.763949] [<ffffffffa004ed7f>] ? ata_bmdma_start+0x2f/0x40 [libata]
[90151.763960] [<ffffffffa01121ad>] ? atiixp_bmdma_start+0x9d/0xe0 [pata_atiixp]
[90151.763969] [<ffffffff811b5445>] __kmalloc_node_track_caller+0xa5/0x240
[90151.763976] [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763984] [<ffffffff81445a11>] __kmalloc_reserve.isra.38+0x31/0x90
[90151.763990] [<ffffffff81445acb>] ? __alloc_skb+0x5b/0x210
[90151.763997] [<ffffffff81445af9>] __alloc_skb+0x89/0x210
[90151.764004] [<ffffffff81445de4>] alloc_skb_with_frags+0x64/0x1e0
[90151.764011] [<ffffffff8143efb9>] sock_alloc_send_pskb+0x219/0x290
[90151.764020] [<ffffffff810136fb>] ? __switch_to+0x1fb/0x600
[90151.764029] [<ffffffff814f886d>] unix_dgram_sendmsg+0x18d/0x690
[90151.764037] [<ffffffff8143ac39>] sock_sendmsg+0x79/0xb0
[90151.764045] [<ffffffff810986da>] ? finish_task_switch+0x4a/0xf0
[90151.764051] [<ffffffff815504c8>] ? __schedule+0x3e8/0xa50
[90151.764059] [<ffffffff8143c76c>] ? move_addr_to_kernel+0x2c/0x50
[90151.764066] [<ffffffff8144a3c7>] ? verify_iovec+0x47/0xd0
[90151.764074] [<ffffffff8143b928>] ___sys_sendmsg+0x408/0x420
[90151.764083] [<ffffffff812149d0>] ? ep_read_events_proc+0xe0/0xe0
[90151.764089] [<ffffffff81440030>] ? sk_prot_alloc.isra.33+0x30/0x130
[90151.764097] [<ffffffff811b291a>] ? kmem_cache_alloc+0x16a/0x170
[90151.764104] [<ffffffff811d100c>] ? get_empty_filp+0x5c/0x1c0
[90151.764112] [<ffffffff8126e496>] ? security_file_alloc+0x16/0x20
[90151.764118] [<ffffffff811d1084>] ? get_empty_filp+0xd4/0x1c0
[90151.764126] [<ffffffff811d118f>] ? alloc_file+0x1f/0xb0
[90151.764134] [<ffffffff8143d651>] __sys_sendmsg+0x51/0x90
[90151.764142] [<ffffffff8143d6a2>] SyS_sendmsg+0x12/0x20
[90151.764149] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[90151.764186] INFO: task txg_sync:583 blocked for more than 120 seconds.
[90151.764325] Tainted: P O 3.18.2-2-ARCH #1
[90151.764418] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.764547] txg_sync D 0000000000000000 0 583 2 0x00000000
[90151.764554] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[90151.764560] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[90151.764566] ffff88018de384b8 ffffffff00000000 ffff88018de38490 00000000025e92cf
[90151.764572] Call Trace:
[90151.764581] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[90151.764588] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.764594] [<ffffffff81550e38>] io_schedule+0x98/0x100
[90151.764607] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[90151.764615] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.764627] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[90151.764651] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[90151.764686] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[90151.764698] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[90151.764735] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[90151.764744] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[90151.764781] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[90151.764818] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[90151.764830] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[90151.764841] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[90151.764847] [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.764855] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.764862] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.764869] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90195.648689] systemd[1]: systemd-journald.service stop-sigterm timed out. Killing.
[90195.649942] systemd[1]: Starting Journal Service...
```",0,0,msr
144,"More of the same
```
[ 1320.575152] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1320.575263] Tainted: P O 3.18.2-2-ARCH #1
[ 1320.575314] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1320.575384] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1320.575390] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1320.575395] ffff88021ba43fd8 0000000000013640 ffff88021dee2840 ffff8800cbfc4670
[ 1320.575399] ffff880131dcc138 ffffffff00000000 ffff880131dcc130 00000000b3f1aac7
[ 1320.575402] Call Trace:
[ 1320.575414] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1320.575420] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1320.575424] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1320.575442] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1320.575447] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1320.575453] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1320.575470] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1320.575490] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1320.575496] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1320.575517] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1320.575522] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1320.575543] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1320.575563] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1320.575569] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1320.575575] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1320.575579] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1320.575583] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1320.575588] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1320.575592] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.661841] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1560.661995] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.662090] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662221] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1560.662232] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1560.662240] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1560.662247] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1560.662255] Call Trace:
[ 1560.662272] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1560.662283] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1560.662292] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1560.662301] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.662307] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1560.662335] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1560.662343] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.662355] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1560.662383] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1560.662418] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1560.662430] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1560.662468] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1560.662477] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1560.662516] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1560.662552] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1560.662564] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1560.662575] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1560.662583] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1560.662590] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662598] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1560.662605] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662617] INFO: task java:1218 blocked for more than 120 seconds.
[ 1560.662753] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.662846] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662975] java D 0000000000000001 0 1218 655 0x00000000
[ 1560.662983] ffff8800c17f7ac8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1560.662990] ffff8800c17f7fd8 0000000000013640 ffff8802240ceeb0 ffff8800c33e1420
[ 1560.662997] 0000008000000000 ffff88002041b7a0 0000000000000000 ffff88002041b778
[ 1560.663003] Call Trace:
[ 1560.663027] [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1560.663049] [<ffffffffa0261eca>] ? dbuf_read+0x8da/0xf20 [zfs]
[ 1560.663061] [<ffffffffa01c8901>] ? kmem_asprintf+0x51/0x80 [spl]
[ 1560.663068] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663080] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663088] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663099] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663126] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663153] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663161] [<ffffffff810e1006>] ? getrawmonotonic+0x36/0xd0
[ 1560.663186] [<ffffffffa0268e1c>] dmu_free_long_range+0x1ac/0x280 [zfs]
[ 1560.663217] [<ffffffffa02daf1c>] zfs_rmnode+0x6c/0x340 [zfs]
[ 1560.663244] [<ffffffffa02fe141>] zfs_zinactive+0xc1/0x1d0 [zfs]
[ 1560.663273] [<ffffffffa02f6974>] zfs_inactive+0x64/0x200 [zfs]
[ 1560.663281] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[ 1560.663307] [<ffffffffa030ef18>] zpl_vap_init+0x838/0xa10 [zfs]
[ 1560.663315] [<ffffffff811eab68>] evict+0xb8/0x1b0
[ 1560.663322] [<ffffffff811eb405>] iput+0xf5/0x1a0
[ 1560.663330] [<ffffffff811df7f2>] do_unlinkat+0x1e2/0x350
[ 1560.663337] [<ffffffff811d4839>] ? SyS_newstat+0x39/0x60
[ 1560.663345] [<ffffffff811e02c6>] SyS_unlink+0x16/0x20
[ 1560.663353] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1560.663371] INFO: task sshd:1381 blocked for more than 120 seconds.
[ 1560.663508] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.663601] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.663791] sshd D 0000000000000001 0 1381 764 0x00000004
[ 1560.663798] ffff880113ad7b38 0000000000000086 ffff8800a7356eb0 0000000000013640
[ 1560.663804] ffff880113ad7fd8 0000000000013640 ffff8802240ceeb0 ffff8800a7356eb0
[ 1560.663810] ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000280 0000000000000001
[ 1560.663816] Call Trace:
[ 1560.663825] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663839] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663847] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663858] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663885] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663912] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663940] [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1560.663951] [<ffffffffa01c8956>] ? kmem_free_debug+0x16/0x20 [spl]
[ 1560.663962] [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1560.663972] [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1560.663998] [<ffffffffa02fe288>] ? zfs_tstamp_update_setup+0x38/0x1c0 [zfs]
[ 1560.664026] [<ffffffffa02f0cfe>] ? zfs_read+0x39e/0x460 [zfs]
[ 1560.664049] [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1560.664056] [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1560.664082] [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1560.664106] [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1560.664114] [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1560.664122] [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1560.664128] [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1560.664137] [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1560.664145] [<ffffffff81554f20>] int_signal+0x12/0x17
[ 1680.702704] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1680.702864] Tainted: P O 3.18.2-2-ARCH #1
[ 1680.702958] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1680.703089] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1680.703100] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1680.703108] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1680.703116] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1680.703123] Call Trace:
[ 1680.703141] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1680.703152] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1680.703161] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1680.703169] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1680.703176] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1680.703203] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1680.703211] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1680.703224] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1680.703252] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1680.703287] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1680.703299] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1680.703338] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1680.703346] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1680.703385] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1680.703422] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1680.703433] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1680.703444] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1680.703452] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1680.703459] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1680.703467] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1680.703474] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.742712] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1800.742869] Tainted: P O 3.18.2-2-ARCH #1
[ 1800.742964] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1800.743095] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1800.743106] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1800.743114] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1800.743121] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1800.743129] Call Trace:
[ 1800.743146] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1800.743157] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1800.743165] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1800.743174] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1800.743180] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1800.743206] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1800.743214] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1800.743227] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1800.743256] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1800.743290] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1800.743302] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1800.743341] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1800.743349] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1800.743388] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1800.743425] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1800.743437] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1800.743448] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1800.743455] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1800.743462] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.743471] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1800.743477] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782195] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1920.782354] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.782449] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.782580] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1920.782590] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1920.782599] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1920.782606] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1920.782614] Call Trace:
[ 1920.782631] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1920.782641] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1920.782650] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1920.782658] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.782665] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1920.782693] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1920.782701] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.782714] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1920.782742] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1920.782777] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1920.782789] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1920.782827] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1920.782836] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1920.782874] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1920.782911] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1920.782923] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1920.782934] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1920.782942] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1920.782949] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782957] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1920.782964] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782976] INFO: task java:1218 blocked for more than 120 seconds.
[ 1920.783136] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.783236] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.783366] java D 0000000000000000 0 1218 655 0x00000000
[ 1920.783373] ffff8800c17f7ab8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1920.783381] ffff8800c17f7fd8 0000000000013640 ffffffff81818540 ffff8800c33e1420
[ 1920.783387] ffff8800c17f7a08 ffffffffa025a872 0000000000000000 ffff8800235be4a0
[ 1920.783393] Call Trace:
[ 1920.783416] [<ffffffffa025a872>] ? arc_buf_eviction_needed+0x82/0xc0 [zfs]
[ 1920.783439] [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1920.783476] [<ffffffffa02b634d>] ? bp_get_dsize+0xad/0xf0 [zfs]
[ 1920.783503] [<ffffffffa02770e4>] ? dmu_tx_callback_register+0x324/0xab0 [zfs]
[ 1920.783512] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.783524] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.783532] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.783544] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.783570] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.783597] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.783627] [<ffffffffa02810e5>] ? dsl_dataset_block_freeable+0x45/0x1d0 [zfs]
[ 1920.783653] [<ffffffffa0276fb9>] ? dmu_tx_callback_register+0x1f9/0xab0 [zfs]
[ 1920.783681] [<ffffffffa02f74a0>] zfs_write+0x3c0/0xbf0 [zfs]
[ 1920.783688] [<ffffffff810a9dee>] ? enqueue_entity+0x24e/0xaa0
[ 1920.783696] [<ffffffff8109a1f0>] ? resched_curr+0xd0/0xe0
[ 1920.783705] [<ffffffff810ec497>] ? wake_futex+0x67/0x90
[ 1920.783711] [<ffffffff810ef856>] ? do_futex+0x8f6/0xae0
[ 1920.783736] [<ffffffffa030d2cb>] zpl_putpage+0x21b/0xd50 [zfs]
[ 1920.783744] [<ffffffff811cf1d7>] vfs_write+0xb7/0x200
[ 1920.783752] [<ffffffff811cfd29>] SyS_write+0x59/0xd0
[ 1920.783760] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1920.783778] INFO: task imap:1389 blocked for more than 120 seconds.
[ 1920.783916] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.784009] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.784138] imap D 0000000000000000 0 1389 770 0x00000000
[ 1920.784145] ffff88000f97fb38 0000000000000082 ffff880222d3bc60 0000000000013640
[ 1920.784151] ffff88000f97ffd8 0000000000013640 ffff88009d290a10 ffff880222d3bc60
[ 1920.784161] ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000fd8 0000000000000001
[ 1920.784168] Call Trace:
[ 1920.784176] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.784190] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.784197] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.784209] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.784236] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.784263] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.784292] [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1920.784303] [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1920.784314] [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1920.784338] [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1920.784345] [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1920.784372] [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1920.784395] [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1920.784403] [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1920.784411] [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1920.784417] [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1920.784426] [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1920.784434] [<ffffffff81554f20>] int_signal+0x12/0x17
[ 2280.901736] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 2280.901897] Tainted: P O 3.18.2-2-ARCH #1
[ 2280.901992] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 2280.902124] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 2280.902134] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 2280.902143] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 2280.902150] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 2280.902157] Call Trace:
[ 2280.902175] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 2280.902186] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 2280.902194] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 2280.902203] [<ffffffff81550b59>] schedule+0x29/0x70
[ 2280.902210] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 2280.902237] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 2280.902245] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 2280.902258] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 2280.902286] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 2280.902321] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 2280.902333] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 2280.902371] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 2280.902380] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 2280.902419] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 2280.902455] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 2280.902467] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 2280.902478] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 2280.902486] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 2280.902493] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 2280.902502] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 2280.902508] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[38275.738573] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
```
Running spl versions from arch:
```
hinoki% pacman -Q | grep spl
spl-git 0.6.3_r54_g03a7835_3.18.2_2-1
spl-utils-git 0.6.3_r54_g03a7835_3.18.2_2-1
```",1,0,msr
147,"I'm seeing similar symptoms after running a `zfs rollback` on a SMB shared dataset. After seeing load quickly rise I've stopped the SMB service and will give the rollback command some time to see if it recovers.
From dmesg, on Ubuntu Server 14.04.2 LTS, ZoL 0.6.3-5 from the Ubuntu PPA:
```
[3764100.795021] smbd D ffff88010b433480 0 34303 60686 0x00000000
[3764100.795025] ffff8800ae491d68 0000000000000082 ffff8800cf951800 ffff8800ae491fd8
[3764100.795029] 0000000000013480 0000000000013480 ffff8800cf951800 ffff8800e48dc3d0
[3764100.795032] ffff8800e48dc3a0 ffff8800e48dc3d8 ffff8800e48dc3c8 0000000000000000
[3764100.795037] Call Trace:
[3764100.795052] [<ffffffff817251a9>] schedule+0x29/0x70
[3764100.795067] [<ffffffffa006b7b5>] cv_wait_common+0x125/0x1c0 [spl]
[3764100.795073] [<ffffffff810ab0b0>] ? prepare_to_wait_event+0x100/0x100
[3764100.795091] [<ffffffffa006b865>] __cv_wait+0x15/0x20 [spl]
[3764100.795130] [<ffffffffa017019b>] rrw_enter_read+0x3b/0x150 [zfs]
[3764100.795181] [<ffffffffa01bf65d>] zfs_getattr_fast+0x3d/0x180 [zfs]
[3764100.795230] [<ffffffffa01d81fd>] zpl_getattr+0x2d/0x50 [zfs]
[3764100.795234] [<ffffffff811c2829>] vfs_getattr_nosec+0x29/0x40
[3764100.795237] [<ffffffff811c28fd>] vfs_getattr+0x2d/0x40
[3764100.795240] [<ffffffff811c29d2>] vfs_fstatat+0x62/0xa0
[3764100.795244] [<ffffffff811c2e5f>] SYSC_newstat+0x1f/0x40
[3764100.795248] [<ffffffff811cdc99>] ? putname+0x29/0x40
[3764100.795252] [<ffffffff811bcfe8>] ? do_sys_open+0x1b8/0x280
[3764100.795256] [<ffffffff811c30ae>] SyS_newstat+0xe/0x10
[3764100.795260] [<ffffffff8173186d>] system_call_fastpath+0x1a/0x1f
[3764100.795263] INFO: task smbd:34313 blocked for more than 120 seconds.
```",0,0,msr
148,"I can trigger the same error by using rsync from two different pools. The rsync process is hanging and can not be killed.
If I won't stop the rsync from the cli, the <my target pool> will endup into a fault status. The fault status is gone after a reboot.
```
May 07 22:43:27 <my host> kernel: INFO: task txg_sync:1408 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel: Tainted: P O 4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: txg_sync D ffff880028b63a28 0 1408 2 0x00000000
May 07 22:43:27 <my host> kernel: ffff880028b63a28 ffff88018c5f3cc0 ffff880070a16540 0000000000000000
May 07 22:43:27 <my host> kernel: ffff880028b63fd8 ffff88021fc93e00 7fffffffffffffff ffff8801b1ccae08
May 07 22:43:27 <my host> kernel: 0000000000000001 ffff880028b63a48 ffffffff8156fa87 ffff88008e7bdcb0
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel: [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel: [<ffffffff8157246c>] schedule_timeout+0x1bc/0x250
May 07 22:43:27 <my host> kernel: [<ffffffff8101f599>] ? read_tsc+0x9/0x10
May 07 22:43:27 <my host> kernel: [<ffffffff810e6757>] ? ktime_get+0x37/0xb0
May 07 22:43:27 <my host> kernel: [<ffffffff8156ef9a>] io_schedule_timeout+0xaa/0x130
May 07 22:43:27 <my host> kernel: [<ffffffffa034daa0>] ? zio_taskq_member.isra.6+0x80/0x80 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa0192248>] cv_wait_common+0xb8/0x140 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel: [<ffffffffa0192328>] __cv_wait_io+0x18/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa034f943>] zio_wait+0x123/0x210 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02d6be1>] dsl_pool_sync+0xc1/0x480 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02f1f80>] spa_sync+0x480/0xbf0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b36>] ? autoremove_wake_function+0x16/0x40
May 07 22:43:27 <my host> kernel: [<ffffffffa0303e06>] txg_sync_thread+0x386/0x630 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff8156fd62>] ? preempt_schedule_common+0x22/0x40
May 07 22:43:27 <my host> kernel: [<ffffffffa0303a80>] ? txg_quiesce_thread+0x3a0/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa018d561>] thread_generic_wrapper+0x71/0x80 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa018d4f0>] ? __thread_exit+0x20/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff81093338>] kthread+0xd8/0xf0
May 07 22:43:27 <my host> kernel: [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel: [<ffffffff81573718>] ret_from_fork+0x58/0x90
May 07 22:43:27 <my host> kernel: [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel: INFO: task rsync:10064 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel: Tainted: P O 4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: rsync D ffff8801a4633a58 0 10064 1 0x00000004
May 07 22:43:27 <my host> kernel: ffff8801a4633a58 ffff880216186f60 ffff880070a56540 ffff8801a4633a68
May 07 22:43:27 <my host> kernel: ffff8801a4633fd8 ffff8800da2a4a20 ffff8800da2a4ae0 ffff8800da2a4a48
May 07 22:43:27 <my host> kernel: 0000000000000000 ffff8801a4633a78 ffffffff8156fa87 ffff8800da2a4a20
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel: [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel: [<ffffffffa019229d>] cv_wait_common+0x10d/0x140 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel: [<ffffffffa01922e5>] __cv_wait+0x15/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa030327b>] txg_wait_synced+0x8b/0xd0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c078c>] dmu_tx_wait+0x25c/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c096e>] dmu_tx_assign+0x9e/0x520 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c8e10>] ? dsl_dataset_block_freeable+0x20/0x70 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02be639>] ? dmu_tx_count_dnode+0x59/0xb0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa033f5de>] zfs_write+0x3ce/0xc50 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff81515821>] ? unix_stream_recvmsg+0x701/0x7e0
May 07 22:43:27 <my host> kernel: [<ffffffffa03558cd>] zpl_write+0xbd/0x130 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff811d56b3>] vfs_write+0xb3/0x200
May 07 22:43:27 <my host> kernel: [<ffffffff811d624c>] ? vfs_read+0x11c/0x140
May 07 22:43:27 <my host> kernel: [<ffffffff811d6399>] SyS_write+0x59/0xd0
May 07 22:43:27 <my host> kernel: [<ffffffff815737c9>] system_call_fastpath+0x12/0x17
May 07 22:44:27 <my host> kernel: WARNING: Pool '<my target pool>' has encountered an uncorrectable I/O failure and has been suspended.
May 07 22:44:27 <my host> zed[12566]: eid=2302 class=data pool=<my target pool>
May 07 22:44:27 <my host> zed[12569]: eid=2303 class=io_failure pool=<my target pool>
```
```
# zpool status -v
pool: <my target pool>
state: ONLINE
scan: scrub repaired 0 in 9h15m with 0 errors on Thu May 7 09:01:37 2015
config:
NAME STATE READ WRITE CKSUM
<my target pool> ONLINE 0 0 0
<my target pool>-crypt ONLINE 0 0 0
errors: No known data errors
pool: <my source pool>
state: ONLINE
scan: scrub repaired 0 in 6h3m with 0 errors on Thu May 7 05:49:33 2015
config:
NAME STATE READ WRITE CKSUM
<my source pool> ONLINE 0 0 0
mirror-0 ONLINE 0 0 0
crypt-<my source pool>-00 ONLINE 0 0 0
crypt-<my source pool>-01 ONLINE 0 0 0
errors: No known data errors
```
```
#modinfo zfs | head
filename: /lib/modules/4.0.1-1-ARCH/extra/zfs/zfs.ko.gz
version: 0.6.4.1-1
license: CDDL
author: OpenZFS on Linux
description: ZFS
srcversion: 8324F6AEA2A06B2B6F0A0F5
depends: spl,znvpair,zunicode,zcommon,zavl
vermagic: 4.0.1-1-ARCH SMP preempt mod_unload modversions ```
```
#modinfo spl | head
filename: /lib/modules/4.0.1-1-ARCH/extra/spl/spl.ko.gz
version: 0.6.4.1-1
license: GPL
author: OpenZFS on Linux
description: Solaris Porting Layer
srcversion: 8907748310B8940C9D0DCD2
depends: vermagic: 4.0.1-1-ARCH SMP preempt mod_unload modversions ```
```
#uname -a
Linux <my host> 4.0.1-1-ARCH #1 SMP PREEMPT Wed Apr 29 12:00:26 CEST 2015 x86_64 GNU/Linux
```
Fingers crossed I've provided good information. I'm running an arch linux with demz repo.",0,0,msr
157,"I'm suffering from the similar problem. `rsync` reads/writes are extremely slow ~3M. After struggling like this ~24h (it's a multi-million file dataset) machine gets bricked. Please advice.
---
```
# uname -a
Linux ip-172-30-0-118 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
```
```
# lsb_release -a
No LSB modules are available.
Distributor ID: Debian
Description: Debian GNU/Linux 8.5 (jessie)
Release: 8.5
Codename: jessie
```
```
# dpkg -l '*zfs*' | grep ii
ii libzfs2linux 0.6.5.7-1 amd64 OpenZFS filesystem library for Linux
ii zfs-dkms 0.6.5.7-1 all OpenZFS filesystem kernel modules for Linux
ii zfs-zed 0.6.5.7-1 amd64 OpenZFS Event Daemon
ii zfsutils-linux 0.6.5.7-1 amd64 command-line tools to manage OpenZFS filesystems
```
```
# dpkg -l '*spl*' | grep ii
ii spl 0.6.5.7-1 amd64 Solaris Porting Layer user-space utilities for Linux
ii spl-dkms 0.6.5.7-1 all Solaris Porting Layer kernel modules for Linux
```
```
# zpool status
pool: zfs-backup
state: ONLINE
scan: none requested
config:
NAME STATE READ WRITE CKSUM
zfs-backup ONLINE 0 0 0
xvdf ONLINE 0 0 0
logs
xvdg ONLINE 0 0 0
errors: No known data errors
```
```
# zpool iostat -v
capacity operations bandwidth
pool alloc free read write read write
---------- ----- ----- ----- ----- ----- -----
zfs-backup 564G 380G 538 0 1.48M 2.42K
xvdf 564G 380G 538 0 1.48M 2.11K
logs - - - - - -
xvdg 0 1.98G 0 0 471 312
---------- ----- ----- ----- ----- ----- -----
```
```
# zpool get all
NAME PROPERTY VALUE SOURCE
zfs-backup size 944G -
zfs-backup capacity 59% -
zfs-backup altroot - default
zfs-backup health ONLINE -
zfs-backup guid 2876612074418704500 default
zfs-backup version - default
zfs-backup bootfs - default
zfs-backup delegation on default
zfs-backup autoreplace off default
zfs-backup cachefile - default
zfs-backup failmode wait default
zfs-backup listsnapshots off default
zfs-backup autoexpand off default
zfs-backup dedupditto 0 default
zfs-backup dedupratio 1.00x -
zfs-backup free 380G -
zfs-backup allocated 564G -
zfs-backup readonly off -
zfs-backup ashift 0 default
zfs-backup comment - default
zfs-backup expandsize - -
zfs-backup freeing 0 default
zfs-backup fragmentation 44% -
zfs-backup leaked 0 default
zfs-backup feature@async_destroy enabled local
zfs-backup feature@empty_bpobj active local
zfs-backup feature@lz4_compress active local
zfs-backup feature@spacemap_histogram active local
zfs-backup feature@enabled_txg active local
zfs-backup feature@hole_birth active local
zfs-backup feature@extensible_dataset enabled local
zfs-backup feature@embedded_data active local
zfs-backup feature@bookmarks enabled local
zfs-backup feature@filesystem_limits enabled local
zfs-backup feature@large_blocks enabled local
```
```
Jun 15 16:55:40 ip-172-30-0-118 kernel: [113640.618167] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:55:41 ip-172-30-0-118 kernel: [113640.621686] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.625572] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.629691] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:55:44 ip-172-30-0-118 kernel: [113640.633524] Call Trace:
Jun 15 16:55:45 ip-172-30-0-118 kernel: [113640.634740] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:55:47 ip-172-30-0-118 kernel: [113640.637492] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:55:48 ip-172-30-0-118 kernel: [113640.640409] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.643375] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.646118] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:55:50 ip-172-30-0-118 kernel: [113640.649130] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:55:51 ip-172-30-0-118 kernel: [113640.651910] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.655111] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.658303] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:55:54 ip-172-30-0-118 kernel: [113640.661609] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:55:55 ip-172-30-0-118 kernel: [113640.664528] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:55:56 ip-172-30-0-118 kernel: [113640.667024] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.670190] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.672778] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:57:39 ip-172-30-0-118 kernel: [113760.681975] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:57:40 ip-172-30-0-118 kernel: [113760.685521] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:57:41 ip-172-30-0-118 kernel: [113760.689295] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:57:42 ip-172-30-0-118 kernel: [113760.693211] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:57:43 ip-172-30-0-118 kernel: [113760.697110] Call Trace:
Jun 15 16:57:44 ip-172-30-0-118 kernel: [113760.698425] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:57:45 ip-172-30-0-118 kernel: [113760.701106] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:57:47 ip-172-30-0-118 kernel: [113760.704131] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:57:48 ip-172-30-0-118 kernel: [113760.707157] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:57:49 ip-172-30-0-118 kernel: [113760.709944] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:57:51 ip-172-30-0-118 kernel: [113760.713084] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:57:52 ip-172-30-0-118 kernel: [113760.715920] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:57:54 ip-172-30-0-118 kernel: [113760.719014] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:57:55 ip-172-30-0-118 kernel: [113760.722283] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.725571] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.728485] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:57:58 ip-172-30-0-118 kernel: [113760.730974] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:58:00 ip-172-30-0-118 kernel: [113760.734102] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:58:02 ip-172-30-0-118 kernel: [113760.736819] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:38 ip-172-30-0-118 kernel: [113880.747394] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.750796] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.754658] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:59:41 ip-172-30-0-118 kernel: [113880.758412] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:59:42 ip-172-30-0-118 kernel: [113880.762234] Call Trace:
Jun 15 16:59:43 ip-172-30-0-118 kernel: [113880.763454] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:59:44 ip-172-30-0-118 kernel: [113880.766117] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.769024] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.772106] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:59:46 ip-172-30-0-118 kernel: [113880.775310] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:59:48 ip-172-30-0-118 kernel: [113880.778331] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:59:50 ip-172-30-0-118 kernel: [113880.781198] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:59:51 ip-172-30-0-118 kernel: [113880.784324] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:59:52 ip-172-30-0-118 kernel: [113880.787500] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:59:54 ip-172-30-0-118 kernel: [113880.790757] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:59:55 ip-172-30-0-118 kernel: [113880.793707] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:59:56 ip-172-30-0-118 kernel: [113880.796081] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:57 ip-172-30-0-118 kernel: [113880.799104] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:59:58 ip-172-30-0-118 kernel: [113880.801724] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:41 ip-172-30-0-118 kernel: [114000.817906] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 17:01:42 ip-172-30-0-118 kernel: [114000.822256] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 17:01:43 ip-172-30-0-118 kernel: [114000.827952] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.833092] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.837533] Call Trace:
Jun 15 17:01:47 ip-172-30-0-118 kernel: [114000.838994] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 17:01:48 ip-172-30-0-118 kernel: [114000.842069] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 17:01:49 ip-172-30-0-118 kernel: [114000.845833] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 17:01:50 ip-172-30-0-118 kernel: [114000.849362] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 17:01:51 ip-172-30-0-118 kernel: [114000.852630] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 17:01:52 ip-172-30-0-118 kernel: [114000.856037] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 17:01:53 ip-172-30-0-118 kernel: [114000.859185] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 17:01:54 ip-172-30-0-118 kernel: [114000.862633] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 17:01:55 ip-172-30-0-118 kernel: [114000.866301] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 17:01:56 ip-172-30-0-118 kernel: [114000.870041] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.873530] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.876425] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:59 ip-172-30-0-118 kernel: [114000.879915] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 17:02:00 ip-172-30-0-118 kernel: [114000.882644] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
```",0,0,msr
159,"Server is on the `EC2`, currently 1 core, 2GB RAM (`t2.small`), HDD - the magnetic one, I believe it is documented [here](https://aws.amazon.com/ebs/previous-generation/).
HDD that I'm copying from is LVM based, consisting of 3 PVs. HDD are not encrypted.
Also FYI, currently `rsync` is calculating deltas and should start moving data again very soon, if that adds something to the stats.
```
# cat /proc/spl/kstat/zfs/arcstats
6 1 0x01 91 4368 35041278654 31678952268315
name type data
hits 4 42347862
misses 4 10285010
demand_data_hits 4 0
demand_data_misses 4 68
demand_metadata_hits 4 34887103
demand_metadata_misses 4 6345542
prefetch_data_hits 4 0
prefetch_data_misses 4 0
prefetch_metadata_hits 4 7460759
prefetch_metadata_misses 4 3939400
mru_hits 4 20830447
mru_ghost_hits 4 2991105
mfu_hits 4 14056656
mfu_ghost_hits 4 1782606
deleted 4 2167955
mutex_miss 4 90369
evict_skip 4 1781974161
evict_not_enough 4 15390660
evict_l2_cached 4 0
evict_l2_eligible 4 35359622656
evict_l2_ineligible 4 55754657792
evict_l2_skip 4 0
hash_elements 4 5452
hash_elements_max 4 48255
hash_collisions 4 57676
hash_chains 4 69
hash_chain_max 4 4
p 4 7007439
c 4 34933248
c_min 4 33554432
c_max 4 1053282304
size 4 105425184
hdr_size 4 1969640
data_size 4 0
metadata_size 4 30842880
other_size 4 72612664
anon_size 4 753664
anon_evictable_data 4 0
anon_evictable_metadata 4 0
mru_size 4 27006976
mru_evictable_data 4 0
mru_evictable_metadata 4 3653632
mru_ghost_size 4 6850560
mru_ghost_evictable_data 4 0
mru_ghost_evictable_metadata 4 6850560
mfu_size 4 3082240
mfu_evictable_data 4 0
mfu_evictable_metadata 4 32768
mfu_ghost_size 4 25972224
mfu_ghost_evictable_data 4 0
mfu_ghost_evictable_metadata 4 25972224
l2_hits 4 0
l2_misses 4 0
l2_feeds 4 0
l2_rw_clash 4 0
l2_read_bytes 4 0
l2_write_bytes 4 0
l2_writes_sent 4 0
l2_writes_done 4 0
l2_writes_error 4 0
l2_writes_lock_retry 4 0
l2_evict_lock_retry 4 0
l2_evict_reading 4 0
l2_evict_l1cached 4 0
l2_free_on_write 4 0
l2_abort_lowmem 4 0
l2_cksum_bad 4 0
l2_io_error 4 0
l2_size 4 0
l2_asize 4 0
l2_hdr_size 4 0
l2_compress_successes 4 0
l2_compress_zeros 4 0
l2_compress_failures 4 0
memory_throttle_count 4 0
duplicate_buffers 4 0
duplicate_buffers_size 4 0
duplicate_reads 4 0
memory_direct_count 4 947
memory_indirect_count 4 259028
arc_no_grow 4 0
arc_tempreserve 4 0
arc_loaned_bytes 4 0
arc_prune 4 3960
arc_meta_used 4 105425184
arc_meta_limit 4 789961728
arc_meta_max 4 816526072
arc_meta_min 4 16777216
arc_need_free 4 0
arc_sys_free 4 32911360
```
```
# cat /proc/cpuinfo processor : 0
vendor_id : GenuineIntel
cpu family : 6
model : 63
model name : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
stepping : 2
microcode : 0x25
cpu MHz : 2394.530
cache size : 30720 KB
physical id : 0
siblings : 1
core id : 0
cpu cores : 1
apicid : 0
initial apicid : 0
fpu : yes
fpu_exception : yes
cpuid level : 13
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm xsaveopt fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips : 4789.06
clflush size : 64
cache_alignment : 64
address sizes : 46 bits physical, 48 bits virtual
power management:
```
```
# dmidecode --type memory
# dmidecode 3.0
Scanning /dev/mem for entry point.
SMBIOS 2.4 present.
Handle 0x1000, DMI type 16, 15 bytes
Physical Memory Array
Location: Other
Use: System Memory
Error Correction Type: Multi-bit ECC
Maximum Capacity: 2 GB
Error Information Handle: Not Provided
Number Of Devices: 1
Handle 0x1100, DMI type 17, 21 bytes
Memory Device
Array Handle: 0x1000
Error Information Handle: 0x0000
Total Width: 64 bits
Data Width: 64 bits
Size: 2048 MB
Form Factor: DIMM
Set: None
Locator: DIMM 0
Bank Locator: Not Specified
Type: RAM
Type Detail: None
```
Thanks",0,0,msr
166,"Okay, so I've replaced the drive which had the high await time, and also increased the memory to 16GB, still having issues:
```
[12785.566973] CE: hpet increased min_delta_ns to 20115 nsec
[25560.317294] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25560.317451] Tainted: P O 4.9.6-1-ARCH #1
[25560.317542] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25560.317668] txg_sync D 0 1392 2 0x00000000
[25560.317678] ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25560.317686] ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25560.317692] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25560.317698] Call Trace:
[25560.317713] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25560.317719] [<ffffffff816061cd>] schedule+0x3d/0x90
[25560.317726] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25560.317781] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25560.317821] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25560.317859] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25560.317867] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25560.317873] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25560.317884] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25560.317891] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25560.317900] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25560.317938] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25560.317984] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25560.318035] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25560.318041] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25560.318091] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25560.318096] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318145] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25560.318154] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25560.318161] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25560.318167] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25560.318171] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318176] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318180] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318184] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[25683.204571] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25683.204722] Tainted: P O 4.9.6-1-ARCH #1
[25683.204814] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25683.204940] txg_sync D 0 1392 2 0x00000000
[25683.204950] ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25683.204958] ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25683.204965] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25683.204970] Call Trace:
[25683.204985] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25683.204992] [<ffffffff816061cd>] schedule+0x3d/0x90
[25683.204999] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25683.205055] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25683.205095] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25683.205134] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25683.205142] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25683.205148] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25683.205159] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25683.205166] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25683.205175] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25683.205213] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25683.205259] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25683.205310] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25683.205315] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25683.205365] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25683.205371] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205419] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25683.205428] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25683.205436] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25683.205441] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25683.205445] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205450] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205454] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205458] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[26051.866268] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[26051.866424] Tainted: P O 4.9.6-1-ARCH #1
[26051.866516] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[26051.866642] txg_sync D 0 1392 2 0x00000000
[26051.866652] ffff8803e6f2c000 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[26051.866660] ffff88041b50b400 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[26051.866667] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[26051.866673] Call Trace:
[26051.866687] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[26051.866694] [<ffffffff816061cd>] schedule+0x3d/0x90
[26051.866700] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[26051.866756] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[26051.866796] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[26051.866834] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[26051.866842] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[26051.866847] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[26051.866859] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[26051.866866] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[26051.866874] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[26051.866913] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[26051.866959] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[26051.867009] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[26051.867015] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[26051.867065] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[26051.867070] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867119] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[26051.867128] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[26051.867135] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[26051.867140] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[26051.867145] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867149] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867153] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867157] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
```",0,0,msr
169,"Just wanted to give a short update to my post in this issue from last year: in the mean time I have upgraded to ZFS 0.6.5.8 from Debian's backports, still using Debian 8. Unfortunately I still get the exact same timeout in the kernel logs.",0,0,msr
172,"@kpande you are right I forgot to give any relevant infos about my underlying hardware. Sorry about that, here are hopefully all the relevant infos:
My server is a virtualization Server running Xen 4.4 with currently 6 virtual machines which all have their logical volumes (LVM) stored on a RAIDZ1 volume with 3x 2TB Seagate SATA enterprise disks (ST32000645NS). The Debian 8 OS is independent and located on two internal SATA-SSD disks of both 16 GB in RAID1 using Linux MD for mirroring. The CPU is an Intel E5-2620 v3 @ 2.40GHz with 6 cores/12 threads. Out of these 6 cores 4 vCPUs have been pinned to the host/hypervisor/dom0 using the `dom0_max_vcpus=4 dom0_vcpus_pin` Linux kernel options. The server has 64 GB of TruDDR4 ECC memory and out of this 64 GB of memory 6 GB has been reserved to the host/hypervisor/dom0 using the respective Linux kernel options `dom0_mem=6G,max:6G`. Finally I have reserved 2 GB RAM of these 6 GB for the ARC using the following `zfs_arc_max=2147483648` zfs module option. I have also disabled ZFS prefetch if that is of any relevance (`zfs_prefetch_disable=1`).
Below is the output of an actual ARC summary (server has bee rebooted 5 days ago):
```
ZFS Subsystem Report	Tue Feb 07 15:11:32 2017
ARC Summary: (HEALTHY)
Memory Throttle Count:	0
ARC Misc:
Deleted:	3.05m
Mutex Misses:	7
Evict Skips:	7
ARC Size:	77.69%	1.55	GiB
Target Size: (Adaptive)	100.00%	2.00	GiB
Min Size (Hard Limit):	1.56%	32.00	MiB
Max Size (High Water):	64:1	2.00	GiB
ARC Size Breakdown:
Recently Used Cache Size:	93.75%	1.88	GiB
Frequently Used Cache Size:	6.25%	128.00	MiB
ARC Hash Breakdown:
Elements Max:	171.83k
Elements Current:	97.97%	168.34k
Collisions:	68.60m
Chain Max:	6
Chains:	12.12k
ARC Total accesses:	102.96m
Cache Hit Ratio:	48.88%	50.32m
Cache Miss Ratio:	51.12%	52.63m
Actual Hit Ratio:	48.88%	50.32m
Data Demand Efficiency:	0.00%	49.34m
CACHE HITS BY CACHE LIST:
Most Recently Used:	63.44%	31.92m
Most Frequently Used:	36.56%	18.40m
Most Recently Used Ghost:	0.08%	42.37k
Most Frequently Used Ghost:	0.00%	0
CACHE HITS BY DATA TYPE:
Demand Data:	0.00%	167
Prefetch Data:	0.00%	0
Demand Metadata:	100.00%	50.32m
Prefetch Metadata:	0.00%	0
CACHE MISSES BY DATA TYPE:
Demand Data:	93.74%	49.34m
Prefetch Data:	0.00%	0
Demand Metadata:	6.26%	3.29m
Prefetch Metadata:	0.00%	145
ZFS Tunable:
metaslab_debug_load 0
zfs_arc_min_prefetch_lifespan 0
zfetch_max_streams 8
zfs_nopwrite_enabled 1
zfetch_min_sec_reap 2
zfs_dbgmsg_enable 0
zfs_dirty_data_max_max_percent 25
zfs_arc_p_aggressive_disable 1
spa_load_verify_data 1
zfs_zevent_cols 80
zfs_dirty_data_max_percent 10
zfs_sync_pass_dont_compress 5
l2arc_write_max 8388608
zfs_vdev_scrub_max_active 2
zfs_vdev_sync_write_min_active 10
zvol_prefetch_bytes 131072
metaslab_aliquot 524288
zfs_no_scrub_prefetch 0
zfs_arc_shrink_shift 0
zfetch_block_cap 256
zfs_txg_history 0
zfs_delay_scale 500000
zfs_vdev_async_write_active_min_dirty_percent 30
metaslab_debug_unload 0
zfs_read_history 0
zvol_max_discard_blocks 16384
zfs_recover 0
l2arc_headroom 2
zfs_deadman_synctime_ms 1000000
zfs_scan_idle 50
zfs_free_min_time_ms 1000
zfs_dirty_data_max 624856268
zfs_vdev_async_read_min_active 1
zfs_mg_noalloc_threshold 0
zfs_dedup_prefetch 0
zfs_vdev_max_active 1000
l2arc_write_boost 8388608
zfs_resilver_min_time_ms 3000
zfs_vdev_async_write_max_active 10
zil_slog_limit 1048576
zfs_prefetch_disable 1
zfs_resilver_delay 2
metaslab_lba_weighting_enabled 1
zfs_mg_fragmentation_threshold 85
l2arc_feed_again 1
zfs_zevent_console 0
zfs_immediate_write_sz 32768
zfs_dbgmsg_maxsize 4194304
zfs_free_leak_on_eio 0
zfs_deadman_enabled 1
metaslab_bias_enabled 1
zfs_arc_p_dampener_disable 1
zfs_object_mutex_size 64
zfs_metaslab_fragmentation_threshold 70
zfs_no_scrub_io 0
metaslabs_per_vdev 200
zfs_dbuf_state_index 0
zfs_vdev_sync_read_min_active 10
metaslab_fragmentation_factor_enabled 1
zvol_inhibit_dev 0
zfs_vdev_async_write_active_max_dirty_percent 60
zfs_vdev_cache_size 0
zfs_vdev_mirror_switch_us 10000
zfs_dirty_data_sync 67108864
spa_config_path /etc/zfs/zpool.cache
zfs_dirty_data_max_max 1562140672
zfs_arc_lotsfree_percent 10
zfs_zevent_len_max 64
zfs_scan_min_time_ms 1000
zfs_arc_sys_free 0
zfs_arc_meta_strategy 1
zfs_vdev_cache_bshift 16
zfs_arc_meta_adjust_restarts 4096
zfs_max_recordsize 1048576
zfs_vdev_scrub_min_active 1
zfs_vdev_read_gap_limit 32768
zfs_arc_meta_limit 0
zfs_vdev_sync_write_max_active 10
l2arc_norw 0
zfs_arc_meta_prune 10000
metaslab_preload_enabled 1
l2arc_nocompress 0
zvol_major 230
zfs_vdev_aggregation_limit 131072
zfs_flags 0
spa_asize_inflation 24
zfs_admin_snapshot 0
l2arc_feed_secs 1
zio_taskq_batch_pct 75
zfs_sync_pass_deferred_free 2
zfs_disable_dup_eviction 0
zfs_arc_grow_retry 0
zfs_read_history_hits 0
zfs_vdev_async_write_min_active 1
zfs_vdev_async_read_max_active 3
zfs_scrub_delay 4
zfs_delay_min_dirty_percent 60
zfs_free_max_blocks 100000
zfs_vdev_cache_max 16384
zio_delay_max 30000
zfs_top_maxinflight 32
ignore_hole_birth 1
spa_slop_shift 5
zfs_vdev_write_gap_limit 4096
spa_load_verify_metadata 1
spa_load_verify_maxinflight 10000
l2arc_noprefetch 1
zfs_vdev_scheduler noop
zfs_expire_snapshot 300
zfs_sync_pass_rewrite 2
zil_replay_disable 0
zfs_nocacheflush 0
zfs_arc_max 2147483648
zfs_arc_min 0
zfs_read_chunk_size 1048576
zfs_txg_timeout 5
zfs_pd_bytes_max 52428800
l2arc_headroom_boost 200
zfs_send_corrupt_data 0
l2arc_feed_min_ms 200
zfs_arc_meta_min 0
zfs_arc_average_blocksize 8192
zfetch_array_rd_sz 1048576
zfs_autoimport_disable 1
zfs_arc_p_min_shift 0
zio_requeue_io_start_cut_in_line 1
zfs_vdev_sync_read_max_active 10
zfs_mdcomp_disable 0
zfs_arc_num_sublists_per_state 4
```
Do you need any more information? and what do you think about this setup? is my hardware undersized?",0,0,msr
176,"I have the same issue or question.
In my case this leads to a huge amount of time spent invoking the factory twice.
Only half of the objects are actually used, because one of the two created objects are discarded anyway.
Is there any way to prevent this from happening?
It would save a lot of time in my test suite.",0,0,msr
183,"**From OP:**
```ruby
class Girl < ActiveRecord::Base
belongs_to :boy
end
class Boy < ActiveRecord::Base
has_one :girl
end
```
---
For real? Girl belongs to boy didn't feel a little gross to anyone?
Let's write examples without reducing women to objects, please, and call this stuff out when we see it.",0,0,msr
189,"I solved this problem by using `:inverse_of` on the association in both models.
```
class Belonging < ActiveRecord::Base
belongs_to :having, inverse_of: :belonging
end
```
```
class Having < ActiveRecord::Base
has_one :belonging, inverse_of: :having
end
```
And then @Manifold0 I used an after-create callback on the factory for the model without the foreign key, so they look like this:
```
factory :belonging do
having
end
```
```
factory :having do
after(:create) do |having, evaluator|
having.belonging || create(:belonging, having: having)
end
end
```
In the callback, `inverse_of` allows for the associated Belonging to be returned from memory even if it isn't saved yet, as is the case when you `create :belonging`. You can do some dynamic checking, while not relying on something like `Belonging.first`.
@Kriechi this will help your test performance by not creating extra throwaway records, and also affect your actual production performance by making fewer trips to the db. Which is the actual purpose of `inverse_of`.",1,0,msr
197,"Using the `term`, `period` and `candidate` example, if I add another layer, say:
```
Class Bear < ActiveRecord::Base
belongs_to :candidate
belongs_to :period
end
```
Is it possible to create the `term` inside of the `bear` FactoryBot beforehand and then get candidate and period to use the `bear` to create internally?
Probably a wrong example here but is it possible to do it inside (sometimes an object belongs to an object that belongs to an object, and if we were to manually create each object from the top and reference the next one outside of the factorybot file then it would be a mess everywhere), something like this:
```
# this is wrong but you get the idea
FactoryBot.define do
factory :bear do
# Run this first so everytime a bear gets created, a term for this bear gets created first to be used later
before(:create) { create(:term }
# Using the term created before this
association :period, factory: :period, term: term
association :candidate, factory: :candidate, term: term
end
end
```",0,0,msr
198,"@CyberMew If you want to complete the period and candidate with Bear#term after Bear/Term are created, I think an `after` block is the prescription.
```
FactoryBot.define do
factory :bear do
association :term
association :period
association :candidate
after(:build) do |bear|
bear.period.term ||= bear.term
bear.candidate.term ||= bear.term
end
end
end
```
I use `after(:build)` instead of `after(:create)` and AR methods instead of passing ids around, so this setup will work for both build(:bear) and create(:bear). The associations follow the build strategy (build/create) of the parent, so if you create they'll assign first and then automatically populate all the ids on save.
You can force build strategies if you want to, but I don't recommend it as you can easily end up with some funky saved/not-saved ""associated"" records. But there's a bit on that here: https://github.com/thoughtbot/factory_bot/blob/master/GETTING_STARTED.md#associations",1,0,msr
204,The documentation of sklearn is really very useful and should answer your question: http://scikit-learn.org (basically you have to put your data in numpy arrays),0,0,msr
205,"This is something that could have a bit more documentation than is in there
currently. You might find Pandas useful.
On 29 October 2014 09:45, Alexander Fabisch notifications@github.com
wrote:
> The documentation of sklearn is really very useful and should answer
> your question: http://scikit-learn.org (basically you have to put your
> data in numpy arrays)
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60845063
> .",0,0,msr
207,"see #2801
On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
wrote:
> @jnothman https://github.com/jnothman Should we reopen this issue and
> add a new section in the documentation? For example in this section:
> http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> your own data"").
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069
> .",0,0,msr
208,"My own dataset means the dataset that I have collected by my self, not the
standard dataset that all machine learning have in their depositories (e.g.
iris or diabetes).
I have a simple csv file and I on my desktop and I want to load it inside
scikit-learn. That will allow me to use scikit-learn properly and introduce
it to my colleges to serve our community.
I need a very simple and easy way to do so.
I will be highly appreciated any useful advice.
On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> see #2801
> > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> wrote:
> > > @jnothman https://github.com/jnothman Should we reopen this issue and
> > add a new section in the documentation? For example in this section:
> > http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> > your own data"").
> > > > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > .
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212
> .",0,0,msr
211,"It probably looks something like:
import pandas as pd
data = pd.read_csv(open('myfile.csv'))
target = data[target_column_name]
del data[target_column_name]
# Then fit a scikit-learn estimator
SVC().fit(data, target)
On 29 October 2014 23:19, MartinLion notifications@github.com wrote:
> Thanks for the link. I checked it out, but the process looks complicated.
> Perhaps if there is a short youtube video explains the process much
> easier,
> otherwise I do not know what to do to solve this matter.
> > On 29 October 2014 19:12, jnothman notifications@github.com wrote:
> > > See http://pandas.pydata.org/pandas-docs/stable/io.html
> > > > On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> > > > > My own dataset means the dataset that I have collected by my self, not
> > > the
> > > standard dataset that all machine learning have in their depositories
> > > (e.g.
> > > iris or diabetes).
> > > > > > I have a simple csv file and I on my desktop and I want to load it
> > > inside
> > > scikit-learn. That will allow me to use scikit-learn properly and
> > > introduce
> > > it to my colleges to serve our community.
> > > > > > I need a very simple and easy way to do so.
> > > > > > I will be highly appreciated any useful advice.
> > > > > > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > > > > > see #2801
> > > > > > > > On 29 October 2014 18:07, Alexander Fabisch <
> > > > notifications@github.com>
> > > > > > > > wrote:
> > > > > > > > > @jnothman https://github.com/jnothman Should we reopen this
> > > > > issue
> > > > > and
> > > > > add a new section in the documentation? For example in this
> > > > > section:
> > > > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html
> > > > > (""Loading
> > > > > your own data"").
> > > > > > > > > > —
> > > > > Reply to this email directly or view it on GitHub
> > > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > > > > .
> > > > > > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > > > > > .
> > > > > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671>
> > > > > .
> > > > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60906075>
> > > > .
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60913843
> .",1,0,msr
218,"We could tell you what the problem is but I think in this case you will learn more from it if you find it on your own. You should read the error message carefully. It is a Python syntax error.
```
File """", line 2
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
^
SyntaxError: invalid syntax
```",0,0,msr
227,"> > should we instead add as a section in the tutorial here?
> > We should reference it. But I don't see this as tutorial material because
> it is outside the scope of scikit-learn. We can only give pointers
> > That's an answer that the users really don't want to hear, because there
> point of view is that they have a lump of data and they want it inside
> scikit-learn. The answer is: this is not a problem that scikit-learn
> solves, go see pandas if you have CSV, scikit-image if you have images,
> database connectors (SQLAlchemy?) if you work on databases...
> > I guess that we should have a sentence like this in the tutorial, where
> you reference, with pointers.
> > As a side note, the kind of errors hit by the users on the thread of this
> issue (lack of basic knowledge of Python for instance) tells me that we
> cannot solve their problem. They need to go to entry-level tutorials on
> Python, and get a bigger picture. Maybe we should make sure that we give
> pointers to these in the right spots, eg early on in the tutorial.
Well, take it easy!!!
I don't know whether you are one of scikit-learn staff or not, but I need
to say that your way of talking is harming both scikit-learn staff and
users (us), due to the two reasons:
First reason, criticizing people (like what you did) and assuming that they
are novice in Python so they don't know how to work with scikit-learn,
means you or the staff are trying to blind their eyes to the truth that
scikit-learn staff are not able to create a clear tutorial to allow loading
the real data, at least. In addition, pretending that the tutorial of
scikit-learn is perfect in spite all the questions regarding loading the
real data (not the toy data as it is too easy to be imported comparing to
the real data) is something needs to be reconsidered, and this means that
scikit-learn staff don't care about the name of scikit-learn at all.
Second, we can understand from your unsuitable way of talking that you
already forgot that scikit-learn is a product, and we as users are
customers, so either you or the staff of scikit-learn should respect all of
us and thank us for any comment or bug fixing. This is the professional way
of behavior. So I recommend you to think of your words before saying them. If
you are knowing the way of loading the real data and you'd like to help,
don't only say go see pandas, better you answer people's question nicely
rather than hurting them with your words, but if you're simply not able to
do that, so keep quite.
On the other hand, regarding the question ""should we instead add as a
section in the tutorial here?"", I would like to say ""_YES_"", you or
scikit-learn staff should add a section in the official tutorial about how
to load your own data either CSV, or ARFF or text or whatever, as users are
interested to load their own data, this is very critical issue should be
considered in the tutorial (not to be ignored). *If you rely on the user,
then what is your work? *
Nevertheless, for those who are still struggling with scikit-learn, I would
like to say, this is not the end of the world, and as I mentioned
previously, find another to tool make your life much easier. For this
reason, and in order to save your time, I would like to recommend some
tools to assist you in data mining procedures. For instance, Waikato
Environment for Knowledge Analysis (WEKA),
http://www.cs.waikato.ac.nz/ml/weka/, last version is WEKA 3-7-13, is a
collection of machine learning algorithms for data mining tasks. WEKA
allows you to use its schemes either from GUI or writing Java code, so its
very easy for non-programmers. Additional to WEKA, R is also an excellent
tool for data mining stuff, you can also perform tasks of R from WEKA or
vice versa. However, if you have a patience to design a prediction process
manually (drag/drop), RapidMiner is a great tool for this propose where you
can design a very nice flow to achieve your target.
Thanks David van Leeuwen for your support.
Good luck in your analysis.
Cheers,
Martin
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-160153930
> .",0,1,msr
228,"Hey Martin,
Kindly don't be offended.
He did not criticize :) He, being one of the top contributors to scikit-learn has to make tough decisions as to what will go into our codebase and what will not, as a more verbose documentation or tutorial might not be preferable for a lot of people. Gael has in fact contributed a lot of user guides himself to scikit learn to help users.
The reason why he was opposing that addition to the tutorial was that there are multitude of ways in which users have their data stored and such a user guide on how to get the input data from all of them (a text file/csv file/database/zipped archive), is indeed out of scope for scikit learn, which is a machine learning library.
The most important thing to note here is that **it is very clearly explained by documentations of libraries which handle data, like numpy or pandas.**
It is expected from the user that he or she knows this! Since it seems to not be very clear, he suggests that we add a FAQ, pointing the user to such userguides, which are more elaborate than we could possibly get :)
It may appear that our tutorial could be a bit more elaborate on how the inputs are obtained. But the thing, in general, with userguides is that, it could _always_ be a little bit more elaborate, which makes us set a hard limit on how detailed our userguides can get, to help contain the userguide in a maintainable format :) If you think from that perspective, you yourself would understand our situation.
As this issue is open someone will indeed send a PR soon adding a nice FAQ entry and an example, maybe, which could help clarify your (or any other new user's) doubts on input formats.
Cheers!",0,0,msr
229,"Hello @MartinLion ,
we understand your eagerness to solve your problem, and your frustration when it is not solved.
However, you seem quite misinformed about what is scikit-learn, how it works, and how the project is developed. Therefore, I would like to make some points clear for you. As you can see from
http://scikit-learn.org/stable/about.html, scikit-learn is a community effort that is developed by a team of volunteers, mostly on their free time. Gaël is one of the creators of the project and its current leader:
scikit-learn would certainly not be the same without his contribution (the same for other volunteers), and he certainly did not deserve your dismissive words.
What I would like to emphasize is that there is no such thing as a scikit-learn ""product"", or scikit-learn ""staff"" (only a handful of people have worked full time on the project). You mention ""we as users are
customers"", but how much are you paying for using scikit-learn? Despite the important development cost, users get scikit-learn for free (and of course that's how it's intended to be). In fact, the development of the project relies on a fragile alchemy: users' needs being a top priority for developers, and users reporting bugs and concerns in the most positive way. The kind of ""ranting"" that you wrote can be very discouraging for developers, who contribute their free time and their expertise just because they believe that scikit-learn is a useful tool for the community. Some prominent developers stopped contributing to open-source software precisely because of such ""customer-like attitude"" of a few people underlining only shortcomings, and dismissing the huge development efforts. Please try to see the bright side as well: you received advice and comments from a lot of people, I'm sure that there was something for you to learn out of it, even if it did not solve your problem. Also, although users' needs are indeed a top priority of scikit-learn (it has an amazing documentation, of which most scientific Python packages can be jealous!), each software addresses a well-targeted niche of users, and it is just normal that scikit-learn cannot fit all users. For example, it is preferable to use scikit-learn with already a good knowledge of Scientific Python. So, I'm really glad that you found a
package that suited your needs better, but please also acknowledge the time and good will that people gave away when answering you.
So, folks, let's all show some good will and keep a constructive dialog.
That's how the project we love will keep on rocking!",0,0,msr
231,"Perhaps I should elaborate on my original frustration, to give you some context. I've been programming in Python almost exclusively for a year now (I am a late convert), and am fairly familiar with the ecosystem---I've done lot's of webservice related things, but also manipulation of resources related to automatic speech recognition. I do my scientific work in [Julia](http://julialang.org/) since a couple of years, and before that, in R, octave, c++/c (some 30 years in total). The Julia ecosystem is quite dynamic, and it is all very exciting, but Python just has this very large ecosystem and very clean coding, which makes it very attractive to use for little side experiments. This time I had to do some topic classification of (single sentence) text documents. Now there is an abundant choice of language technology tools in Python, and I believe that via [lda](https://pythonhosted.org/lda/index.html) I got to scikit-learn. Great tutorials, lovely datasets and all, but I found it very difficult to find out how to organize my own data so that I could load this in. Just now, I browsed through the user guide again to find the docs for ""load_files"", but I could't find an entry. So a google search for ""sklearn.datasets.load_files"" got me there just now, and I happened to remember the particular module path from more painstaking searches yesterday (it is mentioned somewhere in a tutorial). For me, the essential information would have been: ""Organize your data one document per file, one directory per class""---more or less what's under the documentation for [load_files](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html). This all makes perfect sense, but I come from a community where usual formats are ""one datapoint per line"", often with the class label on that line. But having said all this, I am pretty impressed how the Python (text) community has standardized data representation, from what I've seen so far. But perhaps because of the widely used standard data representation, this aspect has naturally less attention in documentation. As a final note, whenever I try to teach students how to use some scientific tool set or another, I have to spend quite some time on ""how to import your data"". Nobody likes to do it, it can be a lot of effort for what you potentially use only once, and is therefore always a difficult threshold.",1,0,msr
232,"@davidavdav I agree that loading data is a difficult and important thing. However, it is a domain-specific problem. You have a particular type of data. I have another. My data is medical images of brain activity. I can tell you how I organize my data and load them. I can even tell you that we have written a whole package about this, with its own documentation. But that will probably not help you.
What you want is something that tells you how to organize and load _your_ data. Now, it may be that your data is something fairly classic, that many people have; for instance tabular data most often stored in CSV files. In which case there is a need for a package doing this data loading. I don't believe that it should be in scikit-learn. It needs to be in a package that is specialized for this data. For instance, we are not going to put image processing in scikit-learn. For tabular data, the dedicated package is pandas; as I mentioned in my reply we need to point to it. We, the scikit-learn team, want to make plugin pandas into scikit-learn easier. But it is not as easy as it may seem and it takes time (one of our core devs is prototyping something).
I realize rereading your post that your data is most likely text documents. So my two examples of data (medical images and tabular data) were both wrong :$. Maybe the documentation on processing text with scikit-learn could indeed be improved and touch a bit on data organization. I don't know, I very seldom process text. But if you want to do add a few words on this, you are most welcomed to do a pull request. Anyhow, this illustrate my point about the diversity of the data: this whole thread is mostly about loading CSV files, as can be seen from earlier comments (before the thread exploded into a rant). The important thing is not the ""CSV"", which is the container, but the data model that underlies a CSV file. This data model is that of columns of data with different nature. It's a very different data model than processing text documents.
And finally, you are unhappy that teaching people ""how to import your data"" is time consuming. I don't think that there is an easy fix for this, even in a specific domain. The reason being that data meaning (ie data semantics) is still very much an open area. It's intrinsically hard to describe what the data means and how it's organized. You can try a simple experience: grab a dataset from someone you don't know, about an experiment you don't know, and try understanding it. Not even loading it, just understanding it. I am sure that it will take time. What takes a human time tends to be very difficult for a computer.",1,0,msr
236,"Well you could add some information about common errors that happen when loading own data. For example I have bumped on `Unknown label type: 'continuous-multioutput'` and not a hint, anywhere, what that could mean. I've tried passing:
- a numpy array created from csv reader
- a numpy array created from pandas
- regular array parsed line by line
- pandas dataframe
- a numpy array created from csv reader converted to regular array
- etc...
I'd recommend adding section summarizing assumptions about data, for example it was sheer luck that I found information that data sets may not contain NaN's Nulls etc. Also, what should be the parameters of a numpy array and how should pandas dataframe look like.",0,0,msr
238,"@MartinLion @MartinLion Thank you to brought up this issue.
I am facing your problem now. I want to upload my IRIS like dataset. Which is 500 rows and 16 columns.
and
Thank you @chenhe95 ,
I have seen your tutorial titled ""Can I go to the bathroom"" , that is a real work , real help to users like us.
And Thank you all developers,
You worked free (or almost free) to deliver this library to people. I appreciate that. @MartinLion has spirit of learning, I praise that.
Actually stubbornness is a virtue in academic.",0,0,msr
245,"I'm experience the same slow experience. Anyone can try aria - http://aria2.sourceforge.net/ and http://stackoverflow.com/questions/3430810/wget-download-with-multiple-simultaneous-connections
It's seems a little bit faster, but, man, you can set this up using default vagrant download mechanism and take a walk or make yourself a sandwich. Get way from screen for a little bit.",0,0,msr
253,"same here
```
vagrant box update
==> default: Checking for updates to 'laravel/homestead'
default: Latest installed version: 0.4.1
default: Version constraints: >= 0
default: Provider: vmware_desktop
==> default: Updating 'laravel/homestead' with provider 'vmware_desktop' from version
==> default: '0.4.1' to '0.4.2'...
==> default: Loading metadata for box 'https://atlas.hashicorp.com/laravel/homestead'
==> default: Adding box 'laravel/homestead' (v0.4.2) for provider: vmware_desktop
default: Downloading: https://atlas.hashicorp.com/laravel/boxes/homestead/versions/0.4.2/providers/vmware_desktop.box
default: Progress: 0% (Rate: 42210/s, Estimated time remaining: 6:10:54))
```",0,0,msr
255,"I guess there's nothing preventing people from sharing boxes via torrent. For example, below is a magnet link for the heroku-cedar-14 box:
> magnet:?xt=urn:btih:5bb1480d5316f229bb71be55b56b06278de41a67&dn=heroku-cedar-14.box&tr=http%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=http%3A%2F%2Fannounce.torrentsmd.com%3A6969%2Fannounce&tr=http%3A%2F%2Fbt.careland.com.cn%3A6969%2Fannounce&tr=http%3A%2F%2Fexplodie.org%3A6969%2Fannounce&tr=http%3A%2F%2Fmgtracker.org%3A2710%2Fannounce&tr=http%3A%2F%2Ftracker.tfile.me%2Fannounce&tr=http%3A%2F%2Ftracker.torrenty.org%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.trackerfix.com%2Fannounce&tr=http%3A%2F%2Fwww.mvgroup.org%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.me%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2710%2Fannounce&tr=udp%3A%2F%2Fcoppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Fexodus.desync.com%3A6969%2Fannounce&tr=udp%3A%2F%2Fglotorrents.pw%3A6969%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.glotorrents.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker4.piratux.com%3A6969%2Fannounce
Anyone know a good website where one can search for torrents of vagrant boxes?",0,0,msr
265,"I can confirm that this is still an issue. All my peers also report times of >1h, while the connection here for other connections is around 200MB/s.
```
vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'ubuntu/trusty32' could not be found. Attempting to find and install...
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'ubuntu/trusty32'
default: URL: https://atlas.hashicorp.com/ubuntu/trusty32
==> default: Adding box 'ubuntu/trusty32' (v20160406.0.0) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty32/versions/20160406.0.0/providers/virtualbox.box
default: Progress: 11% (Rate: 43801/s, Estimated time remaining: 1:36:50)
```",1,0,msr
274,"Sign me upp here, 100mb symmetric connection (Fiber) sloooow as shit, doing 150kb/s",0,1,msr
275,"after looking at the years of complaints of slow download with no effort of resolving the issue,,, i think its time to start emailing Laravel to stop endorsing homestead until the issue is resolved..... maybe that will get their attention!!!! this is a real problem... 15 retries and then 4.6 hours to download a file is irresponsible on their part........",1,1,msr
276,"I bet it gets closed, but if you don't ask you don't get:
https://github.com/mitchellh/vagrant/issues/7307",0,0,msr
279,"In looking to debug curl being slow -- I found a [stackoverflow post ](http://stackoverflow.com/questions/30984641/debugging-slow-download-with-curl) that suggests that --trace-ascii /dev/null makes your curl go at the speed you'd expect. For me, I'm trying to download [CentosOS 7](http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box) and here are my results:
NO --trace-ascii option:
```
$ curl http://cloud.centos.org/centos/nt/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
0 483M 0 489k 0 0 77724 0 1:48:44 0:00:06 1:48:38 69721
```
With trace-ascii the first time:
```
$ curl http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box --trace-ascii /dev/null
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
5 483M 5 24.5M 0 0 4259k 0 0:01:56 0:00:05 0:01:51 4599k
```
Does anyone else see the same behavior?",0,0,msr
285,Guys why is this issue closed? This is still an outstanding issue and needs to be addressed ASAP. I am experiencing the same issue.,1,1,msr
299,"I opened a ticket about customizing the download tool, but it got rejected as too complicated to impliment ;-(
That said, my recent download speeds have been ok from the UK for a while. This example downloaded just now:
```
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'bento/ubuntu-16.04'
default: URL: https://atlas.hashicorp.com/bento/ubuntu-16.04
==> default: Adding box 'bento/ubuntu-16.04' (v2.3.4) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/bento/boxes/ubuntu-16.04/versions/2.3.4/providers/virtualbox.box
default: Progress: 12% (Rate: 8940k/s, Estimated time remaining: 0:01:06)
```
Maybe you guys are just too far from their AWS instances for a good download speed?
All their download servers look to be in New York with no CDN to distribute content.
```
$ dig atlas.hashicorp.com
; <<>> DiG 9.10.3-P4-Ubuntu <<>> atlas.hashicorp.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 44169
;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1
;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4000
;; QUESTION SECTION:
;atlas.hashicorp.com. IN A
;; ANSWER SECTION:
atlas.hashicorp.com. 120 IN CNAME atlas.hashi.co.
atlas.hashi.co. 60 IN A 52.206.86.0
atlas.hashi.co. 60 IN A 52.200.255.5
atlas.hashi.co. 60 IN A 52.55.203.197
;; Query time: 42 msec
;; SERVER: 10.1.1.14#53(10.1.1.14)
;; WHEN: Mon Apr 24 09:20:24 DST 2017
;; MSG SIZE rcvd: 124
```
http://geoiplookup.net/ip/52.55.203.197
http://geoiplookup.net/ip/52.206.86.0
http://geoiplookup.net/ip/52.200.255.5
Maybe its worth the people getting slow downloads detailing what ISP they are using? Maybe you are all on an ISP with a high contention ratio?",0,0,msr
301,"> Maybe its worth the people getting slow downloads detailing what ISP they are using?
Any ISP, in any state I've traveled to in the last couple years.
This problem is squarely on whatever Hashicorp is doing for hosting, and that's where it needs to be fixed. If they're unable or unwilling to fix it, a torrent solution would certainly help without taking their resources aside from development for adding a torrent client to the tool. If all this stuff is on S3 anyway, AWS provides torrent seeding out of the box.",1,0,msr
310,Sooooooooooooooo slowly！！！！:(,0,1,msr
313,I'm also facing the slow download issue.,0,0,msr
320,"I'm seeing the same problem. Running latest Vagrant 2.0.0. When downloading a box (configured using external URL on Vagrant Cloud, so it's actually redirecting to Rackspace Cloud Files) it's painfully slow. If I use cURL or Chrome to download the same URL (https://vagrantcloud.com/joomlatools/boxes/box/versions/1.5.0/providers/virtualbox.box) from the same machine it only takes a couple of seconds to complete the download. What could be the issue here?",0,0,msr
323,"@panozzaj's comment above doesn't quite work if a `Vagrantfile` expects a specific version of a box (that command will add a box without a version). Instead, you can do the below.
# Steps to get a specific box at a specific version without using `vagrant` to perform the download
I'm using laravel/homestead in this example because it's what I was trying to get.
1. Download the box using a better download client (e.g. your browser, curl, wget, whatever).
2. Create a new json file (anywhere), add this to it (note, tweak the `name`, `version` and `url` keys to match the box you want, don't worry about the `checksum` key yet. For `url`, use the path to the file you just downloaded, example below):
```
{
""name"": ""laravel/homestead"",
""description"": ""Whatever you want"",
""versions"": [{
""version"": ""4.0.0"",
""providers"": [{
""name"": ""virtualbox"",
""url"": ""file://c:/users/madmatt/Downloads/47dce273-9892-4691-a746-c4f351ae44a5"",
""checksum_type"": ""sha1"",
""checksum"": ""abc123""
}]
}]
}
```
3. In your `Vagrantfile`, add the following lines `vm.box` and `vm.box_url` keys):
```
Vagrant.configure(""2"") do |config|
config.vm.box = ""laravel/homestead""
config.vm.box_url = ""file://c:/users/madmatt/path/to/the/json-file-you-created-in-step-2.json""
end
```
4. Run `vagrant up` as normal.
5. `vagrant` will complain that the sha hash doesn't match (`The checksum of the downloaded box did not match...`). Take the string that appears next to 'Expected', copy and paste that into the `checksum` key of the json file you created in step 2.
6. Run `vagrant up` again, this time it should load from the local file, store it as the correct name and version, and successfully run it.
Hopefully someone finds these steps useful... funnily enough I have done all this research (having never used vagrant before), have tested it a bunch of times, and the original `vagrant box add laravel/homestead` command that I started running 3 hours ago is still only 8% complete, even though in that time I've downloaded the box file 8 times outside of vagrant.
The rest of this is just my experience, no need to read further ;)
# My experience (aka. why is vagrant so slow to download boxes)
Was getting 420b/sec (yes, that's bytes per second) on a gigabit connection downloading https://vagrantcloud.com/laravel/boxes/homestead/versions/4.0.0/providers/virtualbox.box.
I downloaded the same file via browser, curl and wget with speeds varying between 12 and 27MB/sec. I then tried doing both at the same time - I was able to download via Chrome, Firefox, curl and wget before ```vagrant box add laravel/homestead``` had downloaded 1%.
The URL I got in the browser was ```https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/47dce273-9892-4691-a746-c4f351ae44a5?X-Amz-Algorithm=<snip>&X-Amz-Credential=<snip>&X-Amz-Date=<snip>&X-Amz-Expires=<snip>&X-Amz-SignedHeaders=<snip>&X-Amz-Signature=<snip>```
I don't know what the problem here is, but I can think of a couple:
* Whatever UA vagrant uses is bad, and AWS severely limits it
* Whatever vagrant uses to download isn't following redirects, or somehow never ends up downloading from AWS infrastructure, instead using some other terribly overloaded server/proxy",0,0,msr
325,"Perhaps it's time to fork. This project is core to a lot of development environments, leaving us all subject to the whims of HashiCorp... and on this issue we seemingly cannot even get a reasonable official response.
Vagrant is MIT-licensed. Boxes could easily be distributed via torrent, or we can even just specify URLs in our Vagrantfile. We don't need the Vagrant Cloud dependency if some basic enhancements around box handling are made. Are there any maintained forks already in existence?
Any thoughts from others?",1,0,msr
347,"Well, I cant download a single box tonite. I'm in the UK.
```
The box 'puppetlabs/ubuntu-16.04-64-puppet' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: [""https://atlas.hashicorp.com/puppetlabs/ubuntu-16.04-64-puppet""]
Error: The requested URL returned error: 404 Not Found
```
> config.vm.box = ""puppetlabs/ubuntu-16.04-64-nocm""
> config.vm.box = ""bento/ubuntu-16.04""
> config.vm.box = ""ubuntu/xenial64""
> none of those work.
BUT if you download the box manually via wget, all works fine...
```
nick@TX200-S5:~/workspaces/elk-vagrant$ wget https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
--2018-03-04 00:30:45-- https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 50.17.237.77, 54.221.226.80, 54.243.175.62, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|50.17.237.77|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box [following]
--2018-03-04 00:30:46-- https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box
Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.32.82
Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.32.82|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 666915336 (636M) [application/vnd.previewsystems.box]
Saving to: ‘virtualbox.box’
100%[==========================================================================================>] 666,915,336 2.23MB/s in 4m 48s 2018-03-04 00:35:35 (2.21 MB/s) - ‘virtualbox.box’ saved [666915336/666915336]
nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add bento/ubuntu-16.04
The box 'bento/ubuntu-16.04' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: [""https://atlas.hashicorp.com/bento/ubuntu-16.04""]
Error: The requested URL returned error: 404 Not Found
nick@TX200-S5:~/workspaces/elk-vagrant$ wget -O bento-ubuntu-16.04 https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
--2018-03-04 00:37:41-- https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 54.243.252.123, 50.19.252.69, 54.243.137.45, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|54.243.252.123|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA [following]
--2018-03-04 00:37:41-- https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA
Resolving archivist.vagrantup.com (archivist.vagrantup.com)... 50.16.237.173, 23.21.92.233, 54.221.212.171, ...
Connecting to archivist.vagrantup.com (archivist.vagrantup.com)|50.16.237.173|:443... connected.
HTTP request sent, awaiting response... 307 Temporary Redirect
Location: https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7 [following]
--2018-03-04 00:37:42-- https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7
Resolving vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)... 52.216.164.43
Connecting to vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)|52.216.164.43|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 428203828 (408M) [binary/octet-stream]
Saving to: ‘bento-ubuntu-16.04’
100%[==========================================================================================>] 428,203,828 2.09MB/s in 3m 6s 2018-03-04 00:40:48 (2.19 MB/s) - ‘bento-ubuntu-16.04’ saved [428203828/428203828]
```
A direct box add works too
```
nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add ""bento/ubuntu-16.04"" https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
==> box: Box file was not detected as metadata. Adding it directly...
==> box: Adding box 'bento/ubuntu-16.04' (v0) for provider: box: Downloading: https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
box: Progress: 7% (Rate: 2247k/s, Estimated time remaining: 0:03:13)
```",1,0,msr
353,"Code:
```
$ cat typetest.rs struct Foo<Elem, List:AsSlice<Elem>> {
elems: List,
}
fn main() {
}
```
Compile:
```
$ rustc typetest.rs typetest.rs:1:12: 1:16 error: parameter `Elem` is never used
typetest.rs:1:16: 1:16 help: consider removing `Elem` or using a marker such as `core::marker::PhantomData`
error: aborting due to previous error
```
I'm pretty new to rust, so forgive me if I've missed something. But I can't see a way to say ""my struct contains a type T2, which implements `SomeGenericTrait<T>`"" without my struct also being parameterized over T, as I'm doing here. But rust thinks that I'm not using T.
I tried adding:
```
dummy: Elem,
```
to my struct def, which makes it compile. So it doesn't look like this is just one error message obscuring another, rust does really think this is the only thing wrong with my code.
---
```
$ rustc --version --verbose
rustc 1.0.0-dev (2fc8b1e7c 2015-03-07) (built 2015-03-08)
binary: rustc
commit-hash: 2fc8b1e7c4ca741e59b144c331d69bf189759452
commit-date: 2015-03-07
build-date: 2015-03-08
host: x86_64-unknown-linux-gnu
release: 1.0.0-dev
```",1,0,msr
394,when to achieve?,0,0,msr
395,@vkarpov15 : any progress on this?,0,0,msr
413,"Same here, as Radarr can import Trakt lists I'd love to see this on Sonarr :)",0,0,msr
416,"Works great on Radarr, would be awesome to get it at Sonarr. That would mean one place to select, and let Sonarr/Radarr read list, search, tag, download, done.",0,0,msr
427,If you don't have anything meaningful to add and you want to add your support add a 👍 to the original post and subscribe to the issue. Everyone that has commented doesn't need to receive an alert that you're also interested in the feature.,0,0,msr
437,"``` php
// You could call R directly
\RedbeanPHP\R::freeze(true);
// Or use `use`
use RedbeanPHP\R;
R::freeze(true);
// Or use an alias
use RedbeanPHP\R as Redbean;
Redbean::freeze(true);
// Or create a facade of the R facade
class R extends \RedbeanPHP\R {}
\R::freeze(true);
```
Also, to make sure every database library you use use the same method names, you should create [an interface](http://php.net/interface).",1,0,msr
441,"@gabordemooij Do you know that what you are saying goes against the Unix philosophy:
```
Write programs that do one thing and do it well. Write programs to work together.
```
Tools such as `composer`, or `apt-get`, or any dependency manager, try to make it simpler to link programs together, so that programs can focus on what they must do, and so that they can do it well. If we created the same thing over and over again, every single project would fail, because they would need to do everything from scratch, even though people already created those required programs. Dependency managers are tools to build upon, they are made to create good software based on other software which achieve the task they are asked to almost perfectly.",0,0,msr
442,"@gabordemooij
You should be killed. Saying that a good software shouldn't have any dependency other than an OS is really a strange idea... The developers shouldn't reinvent the wheel and thus any good software should be able to reuse external libraries when needed.
BTW: An OS is nothing more than a bunch of libraries and executable that were compiled to form a system fully usable. Does an OS shouldn't have any external dependencies?",0,1,msr
443,"@tleb composer does not link programs, it links libraries. And, as such it creates huge programs that do everything - which is basically the opposite of the UNIX philosophy. Apt-get also rarely installs programs, it installs mostly lib-this, lib-that, lib-different-version etc. Programs relying on a ton of external libs are definitely not 'Unixy'.
@nuxwin I did not say developers should re-invent the wheel. Neither is this an inevitable consequence of having no dependencies. I simply believe a well designed base system would be preferable - but that's harder to build. @nuxwin Please, let's refrain from statements like 'should be killed' and keep the discussion civilized. Open source forums are already famous for their anti-social tone. We're just people and everyone is entitled to his/her own opinion. If someone disagrees there is either an opportunity to improve knowledge or exchange new ideas.
Also - I believe I accidentally sidetracked this thread with my comment about dependencies in general, sorry for that.",1,0,msr
459,"Sha256 isn't a digital signature.
https://paragonie.com/blog/2015/08/you-wouldnt-base64-a-password-cryptography-decoded
On Mon, Sep 10, 2018, 7:59 AM Gabor de Mooij <notifications@github.com>
wrote:
> Literally everything you just said can apply to any dependency manager
> including Git, Github, apt, whatever.
>
> Git is a VCS, Github is a code hosting platform, so these are not DMs. And
> yes, it applies to apt. That's why I recommend to use Slackware, Crux or
> *BSD instead.
>
> How are redbean releases more secure than another PHP library?
>
>
> 1. 180000+ tests
> 2. 10 years of maturity
> 3. Careful code review process
> 4. Focus on eliminating code, simplicity (minimalism)
> 5. 0 dependencies
>
> Updating RedBeanPHP is also easy: just copy-paste the rb.php file and done!
>
> You aren't signing your releases
> I provide sha256 checksums on the download page:
> https://redbeanphp.com/index.php?p=/download
> So you can check whether the downloaded file has not been corrupted or
> modified - improvements can be made here.
>
> I'll trust the millions of users
> You use Windows I suppose (millions use it!) ? Seriously. How on earth
> does quality improve just by USING something?!
>
> your ORM is straight out of PHP 4.x times
> Therefore it must be bad. Because new is always better?
>
> Why would I implement AWS myself when they give me a library to use?
> AWS is a platform, not a library. You can't implement AWS with just a
> library.
>
> We're all idiots? Really dude?
> I don't know about the rest in this thread but you seem to suffer from the
> Dunning–Kruger effect:
> https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect
>
> You suffer from NIH syndrome
> To a certain extend agreed, and how do you think I that happened after 20
> years of coding?
>
> I'll just kindly ask you, just ASSUME you might be wrong
> That's all I ever do. I know I might be wrong. But to be sure I need good
> arguments.
>
> Should I rewrite those primitives and libraries to deal with SAML and
> OAuth2 so I'm ""safe""
>
> Never use OAuth:
> https://hueniverse.com/oauth-2-0-and-the-road-to-hell-8eec45921529
>
> Never roll your own crypto.
> Use a libreSSL-based crypto solution:
> https://www.libressl.org/
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/gabordemooij/redbean/issues/450#issuecomment-419887784>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ALDfXqfBpQ30trHJ2WyimuRMOIGEXff-ks5uZlQvgaJpZM4EsR4H>
> .
>",0,0,msr
468,Files now contain Signify signatures. GnuPG may follow later.,0,0,msr
478,"Love it. You would need to be able to override it on a per torrent bassis,
but for people who download primarily just a couple of different file types
(cough).
On Sat, Oct 6, 2018, 4:35 PM shula <notifications@github.com> wrote:
> This feature could help security:
>
> If a user downloads only known files (e.g. audio/video), it is wise to
> block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS,
> (PY?), etc.
>
> Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi <>
> Matrix.avi.exe
>
> Some video torrents are bundeled with fake ""codec.zip"" ""driver.exe""
> containing malware. Today I spotted the attached file, an LNK file, an
> extension that is hidden on Windows.
>
> None of my users are downloading software via torrent, so I'd like to
> block it for them, or set blocked extensions; potentially, i'd like to
> block the whole torrent altogether, if a potential software is found in it.
>
> My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY,
> CPL, DLL, LNK, SCR.
>
> In the screenshots below:
> How windows hides LNK extension, which is a sure malware when only
> downloading media:
>
> [image: lnk-virus]
> <https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png>
> [image: virus2]
> <https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-427604310>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGbY-5e2u-o8-6NhlR-Z9UlUBj9q6YGgks5uiRPzgaJpZM4FT95h>
> .
>",0,0,msr
483,"+1 on the feature request.
In the meantime, I wrote a short cmd script which can be referenced in the _Tools->Options->Downloads->Run external program on torrent completion_ which renames files with a suspicious extension (.exe \ .scr \ .cmd \ .bat) to prevent them from running when double clicked.
```
rem Find suspicious files in directory and rename them
rem Usage: fsus.cmd <dirname>
@echo off
SETLOCAL
set extensions=""\.lnk \.exe \.cmd \.scr \.bat""
echo looking in %1 for %extensions%
for /f %%F in ('dir %1 /s /b') do (
(echo %%F | findstr /r %extensions% > NUL) && move %%F bad_%%F.BAD && echo Renamed %%F
)
ENDLOCAL
```",0,0,msr
486,Since 2015 and this still isn’t added yet? Come on. This would be such a useful feature.,0,1,msr
492,"> > Why isn't this issue considered critical?
> > that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK.
As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.
Also, it is arguably because torrent clients aren't smart that these types of malicious torrents are still going around, and that's why I think qBittorrent should provide this feature.
IMHO certain extensions should also be skipped by default, for the same reason. It would be a great security and usability improvement.
This also allows you to skip certain extensions because you prefer so, but the security issue should be considered prominently.",1,0,msr
493,"> As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.
If you are downloading torrents with malicious .lnk files, you need a solution for a more urgent problem: don't download such torrents, use better sites/sources, or pay more attention. After all, it is the user's responsibility to not fall for phishing emails as well. Inspect URLs/files you click.
Alternatively, you could try to enable showing `.lnk` extensions: https://www.tenforums.com/customization/111886-how-show-lnk-extension.html
But because Windows is Windows, this might lead to undesirable presentation elsewhere (such as the start menu).",0,0,msr
494,"Your solution is for power users, has usability drawbacks, and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.
I think the qBittorrent team should step up and fix this. There is almost never a good reason to download certain file extensions, and users should actively check those files for download.",0,0,msr
496,"> Your solution is for power users, has usability drawbacks, Fighting phishing emails is something everyone has to learn to do, no matter the occupation. I think it is reasonable to demand a certain level of proficiency and common sense.
> and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.
""Malicious users are taking advantage of distracted/careless users"" would be a more accurate statement. Do you think the possibility of receiving phishing emails is a flaw of E-mail? If so, is the possibility of hearing the voice of a scammer in real life, believing what they say, and giving them money, a flaw of your ears? Should your auditory system should autoblock certain words/sentences on its own? Perhaps it should be the brain acting on the information instead.
Furthermore, the greater issue of downloading these kinds of torrents should not be underestimated. You have to go out of your way, even when searching for illegal content, to find these kinds of torrents. And no, the `.exe` in RARBG torrents does not count as an example of this practice in a popular site; it is actually just a harmless text file with the `.exe` extension designed to prevent mirroring by software that, ironically, relies on ""file extensions"" to make assumptions about their content.
Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.",0,1,msr
499,"> You don't need to click through anything. Once you've clicked the .lnk file an .exe will download in the background without any warning in less than a second, and it will run again unnoticed at the next restart. There are virtually endless possibilities to the harm that can be done by these attacks.
> Furthermore, they're not even detected by most antivirus software.
> > If the qBittorrent team needs an example, I can provide it.
There are always exceptions to the rule. I am sure there are some examples of software bypassing UAC, or just being dangerous enough without needing to do so in the first place. But this is a secondary point anyway.
I should add to https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-652597093:
Again, I'm not saying it would be bad to have this feature. It would be good for automation purposes, for example. But I don't think it is fair to consider it ""critical due to user security considerations."". Of course just by being there it could serve as an additional safety net. But that's not the main purpose and it's by no means critical for that purpose.",0,0,msr
502,"TL;DR:
- Anyone is welcome to implement this, and I'm also convinced it won't be rejected if implemented. As mentioned above, it may be useful for certain automation scenarios.
- The default should be to not filter anything, IMO. Otherwise, it would be unexpected behavior - people would wonder why certain files don't download by default. If qBittorrent ever gets some sort of ""onboarding"" UX, this could be one of the tunables (e.g. do you want to filter ""potentially malicious files"" by default?).
- No, this is not ""security critical"" (see discussion above).",0,0,msr
505,"This issue still exists in npm version 3.4.0 and is most certainly a bug and not a new feature or user usage problem.
Please see https://github.com/npm/npm/issues/4197 which is correctly tagged as a bug.
#### Details
- OS: Debian GNU/Linux 7.8 (wheezy)
- `npm --version`: `3.4.0`
- `node --version`: `v4.2.2`
- `npm config get umask`: `0022`
- `umask`: `0077`",0,0,msr
508,"Did this issue get lost (honest question, no sarcasm)?
In my opinion it needs lot more love as the possible security implications could be quite catastrophic on a multi-user system and this issue is reported for over a year now.
The worst case scenario is that such directory results in the possibility to replace its content by a non-root user with evil code that may me executed by another user (including possibly root).
What I observed (even with newest npm 3.10.9) that it **sometimes** creates node_modules directories with permission 777. when doing the `npm install` multiple times the results vary, most of the time it results in 755 but sometimes in 777). This seems to have nothing to do with the source tarballs content (retrieved from registry.npmjs.org) but a more general issue. As mentioned, its not deterministic and the tarballs definitively don't contain any files/directories with such permissions.
This problem was observed while creating packages for a Linux distribution and boiled down to finding this ticket.",0,0,msr
512,"Issue not fixed in 30 days?
Must be gone!",1,1,msr
521,"> ""What's common"" is a great design principle. It's also known as the principle of least surprise. I suspect most people will be surprised if they try to plot an error bar and don't get end caps by default
The principle of least surprise is a good one, I agree, and the change I am requesting is not very surprising.
On the other hand, ""What's common"" is a terrible design principle. All progress is about change, not about doing the same thing all over. Do you have an iPhone? Think about why they are so popular. Before we got them, nobody could imagine that phones and software could be that intuitive and playful. Now we cannot imagine how we could live with the clunky interfaces we had before. iPhones were different in a better way, and that's why they took over. I don't want to be dragged into an argument about commonness and the ""majority of users"", because it is not the point.
> I would also be against removing caps as well because it isn't actually ""extra ink"". If you think about it, the lines are really the extra ink.
The command is called ""errorbar"". It draws error _bars_, you can hardly call that the ""extra ink"". If the lines are too thin, they can be made bigger. This is again not the point. I am also not against end caps in principle, as shown in the paper cited initially. The end caps alone (without a bar) are very useful to indicate a secondary uncertainty, typically a systematic uncertainty, in addition to a statistical uncertainty indicated by the error bar.
The default changes are a great chance for matplotlib to go forward. I offered my arguments, and I can continue to defend them, but I respect the decision of the higher-ups whatever they decide. As a friend and promoter of matplotlib, I am just trying to contribute in a positive way.",0,0,msr
531,"I have to say that I find this change simply horrible. It removes the information where error bars end and thus renders them unusable without the caps. Consider the following example:
![without_caps](https://user-images.githubusercontent.com/22542812/45039484-04e2f100-b064-11e8-9be0-704e66d698a9.png)
![with_caps2](https://user-images.githubusercontent.com/22542812/45040016-3f995900-b065-11e8-8390-102fdadfb454.png)
The first plot indicates that the errors of line1 are too small to be seen (hidden behind the marker) and thus negligible. The plot with caps however reveals that this is certainly not true. The vertical lines are arguable only visual clutter however they are a guide to the eye to find the actual errors.
Obviously I can just change the settings (after finally finding out these things are called caps). I find this change however very dangerous, as in a less obvious case I am tricked into reading the plot wrong. This should never happen! Thus, I would argue to make the caps bigger than the actual marker by default.",0,1,msr
534,This is a kind of solution: https://stackoverflow.com/a/49838455/2803344,1,0,msr
535,"I lost this argument a number of years ago, but I will say I have yet to meet anyone who thinks the default is the better choice.",0,0,msr
539,"I agree with @flying-sheep. You need objective rational arguments for your position. The new defaults were chosen so the average plot looks better. There are principles and rational arguments behind this decision, especially ""increase the data-to-ink ratio"", https://infovis-wiki.net/wiki/Data-Ink_Ratio.
You can still change the default to whatever you prefer, on a plot-by-plot basis and even permanently, by changing your `matplotlibrc` file.",1,0,msr
540,"I don't want to reopen the discussion here, I will just accept that matplotlib makes horrible choices and will always keep the overhead in my code, to always remember it.
@HDembinski The data-ink ratio is actually an argument against this choice. The cap contains the complete information, namely how big the uncertainty is. The error line itself contains no information whatsoever. It is solely a guide to the eye, to find the end more easily. What I really want to use this answer for, is to strongly emphasis that the defaults should not focus on making nice looking plots but unambiguous ones. Aesthetics is to a certain point always up to the beholder, and he is encouraged to change the design in a way he finds pleasing. However, wrong information should never be conveyed. That is why it was so important to change the default color map. This belongs to me in the same category.
The plot I showed was exactly such an example (and it is a real-life example). I tried different algorithms and suddenly my plot showed me that algorithm 1 is orders of magnitude better, as there was no more visible error in comparison to plot 2. You can now say, just change the design how you like it. But that just makes it even more dangerous for me, as I might forget about the terrible choice matplotlib does. I go to a different machine and might do the same error again and might lose a workday if I am not careful.
Thus, I just plead, focus that the plots represent the information. What you do afterwards to make the plots look better doesn't bother me. If I want a good-looking plot, I will change it anyway. But never misguide me, when I just want a quick representation of my data.",0,0,msr
541,"W/o wading through the religious war above, I think this behaviour would be an obvious one to expose via an rcParam if it’s not already. If it is an rcParam already, then I’m not sure what the problem is.",1,1,msr
554,Thanks @stek29. Amazing response time there too.,1,0,msr
555,"The user's list from a group, with less than 100 members, just pops up when I move the mouse to the Back Button when I want to go back to main screen. That's pretty annoying, because I end up viewing a user's profile if I just insta-click.
![image](https://cloud.githubusercontent.com/assets/8980291/24885070/c1ce0fa8-1e4c-11e7-9489-1388c6fec3c2.png)",0,0,msr
572,P.S. Thanks for finally responding to this issue.,0,0,msr
574,"The smileyface is too close to the ""scroll to bottom"" thing and the hover thing always opens and instead of scroll to bottom, your cursor now points to a heart icon which is where the scroll to bottom buttom was before the hover thing appeared over it. Annoying.",0,1,msr
576,"I'm here because of the stupid emoji menu that annoys the living daylight out of me. I NEVER use emoji or stickers or gifs, so the only purpose the emoji popup has for me is to drive me insane.",0,1,msr
579,"@S-U-M-1 You're saying ""has taken"" as if it was done.",1,1,msr
612,"@Sega100500 less yelling please.
http://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#create-an-executable-test-case",1,0,msr
618,"> Your point is - only you decide RoR will work correctly or incorrectly.
@Sega100500 pretty much yes. He is part of the core team and he is more than able to decide how Ruby on Rails will work.
But he didn't closed the issue because you are wrong or not. Let me explain why.
[By our licence](https://github.com/rails/rails/blob/master/railties/MIT-LICENSE):
> THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND
which means that nobody, again nobody, has obligation to fix your issues, no matter how wrong they are. We do because we want to, and you removed all the our desire to help you with comments like:
> PLEASE FIX it
> Are you crazy?
> And now turn on the brain!
At this point I may point you to our code of conduct http://rubyonrails.org/conduct. We believe you are not respecting it and we will not accept you behavior here.
@adityashedge I believe it is an issue but I don't care if it will fixed anymore. If you want to fix it please open a PR, otherwise we will fix when we think it is time.",0,0,msr
623,"Comment created by @deeky666:
Hmmm I am not quite sure if the limit/offset is invoked for subqueries but I don't see why it shouldn't. Also I think this is not a DBAL issue because the limit/offset support for MySQL is the easiest we have on all platform. See: https://github.com/doctrine/dbal/blob/master/lib/Doctrine/DBAL/Platforms/MySqlPlatform.php#L51-L63
The query doesn't have to be modified but instead only the limit clause is appended to the query. Can you maybe provide the generated SQL for that query?",0,0,msr
642,"> Then just use the lifecycle system to (de-)hydrate VOs on your own, > no?
Do you mean lifecycle callbacks — maybe `postLoad` — as shown in http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/reference/events.html#lifecycle-callbacks? Looks like those only work on entities, not value objects, as far as I can see? Eg. if I used `postLoad` an embeddable will already have been hydrated with invalid data (if the data in the database is all `null`, for example). Alternatively, if I move the VO properties onto the entity directly I’ve lost the nice encapsulation that embeddable so usefully provides (eg. if I had a `Product` entity with a `SalePrice` with 2 properties, `value` and `currency` I’d have to move those 2 properties onto the entity. Whilst I could then have those properties be private and do an `is_null` check for those 2 properties before instantiating and returning the VO from `getSalePrice() : SalePrice { … }` it does rather compromise my entity.
I’m almost certainly missing something here, sorry. Rather new to Doctrine so still learning!",1,0,msr
644,"> @Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.
OK, gotcha.
So in the example I gave — a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` — would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?
If so, that seems workable and means the entity is responsible for checking if the VO should be instantiated (rather than having the VO able to be ‘invalid’ and have to implement an `isValid()` method).
Example code of what I mean:
``` php
class Product
{
private $sale_price_amount;
private $sale_price_currency;
public getSalePrice() : SalePrice
{
if (
is_null($this->sale_price_currency) || is_null($this->sale_price_amount)
) {
return null;
}
return new SalePrice(
$this->sale_price_currency, $this->sale_price_amount
);
}
}
```
Is that something like what you’re suggesting instead of nullable embeddables?",0,0,msr
645,"> So in the example I gave — a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` — would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?
Correct.
Basically, since this is a scenario that Doctrine can't cover right now (because of how RDBMS DDL works), you can just implement it in userland for the few times where it pops up.",0,0,msr
647,"Thanks, I silently kept up reading your conversation :)
At the moment, my workaround is the following:
```
class Site
{
/**
* @var DomainName
* @ORM\Embedded(class=""DomainName"", columnPrefix=""domain_"")
*/
private $domainName;
public function domainName()
{
return ((string)$this->domainName === '' ? null : $this->domainName);
}
}
/**
* @ORM\Embeddable
*/
class DomainName
{
/**
* Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
*
* @var string
* @ORM\Column(nullable=true)
* @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
* @see https://github.com/doctrine/doctrine2/pull/1275 [2]
*/
private $name;
/**
* Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
*
* @var string
* @ORM\Column(name=""escaped_name"", nullable=true)
* @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
* @see https://github.com/doctrine/doctrine2/pull/1275 [2]
*/
private $escapedName;
public function __construct($name)
{
Assertion::notEmpty($name, 'The domain name must be provided.');
Assertion::regex($name, '/^(?!www\.)([\pL\pN\pS-]+\.)+[\pL]+$/u', 'The domain name ""%s"" must be a valid domain name without the www. subdomain, but might have others.');
$this->name = $name;
$this->escapedName = static::escapeDomainName($name);
}
public function containsSubdomain()
{
return substr_count($this->name, '.') >= 2;
}
public static function escapeDomainName($name)
{
return preg_replace('/\./', '-', $name);
}
public function __toString()
{
return (string)$this->name;
}
}
```",1,0,msr
649,"I would have preferred to comment on #1275, but the present issue has the benefit to be still open.
My 2 cents on the sensitive subject of nullable embedded properties:
- when the Embeddable has at least one non-nullable `@Column`, and this field is null in the database, **there should be no ambiguity** and **`null` should be assigned to the embedded property**. Otherwise (currently!) you get an empty, invalid value object that has non-nullable properties set to `null`. IMHO, the current implementation is broken here.
- when all `@Column` in the Embeddable are nullable, there should be a boolean setting in the `@Embedded` annotation that controls whether or not you want an empty value object or a `null` value when all fields are `null` in the database. **Your choice**.
Finally, you only have a real problem when you have a fully nullable embeddable, **and** want to make the distinction between a `null` property and an empty object. People have suggested to add an extra column in the table, which would work, but would add a ton of complexity for what I think is an edge case. To clarify, **I think this edge case should not be supported by Doctrine**.
The previous two bullet points, however, **I would strongly suggest working on them ASAP**. I'll be happy to help, provided that lead developers are happy with the concept.",0,0,msr
650,"Thanks for that clear explanation, @benmorel. I quite agree with your suggested specification of how Doctrine _should_ behave, and that it should be worked on. This issue is the top priority I'd like to see addressed in doctrine. I too would be happy to help out with the development and testing of this, if the Doctrine team agree.",0,0,msr
656,"That's the dark side of open source: projects rely solely on the free time developers can invest in them, and at some point they're just too busy on other businesses and/or family life to carry on with developments.
I, too, feel like Doctrine is slowing down; it's just unfortunate that there aren't enough (available) lead developers to keep up the pace with pull requests: many developers are there to offer their help, but without enough consideration from project leaders, it's just wasted brain processing time.",0,0,msr
657,"@BenMorel the pace did slow down a bit last year, but the last months have been quite active :) Also see #6211",1,0,msr
660,"> If it really takes that long it would be nice for improvements to still be added to doctrine 2.
It would make it a mess to migrate these additions to something completely redesigned. From what I can see in the last dozen releases, doctrine functionality already abundantly covers the 90% of use-case scenarios, so we could even call it ""feature complete"", if it wasn't for some rough edges that you encounter when you explore more shady features.",0,0,msr
673,"@BenMorel I agree with you. The Doctrine mantra I keep hearing seems to be along the line of ""Well, it kind of works in most cases and changing things is hard so let's not do it"".
They say a picture says a thousand words and I think this picture sums up my feelings about Doctrine right now: ![](https://i.imgur.com/Rl0VmKc.jpg)
Disclaimer: Owners, don't take offense - it's meant in a lighthearted way and is just my personal opinion. I know you work hard on this, and for that I thank you. I just disagree a bit with the general negativity that I personally see towards any major changes. I know you're working on 3.0 but Symfony and others are leaving you in the dust. If there's too much work, consider giving other contributors more rights or bumping the major version more often so that there can be BC breaks.",0,1,msr
686,"It's just not funny anymore. ![https://twitter.com/_sciwoman/status/659519813564424192](https://cloud.githubusercontent.com/assets/1756909/16565049/1dafe0a4-420b-11e6-8899-0af645ee6781.png)
![https://twitter.com/rjw1/status/2741916767](https://cloud.githubusercontent.com/assets/1756909/16565059/30da5254-420b-11e6-8c9a-585bfc90a30c.png)
![https://twitter.com/krausefx/status/659611440911773696](https://cloud.githubusercontent.com/assets/1756909/16565071/3d0c7228-420b-11e6-851e-aab8583ea8aa.png)
## The sane way - NOPE
```
$ sudo gem install nokogiri
Building native extensions. This could take a while...
ERROR: Error installing nokogiri:
ERROR: Failed to build gem native extension.
current directory: /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ext/nokogiri
/System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby -r ./siteconf20160704-86627-119yyod.rb extconf.rb
Using pkg-config version 1.1.7
checking if the C compiler accepts ... yes
checking if the C compiler accepts -Wno-error=unused-command-line-argument-hard-error-in-future... no
Building nokogiri using packaged libraries.
Using mini_portile version 2.1.0
checking for iconv.h... yes
checking for gzdopen() in -lz... yes
checking for iconv... yes
************************************************************************
IMPORTANT NOTICE:
Building Nokogiri with a packaged version of libxml2-2.9.4.
Team Nokogiri will keep on doing their best to provide security
updates in a timely manner, but if this is a concern for you and want
to use the system library instead; abort this installation process and
reinstall nokogiri as follows:
gem install nokogiri -- --use-system-libraries
[--with-xml2-config=/path/to/xml2-config]
[--with-xslt-config=/path/to/xslt-config]
If you are using Bundler, tell it to use the option:
bundle config build.nokogiri --use-system-libraries
bundle install
Note, however, that nokogiri is not fully compatible with arbitrary
versions of libxml2 provided by OS/package vendors.
************************************************************************
Extracting libxml2-2.9.4.tar.gz into tmp/x86_64-apple-darwin15/ports/libxml2/2.9.4... OK
Running 'configure' for libxml2 2.9.4... OK
Running 'compile' for libxml2 2.9.4... OK
Running 'install' for libxml2 2.9.4... OK
Activating libxml2 2.9.4 (from /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ports/x86_64-apple-darwin15/libxml2/2.9.4)...
************************************************************************
IMPORTANT NOTICE:
Building Nokogiri with a packaged version of libxslt-1.1.29.
Team Nokogiri will keep on doing their best to provide security
updates in a timely manner, but if this is a concern for you and want
to use the system library instead; abort this installation process and
reinstall nokogiri as follows:
gem install nokogiri -- --use-system-libraries
[--with-xml2-config=/path/to/xml2-config]
[--with-xslt-config=/path/to/xslt-config]
If you are using Bundler, tell it to use the option:
bundle config build.nokogiri --use-system-libraries
bundle install
************************************************************************
Extracting libxslt-1.1.29.tar.gz into tmp/x86_64-apple-darwin15/ports/libxslt/1.1.29... OK
Running 'configure' for libxslt 1.1.29... OK
Running 'compile' for libxslt 1.1.29... OK
Running 'install' for libxslt 1.1.29... OK
Activating libxslt 1.1.29 (from /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ports/x86_64-apple-darwin15/libxslt/1.1.29)...
checking for main() in -llzma... yes
checking for xmlParseDoc() in libxml/parser.h... no
checking for xmlParseDoc() in -lxml2... no
checking for xmlParseDoc() in -llibxml2... no
-----
libxml2 is missing. Please locate mkmf.log to investigate how it is failing.
-----
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of necessary
libraries and/or headers. Check the mkmf.log file for more details. You may
need configuration options.
Provided configuration options:
--with-opt-dir
--without-opt-dir
--with-opt-include
--without-opt-include=${opt-dir}/include
--with-opt-lib
--without-opt-lib=${opt-dir}/lib
--with-make-prog
--without-make-prog
--srcdir=.
--curdir
--ruby=/System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby
--help
--clean
--use-system-libraries
--enable-static
--disable-static
--with-zlib-dir
--without-zlib-dir
--with-zlib-include
--without-zlib-include=${zlib-dir}/include
--with-zlib-lib
--without-zlib-lib=${zlib-dir}/lib
--enable-cross-build
--disable-cross-build
--with-xml2lib
--without-xml2lib
--with-libxml2lib
--without-libxml2lib
To see why this extension failed to compile, please check the mkmf.log which can be found here:
/Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.8/mkmf.log
extconf failed, exit code 1
Gem files will remain installed in /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8 for inspection.
Results logged to /Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.8/gem_make.out
```
## The use-system-libraries way – NOPE
```
$ sudo gem install -n /usr/local/bin nokogiri -- --with-xml2-include=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/libxml2 --use-system-libraries
$ bundle config build.nokogiri ""--use-system-libraries --with-xml2-include=/usr/local/opt/libxml2/include/libxml2""
$ bundle install
Fetching gem metadata from https://rubygems.org/
Fetching version metadata from https://rubygems.org/
Fetching dependency metadata from https://rubygems.org/
Using RedCloth 4.2.9
Using i18n 0.7.0
Using json 1.8.3
Using minitest 5.8.4
Using thread_safe 0.3.5
Using addressable 2.4.0
Using coffee-script-source 1.10.0
Using execjs 2.6.0
Using colorator 0.1
Using ffi 1.9.10
Using multipart-post 2.0.0
Using gemoji 2.1.0
Using net-dns 0.8.0
Using public_suffix 1.5.3
Using sass 3.4.21
Using rb-fsevent 0.9.7
Using kramdown 1.10.0
Using liquid 3.0.6
Using mercenary 0.3.5
Using rouge 1.10.1
Using safe_yaml 1.0.4
Using jekyll-feed 0.4.0
Using mini_portile2 2.0.0
Using jekyll-paginate 1.1.0
Using jekyll-sitemap 0.10.0
Using rdiscount 2.1.8
Using redcarpet 3.3.3
Using terminal-table 1.5.2
Using bundler 1.12.5
Using jekyll-textile-converter 0.1.0
Using tzinfo 1.2.2
Using coffee-script 2.4.1
Using ethon 0.8.1
Using rb-inotify 0.9.7
Using faraday 0.9.2
Using jekyll-sass-converter 1.3.0
Installing nokogiri 1.6.7.2 with native extensions
Errno::EACCES: Permission denied - /Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.7.2/gem_make.out
Using activesupport 4.2.6
Using jekyll-coffeescript 1.0.1
Using typhoeus 0.8.0
Using listen 3.0.6
Using sawyer 0.7.0
An error occurred while installing nokogiri (1.6.7.2), and Bundler cannot continue.
Make sure that `gem install nokogiri -v '1.6.7.2'` succeeds before bundling.
```
## The use-system-libraries way but put it in /usr/lib – NOPE
```
$ sudo gem install nokogiri -v '1.6.7' -- --use-system-libraries --with-xml2-include=/usr/include/libxml2 --with-xml2-lib=/usr/lib
Building native extensions with: '--use-system-libraries --with-xml2-include=/usr/include/libxml2 --with-xml2-lib=/usr/lib'
This could take a while...
ERROR: While executing gem ... (Errno::EPERM)
Operation not permitted - /usr/bin/nokogiri
```",0,0,msr
691,why not just make it auto-update able?,0,0,msr
704,"A perspective that may not have been considered just yet: For the past year I've been wondering what's wrong with my instance that it's not sending me an email when new versions are released. It only occurred to me as I was typing up a question on the forum that I may have misunderstood the function, and to read the notification option carefully and search for it (which brought me here). Others may be letting their instance go unupdated not realising that there's numerous updates available.
The reason I haven't seen the notifications is because I rarely use the web app, like others here. The sync is done on my server, and I access the synced directory over the network.",0,0,msr
710,Since the addition of `occ update:check` you can run a cronjob to alert you by email when there are updates available or use the ouput in a proper monitoring system.,0,0,msr
711,that wont work if you don't have shell access. This is true for managed nextcloud setups.,0,0,msr
721,"I needed a global install of NVM because I have some node based cron jobs and a few legacy applications that are rather picky about which version of node they are able to work on. For a time I worked around this by sourcing the nvm script everywhere but that seems to be a somewhat unmaintainable solution. The requirements I have:
- NVM should just work in non-interactive sessions
- Every user should be able to select a installed version of node (or use ```$ nvm exec```)
- Some users should be able to install newer versions of node
This is the solution I came up with: (shout out to @icecoldphp, for the initial version)
0. I've done this on a debian 8 machine, as the root user
1. Create a group called ""nvm"", ```# groupadd nvm```
2. Add root to the nvm group ```# usermod -aG nvm root```
3. Goto the ```/opt``` directory and create a directory called nvm - Make sure the groupd owner is nvm ```# chown :nvm ./nvm```
- Set the permissions so that the group is allowed to write in there and all file will inherit the group ```# chmod g+ws ./nvm```
4. Follow the [git install](https://github.com/creationix/nvm#git-install) steps using ```/opt/nvm``` as the directory - To make sure the group can also write aliases, cache downloads and install global packages make sure the directories exist and have the correct permissions:
```
# mkdir /opt/nvm/.cache
# mkdir /opt/nvm/versions
# mkdir /opt/nvm/alias # chmod -R g+ws /opt/nvm/.cache
# chmod -R g+ws /opt/nvm/versions
# chmod -R g+ws /opt/nvm/alias
```
5. Using the following snippet create ```/etc/profile.d/nvm.sh```:
```#/etc/profile.d/nvm.sh
#!/bin/bash
export NVM_DIR=""/opt/nvm""
[ -s ""$NVM_DIR/nvm.sh"" ] && . ""$NVM_DIR/nvm.sh""
```
6. Ensure that the script is executable ```# chmod +x /etc/profile.d/nvm.sh```
7. If you want to use nvm in non-interactive sessions as well make sure to source the nvm file in ```/etc/bash.bashrc``` before the line saying ```# If not running interactively, don't do anything``` by adding ```. /etc/profile.d/nvm.sh```.
8. For bash completion (which is inherently interactive ;) add ```[ -s ""$NVM_DIR/bash_completion"" ] && \. ""$NVM_DIR/bash_completion""``` after the section about bash completion.
Every user can select a version of node (as the permissions for public are ```r-x```) and users in the nvm group can install and remove versions of node (permissions for the group are ```rwx```).
My questions are:
- As a developer I know next to nothing about linux, could this be improved, is it bad style, etc? Any feedback is welcome.
- Should this be documented in the NVM README.md?",0,0,msr
727,"Yes - the hazard is ""multiuser usage"". In your use case, everyone should be running `node` as the same user.",1,0,msr
735,"@hparadiz , cron is notorious for that. Be it node or not. You would want to put in full paths to whatever binary you get to use inside a cron.
Thanks to your question, I just realized cron doesn't even see /usr/local/bin, so will update my role to use the most common of paths: `/usr/bin/node`, and if you did similarly yourself then you could use the absolute path to your gulps etc, like:
`/var/lib/nvm/versions/node/v8.11.3/bin/gulp`
and the hack would be completed, I think.",0,1,msr
739,"Please see https://github.com/creationix/nvm/issues/1533#issuecomment-303203372 if you have any questions - `nvm` is not designed for, or intended for, multiple users, and will never support the same.
This issue remains open because there's clearly some documentation change that could be made in the readme to make this more clear. A PR to do so is welcome.",0,0,msr
744,"Fair enough! Thanks for pointing that out, @AdriVanHoudt",0,0,msr
749,"Just to clarify my issue (let me know if you prefer a separate one)
I have a package.json with
```
""hapi"": 16.2.x
```
I run `npm i`
It installed 16.2.0
I update my package.json to ```
""hapi"": 16.3.x
```
I run `npm update`
it installs 16.3.0 (as it should)
it updates my package.json to ```
""hapi"": ^16.3.0
```
Why would it edit the package.json? npm 4 does all the sames things except that :O",0,0,msr
753,"@zkat I think I will switch to just using the default notation, it is more correct and works better with the lockfile imo",1,0,msr
772,"I've locked this thread to make it clear that this is no longer the place to have feature request discussions. This is not a bug.
If anyone cares enough to see this behavior changed, please [file a formal RFC](https://github.com/npm/rfcs) with more details around expected behavior. We no longer have the time or bandwidth to discuss more informal, off-the-cuff ideas.
Furthermore, if what you want to do is discuss ideas more openly without a formal proposal, with the understanding that the npm team won't really engage with you about it, you can also post in the [ideas category in npm.community](https://npm.community/c/ideas), which is a great place to brainstorm before filing an RFC.",1,1,msr
774,"This is not a bug, this is behaviour as expected.
You have to keep in mind that the option name is zIndex**Offset**, and **not** `zIndex`. The `zIndex` of each marker depends on the *vertical coordinate of the marker*. Look at this screenshot from https://github.com/Leaflet/Leaflet.Icon.Glyph:
![](https://camo.githubusercontent.com/09ba35cbec1a15aaf2a599be333a85d701f69ead/68747470733a2f2f6c6561666c65742e6769746875622e696f2f4c6561666c65742e49636f6e2e476c7970682f64656d6f2e706e67)
Do you see how you can only see the tip of the bottom-most markers? That's because the `zIndex` of the markers in the bottom row is greater than in the second row, which have a `zIndex` greater than those in the third row, and so on. The `zIndex` is not arbitrary. It grows by 1 for every pixel down. It also gets recalculated on zooming and some view resets.
Thus, the zIndex**Offset** option is an **offset** which applies to the seemingly arbitrary `zIndex`. Thus, if I wanted the center marker to be on top of the rest, I'd set a `zIndexOffset` of at least the height of a marker, and it would look like:
![image](https://user-images.githubusercontent.com/1125786/27128107-a2a6bbb4-50fd-11e7-8244-12efd37cc3d3.png)
> All of this doesn't allow me to set different markers on top of others since step 2 change ruins any logic I may use in my code.
> Here's the example of what happens in the leaflet playground:
If the problem is that you can't set which marker goes on top, *show me an example of how that fails*, because that playground only logs a value, it does not show me markers failing to display on top of other markers. You have felt into the [XY problem](https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem).
If you still need fine control over things on top of things, consider [using map panes](http://leafletjs.com/examples/map-panes/), or post a more obvious example.",0,0,msr
778,"I don't know. Show me an example of the problematic case (playground or screenshots or anything) and we might be able to have ideas.
But please remind that this is a bug tracker, for bugs in Leaflet. Don't expect us to offer consultancy on how to build X using leaflet, that's out of the scope.",0,0,msr
783,"@IvanSanchez The active marker: zIndexOffset = 5, zIndex = 239
Inactive marker: zIndexOffset = 10, zIndex = 567
Do you want me to try artificially update the zIndexOffset of the active marker on click and check whether it will reappear on top of inactive one? I really doubt it'll happen.",0,0,msr
785,"@IvanSanchez Actually it did.. I've used marker.setZIndexOffset(100) on the clicked active marker (A) that was below and it appeared on top of inactive one (B) but then the click on B made it on top and clicking A again has left it behind. This means that I'll need to recalculate all markers zIndexOffsets on the map to make it work as I need which is too much. Also in our setup we may have dozens of markers on the map. This way we'll reach enormous values for zIndex which also seems not good...
That's the point that in case I could have managed the actual zIndex of markers on the map it would solve everything since I could have just decided that all active objects may have zIndex above 500 and all others will have below that. Then I should have just change values of last active objects zIndexes below 500 and the newly activated one above etc. And zIndexes of markers within a specific segment also could have similar logic, i.e below 550 inactive, above 550 - active. Anyway then it's my problem to make zIndexes work for me as expected. And currently since the map component decides for me the actual zIndex value, it messes all my logic up.",0,0,msr
787,"> you can manually disable it
Thanks for the suggestion. I'll try it out. It also gave me an insight on how you implement the layouting.
> your logic is infallible
Not at all. :) I just think that the API should allow disabling the recalculation of `zIndex`es and allow the API customer break his own head on how to implement what he needs.
> If you can _isolate_ a case where Leaflet fails...
The scenario is extremely simple. Use `zIndexOffset` for 2 markers with difference of 5. Since the zIndex that each marker gets from your algorithm is hundreds you may come up with the following:
`zIndexOffset(A) > zIndexOffset(B)` set by the code
but
`zIndex(A) < zIndex(B)` calculated by leaflet
And since `zIndex` is recalculated on each map operation: drag, move, zoom etc. there's no way to get the actual difference in `zIndexOffset` for markers to make it work correctly for `zIndex`es that will be calculated.",0,0,msr
788,"Well, finally I came up with a solution that works fine for me:
Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
Anyway, **thanks a bunch for your rapid help**!",0,0,msr
789,"@lentyaishe I was used this gist an run like a charm. Maybe this can be a Feature Request?
https://gist.github.com/up209d/4c68f2391f2302e510eb81aa8bcd4514",0,0,msr
792,"The documentation says:
```
By default, marker images zIndex is set automatically based on its latitude. Use this option if you want to put the marker on top of all others (or below), specifying a high value like 1000 (or high negative value, respectively).
```
It should mention that the ""automatic"" setting based on the latitude shall not be greater than N, instead of ""specifying a high value like 1000"".
Regardless, you need to separate them by thousands for each ""level"" or ""step"" if you want to have them show as you want.
Consider their problem, they want those with a lower latitude to show below the others, but you want them to be displayed in levels not considering the latitude.
They cannot really store all the possible offsets you set, unless you pass a list first. **Basically they should say in the documentation that we use a range of 0-N and you should set each level of z-index you wish to display regardless of location as (N+1 * level_desired). You might as well considering N to be 1000, it seems to work but obviously could change.**",0,0,msr
795,"@Falke-Design does not work.
<img width=""638"" alt=""Screen Shot 2022-02-07 at 4 22 30 PM"" src=""https://user-images.githubusercontent.com/741705/152796083-55234af0-474a-4d07-97ad-e894016f3662.png"">",0,0,msr
807,"@coleifer I'm expecting you will make a deal with static typing fans more and more. I'm partially agree you can do nothing in orm itself and should not, but you should try to understand `static dynamic` fans. Its not about limiting dynamism or to make new java from python, people just want `user.(name/id/group)` in IDE, they are tired to go from view to model definition, to 'google' any functions signature, to debug typo mistakes in runtime. Go has no objects and generics, and any data structure should upcast value to `interface{}`, C is just too low-level. You should check out [typeorm](http://typeorm.io/) and [diesel](http://diesel.rs/), Dart. Static typing was stigmatized long time as a way to make compilable, fast languages like java, cpp. It has different purpose in dynamic languages now.",0,0,msr
813,"This issue was opened nearly 5 years ago. In that time I've had plenty of opportunities to see python's type hinting in the wild. I still believe that it misses the mark, offerning none of the iron-clad guarantees you get from a statically-typed language, while being injurious to Python's inherent dynamism, readability, and simplicity. Since this issue tends to attract drive-by comments of little value, I'm going to close discussion for now.
When I want static typing I reach for a statically-typed language. When I want expressiveness, simplicity and flexibility, I reach for Python.",0,1,msr
815,@alikins I believe you look after vault,0,0,msr
819,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",0,0,msr
821,"I do see this in 2.5.
```
root@ubuntu-xenial:~# ansible --version
ansible 2.5.0rc1 (stable-2.5 36566e62a7) last updated 2018/03/05 13:46:00 (GMT +000)
config file = /etc/ansible/ansible.cfg
configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
ansible python module location = /root/git/ansible/lib/ansible
executable location = /root/git/ansible/bin/ansible
python version = 2.7.12 (default, Nov 20 2017, 18:23:56) [GCC 5.4.0 20160609]
root@ubuntu-xenial:~# cat vars.yaml
ansible_ssh_pass: !vault |
$ANSIBLE_VAULT;1.2;AES256;my_user
31313064366365626535323066613234626234336664333266663161366233396365633063303539
3066363333666236666335656631666663373037643338630a303763363031373337663733326134
38336566366535373561373830386638663635363438333633313536333731646331366138383961
3331346163623661340a663862323337313562376338386539326438323562383136383832376266
31306663393532323761353761353435373432633632626365633734303335633436
nonpass: pass
root@ubuntu-xenial:~# ansible-vault view vars.yaml
Vault password:
ERROR! input is not vault encrypted datavars.yaml is not a vault encrypted file for vars.yaml
```",0,0,msr
822,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",0,0,msr
823,"I poked at this a little yesterday and braindumped some thoughts in code comments at https://github.com/alikins/ansible/commit/603cac4a041a10ec8186617c95ef539a9ece787a
(copied/paraphrased here for discussion)
> Open a file, figure out if it is all vault or yaml with vault strings, edit.
> > if yaml with vault strings, parse the yaml with AnsibleYaml > and secret. Replace with '!vault-plaintext vault-id' and plaintext. > Save, open editor. > On save/reencrypt, reparse the file with AnsibleYaml, get the
> plaintext of the to be reencrypted vaulted string, encrypt it
> (!vault-plaintext -> !vault,
> AnsibleVaultUnencryptedUnicode -> AnsibleVaultEncryptedUnicode).
> > And then, things get complicated... we can't just AnsibleYaml.dumps()
> the data structure out:
> 1. Comments and comment placement is not preserved which > is kind of annoying
> 2. AnsibleYaml can loads things into data structures
> that it can not `dumps()` out.
> Ie, we can't serialize a bunch of stuff we can deserialize.
> > So just AnsibleYaml.dumps'ing the datastructure back
> to a file will usually either fail or do the wrong thing.
> > #2 above is unlikely to get fixed soon if ever.
> #1 is mostly a limitiation of the PyYaml yaml module ansible uses. > Other implementation like Rueyaml can do this, but it is unlikely for > ansible to change this any time soon.
> > So, since we can't just serialize to yaml, we likely need to do some > string manipulation to replace the '!vault ' blob.
> > We would need to know exactly what the before string looked like
> and where in the file it is, and what the new !vault will look like.
> But we don't really know what the new !vault-plaintext > string will look like.
> > For that matter, we don't know if it will be in the same place,
> or if it will exist at all, or if it will be at the same path in the > datastructure after the edit. > > We could limit edit to only try to work in cases where
> those aren't changed. We also have no idea what the
> plaintext will look like.
> > ideas:
> - !vault-plaintext is a compound yaml type, with fields for
> the vault id to use, and for the plaintext. Could also
> possibly include some identifying info for what the !vault
> it replaced looked like. An example:
> > some_var: !vault-plaintext:
> vault_id: 'dev'
> decrypted_from: |
> $ANSIBLE_VAULT;1.1;AES256
> 66393964663765613335633461643334393234346231666665306635323635333137306339356232
> plaintext: |
> The new plaintext to replace decrypted_from with
> > That would give vault-edit enough info to do a reliable job > of replacing the previous content.
> The downside to that approach is that it points out the limitations of the current !vault format. It may also be useful to extend !vault to support getting a data structure with info in it instead of just the plain text scalar. At the moment, I'm not sure if it could do both but it seems possible.
Or could just call the extended info version of !vault !vault-extended or similar. At that point it might be possible to make !vault-extended the default vault blob format for vaulted files as well. ie, instead of
a vaulted file being:
```
$ANSIBLE_VAULT;1.1;AES256
66393964663765613335633461643334393234346231666665306635323635333137306339356232
3533306631646431663239623762366365663137383435380a393139303161383561303336623962
35373663663036333863373666326634616532376335333133326163376136353636633763623739
3736343064326662390a306438356239386665306437646665323836393032393565666136643362
3663
```
It would be yaml something like
``` yaml
--- - !vault-extended:
vault_id: 'dev'
cipher: AES256
encrypted_on: 2018-03-14
ciphertext: |
$ANSIBLE_VAULT;1.2;AES256;dev
66393964663765613335633461643334393234346231666665306635323635333137306339356232
```
ie, more or less like https://github.com/voxpupuli/hiera-eyaml",0,0,msr
825,"Right now the usability of encrypted variables compared to whole encrypted files is rather poor unfortunately. Especially in cases where I quickly need to access an encrypted variable (e.g. a password) I really don't want to google for solutions like https://stackoverflow.com/questions/43467180/how-to-decrypt-string-with-ansible-vault-2-3-0.
It is also a problem for `git diff` use cases (https://stackoverflow.com/questions/29937195/how-to-diff-ansible-vault-changes). Is improving this state still on the roadmap? I didn't find it neither for 2.5 nor 2.6...",0,0,msr
826,"Hi, so I've figured out a way to do this for checking individual values, using a yaml parser, **yq** https://github.com/mikefarah/yq (there's more than one yq project, but I used this). This works with ansible 2.5.2
I have a vars file, with encrypted and unencrypted values, `all.yml`
```
unencrypted_value: 1234
encrypted_value: !vault |
$ANSIBLE_VAULT;1.1;AES256
37316535353565313063353530353539666634363834626664366263666538346131653332353932
3637363030613037316336306466656432353463383230370a396530323164353563363434663238
30336436396264656663663837346162323762333063376631326633356533376566633563386637
6531383261396366640a363339616164333630373730613564646434386364396534653063666238
6131
```
I have a password file, `vault-password`
```
password
```
Using `yq`, I'm able to decrypt the value pretty easily, by selecting the encrypted value and passing it to the decrypt function
```
$ yq read all.yml encrypted_value | ansible-vault --vault-id vault-password decrypt
Decryption successful
secretsecret
```
Hope this helps!",1,0,msr
835,This is still a problem with Ansible 2.8... A solution would be really appreciated!,1,0,msr
836,"For others looking for a quick solution I created this script: https://gist.github.com/steffann/240d4170e45aa3cf7cf0df5e9beaf0ba
It uses [ruamel.yaml](https://yaml.readthedocs.io/), which preserves ordering, comments etc in the YAML file. Great when depending on decent git diffs etc :)",1,0,msr
844,"> I solved this using debug mode. E.g.
> > `ansible localhost -m debug -a var='myVariable' -e ""@myFile.yml"" --ask-vault-pass`
Works beautifully! No need for `--ask-vault-pass` if you have the password in a file identified by the `ANSIBLE_VAULT_PASSWORD_FILE` environment variable:
https://docs.ansible.com/ansible/latest/reference_appendices/config.html#envvar-ANSIBLE_VAULT_PASSWORD_FILE",1,0,msr
846,"A few expansions to @whirlwin's clever workaround:
Instead of a specific var, you can have ansible dump all vars (encrypted and plain) by specifying `-a var=""vars""`. There will be some noise in the result with a few stock vars, but it's a nice way to see everything at once.
You can also specify multiple source files by passing multiple `-e ""@...""` args.
Also, I haven't tested this personally but you can apparently avoid the implicit `localhost` warnings by setting `ANSIBLE_LOCALHOST_WARNING=false` on Ansible 2.6+.
All together:
```
ANSIBLE_LOCALHOST_WARNING=false ansible localhost -m debug -a var=""vars"" \
-e ""@file1.yml"" -e ""@path/to/file2.yml"" -e ""@path/to/file3.yml""
```
plus `--ask-vault-pass` if needed.
Obviously a built-in solution would be much better, but this makes mixed plain/encrypted var files somewhat workable.",0,0,msr
849,+1 for this feature,0,0,msr
870,"This PR is created in response to https://github.com/jbialobr/gitextensions/pull/3
It addresses some issues that bother me in https://github.com/jbialobr/gitextensions/pull/3 These are:
1. WorkingPathProvider depends on Directory, AppSettings and GitModule static methods. These are widely used across the app. I would not like to pass them to various objects through a multiparam constructor.
2. Turning static methods into instance methods: https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376 3. Incompleteness of arranging the exterior. Unconfigured calls return default(T), which may cause false positive tests results. I would prefer to be notified that unit test referenced not configured call. It does not address the separation CommandLineArgs concern https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376
This is only a sketch - there is no proper class per file and file per project separation. I am interested in discussing about the general idea of this solution.
The general idea of this solution is:
1. Wrap static methods in Gateway classes.
2. Gateway classes are singletons, they provide default implementation through public virtual members.
3. In unit tests Gateway classes are substituted in Setup method.
4. After the arrange phase unconfigured calls on substituted Gateways are not allowed.",1,0,msr
873,"To set the context.
The current codebase features heavy use of inheritance and static methods.
OOP in general and inheritance in particular were all the rage about 10-15-20 years ago. As a concept it does have benefits of reuse, but it also carries downsides of maintenance overhead, complicated abstraction layers, difficult testability, side effects and surprises in behavioural changes... These days stateless interface-based implementations have become quite popular (especially with raise of functional languages) where each class has a single responsibility (or as close to that as possible) and gets all the dependencies injected via a constructor. This allows to write much cleaner and maintainable code, easier to unit test (as opposed to inheritance based classes where you would more likely be writing integration tests) and easier to alter behavior by substituting dependencies.
Another benefit of using interface-based architecture that you could plug in IoC container (e.g. autofac).
Over the years I have experience both sides, I was a big proponent of OOP but I have learnt the hard way that it makes it much harder to maintain such code, especially in large projects.
As I observed, a lot of bugs stem from implementations with overloaded concerns and lack of unit tests. As a result the code is quite confusing, inflexible and hard maintain.
In my opinion, one of the ways we could start reducing the tech debt and improving the codebase - start extracting separate concerns into individual classes and compose them as required. Gradually we would be able migrate most of the codebase. Static methods have their use too, but as any tool they have to be used appropriately.",0,0,msr
874,"My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces, leading to the following problems:
1. Tests are no longer executing against reality. Sometimes it's close, but it means tests are prone to both false positives (failed due to incorrect mock) and false negatives (passed even though the situation intended to be tested will fail at runtime).
2. Mocks are effectively a very clumsy DSL where logic must be manually reimplemented. Every mock necessarily reduces the long-term maintainability of the project, as well as the ability of developers to make behavior-preserving changes.
3. Mocks are not a substitute for properly covered tests. You can test code without mocks, or you can test code both with and without mocks, but mocks never save you from writing tests you would otherwise have to write.
As long as there is a clear understanding that mocks are only to be considered in the total absence of the ability to write a test using another approach, then I don't have a particular problem with either using or not using DI.",1,0,msr
876,"> My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
I am not sure if completely I understand your point.
If each class has a single responsibility (SR), then DI should not add any ambiguity - for each use case you set dependencies to imitate boundary conditions, but you **only** exercise and validate functionality of the class under test.
Any changes to dependencies should not have any profound effect because the class is built against a contract and not a particular implementation. Certainly a degree of coupling is possible, but it typically it will be lose coupling.
Also a SR class (usually) define a limited number of methods (accessible via an interface). As a result you test the public surface. Generally there is a limited need to test internal or private methods (due to their absence).
Another benefit of smaller classes with SR - you know exactly what each class depends on. For example right now `GitModule` spans across 3,400+ lines of code carrying hundreds of methods. And we inject this monstrosity to classes which may need to access only one method from the lot...
If a class has multiple responsibilities and/or initialises its own concrete dependencies then it is virtually impossible to perform unit testing - instead you will be writing integration tests. Any changes to dependencies with high degree of probability will necessitate changes to the class, because there is a tight coupling to concrete implementations.
Wrt: code coverage - whilst it is a very useful metric, it shouldn't be viewed as an absolute. 100% CC can be very misleading.
Personally when I write tests I usually strive for 100% coverage taking in consideration various edge cases. I use NCrunch, it helps me to visualise what cases I may have missed.",0,0,msr
877,">Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
Yes, that's true. Regardless of the tools we use there is always a way to use them inappropriately.
I like to be pragmatic. For me it means: achieve maximum benefits at reasonable cost with decent safety.
Weighing those factors we have to take into account the existing code base and resources we have.
Thank you for pointing to codecov.io - it should help to find which pre-condition we lack in the test set (though 100% codecov coverage does not imply 100% pre-conditions coverage). Implementing codecov into GitExt project will show us all the most obvious pre-conditions against which should we write ""units of behavior tests"".
Then we have to write these tests, to do it, we have to weigh the factors I mentioned and decide whether to mock or to configure the exterior. I think we can't make this decision at a general level. Besides, one approach does not have to fit all concerns.
Here we have 3 dependencies, how would you see ideally written [this test](https://github.com/gitextensions/gitextensions/pull/3898/files#diff-67c45fbdcf3a92b53649d84f9f401c3eR26)?
```csharp
public void ReturnsRecentDirectory_if_RecentDirectory_IsValidGitWorkingDir()
{
//arange
DirectoryGateway.Inst.CurrentDirectory.Returns(string.Empty); <-- mocked
_ext.StartWithRecentWorkingDir = true; <-- mocked
string unitTestRecentWorkingDir = ""unitTestRecentWorkingDir"";
_ext.RecentWorkingDir = unitTestRecentWorkingDir;
GitModuleGateway.Inst.IsValidGitWorkingDir(unitTestRecentWorkingDir).
Returns(true);<-- mocked
//act
string workingDir = _workingPathProvider.GetWorkingDir(new string[0]);
//assert
workingDir.Should().Be(unitTestRecentWorkingDir);
}
```",1,0,msr
882,"> To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry
It's been the primary container for Visual Studio since 2010. Over the years it's received a lot of love to meet the performance demands of that application, though not all of those benefits have made their way to the normal distributions of it *yet*. I would classify it as having fewer features, but within the bounds of its capabilities excels in ease of distribution and being generally well-understood.",0,0,msr
885,"I'm not sure what you perceive as a problem in attached links, I'm sorry.
The classes under tests are stateless - they do not have any settable fields or properties (outside the container) nor they access any objects besides those passed via the container. The external dependencies are interface-based, and as such they are stateless too. So there are no side effects and there are external context mutations - so I can assert the correctness of my implementation.
The unit tests exercise my class implementations and check the flow of execution by checking how far each use case progresses and what methods within the method under test were called. There is no magic here :)
Of course there is an arrangement phase, you can't not have it. But the arrangement only sets a possible return from the underlying dependency (boundary conditions). My duty as a class developer write unit tests to account for these boundary conditions and facilitate stable predictable response.
**EDIT:**
Just realised some classes may be accessing `AppSettings` which arguably belongs to the external context, but for the most part we can presume AppSettings to be immutable. They should be abstracted too.",0,0,msr
887,">I'm not sure what you perceive as a problem in attached links, I'm sorry.
""... it often tests the implementation instead of expected behavior.""
The following test tests implementation.
```csharp
public void SetRemoteState_should_call_ToggleRemoteState(string remoteName, bool remoteDisabled)
{
var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
_configFile.GetConfigSections().Returns(x => sections);
_controller.ToggleRemoteState(remoteName, remoteDisabled);
_configFile.Received(1).GetConfigSections();
_module.Received(remoteDisabled ? 1 : 0).RemoveRemote(remoteName);
_configFile.Received(remoteDisabled ? 0 : 1).RemoveConfigSection($""{GitRemoteController.DisabledSectionPrefix}{GitRemoteController.SectionRemote}.{remoteName}"");
_configFile.Received(1).AddConfigSection(sections[remoteDisabled ? 1 : 0]);
_configFile.Received(1).Save();
}
}
```
I would see the assert part along the lines of this:
```csharp
public void ToggleRemoteState_ChangesDisabledValue(string remoteName, bool remoteDisabled)
{
//arrange
var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
_configFile.GetConfigSections().Returns(x => sections);
//act
_controller.ToggleRemoteState(remoteName, remoteDisabled);
//assert
_controler.GetRemote(remoteName).Disabled.Should().Be(!remoteDisabled);
}
}
```
This way we don't care about the implementation details, we only deal with pre and post conditions.
If the implementation changes from `remove then add` to `rename`, the first test will fail while the second will not.
>The external dependencies are interface-based, and as such they are stateless too.
Apparently we have a different understanding of ""stateless"". The config file holds a state. Invoking its methods changes this state.",0,0,msr
901,"@sharwell great write up, thanks.
It looks like we come from different schools of thought and practice. Like yours my position comes from practical experience solving real engineering challenges.
I'm part of a small team building RAD framework and reusable components where pretty much every component can be substituted by a consumer. We have a number of major and minor releases concurrently powering hundreds of apps in production, and we provide full runtime backwards compatibility.
We can assume very little about consumers, their habits and requirements. In turn we must provide a great deal of flexibility but ensure stability of the framework core and consumer applications (to a degree). This is specifically important in security-related areas.
All of this dictates how we architect and engineer the framework, how we deal with dependencies, how we deploy and how we test.
Until we re-engineered the codebase to consist of interface based stateless single responsibility classes we had a number of challenges meeting the requirements (including production incidents). We also couldn't reason about stability and predictability of provided behaviors.
Now we have 1,800+ .NET unit tests, hundreds of JS tests and probably another few thousand of BDD tests. Some tests, especially security-related, test the order of execution - without that we can't reason about predictability, nor we can release new versions or provide backwards compatibility.
Smaller interface-based single responsibility implementations allow us to achieve * isolation (all dependencies are injected), * testability (all dependencies are mocked), * stability (via predictability), * flexibility (consumers only concern themselves with replacing a specific implementation instead of worrying about inter-dependencies). And as a bonus - our APIs are easy to register in an arbitrary IoC container.",0,0,msr
902,"@RussKie Your last post really resonates with me. One of the goals I had when I was pushing so hard to switch away from mocks (on a former project) was based on the type of application being shipped to customers. As you are well aware, anything considered public API surface area has special requirements related to versioning. These requirements constrain the development team substantially, but are a great asset from the customers perspective. However, in an application where the final product is the application (as opposed to a publicly-consumable API), these same requirements *burden* the development team. I didn't like the fact that when we wanted to make a change to our implementation, we kept running into pain points (leading to delays) resembling those faced by developers trying to make changes to public APIs.
Another application (this time a library) I've contributed to over time is StringTemplate. While it has a few known extension points built in, it was never designed to be arbitrarily reconfigurable for end users. Over time it's led to a frustrating experience for a small subset of users trying to do arguably normal things, just things we didn't think of from the start. For example, by failing to abstract the file system (location and reading) away from the template loader, it's not possible to use *some* features of StringTemplate when the templates are stored e.g. in a database, or stored using a compression mechanism we didn't think of from the start. Better adherence to SRP would likely improve the long term durability and flexibility of this library.
It sounds to me like GitExtensions is unlikely to suffer from most of the problems related to reusable components, and more likely to benefit from a testing approach that focuses on application behavior.
> > When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
>
> @sharwell Is there any publicly available project that uses this approach - I would love to see this in action.
Not to the extent of the one I've been referring to. I've started to add tests for UI related bug reports to PerfView, but these are quite limited and not particularly elegant currently.
When I built a system like this in the past, I started with the following:
1. Identify all mutable static state in the application. We will need a way to reset these to known default values between each test, ensuring that the outcome of any given test cannot influence the behavior of another (with a corollary that a test failure uniquely and deterministically identifies the culprit, which can then be executed in isolation to reproduce the failure):
* Mutable `static` fields
* File read and written
* Registry keys/values read/written
2. Identify all asynchronous operations. We will need a way to ensure that a test is not considered complete while one of these is executing (or scheduled for later execution, in the case of timers or delays).
3. Identify external dependencies for which the concrete implementation is likely problematic for high-speed deterministic testing. We will need a way to substitute the behavior of these items during testing. The method of substitution must guarantee that during the execution of any given test, all executing code is using the same implementation (either the original or the substituted one, but never a mix and never multiple substitutes).
* One obvious case is invocations of *git.exe* for many types of tests
* Loading resources from a network
Once you address the above, most tests are still written against module interface boundaries. However, when actually testing UI behaviors it's not particularly challenging to write those tests against the controls. Most of the problems typically associated with testing UI behavior are really just failures to address one of the above underlying concerns.",1,0,msr
903,"I would go as far as proposing moving away from statics altogether.
There are merits for extensions, but like any tool they have to be used sparingly to be used correctly.",1,0,msr
905,"The article is somewhat related, but the single-instance services I would advocate for behave very differently from the classical singleton pattern. Most of the differences were created to directly address one or more costs listed in that article.",1,0,msr
912,"> The word ""Caddy"" in the context of software is under a pending trademark application
So it's not a registered trademark?
> By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)
See issue #1 - this is planned 😃",0,0,msr
919,"@lol768 As long as you're dolling out legal advice on the Internet, you might well quote the other relevant section of the Trade Marks Act of 1994.
> A person may also infringe a registered trade mark where the sign is similar and the goods or services are similar to those for which the mark is registered and there is a likelihood of confusion on the part of the public as a result
See: section 10(2)(b) http://euipo.europa.eu/pdf/mark/nl_uk_1_en.pdf
I am not a lawyer, but I am somewhat versed in trademark law. Your dependence on ""commercial use"" may be unsupportable, but talk to an attorney for legal advice.
LCL are actually doing the right thing to defend their mark by providing notice.",0,1,msr
931,"> I'm confused at what this fork offers that Caddy doesn't... Is it the code that shows the headers that you removed all that's stopping you from using Caddy?
Removal of adware and binaries that can be freely distributed and are not subject to an EULA.
> You do realize that you can compile Caddy yourself without those headers in it, without forking it and maintaining a whole new project, right?
Yes, I didn't overlook that :)
> From my point of view people are upset by 2 things. 1: They can't get pre-compiled binaries without the header and 2: if they can, they have to pay for it.
> If anything, this fork adds MORE work on you than what Caddy says to do to use it free of charge without the headers.
From my point of view (and from reading HackerNews) people are upset about the ads embedded in the webserver and served to the visitors. They're also upset about how the binaries are licensed.
The fork is absolutely more work for me. The point is that I care enough about this that I'm willing to create the fork, remove the code I disagree with, cross-compile it and then make free (as in freedom) binaries available to everyone - including those who may not be technically adept enough to set up their own golang workspace and compile it themselves.",0,1,msr
934,"I find it highly unlikely that's going to happen. I have infrastructure, if you ever need somewhere to host builds or tooling like caddy offers, reach out and I'll share it for free.",0,0,msr
937,"> They stopped the bad faith behavior upstream so there's no longer any reason for this fork to exist. Imagine sticking up for a company that you have nothing to do with when they go around being a trademark bully. You sure showed that petty FOSS volunteer the error of their defiance towards the honorable corporate entity!
Imagine thinking that a solo developer trying to make a living from his work is some trademark bully and evil corporate entity, just because they have an LLC and premium support plans.",0,1,msr
943,"Indeed, that was what I meant by:
> That would generate a Pipfile.lock with updates for only my-awesome-dep and its dependencies.",0,0,msr
946,"Tried to work around this by setting `export PIP_UPGRADE_STRATEGY=only-if-needed` in my shell config. This doesn't work, and `pipenv lock` exhibits these surprising behaviors:
1. It ""upgrades"" packages that don't need to be upgraded (but...)
1. It actually doesn't upgrade the installed versions! i.e. `pip freeze` and `Pipfile.lock` show different versions!
Guessing pipenv is delegating to pip for the install, and pip respects its environment variable settings, but `pipenv lock` doesn't.",0,0,msr
947,"@k4nar What happens right now that you are finding undesirable? Because if you upgrade a dependency that has cascading requirements obviously it will have consequences for other dependencies. Are you suggesting some kind of resolver logic to determine the most current version of a specific package _in the context of the current lockfile_? I am hesitant to encourage too many hacks to resolution logic, which is already complicated and difficult to debug.
@brettdh I think I can shed some light because you have most of the pieces. `pipenv lock` doesn't install anything, and it doesn't claim to. It only generates the lockfile given your host environment, python version, and a provided `Pipfile`. If you manipulate your environment in some other way or if you use pip directly/manipulate pip settings outside of pipenv / are not using `pipenv run` or using `pip freeze` inside a pipenv subshell, it is quite easy for a lockfile to be out of sync from `pip freeze`. The two aren't really related.
To be clear:
1. `Pipfile.lock` is a strictly-pinned dependency resolution using the pip-tools resolver based on the user's `Pipfile`
2. If you want to maintain strict pins of everything while upgrading only one package, I believe you can do this by strictly pinning everything in your `Pipfile` except for the one thing you want to upgrade (correct me if I'm wrong @vphilippon)
As for your lockfile and `pip freeze` disagreeing with one another, I'd have to know more information, but I believe we have an open issue regarding our lockfile resolver when using non-system versions of python to resolve.",0,0,msr
952,"@brettdh The locking mechanism include a notion of ""dependency resolution"" that does not exist in `pip`. Its handled by `pip-tools` (or rather, a patched version of it, integrated in a special way by `pipenv` that bring a few differences with the original tool). In short, the locking mechanism reads the `Pipfile` and performs a full dependency resolution to select a full set of package that will meet *every* constraints defined by the required packages *and their dependencies*.
@techalchemy > [...] it’s resolving with pip-tools that concerns me.
I'm not sure how those `--upgrade-strategy` would affect `pip-tools`, because it works on some low-level internals of `pip`. I have the feeling this would not give the expected result, as these option take into account what's installed, and that's not what's being dealt with in that mechanism. But we have another approach to this in `pip-tools` that could be done here.
The ""original"" `pip-tools` behavior is that it only updates what's is needed in the lockfile (in its context, its the requirements.txt), but this was ""lost"" in the way the resolver was integrated in `pipenv`. Let me explain why.
Pointing back to my resume of how `pip-tools` works: https://github.com/kennethreitz/pipenv/issues/875#issuecomment-337717817
Remember the ""select a candidate"" part? That's done by querying the `Repository` object.
In `pipenv`, we directly configure a `PyPIRepository` for the `Resolver`, but `pip-tools` does something else, it uses a `LocalRequirementsRepository` object, which keeps the existing pins from the previously existing `requirements.txt` (if found), and ""fallbacks"" on `PyPIRepository`.
So in `pip-tools`, the following happens when selecting a candidate:
1. Query `LocalRequirementsRepository` for a candidate that match `foobar>=1.0,<2.0`.
1. Check if an existing pin meets that requirements:
- If yes, return that pin as the candidate.
- If not, query the `proxied_repository` (`PyPIRepository`) for the candidate.
1. Use the candidate returned
Effectively, it means that existing pins are given a ""priority"" as candidate to try first.
But in `pipenv`, currently, it simply:
1. Query `PyPIRepository` (directly) for a candidate that match `foobar>=1.0,<2.0`.
1. Use the candidate returned.
So, I think the same behavior for the locking in `pipenv` could be done by parsing the `Pipfile.lock` to get the existing pins and use a `LocalRequirementsRepository`, like `pip-tools` does in its `pip-compile` command.",1,0,msr
956,"@vphilippon Totally agree that A and B are desirable workflows in different situations. Some of your phrasing around B confused me a bit, though, seeming to say that `pipenv lock` might result in a lockfile that doesn't actually match the environment - I particularly heard this in that one would need to ""run `pipenv update` to benefit from the fresh lock"" - as if the lock is ""ahead"" of the environment rather than matching it.
Regardless of whether you are in an A workflow or a B workflow, a few things seem constant to me, and I think this squares with what @techalchemy is saying as well:
* The result of `pipenv lock` should always be a lockfile that matches the environment.
* The result of `pipenv install` should always be an environment that matches the lockfile.
I'm ignoring implementation details, but that's kind of the baseline behavior I expect from a package manager with a lockfile feature.
Running `pipenv update` periodically allows you to stay in B mode as long as you want everything to be fresh, and having the ability to `pipenv install --upgrade requests` would allow specific updates of one package and its dependencies, without affecting packages that don't need to be upgraded unnecessarily.
Am I missing any use cases? I can think of optimizations for B - e.g. a flag or env var that tells it to always update eagerly - but I think that covers the basics. I also know I'm retreading ground you've already covered; it's just helpful for me to make sure I understand what you're talking about. :)",0,0,msr
965,"I'm interested but I'm not sure to know how would be the best way to implement this. There are a lot of components in action (pip, pip-tools, pipfile, pipenv…) and probably a lot of possible solutions.",0,0,msr
970,"@marius92mc The ""fixed in master"" comment is referring to the `--selective-upgrade` and `--keep-outdated` options added in recent releases: https://docs.pipenv.org/#cmdoption-pipenv-install-keep-outdated
That allows folks that need or want more control over exactly when upgrades happen to opt in to that behaviour, while the default behaviour continues to respect [OWASP A9](https://www.owasp.org/index.php/Top_10-2017_A9-Using_Components_with_Known_Vulnerabilities) and push for eager upgrades at every opportunity.",0,0,msr
976,"Actually, your pipfile/lock would be great, if it contains outdated results.",0,0,msr
979,"There are any updates about this issue, @kennethreitz?",1,0,msr
993,"@l0b0 If your application genuinely cannot handle a particular version of Django, I think it makes sense to state that restriction in your Pipfile?",0,0,msr
1001,@benkuhn Not that I know off - I do the same lock & revert dance all the time.,0,0,msr
1003,"`--keep-outdated` seems to *systematically* keep outdated only the explicit dependencies that are specified (unpinned) in Pipfile, while all the implicit dependencies are updated.",1,0,msr
1010,"@dfee I am not really sure that blurring lines between applications and libraries is the correct answer to dependency management, so I don’t see poetry’s approach as an advantage. I wasn’t involved in whatever your issue was with the recommendation engine, but we took that out some time ago...",0,0,msr
1012,"My apologies to bean so rude. Now reading my comments I realize that despite the info I provided and some of my options are still valid (IMHO), it's wasn't appropriated the way I wrote what I wanted to say.
I understand that the issue tracker is most a place where to discuss bugs and improvements, and discuss whether this is a bug or an error by design is not clear in the thread, but again my apologies.
I thing there are two strong topics here:
- Should pipenv update all your outdated dependencies where you are trying just to install a new dependency: the ones that are not needed to update because the new package / version we are trying to install can works with the existent dependencies, and even the ones that aren't dependencies of the new package we are trying to install? Maybe this is out of scope of this ticket, but it's a really important topic to discuss.
- Do one of these parameters `--keep-outdated` `--selective-upgrade` allow us to avoid these behaviour? It's not clear what these options do, there is a lack of documentation about them, and even in the related issue (#1554) nobody is answering that.
In case it's a bug in on one of these params `--keep-outdated` `--selective-upgrade`, I still thinking that do not set whatever param solves the unnecessary update of the dependencies as default it's a really bad idea.
To compare with a similar scenario, imagine that you execute `apt-get install vim` to just install the `vim` tool in your system (and the necessary vim's dependencies or updates if apply), but imagine also that in this situation apt updates all the other dependencies of your system: python, the QT system, the Linux kernel... and so on. It's not that apt shouldn't allow us to updates other dependencies, but there is a clear command to do that: `apt-get upgrade`, while `apt-get install PACKAGE` just install / update PACKAGE and it's dependencies.",1,0,msr
1013,"@sdispater the distinction is at the heart of every disagreement we've ever had and it's incredibly subtle but I'd point you at https://caremad.io/posts/2013/07/setup-vs-requirement/ or a good article for the elixir use case: http://blog.plataformatec.com.br/2016/07/understanding-deps-and-applications-in-your-mixfile/
`pyproject.toml` isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file. `setup.cfg` exists for that purpose (the actual successor to `setup.py` ) and IMHO both of those should really be supported. A library is published and intended for consumption with abstract dependencies so that they can play nice in the sandbox with others; applications are usually large, complex beasts with sometimes hundreds of direct dependencies. So one of our main divergences is that when we design and build our tooling, we take this into account also
@mrsarm For your first question, the update behavior was intentional (and was discussed extensively at the time, /cc @ncoghlan and related to OWASP security concerns). On the second point, the behavior is currently not properly implemented which is why the issue is still opened, which led us to rewriting the backing resolver behind pipenv, which I mentioned above. It simply didn't support this behavior. `--selective-upgrade` is supposed to selectively upgrade only things that are dependencies of the new package, while `--keep-outdated` would hold back anything that satisfied the dependencies required by a new package. Slightly different, but I am fairly sure neither works correctly right now.",0,0,msr
1014,"> pyproject.toml isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file. setup.cfg exists for that purpose (the actual successor to setup.py ) and IMHO both of those should really be supported.
Well this is certainly off topic but it's an important discussion so I can't help myself.
There is actually no standard around `setup.cfg` right now other than the conventions established by distutils and setuptools. `pyproject.toml` is absolutely for library metadata as the successor to `setup.py` or the community would have placed build requirements in `setup.cfg` instead. `pyproject.toml` describes how to build a project (PEP 518), and part of building is describing metadata. I'm NOT saying that `pyproject.toml` needs a standard location for this metadata, but PEP 518 uses this file to install a build tool and from there it's very reasonable to expect that the build tool will use declarative configuration from somewhere else in the file to determine how to build the project.
Anyway, going back to pipenv vs poetry - there seems to be some idea floating around that applications don't need certain features that libraries get, like entry points, and this is just incorrect. It should be straightforward for an application to be a python package.
The only true difference between an application and a library in my experience with python and with other ecosystems is whether you're using a lockfile or not. Of course there's a third case where you really just want a `requirements.txt` or `Pipfile` and no actual code and that seems to be all that pipenv has focused on so far (`pipenv install -e .` falls into this category as pipenv is still afraid to try and support the package metadata). Unfortunately, while the design of pipenv is cleaner with this approach, it's also way less useful for most applications because PEP 518 decided to punt on how to install projects into editable mode so in order to continue using pipenv we will be stuck on setuptools quite a while longer as you cannot use `pyproject.toml` to switch away from setuptools and still use `pipenv install -e .`.",0,0,msr
1016,"> This came up on the mailing list recently -- nothing anywhere has declared a standard around `pyproject.toml`
That's correct, it is not a ""standard"", yet in that same thread recognise that by calling it `pyproject.toml` they likely asked for people to use this file for other project related settings/config.
So by the same logic you invoked here:
> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly.
`pyproject.toml` is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.",0,0,msr
1018,"> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly. Not to mention it uses the standard outlined in pep 345 for metadata, among others, and can also be used to specify build requirements.
Yes, the build system is expected to output the PKG-INFO file described in PEP 345. This is a transfer format that goes in an sdist or wheel and is generated from a setup.py/setup.cfg, it is not a replacement as such for the user-facing metadata. PEP 518's usage of `pyproject.toml` is about supporting alternatives to distutils/setuptools as a build system, no one is trying to replace the sdist/wheel formats right now. Those replacement build systems need a place to put their metadata and fortunately PEP 517 reserved the `tool.` namespace for these systems to do so. It's not an assumption - **both flit and poetry have adopted this namespace for ""library definition metadata"".**
> Try only defining a build system with no additional information about your project (i.e. no pep-345 compliant metadata) and upload it to pypi and let me know how that goes.
How constructive.
> Who is saying that applications don't require entry points? Pipenv has an entire construct to handle this.
Where is this construct? I cannot even find the word ""entry"" on any page of the pipenv documentation at https://pipenv.readthedocs.io/en/latest/ so ""an entire construct"" sounds pretty far fetched? If you mean editable installs then we have reached the point I was making above - with pipenv deciding to couple itself to `pipenv install -e .` as the only way to hook into and develop an application as a package, for the foreseeable future pipenv's support here is coupled to setuptools. I think the entire controversy boils down to this point really and people (certainly me) are frustrated that we can now define libraries that don't use setuptools but can't develop on them with pipenv. To be perfectly clear this isn't strictly pipenv's fault (PEP 518 decided to punt on editable installs), but its refusal to acknowledge the issue has been frustrating in the discourse as poetry provides an alternative that does handle this issue in a way that's compliant with the `pyproject.toml` format. Pipenv keeps saying that poetry makes bad decisions but does not actually attempt to provide a path forward.",1,1,msr
1020,"@bertjwregeer:
> pyproject.toml is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.
Great, and we are happy to accommodate sdists and wheels built using this system and until there is a standard for editable installs we will continue to pursue using pip to build sdists and wheels and handle dependency resolution that way. Please read my responses in full. The authors and maintainers of pip, of the peps in question, and myself and @uranusjr are pretty well versed on the differences between editable installs and the implications of building them under the constraints of pep 517 and 518. So far All I'm seeing is that the peps in question didn't specifically address how to build them because they leave it up to the tooling, which for some reason everyone thinks means pipenv will never be able to use anything but setuptools? I've said already this is not correct. If you are actually interested in the implementation and having a productive conversation I'm happy to have that. If you are simply here to say that we don't know what we're doing, but not interested in first learning what it is we are doing, this is your only warning. We are volunteers with limited time and I am practicing a 0 tolerance policy for toxic engagements. I do not pretend my work is perfect and I don't pretend that pipenv is perfect. I will be happy to contribute my time and effort to these kinds of discussions; in exchange I ask that they be kept respectful, that they stick to facts, and that those who participate also be willing to learn, listen, and hear me out. If you are here just to soapbox you will have to find another platform; this is an issue tracker. I will moderate it as necessary.
This discussion is **wildly off topic**. If anyone has something constructive to say about the issue at hand, please feel free to continue that discussion. If anyone has issues or questions about our build system implementations, please open a new issue. If you have issues with our documentation, we accept many pull requests around documentation and we are aware it needs work. Please defer **all** of that discussion to new issues for those topics. And please note: the same rules will still apply -- this is not a soapbox, it is an issue tracker.",1,0,msr
1022,"In the project we maintain, we can soapbox. And yes, pip will support all compliant build systems which you both yourselves seem to full well understand will produce consumable metadata, and as pipenv uses pip as the backing tool to drive its installation process, as I described, yes, pipenv will support all compliant tooling. I already said this. So yeah, please take your toxicity somewhere else. Your attitude is not welcome here. Final warning. Persistent attempts to incite conflict won’t be tolerated.",1,1,msr
1024,"How I read PEP-8, it doesn't outlaw bare excepts, but merely recommends to catch more specific exceptions when possible. But what if there is no particular exception you want to catch, but when you just want to do some cleanup before propagating any exception?
```python
try:
self.connection.send(...)
except:
# We only close the connection on failure, otherwise we keep reusing it.
self.connection.close()
raise
```
Using `try...finally` isn't an option here, since we want to reuse the resource on success, and only clean up on failure. I could just explicitly catch `BaseException` instead, but there is no indication in PEP-8 that this is preferable (otherwise why would bare except be supported in the first place).
So how about suppressing E722 if there is a `raise` statement in the `except` block?",0,0,msr
1027,"To be clear, pycodestyle doesn't do look-ahead's so we cannot silence this *if* there is a `raise` in the following block. That's just not how pycodestyle has ever worked and it would require a significant rewrite to enable that kind of behaviour.
----
Quoting the PEP
```
When catching exceptions, mention specific exceptions whenever possible instead of using a bare except: clause.
For example, use:
try:
import platform_specific_module
except ImportError:
platform_specific_module = None
A bare except: clause will catch SystemExit and KeyboardInterrupt exceptions, making it harder to interrupt a program with Control-C, and can disguise other problems. If you want to catch all exceptions that signal program errors, use except Exception: (bare except is equivalent to except BaseException:).
A good rule of thumb is to limit use of bare 'except' clauses to two cases:
1. If the exception handler will be printing out or logging the traceback; at least the user will be aware that an error has occurred.
2. If the code needs to do some cleanup work, but then lets the exception propagate upwards with raise. try...finally can be a better way to handle this case.
```
As a *general* rule, this new check is good. There are specific cases where people will need to do general clean-up work, as described above. Since we cannot check for people re-raising the exception in the except block, it truly is up to the user to determine whether:
1. This check is useful to them at all or whether they feel it should be ignored globally
2. Their particular use of a bare `except` is necessary and it should be ignored in that particular case (e.g., with `# noqa` or `# noqa: E722`).
I would hazard a guess that 90% of pycodestyle's users will find this check worthwhile, useful, and helpful. We can not ever satisfy 100% of our users so I am happy to settle for 90%.",0,1,msr
1028,"I have two (real world) scenarios:
1. Legacy code, initially written by novices, with inappropriate use of bare except all over the place. E722 is helpful there (somewhat since this usually isn't the only issue in such legacy code, and you end up ignoring most errors there anyway).
2. Code written by me (and similarly experienced Python developers), where bare except is used occasionally, but never without re-raising the exception. For reference, E722 is reported inappropriately three times in [this project](https://github.com/snoack/mypass) of mine, where the exception is re-raised.
If the design of pycodestyle doesn't allow considering the following block, in order to avoid inappropriate errors, my understanding is, that due to the potential of false positives this check should not be enabled by default, or such a check should rather be implemented in pyflakes which considers the AST and therefore is able to ignore bare excepts that re-raise the exception.
I'm not going to clutter my code with `#noqa` comments. IMO the purpose of a linter is to help you writing cleaner code, not to require additional boilerplate for legit practices. This isn't any better than using `except BaseException`.",0,0,msr
1033,"![image](https://user-images.githubusercontent.com/13496612/36892494-2c2e0a5c-1e05-11e8-94c0-8daecf633955.png)
autopep8 said use `BaseException`.",0,0,msr
1035,"@hoylemd I appreciate the support.
> Those represent very exceptional cases - cases which shouldn't occur in normal (e.g. production) operation unless something has gone very wrong or the user is explicitly requesting an immediate shutdown.
This sounds like you're assuming all development is on continuously running applications on a remote server. Things like pycodestyle and flake8 are ""production"" applications that should handle `KeyboardInterrupt`. That said, both projects handle it explicitly. I think I understand your point, but it feels like it's imposing a false dichotomy around what is ""production"".
> Why do you want to clean up those temporary files in every possible exception case?
I can imagine a few cases where @lordmauve would want to (and should!) clean up the temporary files. Perhaps those temporary files contain some sensitive information and cleaning them up is the secure thing to do. In reality, neither of us know the details and I think we should be assuming that they know their constraints better than us. Of course having some non-sensitive temporary files lying around can be useful for debugging, but there are certainly valid cases where those shouldn't be left around no matter what exception happens.",1,1,msr
1041,"> Or evidence that this check needs to be moved to Pyflakes, which does consider the AST, in order to eliminate this false positive.
PyFlakes wouldn't accept this for the very reason that it's so controversial. They only accept obvious problems, e.g., unused imports. If this isn't satisfactory here, a different stylistic option for having it would be via a Flake8 AST plugin. Luckily, `flake8-bugbear` already provides this check using the AST but a quick check of the source indicates that it's as strict as this check is. I'd suggest either collaborating with [bugbear](/pycqa/flake8-bugbear) or creating a new plugin that meets the specific needs.
As it stands, this discussion seems to be getting heated. I'm going to lock the thread because it's devolved from constructive conversation to far less productive conversation.",0,0,msr
1043,I can't reproduce this locally I'm afraid. Try `brew reinstall mas`.,0,0,msr
1047,"Decided not to bother because A) the OSX installers aren't typically something you'd want to keep around B) am unable to find anything else that recreates this issue. Either way, thanks for the response @MikeMcQuaid!",0,0,msr
1050,@paulp I cannot reproduce this locally with the installer installed. We will accept a pull request to resolve this (likely just not including anything in a dump missing an ID).,1,0,msr
1051,"Re #321, it's a different bug - as I said, ""the bug in bundle being masked by the bug in mas"" - regardless of your agreement, it's childish and hostile to immediately close and lock the ticket. Sorry to have bothered you with my effort.",0,1,msr
1055,"##### ISSUE TYPE
- Feature Idea
##### COMPONENT NAME
import_playbook
##### ANSIBLE VERSION
<!--- Paste verbatim output from ""ansible --version"" between quotes below -->
```
2.4.0.0
```
##### CONFIGURATION
<!---
If using Ansible 2.4 or above, paste the results of ""ansible-config dump --only-changed""
Otherwise, mention any settings you have changed/added/removed in ansible.cfg
(or using the ANSIBLE_* environment variables).
-->
n/a
##### OS / ENVIRONMENT
CentOS 6
##### SUMMARY
I need to conditionally import a playbook, which isn't possible. The keyword I'm missing is ""include_playbook"" which would allow a ""when"" to apply to it. Why?
I have a playbook that performs some maintenance, OS, Kernel, package, and firmware upgrades. For Major upgrades, we track progress in a ticketing system. So, during those runs, I'd like to conditionally import a playbook that updates the ticket information. In order for that to work, I need to pass the user's password to the ticketing system, so there's a `vars_prompt` in the ticket update playbook. If the user specifies which ticket they're working to the maintenance playbook, I'd like the ticket update playbook to be called after the maintenance is performed.
If there's another way for this to work, I'm open to alternate ideas, but I think conditionally playbook imports would be generically useful. I'm not understanding why tasks could be included dynamically, but a playbook wouldn't be.
##### STEPS TO REPRODUCE
<!---
For bugs, show exactly how to reproduce the problem, using a minimal test-case.
For new features, show how the feature would be used.
-->
<!--- Paste example playbooks or commands between quotes below -->
```yaml
# Maintenance Play Runs first, then conditionally import a second playbook
- import_playbook: update-ticket.yaml
when: ticket_id is defined
```
<!--- You can also paste gist.github.com links for larger files -->
##### EXPECTED RESULTS
<!--- What did you expect to happen when running the steps above? -->
I expect `update-ticket.yaml` to only import if the `ticket_id` is defined.
##### ACTUAL RESULTS
<!--- What actually happened? If possible run with extra verbosity (-vvvv) -->
no parse error, and playbook `update-ticket.yaml` is imported 100% of the time.",0,0,msr
1057,"We have similar need. We have a main playbook that prepares the system, but we allow the user to provide some extra steps by creating a playbook files in a well known directories. We do not know what files will be present there in advance. For this we would really like to have include_playbook that supports with_fileglob (or with_items).",0,0,msr
1063,"Include conditionals are very useful for creating branches in our playbooks. Will it be ensured that import_playbook will support conditionals in the next release citing this issue?
If not, you are losing a lot of power and will end up creating a lot of hacks. Not to mention breaking a ton of include playbook conditionals in end user plays.",1,0,msr
1064,"+1 on implementing a ""when"" conditional.",0,0,msr
1068,"+1 as well, would love to see this feature",0,0,msr
1071,"If someone is interested how to use play-level variables for conditional playbook-import:
1. Set up that variable as a fact in the play
2. Use `when` with `import_playbook` to check this variable (with full path, `hostvars.hostname.a_variable`)
If someone is interested, I managed to make import_playbook be conditional on `--limit` in the command line:
https://medium.com/opsops/import-playbook-with-play-level-condition-775122fe78ff
An example:
```
- hosts: all,localhost
gather_facts: no
run_once: True
tasks:
- set_fact:
full_run: '{{ play_hosts == groups.all }}'
delegate_to: localhost
delegate_facts: yes
- import_playbook: test.yaml
when: hostvars.localhost.full_run
```",0,0,msr
1079,"@Kriechi the engine never supported that, why `include:` was very misleading and we had to separate it into the different include_X/import_X options and make each behaviour explicit. So include_X is dynamic aka runtime, while import_X is 'static' aka 'compile time'.",0,0,msr
1081,"@Kriechi ... please read the subject of this ticket, that is EXACTLY what we are tracking here",0,1,msr
1085,"I've overcome this in my own way as follows. First, my typical playbook directory structure:
```bash
.
├── .ansible-lint
├── .gitignore
├── .yamllint
├── ansible.cfg
├── callback_plugins
│   ├── junit.py
│   ├── log_plays.py
│   ├── profile_roles.py
│   ├── profile_tasks.py
│   ├── timer.py
├── check_ansible_lint.sh
├── check_syntax.sh
├── check_yaml_lint.sh
├── create.yml
├── destroy.yml
├── Jenkinsfile
├── localhost_inventory.yml
├── playbooks
│   ├── ap_linux_instance
│   └── requirements.yml
├── prerequisites.yml
├── README.md
├── reports
├── requirements.txt
├── roles
│   ├── ar_linux_ansible_venv
│   ├── ar_linux_cname
│   ├── config_encoder_filters
│   └── requirements.yml
└── VERSION.md
```
My .gitignore ignores most sane OS/language/IDE things, but also ignores everything in the `roles/` and `playbooks/` folders except for the `requirements.yml` files in each folder. The requirements.yml file within the playbooks folder is similar to your Galaxy-style requirements.yml, but rather than calling out dependent playbooks by Galaxy owner.name, I specify the full Git source (Galaxy supports this of course). The requirements.yml within the roles/ folder is just your traditional Galaxy-style requirements.
I have this `prerequisites.yml` playbook, that looks like this:
```yaml
---
- name: PLAY | Install other required playbooks
hosts: localhost
connection: local
tasks:
- name: INCLUDE_VARS | include variables to discover other needed playbooks
include_vars:
dir: playbooks/
files_matching: requirements.yml
depth: 1
- name: GIT | Clone playbooks
git:
repo: ""{{ item.src }}""
dest: ""playbooks/{{ item.src.split('/')[-1] }}""
version: ""{{ item.version }}""
loop: ""{{ required_playbooks }}""
- name: SHELL | Install included playbooks roles
shell: ansible-galaxy install -r roles/requirements.yml -p roles/ --force
args:
chdir: ""playbooks/{{ item.src.split('/')[-1] }}""
loop: ""{{ required_playbooks }}""
when: item.galaxy
changed_when: false
tags: [ skip_ansible_lint ]
```
And, assuming that my ""big bang"" create.yml depends on a playbook and its roles from another playbook project, I import it like this:
```yaml
# ~~~~~~~~~~
# Ensure that all of the host VMs in the inventory are up and running
# either on-prem or in Azure as specified in the inventory
#
- name: Ensure inventory hosts are present
import_playbook: ""playbooks/ap_linux_instance/create.yml""
tags: [ base_server, hosts ]
```
And so, the work-flow to run my ""big bang"" (e.g. create.yml) is a 3-liner:
```bash
ansible-playbook prerequisites.yml
ansible-galaxy install -r roles/requirements -p roles/
ansible-playbook create.yml -i <path_to_inventory>
```
You could, of course, wrap the above 3-liner in a `create.sh` shell script for convenience. This method has served me well for some fairly complex playbook projects that depend on other playbook projects. This forces us to keep roles and playbooks fairly self-contained and re-usable and factor variables out into their own inventory projects. When performed with discipline, it makes it really easy to migrate unaltered roles/playbooks to other environments, then just update inventory variables that are unique to that environment.
**This doesn't solve the conditional import problem**, mind you, but does help me use the `import_playbook` statement for something that may not exist just yet. It kind of gets around a conditional in my very specific use-case. I make use of tagging on the `import_playbook` to leverage the command-line `--tags` and `--skip-tags` features if I need scalpel-like precision at run-time. But if you had to make an import decision based on some other conditional logic (e.g., OS family), well, we still need that as a language feature I think. For now, I just handle those cases with sub-playbooks and chain them together ensuring I target the appropriate hosts/groups that should or should not be targeted based on how I've setup my inventory (yes, it can get messy).
This is all pretty wild and requires a high degree if what I commonly refer to as ""4th dimensional thinking"", especially when you consider branches/versions of things and running them from CI/CD platforms like Jenkins or even AWX. But I still find Ansible fascinating and use it daily.
HTH,
Ben",0,0,msr
1089,"I'd just like to mention that there is a bit in the documentation that insinuates that the feature requested already exists. See:
https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#applying-when-to-roles-imports-and-includes
Specifically, the note mentions that Ansible allows `when` to work with playbook includes since version 2.0. Perhaps this documentation should also be amended.",1,0,msr
1104,"For windows we even ship debug builds on every nightly.
I guess adding DEBUG=0 is feasible but I don't think it should be the default behavior",0,0,msr
1109,"I'm merging this, it's the right thing to do. If you have a problem with it, take it up with @twinaphex privately, NOT HERE.",1,0,msr
1110,"## If you're having trouble installing Nokogiri ...
**Have you tried following [the installation tutorial][tutorial]?**
yes
**What is the output of `gem install`?**
```
PS C:\Users\replaced> gem install nokogiri -v 1.8.1
ERROR: Error installing nokogiri:
The last version of nokogiri (= 1.8.1) to support your Ruby & RubyGems was 1.8.1. Try installing it with `gem install nokogiri -v 1.8.1`
nokogiri requires Ruby version < 2.5, >= 2.2. The current ruby version is 2.5.0.
```
**What are the contents of the `mkmf.log` file?**
Didnt get so far
**What operating system are you using?**
Windows 10 x64",1,0,msr
1111,Same OS and same problem...,0,0,msr
1116,"The fix is already merged, although a new release is pending. See: https://github.com/sparklemotion/nokogiri/pull/1704 & https://github.com/sparklemotion/nokogiri/issues/1706
Until then, it's better to stick with Ruby 2.4.2 on Windows.
The Nokogiri team removed the GEMSPEC from sources, to disallow people building from master branch, since it is unstable and probably would contain bugs. You can read more about it here:
https://github.com/sparklemotion/nokogiri/blob/master/Y_U_NO_GEMSPEC.md",0,0,msr
1127,"We need a written code of conduct. It should reference the relevant IETF code, and include anything we feel that we need here.
In particular, we need some thing around roles:
* Definition of what it means to be a project Member, and what behavioral standards Members are held to
* Definition of what it means to be a project Collaborator, and what behavioral standards Collaborators are held to
* What behavioral standards are expected of all participants
And some things around discussions:
* What sort of requirements exist for staying on-topic in an issue
* How to bring an end to discussions where it is clear that there will not be consensus (the IETF sets the goal of ""rough consensus"", acknowledging that sometimes you can't get everyone's buy-in)
I'm going to start this issue off locked until we figure out the best / least likely to explode way to take feedback on it.",1,1,msr
1134,The bug is that the interface for a text field and a hidden field are different. IMO They should be the same.,1,0,msr
1136,"```
TextField::create(<name>, <value>); // OK
HiddenField::create(<name>, <value>); // NOT OK. ````",0,0,msr
1137,"@worikgh - I'm sorry, you're not being clear.
both of those work, don't result in error, output expected HTML/fields.
From what I can tell you actually are asking for these APIs to be different, not the same... You'd like `HiddenField`'s second constructor argument to be the value and not the label, is that right?",0,0,msr
1138,"They produce different output.
See above where I created an example and cut and pasted resulting HTML.",1,0,msr
1143,"# Test 1
```php
TextField::create('Test Name', 'Test Value')
```
Creates:
```html
<input type=""text"" name=""Test Name"" class=""text"" id=""Form_ItemEditForm_Test_Name"">
```
# Test 2
```php
TextField::create('Test Name', 'Test Label', 'Test Value'); // Shift 2nd param to 3rd
```
Creates:
```html
<input type=""text"" name=""Test Name"" value=""Test Value"" class=""text"" id=""Form_ItemEditForm_Test_Name"">
```",0,0,msr
1148,"> we have fixed the Marketplace instructions since a while
I've only recently noticed the change (from `ext-name` to `owner.ext-name`) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past?
> Since we now have URL handlers for Mac and Windows, those instructions became almost irrelevant
I would have replaced those `ext install (...)` instructions with URL handlers, but I've tried them once and after that, for a few days/weeks, a ""Do you want to install (...)"" message kept popping up every time I opened a new window. By the time that was fixed I guess I had forgotten about them.
> what if we showed in quick open a list of extensions which match that name? Why providing 2 different interfaces for discovering extensions?
Shouldn't searching ""just work""?",0,0,msr
1152,"@gaurav42 Well, I would still consider whatever algorithm you guys are using broken because if I search for ""open in node modules"" I get these search results (some samples ~~randomly picked):
| Rank | Ext. Name | Ext. Description | My comments |
|--------|-------------|-----------------|----------------|
| `#6` | [Node Exec](https://marketplace.visualstudio.com/items?itemName=miramac.vscode-exec-node) | Execute the current file or your selected code with node.js. | 80k downloads, but no mention of ""open"" or ""module(s)"" anywhere.
| `#10` | [CSS Modules](https://marketplace.visualstudio.com/items?itemName=clinyong.vscode-css-modules) | Visual Studio Code extension for CSS Modules | 22k downloads, but no mention of ""open"" or ""node"" anywhere.
| `#17` | [Node TDD](https://marketplace.visualstudio.com/items?itemName=prashaantt.node-tdd) | Ease test-driven development in Node and JavaScript | 9k downloads, no mention of ""module(s)"", but at least it has a few ""opened"" in its readme.
| `#64` | [Open in Vim](https://marketplace.visualstudio.com/items?itemName=jonsmithers.open-in-vim) | Opens current file in vim | 2k downloads, this is not even mentioning ""node"" anywhere, let alone ""module(s)"". But at least it has ""open"" and ""in"" in it's title.
| `#65` | [Open in node_modules](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-open-in-node-modules) | Open the current selection or arbitrary string in node_modules. | 2 downloads, it matches all the provided keywords, **in its title**.
If I had to guess what's wrong with your approach I'd say:
- Lack of stopwords, kind of meaningless keywords like ""vscode"" or ""in"" are given too much weight.
- Poor query parsing, if keywords are joined in some sort or another the whole thing falls apart (vscode-foo-bar, foo_bar, GitLens etc.)
- Maybe you're giving too much weight to downloads and ratings, the first thing to sort for is relevancy.
> One way to improve your extension rank is to break word node_modules into separate words node modules. This will increase the string matching score and push your extension up in the result.
I'm not going to rename the extension to something wrong (`node_modules` is a folder, I'm not talking about `node modules`, as in ""NPM packages"", here) just to work around this.",0,1,msr
1153,"I agree, the search ranking seems wierd.
Some examples (when searching for ""python""):
Py Files generator is above autoDocString, despite having about 50k less installs and 0 reviews
Same thing with ladieratheme - it is above autoDocString despite having about 49k less installs and 3 less reviews
Trustcode odo snippets and kvlang is above docker linter, despite having 47k less installs
Python paste and indent has 3 stars but apparently it's 2k extra downloads trumps Python (pydev)'s 5 star rating. Seems like download count is weighted more heavily than rating in the ranking (or maybe python paste and indent has better keywords)
Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython (with 742 _thousand_ downloads and 3.5 stars) and several other extensions with far more downloads / good ratings. So you could have a very popular or well-rated extension but if you don't have the right keywords you will still be ranked down.
Wait..,. but looking at the Python Coding Conventions package.json, it doesn't even have any keywords!
https://github.com/harip/python-coding-conventions/blob/master/package.json 🤔🤔🤔
> While searching an extension, we also take into account the community inputs like number of downloads, You mean the download count that is _also_ the update count? This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased.
The update count should not effect the search ranking.
> number of ratings and average rating That's good, but what algorithim are you using to calculate the weighted rating? Not sure if that is open source but hopefully it isn't something like RatingA - RatingB or RatingA/RatingB
http://www.evanmiller.org/how-not-to-sort-by-average-rating.html",0,1,msr
1158,"@fabiospampinato Yes, unfortunately we do not index readme.md.. sorry about that. We use SQL FTS in our backend today and that's kind of the limiting factor. There's only so much custom logic we can run over the results returned by FTS to make it 'more' relevant - and more importantly it doesn't scale in the long run.
We are exploring moving our search platform to Azure Search or Bing which are techologies that are being actively developed and should provide us with more features and capabilities.
I'm afraid we are not going to invest more in trying to optimize search with FTS.",0,0,msr
1163,"~~@kesane-msft~~
~~Which~~
Oops, misclick.",0,0,msr
1164,"> our search algorithm gives more weightage to matches in the extension's display name. If you add the ""vue"" term in your display name your extension would start showing as the top result.
@kesane-msft Which I would refuse. What do I call it? `Vetur Vue`?
Another example: Searching for ""Golang""
![image](https://user-images.githubusercontent.com/4033249/50705744-a2e5c880-1010-11e9-941e-5a1d68cc6848.png)
What should [Go extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.Go) do? Renaming it to Golang? After which the query `Go` wouldn't return it as top result?
Maybe it's better to start a community curation of good extensions for each language/framework. http://howistart.org/ is a good example. At least it wouldn't be impossible for new users to find which extension to use, because the search results are not ranked helpfully.",1,0,msr
1165,"Maybe there could be a keyword (maybe the first one?), that is ranked as if it was part of the title. So that way GitLens could have _git_, Vetur could have _vue_, Go could have _golang_, etc.",0,0,msr
1171,"I can confirm this is still a bad issue. ### This isn't a 'bad' search algorithm, something is broken here.
When searching for ""line endings"" (picture below)<br>
The following 4 circles are all similar extensions. The red circles have MORE downloads, installs, and better ratings, and I checked the `package.json` all of them have ""line endings"" in the title and as keywords.<br>
How can 'Line Note' (4th result) which
- doesn't have 'endings' in the title
- has 203 downloads
- no reviews
- doesn't even have 'endings' in the package.json or readme Beat out both 'line-endings' and 'code-eol (2019) Line Endings'
- both contain the full search term in the title
- they have 4K and 6K downloads respectively
- they have 4 stars and 5 stars respectively
- and have 'line endings', 'line', 'endings' in the keywords of the package.json and readme
And it's not like 'Line Note' is some anomaly, there are 25 just as bad results that come before the relevant one. <img width=""1221"" alt=""Screen Shot 2019-03-31 at 5 02 32 AM"" src=""https://user-images.githubusercontent.com/17692058/55287759-c059d280-5372-11e9-8c99-21549a98d33f.png"">",0,0,msr
1189,"This simplifies the logic a bit and gets rid of the main loop that looks
for where the key is located.
Signed-off-by: Matthew Thode <mthode@mthode.org>
Related: https://github.com/zfsonlinux/zfs/pull/7189
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [x] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux code style requirements.
- [x] I have updated the documentation accordingly.
- [x] I have read the **CONTRIBUTING** document.
- [ ] I have added tests to cover my changes.
- [x] All new and existing tests passed.
- [x] All commit messages are properly formatted and contain `Signed-off-by`.
- [ ] Change has been approved by a ZFS on Linux member.",0,0,msr
1191,"I'm still not an expert in bash or dracut, but the approach looks correct to me.",0,0,msr
1193,"# [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=h1) Report
> Merging [#7194](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=desc) into [master](https://codecov.io/gh/zfsonlinux/zfs/commit/e921f6508b212c61fcedd0eeb2f9cf9da1abc4d1?src=pr&el=desc) will **decrease** coverage by `0.23%`.
> The diff coverage is `n/a`.
[![Impacted file tree graph](https://codecov.io/gh/zfsonlinux/zfs/pull/7194/graphs/tree.svg?width=650&height=150&src=pr&token=NGfxvvG2io)](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=tree)
```diff
@@ Coverage Diff @@
## master #7194 +/- ##
==========================================
- Coverage 76.39% 76.16% -0.24% ==========================================
Files 327 327 Lines 103768 103768 ==========================================
- Hits 79278 79037 -241 - Misses 24490 24731 +241
```
| Flag | Coverage Δ | |
|---|---|---|
| #kernel | `76.16% <ø> (+0.22%)` | :arrow_up: |
| #user | `65.43% <ø> (-0.46%)` | :arrow_down: |
------
[Continue to review full report at Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=footer). Last update [e921f65...e5619df](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",1,0,msr
1198,"@prometheanfire unfortunately, Github won't let me reopening this issue either. But please open a new PR with your updated version. I'd like to think we can continue discussing the relative merrits of the two proposed solutions and then move forward with one of them.",0,0,msr
1202,This is the first time I've had any problem at least.,0,0,msr
1203,"no better place becaus you ban me from IRC. respond to criticism is lies, everyone who do not think like yourself you consider stupid",1,1,msr
1215,花Q,1,0,msr
1220,"**Do you want to request a *feature* or report a *bug*?**
Bug
**What is the current behavior?**
If I run .save on a mongoose schema without being connected to the database, the callback is never called. I don't get any errors either. The process seems to be running forever. Also, if I'd run an empty .save(), `message.save()` before the actual save, I get the 200 response back. Not sure if that's the expected behaviour or not.
**If the current behavior is a bug, please provide the steps to reproduce.**
Controller:
```
var message = new Message({
content: ""Much content""
});
});
// message.save(); <--- run this to get the 200 response
message.save(function (err) {
if (err){
console.log('Something went wrong: ' + err);
return res.status(500).send(""Something went wrong: "" + err);
}
return res.status(200).send(""Saved!"");
});
```
Model:
```
var mongoose = require('mongoose');
var Schema = mongoose.Schema;
// Schema.set('validateBeforeSave', false); // to validate data manually
//mongoose.connect('mongodb://localhost:27017/userFeedback'); <--- don't connect to database
mongoose.set('debug', true);
var MessageSchema = new Schema({
content: String
});
module.exports = mongoose.model('Message', MessageSchema);
```
**What is the expected behavior?**
Some sort of error message telling me that I'm not connected to the database and that the data will not be saved.
_I am using version 5.0.9 which is the latest version at this point._",0,0,msr
1229,"No, the problem also happens with the `{% highlight %}` syntax.",0,0,msr
1233,"> What do you want me to do? I don't even know what Rouge is!
I’ve provided a link to the repository so that you can open an issue there and explain the problem you are having.
I do not know what “Twig” is, so it would not make sense for me to be the one to explain what needs changing in Rogue.
There is nothing in Jekyll’s code that can be changed to fix this issue; the fix will have to come from Rogue.
Here is a link to Rogue’s Twig lexar: https://github.com/jneen/rouge/blob/master/lib/rouge/lexers/twig.rb",0,0,msr
1234,"> There is nothing in Jekyll’s code that can be changed to fix this issue; the fix will have to come from Rogue.
That's not my point. You are the one using Rouge to implement a feature that you advertise explitely on your docs! That's your responsibility to take care of things that don't work as expected in the dependencies of your project. As a consumer of your product, I expect it to work as advertised:
https://jekyllrb.com/docs/templates/#code-snippet-highlighting
You are advertising syntax highlighting, you are supposed to deliver! And if you don't, you are supposed to take care of whatever is needed to have your product work as expected.",0,1,msr
1237,"@ericmorand Please take a step back and consider that this is an entirely volunteer-run project. We're not contractually obligated to work on every bug and answer every question, seeing as we simply don't have enough resources. So our apologies if some things take too long, or don't end up happening, but it's wrong to blame the maintainers for this.",0,0,msr
1241,"This issue has been automatically marked as stale because it has not been commented on for at least two months.
The resources of the Jekyll team are limited, and so we are asking for your help.
If this is a **bug** and you can still reproduce this error on the <code>3.3-stable</code> or <code>master</code> branch, please reply with all of the information you have about it in order to keep the issue open.
If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced [hooks](http://jekyllrb.com/docs/plugins/#hooks) which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.
This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions.",1,0,msr
1249,@pmiossec whats the problem. is there a fix or not. I am using this globally in my company and personal.,0,1,msr
1253,"This patch accommodates some breaking changes introduced with MySQL 8.
Closes #1959 In a nutshell, the `caching_sha2_password` plugin (which is used by default since MySQL 8.0.4) hashes the password using SHA-256 and, after a first successful authentication attempt, saves it in a cache. That first attempt needs to be done under one of two conditions. The client either uses SSL and sends the password as clear text or it encrypts the password using the RSA public key generated by the server. On any subsequent attempt, until the password is somehow removed from the cache or the server shuts down, these rules no longer apply.
The handshake process remains unchanged when connecting to any server with version lower or equal to `8.0.3`. Whereas for `8.0.4` or above, the process is now the following:
- the client sends a `ClientAuthenticationPacket` with a scramble computed using a SHA-256 hash
- if the password is not cached, the server sends back a `PerformFullAuthenticationPacket`
- if the client uses SSL, the password is sent to the server (as clear text) via a `ClearTextPasswordPacket` to which the server replies with a `OkPacket`
- otherwise it uses the server authentication public key compute the scramble, sending a `AuthSwitchResponsePacket` to which the server replies with a `OkPacket`
- if the client does not know the server public key (is not provided by the user), it requests it from the server, which sends it back using a `AuthMoreDataPacket`
- after a first successful authentication attempt, and until the password is cached, the server will reply to the initial `ClientAuthenticationPacket` with a `FastAuthSuccessPacket` (which basically just signals that an `OkPacket` will follow)
If the account is created using the `mysql_native_password` authentication plugin, the client will just fall back to the ""traditional"" process during the handshake, keeping compatibility, by default for any previously supported server version.
MySQL 8.0.2 disables the `local_infile` server variable by default, which breaks a couple of integration tests. The tests were updated to enable the feature by themselves (something that does not have any effect on older server versions and allows the tests to pass with newer versions).
Additionally, one of the integration tests was updated to avoid failing after the first run (using any server version) since it tried to create a table that already existed from the previous runs.",1,0,msr
1266,"@dougwilson I've kept the option for providing a custom key padding, to allow users to exceptionally connect to a MySQL `8.0.4 RC` server. In any case, the default value is always `RSA_PKCS1_OAEP_PADDING` which means it will work, by omission, with the latest `8.0.11 GA` version.
I couldn't find a sane way to add an additional Travis CI test matrix for users with non-empty passwords though. I was constantly bumping into an authentication error (I believe it might be related to the `mysql` docker setup and environment variables mixup), but maybe you can give it a try.
Let me know if there is something else you would like to see in this PR. Otherwise, feel free to merge it! :wink:",1,0,msr
1270,"In addition to what @ruiquelhas said, I also had to:
```
mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)
```
And then restart the server.
If you are using multiple MySQL users, ensure that you update all of them that are connecting from remote (ie, not localhost). In my case I also had to use `'%'` instead of `'localhost'`.",1,0,msr
1282,"Alright, perfect. Let me know if there is something I can help with.",0,0,msr
1283,I think you should keep track of this issue :) still on 8.0.,0,0,msr
1285,am trying to make a script to search by user by entering in there first name it should display the detail of that user this interacts with the file email.xml please help me !!! i can get it to read the xml doc and display all users i just need someone to give some exmaple code or show me how the hell i am supposed to do this thanks :),1,0,msr
1292,"> Usually, when waiting for a UDP reply the src port isn't taken into consideration. That's not how 99% of UDP protocols work. And definitely not how RADIUS works. This is all documented in RFC 2865.
> The radsec RFC does not mention that the proxy should be rewriting/translating attributes and recalculating the message authenticator.
That's how RADIUS works. This is also clear in RFC 2865. Nothing was changed in later RFCs.
Your proxy might work for one RADIUS client. It definitely won't work for multiple RADIUS clients. Or, it will, but they all have to use the same shared secret.
RFC 2865 explains that you find the shared secret by looking up the source IP of the packet. This means that all downstream servers will look up *your* proxy IP to get *one* shared secret. If the originating UDP clients use *multiple* shared secrets, it won't work.
And no, you can't change shared secrets per packet. And no, you can't change shared secrets per destination.
If you want to learn other UDP to TCP proxy issues, read RFC 6613. It explains all of these issues in excruciating detail.
TBH, you can't just write a ""UDP to TCP"" translator for RADIUS and expect it to work. You MUST implement the RADIUS protocol correctly. This involves reading, and following, the RFCs.
Any short-cut you take means that you have a toy implementation. It might work for simple tests, but it will fall over and die in any real-world environment.",1,1,msr
1296,"With my aging eyes that orange color is increasingly hard to read against a black background, and there is no way to change it. Yes I can copy and paste it into another editor, but that shouldn't be necessary just to see a simple message.
This is bad for accessibility and hard on your users. Please change to something more readable",0,1,msr
1316,"> If we must have red/orange, why not put a ""ERROR:"" prefix to the message and make that red/orange, and then let the actual message be one of the two most readable colors we have (i.e. white).
I do not think this is a good idea. The text that is currently printed in a distinct color, is the text that was printed on the ""stderr"" output of subprocesses. Typically, this contains errors, but this is not necessarily so. For the compiler, it is also used for warnings. Avrdude even prints all of its verbose output to stderr. Prefixing each line with ""ERROR"" or something similar would probably only confuse users. More correct would be to prefix each line with ""stdout"" or ""stderr"", but that would still confuse novice users who have no clue what these terms mean.
The current approach of using a different color seems to bypass this problem by not explicitly stating what the colored text means, it just draws more attention to it.
As for using a different color to improve readability: That seems like a viable option to me. Of course, using a red or red-ish color for errors (and some other stuff, as pointed out above) seems common, which is probably why it is done now. Using e.g. green would also draw attention to the text, but I *think* users would not expect green text to contain error messages, so it might be better to consider other colors. @tofrnr also suggested a yellow color, which could work IMHO.
As for making things bold: I'm not sure if that would stand out sufficiently, but that might just need an experiment perhaps?",0,0,msr
1321,"# Using Zip files for themes
## Hexcode for colors
http://www.javascripter.net/faq/rgbtohex.htm
https://www.rapidtables.com/web/color/green-color.html
## File locations:
* on mac:
Arduino.app/Contents/Resources/Java/lib/theme/theme.txt * on Win7: C:\Program Files\Arduino\lib\theme\theme.txt
* on Linux: .../arduino-1.6.3/lib/theme/theme.txt
* on Windows 10: when installed as an App, C:\Program Files\WindowsApps\ArduinoLLC.ArduinoIDE_1.8.10.0_x86__mdqgnx93n4wtt\lib\theme\theme.txt, however it does not seem possible to edit it - it can be opened in notepad++ but a save fails.
## Procedure Using zip application create a zip file containing the original theme.txt found in the location of your operating system.
call it theme.txt.original.zip
Now modify the theme.txt file for colors, font sizes, etc
using zip application create a zip file containing the edited theme.txt found in the location of your operating system.
call it theme.txt.custom.zip
In the Arduino IDE goto the menu Arduino-->preferences under the option theme select theme.txt.custom.zip from the drop down list and restart Arduino IDE and the custom theme will take effect.
Copy the theme.txt.custom.zip file to a safe location so it will be available when there is an upgrade or reinstallation.
## Some suggested changes
changed console.font to 18
changed console.error.color to #7CFC00 - lawngreen
# GUI - CONSOLE
console.font = Monospaced,plain,11
console.font.macosx = Monaco,plain,18
console.color = #000000
console.output.color = #eeeeee
# original orange #E34C00
# lime green #BFFF00 # lawngreen	#7CFC00
console.error.color = #7CFC00
changed editor.comment2 to #434F54,bold - dark grey and bold
# TEXT - COMMENTS
editor.comment1.style = #434F54,bold
editor.comment2.style = #434F54,bold
Increased linestatus.font to 14
increased linestatus.height to 30
# LINE STATUS - editor line number status bar at the bottom of the screen
linestatus.font	= SansSerif,plain,14
linestatus.height = 30
you could also copy the attached file to the theme directory
[theme.txt_visually_improved pacav.zip](https://github.com/arduino/Arduino/files/2449671/theme.txt_visually_improved.pacav.zip)",0,0,msr
1322,"@pacav69 in Arduino IDE 1.8.6 and newer, themes can be placed in the `theme` subfolder of the sketchbook folder. You can find the location of the sketchbook folder at **File > Preferences > Sketchbook location**. This way, custom themes will persist through updates to a new IDE version. This should also make it possible to use custom themes with the Windows App Store version of the Arduino IDE:
https://github.com/arduino/Arduino/pull/7115
You can put multiple themes stored in .zip files in that folder and select which one you want from the **File > Preferences > Theme** menu. Note that the theme files must be directly under the .zip file, not in a subfolder of the .zip file:
https://github.com/arduino/Arduino/pull/7124",0,0,msr
1329,"@per1234 And how many more years must we wait for accessibility to be a
consideration in 2.0? My problem is that this has been sat on long enough for you to say lets sit on it longer. Does 2.0 have a release date? How long will 1.x be supported after 2.0 is out? Why wait any longer for the simple option of a high contrast/colorless option for people with visual disabilities?
I do not say this lightly: it genuinely feels like you do not care, not in an active hostile way, but in the soft neglectful way that someone who doesn't actually understand the problem and thinks it is fine asking people underserved to wait even longer",1,1,msr
1339,"Both are bound as `string` there - can you try passing in an integer and running the SQL statement manually with either `string` or `int`, and see if there is a difference?",1,0,msr
1340,"native sql..
![grafik](https://user-images.githubusercontent.com/38318899/38754033-914eaabe-3f60-11e8-90ea-475087e3c644.png)
and the result of my script..
![grafik](https://user-images.githubusercontent.com/38318899/38754086-c5245d48-3f60-11e8-87b9-fd36462da3c9.png)
my script..
```
<?php
include ""Bootstrap.php"";
require_once 'Entities/countries.php';
error_reporting(E_ALL);
$stack = new \Doctrine\DBAL\Logging\DebugStack();
$stack->enabled;
$em->getConfiguration()->setSQLLogger($stack);
$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');
$LNG = 'DE';
$NUM = 4;
$query->setParameters(array(
'lng' => $LNG,
'num' => $NUM,
));
$result = $query->getResult();
if (!$result) {
echo 'Noting found!';
} else {
foreach ($result as $row) {
$TXT=$row->getContxt();
echo $TXT; } } echo '<pre>';
\Doctrine\Common\Util\Debug::dump($stack, 5); ```",0,0,msr
1345,"Yes, that is correct That should be the defaults on arch, if you refresh, you need to do an upgrade, otherwise you end up with broken so names because pacman does not resolve upgrades to sonames and force other packages to upgrade.
If you want to do only a refresh and install, and not upgrade you can end up with an unbootable system, but you can do this by setting `sysupgrade=False` on the commandline with the `refresh=true`",0,0,msr
1353,"Even though thus is a EU regulation, the actual implementation is in national law so answers will vary across legislations. Starting with a privacy statement or agreement in which you explain what is collected and why and is accessible to whom and when it will be deleted is always a good start.",0,0,msr
1354,"From some fresh readings, this regulation is a law that applies to all EU members with extraterritorial involvements (for outside EU working with Europeans), probably with some variation by land, but not so much (e.g. sensitive data ). To get an quick overview, WordPress has a [roadmap](https://make.wordpress.org/core/2018/03/28/roadmap-tools-for-gdpr-compliance) to address GDPR and its first [implementations](https://core.trac.wordpress.org/query?status=!closed&keywords=~gdpr). It is quite complex stuff but seems logic and normal (see [Max Schrems](https://en.wikipedia.org/wiki/Max_Schrems ) ). Privacy and consent by design, right to erasure, personal data backup, encrypted data, ...etc, just good sense. At this stage, I am not sure a lawyer is necessary but companies would need to use a DokuWiki core and plugins GDPR-compliant.",0,0,msr
1356,"> I started a privacy policy at https://www.dokuwiki.org/privacy -- keeping
> it understandable (as requested by the GDPR) and complete is quite hard.
> Any hint on what's missing is welcome.
I'm rewriting ours tomorrow. I'll take a look and make some suggestions.
UPDATE : The one at the above link seems to have most of it covered well. The additional sections that we have in ours are mostly to do with marketing which is most likely not relevant.",1,0,msr
1366,"For your inspiration, I've edited Greebo (the installed release) to make the *DOKU_PREFS* cookie a session cookie. A session cookie means no permanent storage, so no user consent required.
The second commit removes recording of IP addresses from the logs. Quite some places need code removal, still the result works just fine. All new changelog entries no longer receive the IP address, so nothing can go wrong. Some retrocompatibility code for dealing with older records is also included.
As making a pull request on Github is a chore and Github refuses to accept patches, I made a Gist: https://gist.github.com/Traumflug/74fd0b4c8968fd0184e503d221b13310 with both patches.
With these patches applied the privacy statement reduces to about this (DokuWiki markup):
----
==== General Data Protection Regulation (GDPR) ====
We're neither interested in personal data, nor do we try to collect or use such data. In detail:
* Pages at reprap-diy.com do not use trackers.
* Visiting pages at reprap-diy.com stores up to three cookies in your browser to follow the session. These cookies get deleted when the session ends (when you close your browser).
* Creating an account at reprap-diy.com stores your email address, content of the //Real name// field and an encrypted hash of your password.
* Logging into an account and checking the //Remember me// checkbox stores another, permanent cookie (valid for one year) to keep you logged in. To remove this cookie, log out.
* Each page edit stores the username of the user who did the edit. This information cannot get removed, but if the related account was removed, it also cannot be mapped to an email address or other personal data.
* During page editing your IP address is used to lock the page against a competing edit. The address gets removed when the edit gets saved.
* Some of the pages on reprap-diy.com may contain external videos. For YouTube we use the ""privacy enhanced"" youtube-nocookie.com domain that will not track your visit. Your IP address will be visible to the server providing the video, though. * To view the data stored about you at reprap-diy.com, look at your [[start?do=profile|user profile]].
* To remove this data, go to your [[start?do=profile|user profile]] and delete your account.
----
Voilá, no user consent required, problem solved.
The only issue which might remain is fighting spammers. No IP address, no entry into blacklists. But we all have secured account registration against spammers, right?
----
In case somebody doesn't believe that session cookies need no user consent, he may have a look at this pretty official page: http://ec.europa.eu/ipg/basics/legal/cookies/index_en.htm#section_2. It states:
> Cookies clearly exempt from consent according to the EU advisory body on data protection include:
>
> * user‑input cookies (session-id) such as first‑party cookies to keep track of the user's input when filling online forms, shopping carts, etc., for the duration of a session or persistent cookies limited to a few hours in some cases
> * authentication cookies, to identify the user once he has logged in, for the duration of a session
user‑centric security cookies, used to detect authentication abuses, for a limited persistent duration
> * multimedia content player cookies, used to store technical data to play back video or audio content, for the duration of a session
> [...]",0,1,msr
1371,"Changing `DOKU_PREFS` into a session does not fix anything but breaks the usability imho. `DOKU_PREFS` is used e.g. for storing the size of the edit window and this should persist across sessions imho. As Anika says, cookies do not store the IP (or even DNS entry) of the user, the IP address is instead sent with every request.
Not storing IP addresses is also not the solution as there is a very valid reason to store them at least temporarily: detect and remove vandalism (by IP address you can identify the connection between several edits, possibly even several user accounts) and to be able to identify the author (at least in court) if the content posted was illegal and the site owner gets sued because of that.
What I think would be a good thing is to have some automatic way to remove IP addresses after some time, at least for changes where the user has been logged in (this could be a plugin of course). For anonymous edits I'm not sure if the IP address can be interpreted as an author identification that needs to be stored because of the license (but this of course depends on the selected license).
Concerning the removal of the user name: my personal (non-lawyer) interpretation is that due to the license of the content (creative commons license at least with attribution), DokuWiki has a legitimate interest to store this attribution as it otherwise cannot use the content and as the Wikimedia issue tracker says ""the right of erasure only exists when the processing is not necessary for some legitimate interest of the data controller"".",1,0,msr
1374,"> would it be desirable to extend that with technology (eg. automatically list what data is collected when, why and how long) and provide a way for plugins to hook into that?
I would definitely say Yes to that. That should also include templates, not just plugins. (I know e.g. some templates include Google Analytics.) Maybe have a hook per section (cookies, third party, etc)?",0,0,msr
1377,"Balancing artillery
I'd like to get the stuff I've already tested out of the way. The current balance team is essentially doing everything I already did in terms of making balance changes and testing them.
A lot of time is being wasted doing what I've already done. I've tested my changes in hundreds of matches with great feedback. The only negative opinions I get are from trolls or people who need an excuse for why they suck at the game.
So, first up, ARTILLERY.
These values allow the artillery to remain effective, but not overpowered. Instead of 3 arty destroying a huge infantry blob, you'll 4-6 of them.
Damage vs vehicles has been buffed slightly to compensate for lack of infantry killing.
A direct hit will kill the infantry, but if it lands off to the side, it can take anywhere from 1-4 shots. With a small group of artillery firing, the aoe will make it a lot more effective.
Cost also went up, and it's speed has been slowed down. I think I buffed it vs buildings too, maybe not.
Now, the artillery infantry tradeoff is fair, and for what it lacks in anti infantry, it makes up for by doing roughly 10-20% more dmg to everything else.
vehicles.yaml:
ARTY:
Mobile:
TurnSpeed: 2 (used to be 4 I think)
Speed: 60 (I think -10 points?)
RevealsShroud:
Range: 8c0 (remains same, i think, mightve increased by 1-2 cells)
RevealGeneratedShroud: False
RevealsShroud@GAPGEN:
Range: 4c0
Passenger:
Weight: 4
----------
ballistics.yaml:
^Artillery:
Range: 12c0 (it shouldnt outshoot a v2 by 4 cells, or at all, really)
Projectile: Bullet
Speed: 200 (little bit slower to help nerf vs infantry.
Blockable: false
LaunchAngle: 110
Inaccuracy: 1c256
Warhead@1Dam: SpreadDamage
Spread: 420
Versus:
None: 40
Light: 60
Heavy: 35
Concrete: 50
155mm:
MinRange: 4c0 (minimum range back to 4 cells instead of 5)",1,1,msr
1380,balanced weapon for artillery,0,0,msr
1383,"#### I'm opening this issue because:
- [ ] npm is crashing.
- [x] npm is producing an incorrect install.
- [ ] npm is doing something I don't understand.
- [ ] npm is producing incorrect or undesirable behavior.
- [ ] Other (_see below for feature requests_):
#### What's going wrong?
using `npm i --no-optional`, but the optionalDependencies still affect the version of other dependencies.
#### How can the CLI team reproduce the problem?
1. `git clone https://github.com/mapleeit/how-to-reproduce-npm-bug.git`
2. `npm i --no-optional`
3. `npm ls | grep request`
```
// it prints │ ├─┬ request@2.81.0
│ │ │ │ ├─┬ request@2.87.0
│ │ │ │ ├─┬ request-promise-native@1.0.5
│ │ │ │ │ ├─┬ request-promise-core@1.1.1
├── request@2.81.0 deduped
```
__the version of `request` is 2.81.0__
4. `open package.json`
5. delete the `optionalDependencies`
6. `rm -R node_modules && rm package-lock.json`
7. `npm i --no-optional` or `npm i`
8. `npm ls | grep request`
```
// it prints │ ├─┬ request@2.87.0
│ │ │ ├── request@2.87.0 deduped
│ │ │ ├─┬ request-promise-native@1.0.5
│ │ │ │ ├─┬ request-promise-core@1.1.1
```
__the version of `request` is 2.87.0__
### supporting information:
- `npm -v` prints: 6.1.0 - `node -v` prints: v8.1.2
- `npm config get registry` prints: https://registry.npmjs.org/ - Windows, OS X/macOS, or Linux?: OS X
- Network issues:
- Geographic location where npm was run:
- [ ] I use a proxy to connect to the npm registry.
- [ ] I use a proxy to connect to the web.
- [ ] I use a proxy when downloading Git repos.
- [ ] I access the npm registry via a VPN
- [ ] I don't use a proxy, but have limited or unreliable internet access.
- Container:
- [ ] I develop using Vagrant on Windows.
- [ ] I develop using Vagrant on OS X or Linux.
- [ ] I develop / deploy using Docker.
- [ ] I deploy to a PaaS (Triton, Heroku).
<!--
Thank you for contributing to npm! Please review this checklist
before submitting your issue.
- Please check if there's a solution in the troubleshooting wiki:
https://github.com/npm/npm/blob/latest/TROUBLESHOOTING.md
- Also ensure that your new issue conforms to npm's contribution guidelines:
https://github.com/npm/npm/blob/latest/CONTRIBUTING.md
- Participation in this open source project is subject to the npm Code of Conduct:
https://www.npmjs.com/policies/conduct
For feature requests, delete the above and uncomment the section following this one. But first, review the existing feature requests
and make sure there isn't one that already describes the feature
you'd like to see added:
https://github.com/npm/npm/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+label%3Aalready-looked-at
-->
<!--
#### What's the feature?
#### What problem is the feature intended to solve?
#### Is the absence of this feature blocking you or your team? If so, how?
#### Is this feature similar to an existing feature in another tool?
#### Is this a feature you're prepared to implement, with support from the npm CLI team?
-->",0,0,msr
1389,"Same here, started today. Error on my local machine (windows) and a VM (linux).",0,0,msr
1392,"Running into the same issue here, running into problems on both local and build server
npm ERR! 418 I'm a teapot: express@^4.15.3",0,0,msr
1394,"Ditto, npm config get registry
https://registry.npmjs.org/
npm -v
5.6.0
node -v
v8.9.4
npm install
npm ERR! code E418",1,0,msr
1396,"I know the Internet loves being cute, but this error is unhelpful.",0,1,msr
1399,"My colleague tried to run `npm install` and he was succeeded. His environment:
```
$ npm -v
3.5.2
$ node -v
v4.2.6
$ npm config get registry
https://registry.npmjs.org/
```
log:
```
$ npm install express
/home/username
tqq express@4.16.3
tqq thomash-node-audio-metadata@1.0.1 (git://github.com/osyo-manga/thomash-node-audio-metadata.git#c8af2b43ef97b2753f2ba712e828e0ff131976e8)
mqq xx@0.0.3
```
I also tried that in an older version of npm and node.
```
$ nvm install 4.2.6
Downloading and installing node v4.2.6...
Downloading https://nodejs.org/dist/v4.2.6/node-v4.2.6-linux-x64.tar.xz...
######################################################################## 100.0%
Computing checksum with sha256sum
Checksums matched!
Now using node v4.2.6 (npm v2.14.12)
$ npm -v
2.14.12
$ npm install express
npm WARN package.json rest-test@1.0.0 No description
npm WARN package.json rest-test@1.0.0 No repository field.
npm WARN package.json rest-test@1.0.0 No README data
express@4.16.3 node_modules/express
$ ls node_modules/ | grep express
express
```
Does NOT the issue reproduce in older versions of npm?",0,0,msr
1403,"The body of the 418 is `{""error"":""got unknown host (registry.npmjs.org:443)""}`. Looks like some npm clients are appending the port to the Host header, but only when going through a proxy, and that's confusing the registry:
```
$ wget --header='Host: registry.npmjs.org:443' https://registry.npmjs.org/
--2018-05-29 01:22:08-- https://registry.npmjs.org/
Resolving registry.npmjs.org (registry.npmjs.org)... 104.18.95.96, 104.18.97.96, 104.18.94.96, ...
Connecting to registry.npmjs.org (registry.npmjs.org)|104.18.95.96|:443... connected.
HTTP request sent, awaiting response... 418 I'm a teapot
2018-05-29 01:22:08 ERROR 418: I'm a teapot.
```",0,0,msr
1410,@dincamihai There are a few lint errors here: https://jenkins.saltstack.com/job/PR/job/salt-pr-lint-n/22487/violations/file/salt/client/ssh/__init__.py/,1,0,msr
1411,"@terminalmage I've forgot to change the last error message, which is ""invalid Python"". That just misleads people and they don't understand what is going on, hence the fix. Salt SSH still needs to have both library sets (dependencies) on the Master for the specific Python versions, so the `.tar.gz` is carrying over those in `py<major version>` subdirectories.
@dincamihai NOTE: actually if you are running Salt on a Master from the specific version, you _do not_ need to install Salt again for the alternative version, as it is anyway works for both versions. This is just packaging convenience. But in fact you need only couple of version-specific libraries to be installed so they will be picked up by the `thin` creation procedure. That is, probably we should not say ""install Salt for alternative Python X"" (which implies you will get all the needed dependencies automatically), but ""install Salt _dependencies_ only for the alternative Python X"".
@gtmanfred nope, this is only needed in Fluorine.",1,0,msr
1413,"@dincamihai I would still minimise the text according to the example I showed above. As well as the ""3.x"" thing.",1,0,msr
1415,"@dincamihai the ""3.x"" is the same as ""3"", as I see it. I also suggested to use integers as in the `version_info` instead of just strings. And so if we have some issue with the 3.8 (e.g.) one would just add a minor version key. Otherwise default should go. But ""3.x"" is more to me looks like a hack.
The ""origin"" is something that might not be understood. Personally to me it is very odd terminology here. And you are running ""SaltSSH"" from the Master (or want-to-be-master).",0,1,msr
1420,"@isbm ok. thanks for the suggestion, but if the upstream is fine with my version (changes approved) please merge the PR. thanks!",0,1,msr
1423,"One note:
sometimes, when targeting 1000 machines, maybe 999 are ok but one has an old python. there are two options and we don't know what it is better for the user:
- update the origin machine (that works on 999 of the clients) or
- update the one client that doesn't work
Other than that, this is open source, so I don't have any problem with adapting my PR to whatever is a better fit for saltstack/salt.",0,0,msr
1427,"@dincamihai actually ""approved"" here means _everyone_ agrees, JFYI. And your google result is suggesting exactly what I mean.",0,1,msr
1441,"This is the current (july 2018) GLTF status.
Test case: the same skinned mesh exported from 3DS Max 2018.
Standard material: Diffuse + Specular + Normal
1) SEA3D exporter + SEA3D importer:
http://necromanthus.com/Test/html5/Lara.html SEA file size: 658 KB
Result: close to perfect
2) Babylon3D GLTF exporter + GLTF importer:
http://necromanthus.com/Test/html5/Lara_gltf.html GLB file size: 1,850 KB
Result: messed up materials
I've also tested the FBX2GLTF utility (by Facebook): the same wrong results
Important note: there is nothing wrong with THREE.js and PBR materials:
http://necromanthus.com/Test/html5/Lara_PBR.html In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.",0,0,msr
1442,"I don't think this repo is the right place for this post. It's neither a feature request, nor a bug. So my question is: What are you trying to accomplish with this issue? Bashing `glTF`?
If you encounter problems with an exporter or converter, I suggest you open an issue at the respective github repo.
> In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.
I generally reject such Trump-like statements. They have a provocative nature and are not objective at all.",1,1,msr
1454,If one could quickly prototype some hacks over the existing phong / standard implementations i bet it would be pretty useful 😉,0,0,msr
1460,"The glTF format supports PBR and unlit shaders at this time. Whether the BabylonJS and FBX2GLTF tools do the Phong-to-PBR conversion in a way that preserves Phong specular maps, I don't know — that would be a question for the repos of those tools. If you are trying to preserve the exact appearance of models using classic Phong shaders, you may have an easier time with other formats.
> > The last thing to do is setting renderer.gammaOutput = true.
> > That indicates buggy GLTF Loader (and it has to be fixed).
This is a deliberate decision and not a bug. Base color and emissive textures in glTF (and, typically, diffuse textures in any format...) are in sRGB colorspace. GLTFLoader marks them as such (`material.map.encoding = THREE.sRGBEncoding`), so that they're converted to linear colorspace for correct PBR lighting calculations. Finally colors should be converted back to sRGB (e.g. `renderer.gammaOutput=true`).
If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the `.encoding` property of any texture, and leave it to the end user to change texture colorspace and renderer colorspace. I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace, and getting results that are ""good enough"" but inconsistent with other engines and authoring environments. For correct results you should be using `renderer.gammaOutput=true`, and marking sRGB textures as sRGB.
None of these issue are specific to glTF (see https://github.com/mrdoob/three.js/issues/11337), but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments, and have chosen to treat all sRGB textures as sRGB for a first step. If you're mixing models from other formats in the scene, then yes it's awkward, and you'd need to either mark the diffuse textures of those formats as sRGB or mark the colorspace on the glTF models to linear (the latter is incorrect for all model formats involved, but may look good enough if you don't need precise colors).
***
It does not seem like there is anything actionable here, unless there are specific issues we can report to the tools mentioned. @RemusMar if you are happy with your SEA3D workflow, that's great — I'm not interested in debating formats or persuading you to change from something that is already working well for you.",0,0,msr
1461,"> Could you update the online samples?
Ricardo,
PHONG looks great with Diffuse + Specular only.
PBR does not look great with BaseColor + MetallicRoughness only.
That's the main problem here.
The Normal/Bump and Environment textures are irrelevant at this point.
On top of that: more texture layers = bigger file size and performances drop
> If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the .encoding property of any texture
That was a wise decision.
> I'm not interested in debating formats or persuading you to change from something that is already working well for you.
Don,
I'm not debating the ""PBR only"" bad choice for GLTF.
This topic shows GLTFLoader design flaws.
We don't reinvent the wheel here, so ""renderer.gammaOutput = true"" is not an option now, when GLTF represents less than 1% of the market.
cheers",1,0,msr
1466,"> You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?
Yes, the problem is the same for Phong materials or PBR materials loaded in any other format.
> ...should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible.
If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB. But making the change now would cause confusion and break backward-compatibility, and the `gammaOutput=true` setting needed to fix output is not intuitive — I don't think changing other loaders can be justified given those issues.
Let's keep an eye on https://github.com/mrdoob/three.js/issues/11337. I hope the resolution there will make color workflows more intuitive. With that and NodeMaterial, there may be opportunities to fix some existing issues without breaking anyone's existing applications.",0,0,msr
1468,"> If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB. Yeah, it's unfortunate but I agree that it's not worth breaking backwards compatibility over this. However, as I've been working with larger FBX scenes consisting of multiple models, animated cameras and so on I've found myself wishing that the output of the loader was something more like GLTFLoader's output - that is, it should return an `fbx` object with properties:
```
fbx.animations; // Array<THREE.AnimationClip>
fbx.models; // Array <THREE.Group, THREE.Mesh, THREE.SkinnedMesh>
fbx.cameras; // Array<THREE.Camera>
fbx.asset; // Object
```
There may be other loaders that would benefit from a similar change. We should add this to the backburner (and certainly wait on #11337), but if any loaders do have breaking changes made for whatever reason, then we can use that as opportunity to apply this change as well. Perhaps we should open a new issue to keep track of this?",0,0,msr
1474,">That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( @mrdoob ) if they will be removed or not.
Errr.... this is extremely fuzzy. `GLTFLoader` appears to be a community provided example, that lives in `/examples`. If you load three.js alone (`three.min.js`) there wont be any mention of GLTF.
If you use three.js off the shelf you get a scenegraph and various webgl abstractions. In this context, GLTF is just another one of many many examples of how you can translate some generic 3d data / scene file into three.js's structs. So, at a glance, three.js seems like this generic library, that draws stuff to screen. It doesn't care if you fetch that data from some remote server, and it doesn't care how you parse it (collada, fbx, sea3d, gltf... and 40 others). At the end of the day, you are rendering a `THREE.Mesh` with `THREE.Geometry` and `THREE.Material`. **Absolutely all of the loaders share this feature. All of them result in this data structure.**
However, in practice, this is not the case, and you seem to be in the right. GLTF is a ""first class citizen"" of three.js. @mrdoob wrote that several times. SEA3d is some **random format** written by **some unknown people** while GLTF has the backing of THE khronos group. On top of that, it's probably meant to be the backbone of the whole VR/AR revolution, hence so much backing by other big players. This is just an unfortunate circumstance that three.js found itself in as the most widely used webgl library. People want to do 3d, which three.js solves really well with it's scene graph and other abstractions (things like `Mesh`, `Geometry`, `Material`, `Texture` etc.). Unfortunately, people also want to make experiences and expect three.js to be able to do that. This is of course complicated, hence, favoring one particular format and giving it preferential treatment makes sense. It may somewhat hurt the very essence of the library (draw stuff on screen) but it's a trade off. If you care to read all the guidelines, there's a document called [owners](https://github.com/mrdoob/three.js/wiki/Owners). @donmccurdy owns that particular loader, so his word carries as much weight as @mrdoob's, so good luck there :)
The directive right now is:
>three.js must support gltf
So anything that the khronos group comes up with has to be reflected in three.js. If you look at the discussion historically, @mrdoob doesn't really follow what's going on and @donmccurdy is the authority on all things khronos/gltf related.
So for example, khronos has defined a specification for gltf ""extensions"". Out of infinitely many extensions, one involves texture transformations. It has been ratified by the khronos group and because of that, **three.js absolutely must support it**. This warrants a PR with **11 thousands line of code** and increasing the size of the library by 1/3. I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed.",1,1,msr
1475,"> I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed.
good point
As I said before, Google already failed with UTF8.
In the current development status, GLTF (and PBR) follow the same path.",0,1,msr
1482,"I think the documents wouldn't hurt by having some guideline on how to author textures. The texture can still be in the wrong space regardless of the gltf format, like under calibrated or over calibrated. Something saying ""when making a texture in photoshop do that, when making it in GIMP do this"".",0,0,msr
1486,"> I'm the one who was doing this. As collaborators we have to moderate issues and ensure correct language and behavior.
Grow up first!",0,1,msr
1488,"when we have storage OOM gradle doesn't sygnalize that, instead proper info we have a bullshit :)
`$ gradle clean`
>FAILURE: Build failed with an exception.
>
>* What went wrong:
>Unable to start the daemon process.
>This problem might be caused by incorrect configuration of the daemon.
>For example, an unrecognized jvm option is used.
>Please refer to the user guide chapter on the daemon at >https://docs.gradle.org/4.7/userguide/gradle_daemon.html
>Please read the following process output to find out more:`
>-----------------------
>* Try:
>Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log >output. Run with --scan to get full insights.
Unable to start the daemon process... **when explicity run with false flag in props file**
the next try maybe gradle didn't read the prop file proper .. **so explicity set no deamon arg**
`$ gradle --info --no-daemon clean`
>Initialized native services in: /opt/gradle/ceph3us/native
To honour the JVM settings for this build a new JVM will be forked. Please consider using the daemon: https://docs.gradle.org/4.7/userguide/gradle_daemon.html.
Starting process 'Gradle build daemon'. Working directory: /opt/gradle/ceph3us/daemon/4.7 Command: /opt/jdk1.8/bin/java -XX:+AggressiveOpts -XX:+UseG1GC -Xmn512m -XX:MaxMetaspaceSize=1g -XX:SurvivorRatio=40 -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:-OmitStackTraceInFastThrow -XX:SoftRefLRUPolicyMSPerMB=100 -XX:-HeapDumpOnOutOfMemoryError -Xms512m -Xmx3g -Dfile.encoding=UTF-8 -Duser.country=PL -Duser.language=pl -Duser.variant -cp /opt/gradle/lib/gradle-launcher-4.7.jar org.gradle.launcher.daemon.bootstrap.GradleDaemon 4.7
Successfully started process 'Gradle build daemon'
An attempt to start the daemon took 1.005 secs.
FAILURE: Build failed with an exception.
>* What went wrong:
Unable to start the daemon process.
This problem might be caused by incorrect configuration of the daemon.
For example, an unrecognized jvm option is used.
Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
Please read the following process output to find out more:
-----------------------
>* Try:
Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output. Run with --scan to get full insights.
>* Get more help at https://help.gradle.org
still some daemon shit WTF ??? one more try brings same results of bulshit doeasnt reveal the TRUE CAUSE for BUILD FAILED '$ gradle --debug --no-daemon clean'
>{ unrelated sensitive data cut}
>16:50:14.639 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTING
16:50:14.640 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Waiting until process started: Gradle build daemon.
16:50:14.655 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTED
16:50:14.656 [INFO] [org.gradle.process.internal.DefaultExecHandle] Successfully started process 'Gradle build daemon'
16:50:14.656 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is starting. Waiting for the daemon to detach...
16:50:14.657 [DEBUG] [org.gradle.process.internal.ExecHandleRunner] waiting until streams are handled...
16:50:14.659 [DEBUG] [org.gradle.launcher.daemon.bootstrap.DaemonOutputConsumer] Starting consuming the daemon process output.
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: DETACHED
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Process 'Gradle build daemon' finished with exit value 0 (state: DETACHED)
16:50:15.611 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is now detached.
16:50:15.613 [INFO] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] An attempt to start the daemon took 0.982 secs.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Unable to start the daemon process.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] This problem might be caused by incorrect configuration of the daemon.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] For example, an unrecognized jvm option is used.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please read the following process output to find out more:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] -----------------------
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Try:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Run with --stacktrace option to get the stack trace. Run with --scan to get full insights.
16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Get more help at https://help.gradle.org
**pleaes add OOM storage watcher during task execution that will throw some sort of StorageOOM exception**
**distracts from the fact that there was no memory left at the start of the gradle .. which should be signalized apriori any taskl start / evaluate**
tneet to consider the SPACE -- WHERE PROJECT IS EXECUTED (build output) -- WHEN GRADLE WRITES (cache, etc) **those places should be checked during start gradle (LOW MEM WARNING) and monitored as mentioned above**
i have wasted 5 min befor i did to know the real cause..",1,1,msr
1489,Closing due to abusive language. Please read our code of conduct and open an issue using appropriate language.,0,1,msr
1498,The answer @pink-mist gave was 100% correct. So this is not a bug. If you require additional help please use our official support channels. https://mojolicious.org/perldoc#SUPPORT,0,0,msr
1506,3.7.2 seems to have been unpublished now. Would love to hear what actually happened.,0,0,msr
1507,"@akx shortly, the npm credentials have been stolen and the malicious release has been made directly to npm (not GitHub). The malicious code ran upon `npm install` and attempted to transfer the npm authentication token from `.npmrc` to remote server. Surprisingly, the tampered code only contained a bootstrap script that downloaded and executed the main script (with `eval`) from pastebin and the pastebin script had a syntax error in it so that's how it revealed itself. All of what happened is something to think about, why didn't npm spot the calls to `eval` and reject the update. I know many packers use `eval` as well but come on, it's not 1999 again and there are simple measures and protocols that can be implemented in order to strengthen the security of npm ecosystem.",0,0,msr
1519,I +1 the global unpublishing of all packages and revoking all tokens until this is resolved. This does NOT look good.,0,0,msr
1520,"@branneman and others suggesting npm integration into git,
Not every npm package uses git, and certainly not everyone uses github. Marrying the two concepts doesn't make sense to me.
I think the best solution is faster communication and awareness of exploited packages. Some brainstorming ideas:
* Maybe integrate the npm ecosystem into something like node security project. Warnings should be shown inside of NPM cli when a vulnerable package is found in the dep tree.
* Easy flagging of infected code to notify maintainers.
* Have stricter publish regimens for foundational (heavily depended on) dependencies. Maybe enforce manual audits with two factor authentication. Realistically dependencies that have thousands and thousands of dependents should not be modified so frequently, or they must go through a manual audit step. * Perhaps have a badge awarded to releases that have been manually audited and approved with a 2 factor verification.
... I could go on",1,0,msr
1522,"@gedgaudasnikita how would that help? then they write the code to a file, and `require()` it.. or download a compiled binary instead of using JS for their malware.
btw, why do you have two accounts? @nikita-gedgaudas-ht / @gedgaudasnikita - AFAIK the GitHub ToS only allow one account per human",0,0,msr
1527,@OliverUv +1 on that but I don't know why NPM rejected this - it was a majorly a good security practice,0,0,msr
1529,"(update: apologies, my original post included a link to this issue instead of the actual status page. The link has been updated)",0,0,msr
1533,"[I have read the previous stance on the idea of allowing composer to manage multiple versions of a given package.](https://github.com/composer/composer/issues/2167) However, I would like to re-open the discussion as there are valid use cases for wanting multiple versions installed in parallel. I would counter that this is completely possible based on the following arguments:
1. It's not composer's responsibility to ensure that a developer doesn't try to do something they aren't supposed to do; mind you, it would be fair of composer to issue a stern warning, but ultimately allow a developer to shoot themselves in the foot if that's what they so choose to do.
1. PHP's namespace aliasing and/or composer's autoloader might be a way to work around loading two classes from different versions of a package -- in other words, manipulate the namespace in the autoloader so that if I install versions `2.0` and `3.0` of `package/xyz` (with a namespace of `Package\Xyz`) I could reference them in code as either `Package\Xyz\2.0\MyClass or `Package\Xyz\3.0\MyClass`. And if I wanted to import both of them, I could alias the second one to a non-conflicting class name (see point 1 above).
1. There are use-cases where packages are installed to be run as commands (e.g., phpunit, pdepend, phpcs, phpdoc, phpstan, et al) and would not be included/required directly into a PHP project's code base
One use case I've run into is that I want to install `ramsey/jenkins-php`, which inheritantly depends on `nikic/php-parser: ^1.0`, as well as `phpstan/phpstan-symfony` which inheritantly depends on `nikic/php-parser: ^4.0`. Both are commands to be run from a shell, so there would be no actual conflict with each other. The only thing stopping them from both being installed side by side is that composer does not support multiple versions of the same library.
I understand that this is probably going to be a decisive feature request. I also understand I've probably not thought out every single possible hurdle to making this happen. But I feel like this is an important request to discuss with you and the composer community.
Thank you!",0,0,msr
1536,"Unfortunately, no. There are currently no viable encoders for this that can be included within HandBrake releases for Windows and Linux.
If you compile from source, you can enable the fdk-aac encoder which supports it, but sadly this encoder is not compatible with HandBrakes GPL license so we can not include this in official releases. If the situation changes, we'll look into it.",0,0,msr
1544,"but not shipped in many distributions due to possible patent problems, still compatible?",0,0,msr
1545,"There are a couple reasons distributions don't ship dvdcss. In the US, the big one would be aiding piracy. They can't afford to be sued by the MPAA. Second would be patents, but I'm not sure those patents haven't expired by now. They are quite old at this point. Regardless, patents have nothing to do with copyright. The software license is compatible so there is no copyright issue that prevents using it.",0,0,msr
1546,"fdk-aac license is incompatible with gpl because of patents, otherwise it would be a simple BSD, even all mp3 encoding libraries have same problem, not having explicitly stated free patent licensing does not mean you have the rights.
Anyway it's not the point, if you dynload fdk-aac and you do not distribuite the dll, you are violating nothing, the code you compiled in is the wrapper that is still compatible with gpl.
I suspect even coreaudio on osx may have a licensing problem a this point.",1,0,msr
1549,"To your previous point, we believe Core Audio AAC is not in violation of any license, as it is a system library. We also have periodic contact with Apple, such as recently when they suggested some threading changes to improve compatibility with future macOS versions, and they have never expressed any of the concerns you have.",0,1,msr
1551,"I've already made my researches:
```
Fraunhofer FDK AAC license (#fdk)
This is a free software license as far as it goes. It is incompatible with any version of the GNU GPL.
It has a special danger in the form of a term expressly stating it does not grant you any patent licenses, with an invitation to buy some. Because of this, and because the license author is a known patent aggressor, we encourage you to be careful about using or redistributing software under this license: you should first consider whether the licensor might aim to lure you into patent infringement. If you conclude that the program is bait for a patent trap, it would be wise to avoid the program.
It is possible that the pertinent patents have expired. Depending on whether Fraunhofer still has active patents covering the work, the software might be a trap now, or not. (Of course, any program is potentially threatened by patents, and the only way to end that is to change patent law to make software safe from patents.)
```
The fdk aac license: https://directory.fsf.org/wiki/License:Fdk
look at point 3, even ffmpeg aac encoder that is lgpl does not explicit says that has no patent grant, but it does not means that it has, the only difference is that GNU FSF does not like Fraunhofer:
```
It has a special danger in the form of a term expressly stating it does not grant you any patent
licenses, with an invitation to buy some. Because of this, and because the license author is a known patent aggressor, we encourage you to be careful about using or redistributing software under this
license: you should first consider whether the licensor might aim to lure you into patent infringement
If you conclude that the program is bait for a patent trap, it would be wise to avoid the program.
```
about coreaudio, https://github.com/nu774/qaac
uses coreaudio dll on windows, it needs application support library included in itunes, I suspect they are not system libraries, at least on windows, but the api is the same on osx
My idea is the incompatibily is bogus, at least with gplv2, v3 has explicit denial",0,1,msr
1557,The user must be able to configure the names (not only the admin). The user name is not used as a name for the mail address anymore.,0,0,msr
1561,I just started using Tutanota and i'm not sure if i will continue. Protonmail allowed me to have a different sender name for each email as also filtering emails per email address. I think it is critical for tutanota to implement this feature. I really hope they do it soon. It's the only thing i miss in protonmail.,0,0,msr
1564,1,0,0,msr
1566,+1 Essential!,0,0,msr
1571,1,0,0,msr
1577,"Short answer is that we need to move name from `GroupInfo` (if I remember correctly). There's currently no place to put them so we need to create this mapping. This may affect some other things like user management and creation so it ripples.
In theory it's not hard but it's something that needs time investment.",1,0,msr
1578,"Fair enough.
I hope you get to this soon. For months I have been switching my ""Sender Name"" 2-3 times per day, which is quite annoying, not to mention error-prone.",0,0,msr
1580,Me too is desperate for this feature. I can't get it together. It's a part of privacy in general. But also in just simple daily use. For example: In some official mail I want to use my first and last name. In private mail just my first name.,0,0,msr
1583,1,0,0,msr
1585,This is a big problem. Please prioritize this.,0,0,msr
1592,"Och nö, jetzt wollte ich endlich mal zu Tutanota umziehen und finde das hier. Ich hatte angenommen so etwas sei eine Standardfunktion. Ich bin enttäuscht :(.
Ich habe mich trotzdem für Tutanota entschieden. Die Vorteile überwiegen. Dass Kommentare ausgeblendet werden...nun ja...",0,0,msr
1595,"(Per @mlubin, this is the relevant change https://github.com/JuliaLang/julia/pull/19324)",1,0,msr
1597,"But wouldn't that be confusing in that you couldn't do
```
a = 1
```
and use `a` after that? Unless `global` is inserted for all the toplevel assignments, I guess?",1,0,msr
1598,The behavior wouldn't be just to wrap everything in a `let` block—it's more complicated than that. You need to let-bind any global that's assigned inside the expression and then extract the let-bound value to a global at the end of the expression.,0,0,msr
1599,"So you would turn `a = 1` into something like `a = let a; a = 1; end`. And something like
```jl
for i in 1:2
before = false
end
```
would be turned into this:
```jl
before = let before = before
for i in 1:2
before = false
end
end
```
Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months.",0,1,msr
1602,"> Frankly, I'm pretty annoyed that people are only giving this feedback now. This has change has been on master for ten months
Unfortunately for those of us who can not handle living on the edge, its brand-new from our perspective.",0,0,msr
1610,"> Having an option to choose between the old behavior and the new one is interesting but it feels very hacky.
If someone implements an ""unbreak me"" soft-scope AST transformation, it will be very tempting to use it in IJulia, OhMyREPL, etcetera, at which point you get the even more problematic situation in which the default REPL is seen as broken.",1,1,msr
1624,"I was interested to see if it was possible to monkey patch the REPL to use @stevengj's `globalize` function and it appears it is without too much effort (though quite hacky). See the [gist](https://gist.github.com/dawbarton/0388715fb56fb5cd05e0e4b12c322815). This doesn't work with Juno (or anything else that calls `Core.eval` directly).
I'm **not** going to be recommending this to people, but it's quite useful to me when doing quick-and-dirty data analysis. I would very much like to see a (better thought out) solution since it really is quite confusing for inexperienced and often reluctant coders (i.e., my students) when you can't copy and paste in code from a function into the REPL to see what it does and vice-versa.
```julia
julia> a = 0 0 julia> for i = 1:10 a += i end ERROR: UndefVarError: a not defined Stacktrace: [1] top-level scope at .\REPL[2]:2 [inlined] [2] top-level scope at .\none:0 julia> using SoftGlobalScope [ Info: Precompiling SoftGlobalScope [363c7d7e-a618-11e8-01c4-4f22c151e122] julia> for i = 1:10 a += i end julia> a 55 ```
(BTW: the above is about as much testing as it has had!)",0,0,msr
1625,"> What would the proposed REPL-mode do to included scripts?
Nothing. Basically, the proposal is that this would only be for code entered at an interactive prompt. As soon as you start putting things in files, you need to learn the ""hard scope"" rules. Hopefully, when you start putting code into files you should start using functions.
It's not ideal for there to be pickier scoping rules for global code in files than at the prompt. But I think that #19324 combined with the Julia 1.0 stability promise leaves us with no ideal options.",0,0,msr
1632,"> The point that I was trying to make is that it is reasonable to expect a certain level of difference between Julia and language X (which may be Matlab)
Obviously. When I say ""use Julia instead of Matlab"", I don't mean I'm trying to teach them Matlab syntax in Julia, nor am I specifically targeting former Matlab users.
> I prefer to face these issues early on
It's not about differences from Matlab per se. I would really rather not talk about global vs local scope and the utility of a `global` keyword for static analysis the first time I write a loop in front of non-CS students, or the first time they paste code from a function into the REPL to try it interactively. I would rather focus on the math I'm trying to use the loop to express.
**No one here is arguing for soft interactive scope just because that's what Matlab users expect. We are arguing for it because that is what *all* first-time users will expect,** and because long **digressions into the unfamiliar concept of ""scope"" are certain to derail** any non-CS lecture where you are showing a loop for the first time. (And even for experienced users, it's rather inconvenient to be forced to add `global` keywords when we are working interactively.)",0,0,msr
1634,"@vtjnash, I'd rather focus this discussion on things that we can do *before* Julia 2.0. I agree that having interactive mode behave differently is only a stopgap, though, and we should seriously contemplate changing the scoping rules in a few years.",0,0,msr
1639,"Some thoughts I wrote down last night while trying to wrap my head around this issue (yet again) to try to figure out what the best course of action might be. No conclusion, but I think this lays out the problem quite clearly. After having thought about this issue for some years, I don't think there is any ""ideal solution""—this may be one of those problems where there are only suboptimal choices.
---
People naively view global scope as a funny kind enclosing local scope. This is why global scopes worked the way they did in Julia 0.6 and prior:
- If an outer local scope creates a local variable and an inner local scope assigns to it, then that assignment updates the outer local variable.
- If an outer global scope creates a global variable and an inner local scope assigns to it, then that assignment previously updated the outer global variable.
The main difference, however, is:
- Whether an outer local variable exists, by design, does not depend on the order of appearance or execution of the expressions in the outer local scope.
- Whether a global variable exists, however, cannot be independent of order, since one evaluates expressions in global scope, one at a time.
Moreover, since global scopes are often quite lengthy—not infrequently spread across multiple files—having the meaning of an expression depend upon other expressions an arbitrary distance from it, is a “spooky action at a distance” effect, and as such, quite undesirable.
---
This last observation shows why having the two different versions of a for loop at global scope behave differently is problematic:
```jl
# file1.jl
for i = 1:5
a += 1
end
```
```jl
# file2.jl
a = 1
```
```jl
# file3.jl
for i = 1:5
a += 1
end
```
```jl
# main.jl
include(""file1.jl"")
include(""file2.jl"")
include(""file3.jl"")
```
Also note that the contents of `file1.jl` and `file3.jl` are identical and we could simplify the example by including the same file twice with a different meaning and behavior each time.",0,0,msr
1641,"My small suggestion, that does not deal with the repl problem, but would be useful for didactic purposes when teaching the language not-interactively, at least: define a main block named ""program"", like can be done in fortran (it is the same as the ""let...end"" above, just with a more natural notation):
program test
...
end one could teach the language without going into the scope details and only eventually discuss that point.",1,0,msr
1643,"> we should seriously contemplate changing the scoping rules in a few years.
Absolutely not. Do you seriously want to go back to the pre-v0.2 world (see #1571 and #330) of loop scope?
We have actually never fully supported copying and pasting code from a function line-by-line into the REPL. So we can view this as an opportunity to make that work. Specifically, while it ""worked"" for `for` loops, it did not work for inner functions:
```
x = 0
f(y) = (x=y)
```
Inside a function, `f` will mutate the `x` from the first line. In the REPL it won't. But with a transformation like that in SoftGlobalScope.jl it could work. Of course, we probably wouldn't want that on by default since then pasting stand-alone function definitions wouldn't work. The first thing that comes to mind is a REPL mode for line-by-line function debugging.",0,0,msr
1645,"I guess I was responding more to:
> One other fix not mentioned here is to simply stop making ‘for’ define a scope-block",0,0,msr
1646,"> We have actually never fully supported copying and pasting code from a function line-by-line into the REPL. So we can view this as an opportunity to make that work.
I very much appreciate this sentiment and for my use cases it would really help. From my perspective it is really about making the REPL as useful as possible rather than changing the scoping rules of the language directly. That said, the more I think about this problem the more I see the conflicting views I (personally) hold as to what the REPL should do.
To be concrete, I'd very much like it if the REPL matched the scoping rules of a function body; i.e., variables are local rather than global and you can just copy-and-paste code directly from a function and know that it will work. I imagine a naive implementation would be something like let-block wrapping (as has been mention previously) of the form
```
julia> b = a + 1
```
being transformed into
```julia
let a = _vars[:a]::Float64 # extract the variables used from the backing store
# Code from the REPL
b = a + 1
# Save assigned variables back to the backing store
_vars[:b] = b
end
```
Done properly (i.e., by someone who knows what they are doing), I imagine that this would have a number of benefits over the existing REPL. 1. previous workflows with interactive data analysis/computation just work. 2. far fewer posts on Discourse where the basic response is ""stop benchmarking with global variables"" - everything would be local and so hopefully fast! :) 3. copy-and-paste to/from a function body works as expected. 4. a `workspace()` like function is trivial if the backing store is some sort of Dict; just clear it out. 5. globals become explicit - things are local unless you specifically ask for them to be global; this is a big advantage from my perspective, I don't like implicitly creating globals. A very minor final point (and I hestiate to add this!), this would match the behaviour of Matlab making it easier for people transitioning - at the Matlab REPL all variables seem to be local unless explicitly annotated as global. Until a few hours ago this story sounded great to me. But after Jeff's comment about functions I thought about pasting in stand-alone function definitions and how this approach would basically prevent that since function definitions should go in the global scope (at least, that is probably what is intended); but then what if they *were* intended to go into the local scope (an inner function)? There is no information to disambiguate the two possibilities. It would seem that two REPL modes are needed, one with local scope and one global scope. On one hand that could be very confusing (imagine the Discourse posts...) but on the other it could be extremely useful. (Having both REPL modes would also be non-breaking since you are just introducing new functionality :) )
Going for the halfway house of `SoftGlobalScope.jl` might end up being the least confusing compromise but my worry is that it's just another set of rules to remember (which things work in the REPL but not in my function body/global scope and vice-versa).
Apologies for the long post but I think this is important for usability (and it helped me think it through!).",1,0,msr
1647,"> How many mailing-list complaints and github issues have been filed about this by upset users? Zero, by my count. Why? Probably because this behavior is fundamentally unsurprising to people — if you work in global scope, you depend on global state.
Hmm, did you really make a systematic study of this? I must have missed that. Nevertheless, this does not mean that this behavior is not a source of bugs or unexpected results; just that after the user has figured it out, it was recognized as correct behavior and thus did not prompt an issue/complaint.
> In Julia 1.0, I'm honestly worried about what I will do if I'm in the middle of a linear-algebra lecture and have to mysteriously type a global keyword
I sympathize with this problem. When I taught into some simple programming to econ students necessary for a course, I usually suggested that they go back and forth between wrapping code in functions, and simply commenting out `function` and `end` and running things in the global scope, so they could inspect what is happening. This pretty much made up for the lack of debugging infrastructure at that time in Julia.
It appears this approach is no longer feasible. But I wonder if it was really the right way to do it anyway, and in the meantime various things have improved a lot (#265 was fixed, [Revise.jl](https://github.com/timholy/Revise.jl) and recently [Rebugger.j](https://github.com/timholy/Rebugger.jl) have improved workflow/debugging considerably).
It seems that this issue does not bother experienced users very much, the main concern is confusion in a pedagogical setting. I have not experimented with this myself yet, but I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope. These are elements of good style anyway and would benefit students.",1,0,msr
1649,"> I wonder if we could adapt our approaches to teaching instead, eg introduce functions before loops, avoid loops in global scope.
This is totally impractical in a class that is not focused on teaching programming. I might as well not use Julia in my classes if I can't use it interactively and/or have to write functions for everything first.
(And it's not just pedagogical. Loops in global scope are *useful* for interactive work. And one of the main reasons people like dynamic languages for technical computing is their facility for interactive exploration. Not all coding is performance-oriented.)",0,0,msr
1651,"@StefanKarpinski, I'm specifically referring to people complaining that a global loop depends on global state. I don't recall anyone complaining that this was bad behavior, nor can I find any examples of this.
I agree that people have been confused about when and where assignment defines new variables, but it has usually been in the other direction — they wanted local scopes to act more global (rather than vice versa), or to not have a distinction between `begin` and `let`. IIRC, the complaint was never that assigning to a global variable in a global loop had the surprising side effect of modifying a global.
The whole issue of scoping is confusing to new users, and it will continue to be so. But the confusing part was not cases where assigning to a global variable name affected the global state. The current behavior makes this worse, not better.",1,0,msr
1658,"From a practical point of view, getting this ""fixed"" in the REPL is not
only important for teaching/non-programmer users. This behaviour also
makes interactive debugging via the REPL (by copy-pasting parts) very
unpractical. This mode of debugging can sometimes be preferable (even to a
good debugger and) even for experienced programmers (and is often one of
the reasons to prefer a dynamic language). Of course for the experienced
programmers, being optional shouldn't be a problem; For novice users it
would be preferably the default.
@StefanKarpinski As a naive programmer, I don't really see what is so wrong in viewing the
global scope as a funny kind enclosing local scope, especially in dynamic
languages. I do understand that from a compiler point of view it is not
necessarily correct (in Julia), but it is a nice, easy and useful model
for a (naive) programmer. (I also suspect it might be actually implemented that way in
some languages).
Julia also seems to presents it that way to the programmer:
The following function function will give the error ""a not defined"", which
it will not do if a=1 is put before the for loop.
function test()
for i = 1:10
a=a+i
end
a=1
@show a
end
which, unless I complete misunderstood, seems at odds with ""Whether an
outer local variable exists, by design, does not depend on the order of
appearance or execution of the expressions in the outer local scope"".
I very much agree with avoiding ""spooky action at a distance"", and much
prefer explicit definition for using globals at the function/call stack
level and would personally also like having something like loading from a file in
its own scope, and requiring explicit definition for using global variables.
At the level of loops is going a bit to far for me though, as the
definitions/context is usually quite near.
The 3 files example is a bit contrived (and fails with the expected ""a not
defined"" error): You would normally put the initial definition in the same
file. There is actual spooky danger in this (and I have been bitten by it
in other languages) in that includes are run in the global scope, so you
are inadvertently defining a global variable that may interfere with other
code. However, having to use global in the loop is not a solution to
this problem.
wrt to the long-running REPL session:
The current behaviour replaces a very rare and easy to spot failure mode
for running an online example in the REPL (you miss copy/pasting the
initial definition of the variable before the loop, and already have the
same variable defined globally from something previous) with not being
able to run an online example correctly if it is part of a function
(without adding global everywhere), and not solving the problem if it is
not (if global is already there in the online code, you will still use the
wrong value in the already existing global variable)",1,0,msr
1662,"What is the time-line on this? It seems it would be a great improvement to user usability. And at this ""critical"" time of Julia with 1.0 out, it would seem advantageous to get this fixed asap (in the way suggested by Jeff above) and tag a new Julia version or REPL version. (Sorry for this arm-chair comment, as I certainly will not fix this!)",0,0,msr
1665,"@derijkp A short answer is that I think it's easier if the scope of a variable corresponds to some block construct (e.g. the body of a function or loop). With your suggestion, the scope of a variable would be some subset of a block, which I think is ultimately more complex and confusing --- you can't point to a syntactic form that corresponds to the scope of the variable.
Yes, I can believe this is a mismatch for some people's intuition. But you can only optimize for the first ten minutes of using a language up to a point. The real question is, how hard is it to teach/learn how it works, and which design will save time in the long run (by making the language simpler, making it easier to develop tooling, etc.)?",0,0,msr
1675,"(I guess) scoping rules cannot be changed in scripts because it would be backwards incompatible, i.e. would break the promise that any code written for 1.0 will run on any 1.* version. You are correct though that the same problem with scoping for the REPL also applies to scripts (naive user at a complete loss why his/her code does not work properly when run as a script). A way to solve/alleviate this problem without major incompatibilty would be to add an option to the julia cmdline to use softscope (or alternative) , e.g. julia -f programfile, and show this option in any description/tutorial that a beginner is likely to come across.
I also see a potential alternative for the softscope that may have some advantages (though i am probably overlooking disadvantages): What if a file (a called script) would always introduce its own local scope: scoping rules would be in complete consistency with those in functions, and with the expectations of a lot of users. It would also remove a lot of the performance liabilities with new users: No more unneeded globals (globals would have to be explicitly defined), and code might be compiled
(How many times have you had to say to put everything in a function, and to avoid using globals?)",1,0,msr
1683,"> I reeeally don't think there should be a command line option for this. Then every piece of julia code will have to come with a comment or something telling you which option to use. Some kind of parser directive in a source file would be a bit better, but even better still would be to have a fixed rule
I agree. Sounds like a teaching and communication headache to me.
> For example, hard scope inside modules only might make sense.
Just so I understand: if I had a short script (not in a module!) in a `.jl` file which I had copied from an IJulia notebook, then if I ran that code in either the REPL directly or shift-enter in Juno, then it would behave consistently as soft-scope... but if I copied it instead of a `module` block then it would yell at me about globals? But if I copied that code inside of functions inside of a module, then it should work.
If so, that makes complete sense,is very teachable and coherent. Top-level scripts are an interactive interface for exploration, etc. but you would never put that kind of code in a module. Modules are something that you should fill with functions are very carefully considered globals. It would be easy to tell people about those rules.",1,0,msr
1687,"> We end up down a rabbit-hole of ""just try it and work out for yourself whether it's local or global, good luck"".
Pop quiz: in julia 0.6, is `x` global or local:
```
for i = 1:10
x = i
end
```
The answer is that there's no way to know, because it depends on whether a global `x` has been defined before. Now, you can say for sure that it is local.",0,0,msr
1689,"One thought that I had but we dismissed as being ""too annoying"" and ""likely to cause the villagers to get out their pitchforks"" was that in non-interactive contexts, we could require a `local` or `global` annotation in ""soft scope"". That would guarantee that code from a module would work the same if pasted into the REPL. If we applied that to ""scripts""/""programs"" then the same would be true of them.",1,0,msr
1690,"When I was first introduced to Julia (not a long time ago, and I come from a Fortran background mostly), I was taught that ""Julia is compiled and fast at the function level, thus everything that must be efficient must be done inside functions. In the main 'program' it behaves like a scripting language"". I found that fair enough, as I cannot imagine anyone doing anything too computationally demanding without understanding that statement. Therefore, if there is any sacrifice in performance at the main program for using the same notation and constructions than in the functions, I find that totally acceptable, much more acceptable than trying to understand and teach these scoping rules and not being able to copy and paste codes from one place to another.
By the way, I am a newbie in Julia yet, having chosen it to teach some high-school and undergraduate students some basics of simulations of physical systems. And I am already hopping this issue returns to the 'normal' behavior of previous versions, because it gives us quite a headache.",0,0,msr
1693,"Per https://twitter.com/dhh/status/1032050325513940992, I'd like for Rails to set a good example and tone by using better terminology when we can. An easy fix would be to replace our use of whitelist with allowlist and blacklist with denylist.
We can even just use them as verbs directly, as we do with the former terms. So something is allowlisted or denylisted.
I took a quick look and it seems like this change is mostly about docs. We only have one piece of the code that I could find on a search that uses the term whitelist with `enforce_raw_sql_whitelist`. Need to consider whether we need an alias and a deprecation for that.",0,0,msr
1697,"1. [etymology is quite important](
https://www.quora.com/Is-the-term-blacklist-racist?share=1). in the end, we might consider plain words „black“ and „white“ racist and enter the realms of newspeak which i figure you especially, @dhh, are familiar with.
2. “allow/deny are simply clearer terms” — now that’s an actual, technically useful argument.
3. can we please stop jumping onto political bandwagons? i am here for the sanity.",1,1,msr
1709,@ulrichb Are you sure that you're using mongoose@5.2.10 that released an hour ago?,1,0,msr
1710,"I can replicate what @ulrichb is seeing on 5.2.10 with:
### 6922.js
```js
#!/usr/bin/env node
'use strict';
const mongoose = require('mongoose');
mongoose.connect('mongodb://localhost:27017/test', { useNewUrlParser: true });
mongoose.set('useCreateIndexes', true);
const conn = mongoose.connection;
const Schema = mongoose.Schema;
const schema = new Schema({
name: {
type: String,
unique: true
}
});
const Test = mongoose.model('test', schema);
const test = new Test({ name: 'one' });
async function run() {
console.log(`mongoose version: ${mongoose.version}`);
await conn.dropDatabase();
await test.save();
return conn.close();
}
run();
```
### Output:
```
issues: ./6922.js
mongoose version: 5.2.10
(node:7186) DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.
issues:
```",1,0,msr
1715,"@adamreisnz yeah we should add an alias `doc.delete()`, ditto for `doc.update()` @mycompassspins",0,0,msr
1718,Using mongoose 5.2.12. I also get this: `the options [useCreateIndex] is not supported` in addition to the deprecation warning. More info: I only get this extra message when pass `useCreateIndex` as a connection option. When i use `mongoose.set()` I don't get that message.,0,0,msr
1720,"@Fonger @govindrai I tried doing this through connection options and though mongoose.set()
before and after connection to the DB.
the 'DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.' always seems to persist.",0,0,msr
1722,"@ParikshitChavan @govindrai @Avcajaraville You have to put `mongoose.set('useCreateIndex', true)` before **EVERY** `mongoose.model()` call
(Note: not `mongoose.connect()` ).
See the discussion in #6890. This is fixed in 5.2.13 (but it's not released yet).",1,0,msr
1724,"@arastu can you create a reproducible example with one or more files where you demonstrate that you are using 5.2.13 and are seeing one or more of these warnings?
*Note that I am calling `mongoose.set()` **before** requiring my models*
For Example:
### index.js
```js
#!/usr/bin/env node
'use strict';
const assert = require('assert');
const mongoose = require('mongoose');
mongoose.set('useCreateIndex', true);
mongoose.set('useFindAndModify', false);
mongoose.connect('mongodb://localhost:27017/test', { useNewUrlParser: true });
const conn = mongoose.connection;
const One = require('./one');
const Two = require('./two');
const one = new One({ name: '1' });
const two = new Two({ name: '2' });
async function run() {
assert.strictEqual(mongoose.version, '5.2.13');
await conn.dropDatabase();
await Promise.all([One.init(), Two.init()]);
await one.save();
await two.save();
await One.findOneAndUpdate({}, { name: 'one' });
await Two.findOneAndUpdate({}, { name: 'two' });
console.log(`${mongoose.version} should show no deprecations with the new settings.`);
return conn.close();
}
run();
```
### one.js
```js
'use strict';
const mongoose = require('mongoose');
const schema = new mongoose.Schema({ name: { type: String, unique: true } });
module.exports = mongoose.model('one', schema);
```
### two.js
```js
'use strict';
const mongoose = require('mongoose');
const schema = new mongoose.Schema({ name: { type: String, unique: true } });
module.exports = mongoose.model('two', schema);
```
### Output:
```
6922: ./index.js
5.2.13 should show no deprecations with the new settings.
6922:
```",0,0,msr
1729,"Here's the stack:
```
DeprecationWarning: collection.update is deprecated. Use updateOne, updateMany, or bulkWrite instead.
at NativeCollection.(anonymous function) [as update] (/MyProject/node_modules/mongoose/lib/drivers/node-mongodb-native/collection.js:143:28)
at NodeCollection.update (/MyProject/node_modules/mquery/lib/collection/node.js:66:19)
at model.Query._updateThunk (/MyProject/node_modules/mongoose/lib/query.js:3233:23)
at model.Query.Query._execUpdate (/MyProject/node_modules/mongoose/lib/query.js:3245:23)
at process.nextTick (/MyProject/node_modules/kareem/index.js:333:33)
at process._tickCallback (internal/process/next_tick.js:150:11)
```
Obviously, `Model.update` is actually executed, even though I'm calling `MongooseDocument.update`. This is what I'm doing, more or less:
```
async UpdateMyDocument(id:string, update:MyDocument):Promise<MyDocument>
{
const doc = await MyModel.findById(id)
.populate({ path: 'somePath', populate: [{ path: 'somePath.nested' }] });
return await doc.update(update); // <= MongooseDocument.update is producing a Model.update deprecation warning
}
```",0,0,msr
1732,"(node:6455) DeprecationWarning: GridStore is deprecated, and will be removed in a future version. Please use GridFSBucket instead. What about this ??.
""gridfs-stream"": ""^1.1.1"",
""mongoose"": ""^5.3.1"",
""multer"": ""^1.4.0"",
""multer-gridfs-storage"": ""^3.0.0""",1,0,msr
1737,"Hello, as referenced by others like this issue [here](https://github.com/zfsonlinux/zfs/issues/6001), I have a pool that was mistakenly upgraded under Linux that needs to go back to FreeBSD. Unfortunately userobj_accounting is enabled, which means that FreeBSD won't take the pool due to an unsupported feature.
I saw mention of [this commit](https://github.com/zfsonlinux/zfs/commit/83025286175d1ee1c29b842531070f3250a172ba) enabling feature disable functionality. I grabbed a Ubuntu 14.04 boot disk and installed ZoL (and an SPL from about that time) from that pull request, but am still getting reference to userobj_accounting being an ""unsupported feature"" when I try to import the pool.
I'm not sure if the pool should be mounted or not?
With the pool exported, trying zhack returns:
```
root@ubuntu:~/zfs# zhack feature disable media org.zfsonlinux:userobj_accounting
zhack: feature 'org.zfsonlinux:userobj_accounting' is active, can not disable
```",0,0,msr
1741,"We should release the next version asap and rebuild our packages.
~~~
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1) but 1.14.0-0ubuntu1.1 is to be installed
E: Unable to correct problems, you have held broken packages.
~~~",0,0,msr
1743,"As a workaround while they rebuild passenger, you can install the old version with:
```sh
sudo apt install \
libnginx-mod-http-auth-pam=1.14.0-0ubuntu1 \
libnginx-mod-http-cache-purge=1.14.0-0ubuntu1 \
libnginx-mod-http-dav-ext=1.14.0-0ubuntu1 \
libnginx-mod-http-echo=1.14.0-0ubuntu1 \
libnginx-mod-http-fancyindex=1.14.0-0ubuntu1 \
libnginx-mod-http-geoip=1.14.0-0ubuntu1 \
libnginx-mod-http-headers-more-filter=1.14.0-0ubuntu1 \
libnginx-mod-http-image-filter=1.14.0-0ubuntu1 \
libnginx-mod-http-lua=1.14.0-0ubuntu1 \
libnginx-mod-http-ndk=1.14.0-0ubuntu1 \
libnginx-mod-http-perl=1.14.0-0ubuntu1 \
libnginx-mod-http-subs-filter=1.14.0-0ubuntu1 \
libnginx-mod-http-uploadprogress=1.14.0-0ubuntu1 \
libnginx-mod-http-upstream-fair=1.14.0-0ubuntu1 \
libnginx-mod-http-xslt-filter=1.14.0-0ubuntu1 \
libnginx-mod-mail=1.14.0-0ubuntu1 \
libnginx-mod-nchan=1.14.0-0ubuntu1 \
libnginx-mod-stream=1.14.0-0ubuntu1 \
nginx-common=1.14.0-0ubuntu1 \
nginx-core=1.14.0-0ubuntu1 \
nginx=1.14.0-0ubuntu1
```
You will have to uninstall the new version before trying the command above.",0,0,msr
1744,"Same problem ```
xxx:~# apt install libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1.1) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
xxx:~# aptitude install libnginx-mod-http-passenger
The following NEW packages will be installed:
libnginx-mod-http-passenger{b} passenger{a} passenger-dev{a} passenger-doc{a} ruby-rack{a} 0 packages upgraded, 5 newly installed, 0 to remove and 0 not upgraded.
Need to get 6,823 kB of archives. After unpacking 50.9 MB will be used.
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1.1) but it is not going to be installed
The following actions will resolve these dependencies:
Keep the following packages at their current version:
1) libnginx-mod-http-passenger [Not Installed] Accept this solution? [Y/n/q/?] ^C
xxx:~# cat /etc/lsb-release DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION=""Ubuntu 18.04.1 LTS""
```",1,0,msr
1746,"Like @nleo , I'm also running into the issue, today with a fresh install of **Ubuntu 18.04.1 LTS** after following the install instructions on the PhusionPassenger website here:
https://www.phusionpassenger.com/library/install/nginx/install/oss/bionic/
Here's my output:
```
deploy@localhost:~$ sudo apt-get install -y libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: nginx-common (= 1.14.0-0ubuntu1.1) but 1.14.0-0ubuntu1.2 is to be installed
E: Unable to correct problems, you have held broken packages.
deploy@localhost:~$ The following packages have unmet dependencies:
```",0,0,msr
1748,"Oh boy. It's not good when I see @excid3 show up with the same isse as me :/
I tried following @hamiltonc 's instructions above 👆 and while it appears to have installed no problem, I am unable to start NGINX.
```
deploy@localhost:~$ sudo apt install \
> libnginx-mod-http-auth-pam=1.14.0-0ubuntu1 \
> libnginx-mod-http-cache-purge=1.14.0-0ubuntu1 \
> libnginx-mod-http-dav-ext=1.14.0-0ubuntu1 \
> libnginx-mod-http-echo=1.14.0-0ubuntu1 \
> libnginx-mod-http-fancyindex=1.14.0-0ubuntu1 \
> libnginx-mod-http-geoip=1.14.0-0ubuntu1 \
> libnginx-mod-http-headers-more-filter=1.14.0-0ubuntu1 \
> libnginx-mod-http-image-filter=1.14.0-0ubuntu1 \
> libnginx-mod-http-lua=1.14.0-0ubuntu1 \
> libnginx-mod-http-ndk=1.14.0-0ubuntu1 \
> libnginx-mod-http-perl=1.14.0-0ubuntu1 \
> libnginx-mod-http-subs-filter=1.14.0-0ubuntu1 \
> libnginx-mod-http-uploadprogress=1.14.0-0ubuntu1 \
> libnginx-mod-http-upstream-fair=1.14.0-0ubuntu1 \
> libnginx-mod-http-xslt-filter=1.14.0-0ubuntu1 \
> libnginx-mod-mail=1.14.0-0ubuntu1 \
> libnginx-mod-nchan=1.14.0-0ubuntu1 \
> libnginx-mod-stream=1.14.0-0ubuntu1 \
> nginx-common=1.14.0-0ubuntu1 \
> nginx-core=1.14.0-0ubuntu1 \
> nginx=1.14.0-0ubuntu1
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
Setting up libnginx-mod-mail (1.14.0-0ubuntu1) ...
Setting up libxpm4:amd64 (1:3.5.12-1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
Setting up libnginx-mod-http-xslt-filter (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-upstream-fair (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-geoip (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-ndk (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-perl (1.14.0-0ubuntu1) ...
Setting up libwebp6:amd64 (0.6.1-2) ...
Setting up libnginx-mod-http-lua (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-uploadprogress (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-fancyindex (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-stream (1.14.0-0ubuntu1) ...
Setting up libnginx-mod-http-echo (1.14.0-0ubuntu1) ...
Setting up libgd3:amd64 (2.2.5-4ubuntu0.2) ...
Setting up libnginx-mod-http-image-filter (1.14.0-0ubuntu1) ...
Setting up nginx-core (1.14.0-0ubuntu1) ...
Job for nginx.service failed because the control process exited with error code.
See ""systemctl status nginx.service"" and ""journalctl -xe"" for details.
invoke-rc.d: initscript nginx, action ""start"" failed.
● nginx.service - A high performance web server and a reverse proxy server
Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)
Active: failed (Result: exit-code) since Thu 2018-11-08 15:53:24 UTC; 13ms ago
Docs: man:nginx(8)
Process: 10420 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=1/FAILURE)
Main PID: 968 (code=exited, status=0/SUCCESS)
Nov 08 15:53:24 localhost systemd[1]: Starting A high performance web server and a reverse proxy server...
Nov 08 15:53:24 localhost nginx[10420]: nginx: [emerg] unknown directive ""passenger_root"" in /etc/nginx/conf.d/mod-http-passenger.conf:1
Nov 08 15:53:24 localhost nginx[10420]: nginx: configuration file /etc/nginx/nginx.conf test failed
Nov 08 15:53:24 localhost systemd[1]: nginx.service: Control process exited, code=exited status=1
Nov 08 15:53:24 localhost systemd[1]: nginx.service: Failed with result 'exit-code'.
Nov 08 15:53:24 localhost systemd[1]: Failed to start A high performance web server and a reverse proxy server.
dpkg: error processing package nginx-core (--configure):
installed nginx-core package post-installation script subprocess returned error exit status 1
E: Sub-process /usr/bin/dpkg returned an error code (1)
```
@FooBarWidget Any chance you can reopen this issue? It doesn't appear to have been fixed and from my basic testing (it works on 18.04 but not 18.04.1) it seems to be a bad build.",0,1,msr
1750,"I was able to get things going by going all the way back to Passenger 5.3.4. ```
sudo apt install libnginx-mod-http-auth-pam=1.14.0-0ubuntu1 libnginx-mod-http-cache-purge=1.14.0-0ubuntu1 libnginx-mod-http-dav-ext=1.14.0-0ubuntu1 libnginx-mod-http-echo=1.14.0-0ubuntu1 libnginx-mod-http-fancyindex=1.14.0-0ubuntu1 libnginx-mod-http-geoip=1.14.0-0ubuntu1 libnginx-mod-http-headers-more-filter=1.14.0-0ubuntu1 libnginx-mod-http-image-filter=1.14.0-0ubuntu1 libnginx-mod-http-lua=1.14.0-0ubuntu1 libnginx-mod-http-ndk=1.14.0-0ubuntu1 libnginx-mod-http-perl=1.14.0-0ubuntu1 libnginx-mod-http-subs-filter=1.14.0-0ubuntu1 libnginx-mod-http-uploadprogress=1.14.0-0ubuntu1 libnginx-mod-http-upstream-fair=1.14.0-0ubuntu1 libnginx-mod-http-xslt-filter=1.14.0-0ubuntu1 libnginx-mod-mail=1.14.0-0ubuntu1 libnginx-mod-nchan=1.14.0-0ubuntu1 libnginx-mod-stream=1.14.0-0ubuntu1 nginx-common=1.14.0-0ubuntu1 nginx=1.14.0-0ubuntu1 nginx-extras=1.14.0-0ubuntu1 libnginx-mod-http-passenger=1:5.3.4-1~bionic1 passenger=1:5.3.4-1~bionic1
```",1,0,msr
1754,We're aware of the issue and are working on a fix.,0,0,msr
1765,"@tbonz how are you installing nginx 1.15.8? Ubuntu only ships 1.14.0. Our nginx module package targets the ubuntu provided nginx package, if you want to target another nginx, you can use the instructions here: https://www.phusionpassenger.com/library/install/nginx/install_as_nginx_module.html.",0,0,msr
1769,"I'm having the same problem, fresh ubuntu 18.04.2 LTS install. Followed the steps described in the docs:
```
sudo apt-get install -y dirmngr gnupg
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7
sudo apt-get install -y apt-transport-https ca-certificates
sudo sh -c 'echo deb https://oss-binaries.phusionpassenger.com/apt/passenger bionic main > /etc/apt/sources.list.d/passenger.list'
sudo apt-get update
sudo apt-get install -y libnginx-mod-http-passenger nginx
```
```
# apt-get install -y libnginx-mod-http-passenger nginx
Reading package lists... Done
Building dependency tree Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: passenger (= 1:6.0.2-1~bionic1) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
```
I'm not using any custom repositories for nginx:
```
# apt-cache policy nginx
nginx:
Installed: (none)
Candidate: 1.14.0-0ubuntu1.2
Version table:
1.14.0-0ubuntu1.2 500
500 http://archive.ubuntu.com/ubuntu bionic-security/main amd64 Packages
500 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages
1.14.0-0ubuntu1 500
500 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages
```
Any clues?",1,0,msr
1770,"I dont know the reason, but my /etc/apt/sources.list was missing these lines:
```
deb http://archive.ubuntu.com/ubuntu/ bionic universe
deb http://archive.ubuntu.com/ubuntu/ bionic-updates universe
```
After added, the instalation succeded.",0,0,msr
1780,"@CamJN Is there a possibility to get support soon so that I can install Passenger/Nginx on Ubuntu 19.04?
It'd be great if you can please share the alternative solution for the installation.",0,0,msr
1785,"EDIT: Booyah! Fixed it. In my case, I had ""stretch"" in my passenger.list file instead of ""bionic"". Changed to ""bionic"" and viola! Installed successfully.
Resolved.
Heyo,
After a couple installation cycle and purges of nginx, I'm still getting a similar response:
sudo apt-get install libnginx-mod-http-passenger
Reading package lists... Done
Building dependency tree Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: passenger (= 1:6.0.2-1~stretch1) but it is not going to be installed
Depends: nginx-common (>= 1.10.3-1+deb9u2) but it is not going to be installed
Depends: nginx-common (< 1.10.4) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
I guess perhaps I'll have to tinker with installing older versions :/
Thanks",1,0,msr
1789,"> > > @activestylus You haven't described what you did or what output you saw. But I can assure you that installing passenger and nginx on a fresh bionic (title of this issue) install does work if you follow the instructions: https://www.phusionpassenger.com/docs/tutorials/deploy_to_production/installations/oss/ownserver/ruby/nginx/
> > I've just gone through the process myself to verify it.
05/05/2020 and this is still an issue, followed every piece of instructions specially the link below, still same error. Ubuntu 18.04 (aws ec2). Error occurs when entering in sudo apt-get install -y libnginx-mod-http-passenger
[https://www.phusionpassenger.com/docs/tutorials/deploy_to_production/installations/oss/ownserver/ruby/nginx/](url)
The following packages have unmet dependencies:
libnginx-mod-http-passenger : Depends: passenger (= 1:6.0.4-1~bionic1) but it is not going to be installed
Depends: nginx-common (>= 1.14.0-0ubuntu1.6) but 1.10.3-0ubuntu0.16.04.5 is to be installed
E: Unable to correct problems, you have held broken packages.",0,0,msr
1794,"BTW, I also tried gcc 7.3 on gentoo-sources-4.18.10 (different machine) and the compile worked fine as well.",0,0,msr
1795,"Thanks for replies. About zfs binary stopped working I know, it is because I compiled a new version of zfs and still use old version of zfs-kmod.
I am using this kernel for a long time and compilation of zfs worked well always. My last successful compilation was about 4-6 commits ago to the master. I read this link about grsecurity, but didn't found a solution. Also, I don't see any grsecurity errors at dmesg while compiling. So now I have new zfs and old zfs-kmod and I am afraid to reboot, I don't know how to compile the same version in gentoo or back-forward for some commits.
I think the problem is related to some new code in last commits, the kernel version or gcc version. I am using zfs (master) 1.5+ years at 4.9.24-grsec and about a year on 4.9.74-grsec (minipli), and compilation always was successful. Please help me resolve these issues. Glibc-2.26-r7, gcc-7.3.0-r5 hardened profile, binutils-2.31.1",1,0,msr
1798,"Hello. Anything on this? Because of some commit (I think it is related to https://github.com/zfsonlinux/zfs/commit/7a23c81342df05ace730bd303b4a73854dba43dd), I can't use dappersec kernel anymore. I also tried with 4.9.132-dappersec, and the error is the same.
It was always compatible with grsecurity patches, please fix it. Thanks so much",0,0,msr
1802,"Hi, I just saw this. I will let someone know so a PR can be opened, meanwhile I had some other fixes written for a few other spots that need testing.",1,0,msr
1804,"Eh, after reviewing that other issue where someone is calling for a Code of Conduct (which is a fairly dumb idea), I'm surprised to see that you assumed that person was me (perhaps the intention of the troll all along). Since this begs finally some proper attention, I will describe this issue (feel free to remove the comment later as you obviously are seemingly easily to have your feelings hurt regardless of whether the other person intended so or not...).
I'll just clarify a few things: I've never asked for help in the channel and treated someone ""poorly"". I responded ironically to ptx0's messages, after he spent the past few hours being dismissive or condescending to other people, obviously in some cases users with a very limited skill around ZFS, but that does not merit acting like an imbecile. None of them were insults or hostile, they simply pointed out that ptx0 was assuming things constantly and then replying with an air of arrogance and usually in a condescending tone, despite himself being far from a beacon of excellence as far as technical skill or coding go. Immediately after he proceeded to ban me from the channel, to which I responded in -social pointing out that his attitude and actions making him look puerile and petty regardless of the nature of the ""offense"" by someone else. This knee-jerk reaction came motivated more by petty personal sentiments (and sharing those with some others who, well, fit that profile) than any actually reasonable reasons.
Now, on the project: you are routinely accepting patches quite obviously without really doing your due diligence, ensuring proper QA, etc. Case in point, the one we have here. It's painful to see that you waste more time on petty nothings over actually making your code or merged patches look like something with a modicum of technical quality. The control over whether contributors are just copy-pasting from other spots (*hint* licensing violations get people sued), if the patches actually conflict with major kernel features, or symbol exports, or..... is lacking all over the place.
How about instead of making up half-truths about people offending your feelings, or your buddies' feelings, you go and properly benefit from the resources you so adamantly defend (and ask others to work with): How difficult was it to grab the patch I sent you from the link, test it, test it again, sign it off and merge it? You need to go around deleting comments and engaging in conversation with the other troll, but you are too busy to actually merge an out of band patch while you waste time over petty fights? Well, good luck with that. And you are surprised ZoL is not taken seriously anywhere but by the homelab/non-enterprise crowd. If you want to be a primadonna, at least have the cards in your sleeve to back it up: you can't justify it right now with the standard you are providing your users. And it runs in ring0, so go figure a better way to fend off all those users popping up on IRC asking about destroyed data because you merged a patch from a pull-request that would not stand a moment in the LKML.",0,1,msr
1806,"The discussion in this thread (now hidden) has not been productive. In the future, please speak respectfully to fellow contributors. @behlendorf and I are working on putting a process in place for handling inappropriate behavior. In the mean time, please feel free to contact either Brian or I privately if you have concerns about the behavior of ZFSonLinux community members.",0,0,msr
1808,"Hello,
GitHub is not a support forum; it's an issue tracker. For support issues, please post in the [podmin support section](https://discourse.diasporafoundation.org/c/support/podmin-support) of our Discourse forum.",0,0,msr
1812,"Before you contribute, comment, or somehow interact with this project any further, I'd like you to read [our community guidelines](https://diasporafoundation.org/community_guidelines), as well as our [Code of Conduct](https://github.com/diaspora/diaspora/blob/develop/CODE_OF_CONDUCT.md). If you disagree with those rules and do not want to follow them, please stop interacting with this project.
When members of the project team, clearly indicated by a ""member"" badge in the header of all GitHub comments, ask you in a friendly tone do do something, then they do this because our project has rules, and everyone needs to follow them. In this case, GitHub is there to track software issues and track actionable feature requests. Support requests of any kind are handled on Discourse, as this allows more people to join. If you don't like this, well, too bad. This decision has been made by the majority of our community, and we for sure will not change our work just because you feel like it.",0,1,msr
1822,"Sorry if I sounded harsh, that wasn't my intent.
We've drawn a clear line between what goes in Gatling OSS and what goes in Gatling FrontLine:
all new things related to stats/exports/integrations go into Gatling FrontLine.
We're more than happy with contributions related to core Gatling features such as HTTP and users orchestration. But we can't make any exception to the above rule.",0,0,msr
1825,"@slandelle Thanks for the reply, but I still don't think you actually addressed the points I was trying to raise regarding this particular issue. I still don't understand how the suggested change is against your commercial strategy. The points I was trying to articulate are that this request:
- doesn't expose any data that would previously be inaccessible / enterprise.
- doesn't suggest structuring the results in a way that is easier to parse/interpret programatically
- only suggests what we believe is better practice by outputting using a logger which is the standard practice
- would allow for common, modern log shippers to ship your output with our creating message noise, where each println is interpreted as a new log event.
I literally don't see a single reason you would be against it this request. I've read through your comments multiple times and I still don't see a single argument you have given explaining why . > Sorry, but all new features regarding stats (additional stats, pushing stats elsewhere) fall into the scope of FrontLine, our Enterprise version.
Fair enough to that statement, but we're not talking about additional stats, pushing stats or anything about stats. We're talking about not using a logger for your output which causes problems when using industry standard shipping practices that expect structured logs.",1,0,msr
1829,"**This issue tracker is a tool to address bugs in Flask itself.
Please use the #pocoo IRC channel on freenode or Stack Overflow for general
questions about using Flask or issues not related to Flask.**
If you'd like to report a bug in Flask, fill out the template below. Provide
any extra information that may be useful / related to your problem.
Ideally, create an [MCVE](http://stackoverflow.com/help/mcve), which helps us
understand the problem and helps check that it is not caused by something in
your code.
---
### Expected Behavior
```flask-tutorial
$ coverage run -m pytest
Name Stmts Miss Branch BrPart Cover
------------------------------------------------------
flaskr/__init__.py 22 0 2 0 100%
flaskr/auth.py 54 0 22 0 100%
flaskr/blog.py 54 0 16 0 100%
flaskr/db.py 24 0 4 0 100%
------------------------------------------------------
TOTAL 154 0 44 0 100%
```
### Actual Behavior
```flask-tutorial
$ coverage run -m pytest
============================================================== test session starts ==============================================================
platform darwin -- Python 2.7.10, pytest-3.9.1, py-1.7.0, pluggy-0.8.0
rootdir: /Users/adamg9999/Development/flask-tutorial, inifile: setup.cfg
collected 24 items
tests/test_auth.py ........ [ 33%]
tests/test_blog.py ............ [ 83%]
tests/test_db.py .. [ 91%]
tests/test_factory.py .. [100%]
=========================================================== 24 passed in 1.15 seconds ===========================================================
```
for coverage > 4.0 need to first run `coverage run -m pytest` this will print the pytest output to STDOUT, then a followup `coverage report` will output the expected report:
```$ coverage report
Name Stmts Miss Branch BrPart Cover
------------------------------------------------------
flaskr/__init__.py 22 0 2 0 100%
flaskr/auth.py 54 0 22 0 100%
flaskr/blog.py 54 0 16 0 100%
flaskr/db.py 24 0 4 0 100%
------------------------------------------------------
TOTAL 154 0 44 0 100%```
### Environment
$ python --version
Python 2.7.10 :: Continuum Analytics, Inc.
(py2.7.10) adamg9999 flask-tutorial
$ flask --version
Flask 1.0.2
$ coverage --version
Coverage.py, version 4.5.1 with C extension",0,0,msr
1834,"I need to be able to push generated results to a DB, by exposing the report generator as public interface, I can provide a custom report generator which can push results to DB.",0,0,msr
1836,"Thanks, will have to move to a real open source tool like JMeter instead of gatling. Sad.",1,1,msr
1844,"I am having the same issue also with 1.7.3
```
Do not run Composer as root/super user! See https://getcomposer.org/root for details
Loading composer repositories with package information
Installing dependencies (including require-dev) from lock file
Dependency resolution completed in 0.003 seconds
Your requirements could not be resolved to an installable set of packages.
Problem 1
- Conclusion: remove symfony/polyfill v1.7.0
- Installation request for symfony/polyfill v1.7.0 -> satisfiable by symfony/polyfill[v1.7.0].
- Conclusion: remove symfony/stopwatch v4.1.1
- symfony/polyfill v1.7.0 requires symfony/intl ~2.3|~3.0|~4.0 -> satisfiable by symfony/symfony[v3.4.12].
- don't install symfony/stopwatch v4.1.1|remove symfony/symfony v3.4.12
- don't install symfony/symfony v3.4.12|don't install symfony/stopwatch v4.1.1
- Installation request for symfony/stopwatch v4.1.1 -> satisfiable by symfony/stopwatch[v4.1.1].
```",0,0,msr
1851,Why this change?,1,0,msr
1859,"Plase, add a step on the Step by Step Tutorial to demonstrate how to theme the just finished demo app.
## Motivation
1. Theming is not an obvious task for a newbie like me. Also, doc speaks about gem based and 'regular' file themes. It's confusin a bit.
2. At the end of demo app it's more beautiful to leave in the han of the user a well looking demo
## Idea
Please include istructions on - how to enable default gem base theme
- how to customized if possible a gem theme
- how to switch to a regular file based theme
- idem, how to customize it
... or ...
Create a guide on how to start theming AND then include previous steps into the new tutorial.
Thanks for this excellent piece of software !!!!",0,0,msr
1867,"If I see a tutorial on an external website it soon or later will become obsolete.
If I see a tutorial on main site I think that must be ok, tested, and updated. Simply. Close this issue
A company that does not understand the need of a full doc do not will offer a serious support in the long time
Bye bye.",0,0,msr
1873,"> The GeoJSON field wasn't one of priority. The GeoPoint (aka Map) is a priority for example. > However, the work done here seems pretty good. I would love to see it merged to the project.
Thanks! I would be more than happy to help in any way.
> I'm not sure to understand the questions:
> > Where do you expect to store the vectors tiles?
The vector tiles that I'm currently using comes from MapTiler Cloud ([see pricing here](https://www.maptiler.com/cloud/plans/)). I'm currently using the free plan which allows upto 100,000 requests a month. I guess what I'm asking is if that would be enough. If not, we will have to look for other free options (which I'm sure are available, I just haven't spent enough time researching on the same).
> Can you submit the PR so we'll be able to review the code and continue the work together,
I did this the same time I opened this issue. You can find it [here.](https://github.com/strapi/strapi/pull/2526)",0,0,msr
1889,"This will come with the custom field feature. With that, every field type will be able to be supported.
I'm locking this issue. We well understood the need.
Thank you all for your contribution.",0,0,msr
1892,"# [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=h1) Report
> Merging [#8255](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=desc) into [master](https://codecov.io/gh/zfsonlinux/zfs/commit/0a10863194b0e7c1c64f702f868c10d5dac45ea5?src=pr&el=desc) will **decrease** coverage by `0.04%`.
> The diff coverage is `88.01%`.
[![Impacted file tree graph](https://codecov.io/gh/zfsonlinux/zfs/pull/8255/graphs/tree.svg?width=650&token=NGfxvvG2io&height=150&src=pr)](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=tree)
```diff
@@ Coverage Diff @@
## master #8255 +/- ##
==========================================
- Coverage 78.33% 78.29% -0.05% ==========================================
Files 380 382 +2 Lines 115719 116678 +959 ==========================================
+ Hits 90653 91350 +697 - Misses 25066 25328 +262
```
| Flag | Coverage Δ | |
|---|---|---|
| #kernel | `78.93% <86.25%> (+0.04%)` | :arrow_up: |
| #user | `65.8% <87.99%> (-1.21%)` | :arrow_down: |
------
[Continue to review full report at Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=footer). Last update [0a10863...3a184b8](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",0,0,msr
1897,Is it policy to license all new files under CDDL? May authors supply code under a more permissive license? (Sorry for the OT),1,0,msr
1899,"I'm going to be closing this PR. @behlendorf has been doing some major re-working of the original TRIM work to, among other things:
- Harmonize its operation with that of device initialization so it's possible, for example, to TRIM individual vdevs.
- Add per-vdev trim stats
- Re-work the underlying discard mechanism to use a new ZIO type rather than the DKIOCFREE ioctl.
- Utilize the new ""xlat"" vdev method from device initialization where appropriate.
- Switch from the legacy ioctl interface to the new nvlist-style ioctl interface.
- Plenty of other restructuring, in particular w.r.t. the management of the trim threads.
Expect a new PR to be posted soon.",0,0,msr
1900,"### System information
<!-- add version after ""|"" character -->
Type | Version/Name
--- | --- Distribution Name	| Fedora
Distribution Version	| 30 (rawhide)
Linux Kernel	| 5.0.0-0.rc1.git0.1.fc30.x86_64
Architecture	| x86-64
ZFS Version	| master
SPL Version	| master
<!-- Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version -->
### Describe the problem you're observing
`current_kernel_time64()` was removed from recent kernels. Looks like `ktime_get_coarse_real_ts64()` should be used instead.
https://lkml.org/lkml/2018/7/11/202
https://lkml.org/lkml/2018/12/17/110
### Describe how to reproduce the problem
Build master in rawhide
### Include any warning/errors/backtraces from the system logs
``` CC [M] /home/hutter/zfs/module/icp/illumos-crypto.o
In file included from /home/hutter/zfs/include/spl/sys/condvar.h:33,
from /home/hutter/zfs/include/sys/zfs_context.h:38,
from /home/hutter/zfs/include/sys/crypto/common.h:39,
from /home/hutter/zfs/module/icp/illumos-crypto.c:35:
/home/hutter/zfs/include/spl/sys/time.h: In function ‘gethrestime’:
/home/hutter/zfs/include/spl/sys/time.h:76:8: error: implicit declaration of function ‘current_kernel_time64’; did you mean ‘core_kernel_text’? [-Werror=implicit-function-declaration]
*ts = current_kernel_time64();
^~~~~~~~~~~~~~~~~~~~~
core_kernel_text
/home/hutter/zfs/include/spl/sys/time.h:76:6: error: incompatible types when assigning to type ‘inode_timespec_t’ {aka ‘struct timespec64’} from type ‘int’
*ts = current_kernel_time64();
^
/home/hutter/zfs/include/spl/sys/time.h: In function ‘gethrestime_sec’:
/home/hutter/zfs/include/spl/sys/time.h:86:24: error: invalid initializer
inode_timespec_t ts = current_kernel_time64();
^~~~~~~~~~~~~~~~~~~~~
cc1: all warnings being treated as errors
```",0,0,msr
1901,Looks related to https://lore.kernel.org/patchwork/patch/562186/,0,0,msr
1903,"The underlying issue comes from kernel commit 12209993e98c5fa1855c467f22a24e3d5b8be205 (""x86/fpu: Don't export __kernel_fpu_{begin,end}()""), which unexports __kernel_fpu_begin(), and causes the configure tests not to define HAVE_FPU_API_H. Changing the header locations will just lead to another failure later because kernel_fpu_begin/end() are exported GPL, and can't be used directly from the zfs module.
See the thread on LKML starting here: https://marc.info/?l=linux-kernel&m=154689892914091
Marc",0,0,msr
1912,"> You probably don't understand what `git remote prune origin` does...
Every time I disagree with you, things escalate quickly. I do not want to do this again.",0,1,msr
1914,"e.g. something like this:
```lua
Spring.GiveOrderToUnit(unitID,
CMD.INSERT,
{-1,CMD.ATTACK,''CMD.OPT_SHIFT'',''unitID2''},
{""timeout"" = 500 }
);
```
Related to https://springrts.com/mantis/view.php?id=6128",0,1,msr
1920,"@abma I wasn't aware, perhaps a [pull request template](https://github.blog/2016-02-17-issue-and-pull-request-templates/) would help document this? That way it would be clear to anybody creating a pull request that they need to apply it to the `develop` branch not the `master` branch, even a prompt to link to any related Mantis tickets
---
As a general note, as a part of my job and general activities I regularly make and comment on pull requests on a wide range of projects on a daily basis, but this experience was poor, and below average, and entirely avoidable.
I don't know if the rude response from @rtri was because english isn't the primary language, or habit, but it's very easy to be polite and at the same time close responses. GitHub provides various processes to avoid these.
For example, the original comment doesn't explain why the PR was closed. If it had instead just been:
> Thanks, it's been fixed in [be99910](https://github.com/spring/spring/commit/be999101be20d10fe46ce6921d7ec7d88ccdece5)
That would have been perfectly acceptable on its own. The follow up comment was also catty and rude. Or a review left that was literally just the comment he left, requesting changes.",0,1,msr
1929,@Aurelsicoko for now I will not be able to dedicate time. 😕,0,0,msr
1934,"And you removed my issue now, wow, good!
Do you know what ? I'll try to set my framework without SFML because I see SFML is not a serious projet.",0,1,msr
1939,"I'm finding a similar issue when you try and parse a time that is incompatible with daylight savings time (i.e. 2:00 AM until 3:00 AM doesn't exist when we ""spring ahead"" in the states). You get an NA and a failed to parse warning. Technically this makes sense but maybe this should be an Error rather than a warning, and maybe a more informative message. Surprisingly this doesn't break the ""strptime"" function. ```r library(lubridate)
test_date <- '03/10/19 02:00 AM'
mdy_hm( test_date, tz = 'America/New_York')
#[1] NA
#Warning message:
# 1 failed to parse. ```
Using strptime ```r d2 <- strptime(test_date, format = '%m/%d/%y %I:%M %p', tz = 'America/New_York')
print( d2 )
# [1] ""2019-03-10 02:00:00""
attributes(d2)
# $names
# [1] ""sec"" ""min"" ""hour"" ""mday"" ""mon"" ""year"" ""wday"" ""yday"" ""isdst"" # [10] ""zone"" ""gmtoff""
# $class
# [1] ""POSIXlt"" ""POSIXt"" # $tzone
# [1] ""America/New_York"" ""EST"" ""EDT"" ``` ```r sessionInfo()
R version 3.5.3 (2019-03-11)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.4
Matrix products: default
BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
attached base packages:
[1] stats graphics grDevices utils datasets methods base other attached packages:
[1] shiny_1.3.0 reprex_0.2.1 lubridate_1.7.4
loaded via a namespace (and not attached):
[1] Rcpp_1.0.1 compiler_3.5.3 later_0.8.0 remotes_2.0.3 [5] prettyunits_1.0.2 tools_3.5.3 pkgload_1.0.2 digest_0.6.18 [9] pkgbuild_1.0.3 jsonlite_1.6 evaluate_0.13 memoise_1.1.0 [13] rlang_0.3.4 cli_1.1.0 rstudioapi_0.10 yaml_2.2.0 [17] xfun_0.6 withr_2.1.2 stringr_1.4.0 knitr_1.22 [21] desc_1.2.0 fs_1.2.7 devtools_2.0.2 rprojroot_1.3-2 [25] glue_1.3.1 R6_2.4.0 processx_3.3.0 rmarkdown_1.12 [29] sessioninfo_1.1.1 callr_3.2.0 clipr_0.5.0 magrittr_1.5 [33] whisker_0.3-2 usethis_1.5.0 backports_1.1.3 ps_1.3.0 [37] promises_1.0.1 htmltools_0.3.6 rsconnect_0.8.13 assertthat_0.2.1 [41] mime_0.6 xtable_1.8-3 httpuv_1.5.1 stringi_1.4.3 [45] miniUI_0.1.1.1 crayon_1.3.4 ```",0,0,msr
1945,"This is a bug and it has to do with the fact that arithmetics with periods are not well defined. > I've read the docs in ?Period-class, and I know you want this reversible property:
That rule is unfortunate. There is no solid reason to follow it and it caused a lot of trouble with the package. There is a new lower level package `timechange` which has a clear defined semantics and allows a full control over how things are rolled both on month change and DST.
```R
> timechange::time_add(time, minutes = 1160)
[1] ""2019-03-10 03:42:03 PDT""
```
I wanted to roll lubridate on top of timechange for at least a year now, but just don't have time. Hopefully by the end of this year.",1,0,msr
1950,"For anyone doing arithmetic with _hours, minutes, or seconds_ with lubridate, you likely want to use Durations objects (like `dhours()`) rather than Period objects (like `hours()`). For arithmetic with more granular units of time like days, months or years, you probably want to stick with Period objects.
So, if using lubridate, my advice for @Demetrio92 and @robertwwalker is to switch to `dminutes()` and `dhours()` as that seems to be what you are expecting. That will match your intuition if you were expecting a result equal to ""sitting in a chair for 1160 minutes, then looking at the clock"" (assuming that your clock automatically adjusts itself for DST).
You could also try using [clock](https://clock.r-lib.org/). The POSIXct API in clock defaults to the behavior I described in the first paragraph, so it might feel more intuitive. It also errors when landing on a nonexistent time in a DST gap, or on an ambiguous time in a DST fallback, rather than returning `NA`.
@Demetrio92's original example:
```r
library(clock)
library(lubridate)
library(magrittr)
# This example has to do with DST Gaps (nonexistent times)
x <- as.POSIXct(""2019-03-09 07:22:03"", tz = ""US/Pacific"")
x
#> [1] ""2019-03-09 07:22:03 PST""
# This is lubridate Period addition:
# - converts to naive (2019-03-09 07:22:03, with no assumed time zone)
# - adds 1160 minutes (2019-03-09 07:22:03 -> 2019-03-10 02:42:03)
# - converts back to US/Pacific, but this is now in a DST gap from
# 01:59:59 -> 03:00:00, so lubridate returns NA
x + minutes(1160)
#> [1] NA
# clock - With POSIXct, hours, minutes, and seconds add like lubridate Durations
add_minutes(x, 1160)
#> [1] ""2019-03-10 03:42:03 PDT""
# so clock matches this result of lubridate Duration arithmetic:
x + dminutes(1160)
#> [1] ""2019-03-10 03:42:03 PDT""
# To get the lubridate Period behavior in clock if you want it
nt <- x %>%
as_naive_time() %>%
add_minutes(1160)
# naive time with no implied time zone - note that this would be in a # DST gap if we tried to apply a US/Pacific zone to it
nt
#> <time_point<naive><second>[1]>
#> [1] ""2019-03-10 02:42:03""
# unlike lubridate, clock treats this as an error rather than returning NA
as.POSIXct(nt, tz = date_zone(x))
#> Error: Nonexistent time due to daylight saving time at location 1. Resolve nonexistent time issues by specifying the `nonexistent` argument.
# control how to handle this nonexistent time with `nonexistent`,
# use `""NA""` to match lubridate behavior
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""roll-forward"")
#> [1] ""2019-03-10 03:00:00 PDT""
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""roll-backward"")
#> [1] ""2019-03-10 01:59:59 PST""
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""NA"")
#> [1] NA
```
@robertwwalker's example:
```r
library(clock)
library(lubridate)
library(magrittr)
# This example has to do with DST Fallbacks (ambiguous times)
x <- as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
x
#> [1] ""2021-04-04 02:15:00 NZDT""
# This is lubridate Period addition:
# - converts to naive (2021-04-04 02:15:00, with no assumed time zone)
# - adds one hour (02 -> 03)
# - converts back to Pacific/Auckland with no issues, but the result is
# potentially not intuitive
x + hours(1)
#> [1] ""2021-04-04 03:15:00 NZST""
# clock - With POSIXct, hours, minutes, and seconds add like lubridate Durations
add_hours(x, 1)
#> [1] ""2021-04-04 02:15:00 NZST""
# so clock matches this result of lubridate Duration arithmetic:
x + dhours(1)
#> [1] ""2021-04-04 02:15:00 NZST""
# To get lubridate Period behavior in clock if you want it
x %>%
as_naive_time() %>% # this is the key
add_hours(1) %>%
as.POSIXct(tz = date_zone(x))
#> [1] ""2021-04-04 03:15:00 NZST""
```",0,0,msr
1953,"> > However: dminutes should not act different from minutes. This is confusing at best.
> > I disagree. The whole reason there are two functions for adding minutes is that they work differently. I would encourage you to read the docs for [Durations](https://lubridate.tidyverse.org/reference/duration.html) and [Periods](https://lubridate.tidyverse.org/reference/period.html) again, as they are quite different.
Reading `period` docs
> Within a Period object, time units do not have a fixed length (except for seconds) until they are added to a date-time. ... When math is performed with a period object, each unit is applied separately. How the length of a period is distributed among its units is non-trivial. For example, when leap seconds occur 1 minute is longer than 60 seconds.
> Periods track the change in the ""clock time"" between two date-times. They are measured in common time related units: years, months, days, hours, minutes, and seconds. Each unit except for seconds must be expressed in integer values.
I do not see why this description contradicts my expected output. Specifically it says stuff like >or example, when leap seconds occur 1 minute is longer than 60 seconds. My impression here would be that periods are more robust than durations. -----
I generally agree, the issue is more deep than just two functions that supposed to do the same. Still, worst case, rename `minutes` into `pminutes`. ----
I am glad `period` now contains a warning. At least people who carefully study the docs will be aware. > Note: Arithmetic with periods can result in undefined behavior when non-existent dates are involved (such as February 29th in non-leap years). Please see Period for more details and %m+% and add_with_rollback() for alternative operations.
Coming from postgres I just did `+minutes()` and it worked as expected without any issues until it didn't. I wasn't even aware there were two ways of doing this in `lubridate`",0,0,msr
1957,"I've been thinking (too much probably) about the period-duration distinction.
Imagine the (completely implausible I hope) scenario where the time-lords decide that they're going to do a leap second at the same time as a daylight savings change.
Here's how I'd expect `hours` and `dhours` to act:
""2021-04-04 02:15:00 NZDT"" + hours(1) == ""2021-04-04 02:15:00 NZST""
""2021-04-04 02:15:00 NZDT"" + dhours(1) == ""2021-04-04 02:14:59 NZST""
On the other hand when it comes to `days` I have no idea what I should even expect. It doesn't appear to be reversible currently though:
```
> as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
[1] ""2021-04-04 02:15:00 NZDT""
> > as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"") + days(1)
[1] ""2021-04-05 02:15:00 NZST""
> > as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"") + days(1) - days(1)
[1] ""2021-04-04 02:15:00 NZST""
```
I'd also like to add that all of this is really annoyingly complex and I'm really glad there are people who are not me making it easier to deal with. So a big thank you to everyone involved in lubridate development.",0,0,msr
1958,"> Here's how I'd expect hours and dhours to act:
Hmm, @pitakakariki from this example it looks to me like you might misunderstand how Period and Duration are supposed to be working. Let me try explaining. I'll use types from clock, since they allow me to explicitly show the intermediate steps that are happening under the hood.
``` r
library(clock)
x <- date_time_parse(""2021-04-04 02:15:00"", ""Pacific/Auckland"", ambiguous = ""earliest"")
x
#> [1] ""2021-04-04 02:15:00 NZDT""
# Adding a lubridate ""Period"" is like:
# - Dropping the original time zone completely
# - Adding the unit of time
# - Adding the original time zone back (if possible)
# Adding a lubridate ""Duration"" is like:
# - Converting the original time zone to UTC (where DST never affects you)
# - Adding the unit of time
# - Adding the original time zone back (which is always possible)
# We can show these explicitly with clock's naive-time and sys-time types, which
# mimic dropping the original time zone and converting to UTC respectively.
```
``` r
## Period example:
# Notice this has no implied zone attached
nt <- as_naive_time(x)
nt
#> <time_point<naive><second>[1]>
#> [1] ""2021-04-04 02:15:00""
# Now we add the time. Since there is no implied time zone, this doesn't
# have to do with the DST fallback whatsoever.
nt_plus_hour <- nt + duration_hours(1)
nt_plus_hour
#> <time_point<naive><second>[1]>
#> [1] ""2021-04-04 03:15:00""
# Now we convert back to Pacific/Auckland. Since the 3 o'clock hour is past
# the DST fallback, there is no ambiguity in the conversion, but it may or
# may not be what you want.
as.POSIXct(nt_plus_hour, ""Pacific/Auckland"")
#> [1] ""2021-04-04 03:15:00 NZST""
x + lubridate::hours(1)
#> [1] ""2021-04-04 03:15:00 NZST""
```
``` r
## Duration example:
# This is in UTC time, notice how the hour has shifted
st <- as_sys_time(x)
st
#> <time_point<sys><second>[1]>
#> [1] ""2021-04-03 13:15:00""
# Now we add the time in UTC. DST can never affect this.
st_plus_hour <- st + duration_hours(1)
st_plus_hour
#> <time_point<sys><second>[1]>
#> [1] ""2021-04-03 14:15:00""
# Now convert back to Pacific/Auckland. This just shifts by the appropriate
# UTC offset to get the Pacific/Auckland time.
as.POSIXct(st_plus_hour, ""Pacific/Auckland"")
#> [1] ""2021-04-04 02:15:00 NZST""
x + lubridate::dhours(1)
#> [1] ""2021-04-04 02:15:00 NZST""
```
<sup>Created on 2021-04-07 by the [reprex package](https://reprex.tidyverse.org) (v1.0.0)</sup>",0,0,msr
1960,"As Vitalie mentioned here https://github.com/tidyverse/lubridate/issues/759#issuecomment-536286765, the reversibility rule is probably not the best idea for date time arithmetic, even if it sounds like a good idea (I agree with him now). So I wouldn't base too many assumptions on it.",0,0,msr
1964,"I have 20+ screenshots of moderators running Slack channels (1000's of members in Rails) who are using Slack as their personal bully pulpit. Last night was the last straw for me. I saw a moderator call a girl a troll after another guy made sexist comments to her. The moderators are acting like bullies. I love Rails. But I am going to drop it like a hot potato if the creators of Ruby and Rails don't come up with some way to establish specific CoC's. Right now, moderators are going around calling people names like trolls, cursing left and right and causing arguments. Then they accuse everyone else of what they started. There needs to be some kind of controls and certifications that is granted as an authorized representative of Rails before anyone can open a group under the copyright. A lot of the Rails groups have chat rooms for politics and unrelated matters. Yet, half the CoC forbids talking about those things. I've seen entire Rails groups where a few friends of the moderators use it as their political pulpit. Then they lure people into discussions and ban them when they don't agree.
This is just 1 example of how a moderator talks to people.
![image](https://user-images.githubusercontent.com/4573756/55576276-ea96f180-56de-11e9-96bf-ee91f6edd312.png)
I have not seen this level of behavior on any other community. Not Python, NYC, Clojure, Java etc... It's only been on Rails/Elixir groups over the past 2 years.",1,1,msr
1970,"it looks like your talking about capcom games watch this to understand whats going on and it is indeed 4:3
https://www.youtube.com/watch?v=LHfPA4n0TRo&feature=youtu.be",1,0,msr
1974,You just aren’t getting what I’m saying. I’m going to have to post some screenshots later to help illustrate what’s going on currently and what should be going on instead.,0,1,msr
1999,"> > > there is a reason for it to be there clearly this isint a front end issue. It cant guess if you have rotated you monitor 90 degrees physically you need to deal with this as a core option if that is the case.
The resolution/aspect ratio output by the emulator should remain unchanged by the core- see above. A vertical game that is 320x240 should be output as 320x240. The frontend options for video rotation and aspect ratio are more than sufficient and solve the problems related to the way things are currently handled, which are, to wit:
1. it's incorrect according to what the original hardware does
2. it's counter-intuitive
3. it results in scaling artifacts with integer scaling
If people want to play vertical games rotated 90 degrees on their monitors, then they should use the frontend for that. The emulator/core shouldn't alter the actual output.",0,1,msr
2010,"there is no front end issue some arcades have different rotations not just 90 degrees. The core needs to work this out the user will need to be more specific from what he says he wants vertical games to display like this by default which is perfectly valid if you have a rotated monitor.
![fshark-190407-183227](https://user-images.githubusercontent.com/6128601/55691157-04605f00-5993-11e9-8e9b-f51fa4f66dde.png)
this is how it displays when you dont have a rotated monitor the user seems to think this is wrong
![fshark-190407-183154](https://user-images.githubusercontent.com/6128601/55691184-54d7bc80-5993-11e9-8fe8-57f884e15aa1.png)",0,0,msr
2013,"> This whole discussion is pointless.
> > > and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic
> > There is not need to guess anything, this is software development, there is an API, an implementation, and a frontend.
> > There is an API for this:
> https://github.com/libretro/RetroArch/blob/master/libretro-common/include/libretro.h#L486
> > So the players are:
> > * core
> > * api
> > * frontend
> > > Who knows the content needs rotation? The core does.
> The environment callback is a set, which means it's telling the frontend to ""do something""
> > So what should happen could go two ways
> > a. The core tells the frontend: ""hey video is rotated, adjust aspect ratio from what I reported accordingly""
> b. The core tells the frontend: ""hey this content requires rotation, do whatever you need to do so it works properly""
> > That's all. This is a frontend problem, but before anything can be do about it what we need is clarification from the API side so we can adjust both the frontend and the cores to do whatever needs to be done.
> That's all.
I don't understand all of this, but it sounds very reasonable. Yes, it does seem grant2258 and I have been going in circles with this. I think I've provided enough info on the problem as it currently stands to work towards a solution, but since I'm not a programmer, I've probably done all I can by this point.",1,0,msr
2016,"> > > i agree with that @CharlesBukowski im sure @fr500 has some idea looking forward to seeing what he is going to do since he thinks there is an issue and what exactly is wrong as i cant see any issue at all with mame2003 or fba on libretro it both have the ability rotate or not rotate vertical games. It does it the same way as mainline fba and mame.
> > So i wll digress im at a bit of a loss what you two seem to think the issue is is you want to maintain a 4:3 ratio you need to rotate the Monitor or physically force the aspect ratio to 4:3 when rotating and put up with the streched gfx
1. automatically switching the width/height for vertical oriented games is altering the output resolution, which is incorrect. The emulator should just output the resolution completely unaltered. Altering what is output by the emulator should always be done by the frontend, or through an option that the user has to manually select, but maybe that's just my opinion. The default should just be whatever the emulator spits out before you start doing stuff to it. 2. rotating the game is something that can be easily handled in the frontend without all of the confusion that currently exists 3. 2 is something that can even be done automatically if desired by the user, through the frontend as explained by fr500.
4. the current method of automatically switching the height/width with vertical games will always result in scaling artifacts no matter what you set the aspect ratio to (unless it's a multiple of both 240 and 320). How can this be considered correct? 5. the current method of automatically switching the height/width of vertical games isn't even making things easier for the user in all cases, because video_allow_rotate = true/false isn't doing the same thing in all cores. The current method is just adding more confusion. My brain is tired. Hopefully the thread I linked to above sheds some additional light on the problem.",0,0,msr
2018,"Ok here is the screenshots with you information the core geometry is reporting the right resolution. I explained before above that the information you need. Fba by default comes with setup like rotated i installed it and checked. It does not display vertical none rotated by default. Just giving you information in one place here not debating at all
rotated
![Untitled](https://user-images.githubusercontent.com/6128601/55694766-85c4eb00-59ad-11e9-8938-fbe965c7f255.png)
tate mode
![Untitled1](https://user-images.githubusercontent.com/6128601/55695277-cd4c7680-59af-11e9-81de-deb700b117b2.png)
fba default install no setting changed
![fba default](https://user-images.githubusercontent.com/6128601/55695612-426c7b80-59b1-11e9-8938-b8717439adfc.png)
Ill add one more this is screen shot with fba pay attention to the width and height
![res](https://user-images.githubusercontent.com/6128601/55737000-526d7500-5a1c-11e9-9815-dc3a78be4ad0.png)",0,0,msr
2027,"i literally cant even figure your problem out the games will show sideways if they arent rotated. mame fba and lr cores do this. So i guess i leave it at this not wasting more time on you back tracking it not a good default leaving vertical games sideways. Something you claimed fba done
https://github.com/libretro/RetroArch/issues/8551#issuecomment-480630788",0,1,msr
2031,"> > > > you rotated the image what do you expect to happen? you creen shots prove nothing accept the video is rotated fba and mame2003 create the same images
> > That's exactly what I expect to happen. Do you see that you're just not following what I'm saying very well?
No they don't, and I've provided numerous examples demonstrating this.",0,1,msr
2049,"FreeRADIUS 3.0.19 doesn't build with libressl 2.8.3 any more, it shows the following build warnings:
src/main/tls.c:3286:33: warning: incompatible pointer types passing 'SSL_SESSION *(SSL *, unsigned char *, int, int *)' (aka 'struct ssl_session_st *(struct ssl_st *, unsigned char *, int, int *)') to parameter of type 'SSL_SESSION *(*)(struct ssl_st *, const unsigned char *, int, int *)' (aka 'struct ssl_session_st *(*)(struct ssl_st *, const unsigned char *, int, int *)') [-Wincompatible-pointer-types]
SSL_CTX_sess_set_get_cb(ctx, cbtls_get_session);
^~~~~~~~~~~~~~~~~ /usr/local/include/openssl/ssl.h:730:20: note: passing argument to parameter 'get_session_cb' here
SSL_SESSION *(*get_session_cb)(struct ssl_st *ssl,
^
src/main/tls.c:3383:3: warning: implicit declaration of function 'SSL_CTX_set_num_tickets' is invalid in C99 [-Wimplicit-function-declaration]
SSL_CTX_set_num_tickets(ctx, 1);
^
src/main/tls.c:3396:3: warning: implicit declaration of function 'SSL_CTX_set_num_tickets' is invalid in C99 [-Wimplicit-function-declaration]
SSL_CTX_set_num_tickets(ctx, 0);
^
This leads to this linker error:
LINK build/bin/radiusd
/usr/bin/ld: error: undefined symbol: SSL_CTX_set_num_tickets
>>> referenced by tls.c
>>> build/objs/src/main/tls.o:(tls_init_ctx)
See also [FreeBSD Bug 237216](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=237216)
The following simple patch fixes the build:
--- src/main/tls.c.orig 2019-04-12 06:47:56 UTC
+++ src/main/tls.c
@@ -1579,7 +1579,7 @@ done:
return 0;
}
-#if OPENSSL_VERSION_NUMBER < 0x10100000L || defined(LIBRESSL_VERSION_NUMBER)
+#if OPENSSL_VERSION_NUMBER < 0x10100000L
static SSL_SESSION *cbtls_get_session(SSL *ssl, unsigned char *data, int len, int *copy)
#else
static SSL_SESSION *cbtls_get_session(SSL *ssl, const unsigned char *data, int len, int *copy)
@@ -3379,14 +3379,14 @@ post_ca:
*/
SSL_CTX_sess_set_cache_size(ctx, conf->session_cache_size);
-#if OPENSSL_VERSION_NUMBER >= 0x10101000L
+#if OPENSSL_VERSION_NUMBER >= 0x10101000L && !defined(LIBRESSL_VERSION_NUMBER)
SSL_CTX_set_num_tickets(ctx, 1);
#endif
} else {
SSL_CTX_set_session_cache_mode(ctx, SSL_SESS_CACHE_OFF);
-#if OPENSSL_VERSION_NUMBER >= 0x10101000L
+#if OPENSSL_VERSION_NUMBER >= 0x10101000L && !defined(LIBRESSL_VERSION_NUMBER)
/*
* This controls the number of stateful or stateless tickets
* generated with TLS 1.3. In OpenSSL 1.1.1 it's also",0,0,msr
2051,"It isn't that simple. The two checks I added are about OpenSSL 1.1.1, there is no libressl version yet that's compatible with that. So there's no way to do these checks for specific version numbers.
Leaves me with the one I removed; this could indeed be replaced with a specific version -- I assume libressl did that API change in version 2.8.0, but I'm not sure about it.",0,0,msr
2071,"Please stop being so negative.
> The RA project made it clear that the scanner is a afterthought with a rather impressive number of ways it can go wrong and no interest in doing anything to get data the original dump sets didn't provide
Send us PRs to deal with these issues. The communication couldn't have been clearer. No walls of text are going to fix anything here on its own.
You might be mistaken into thinking we have infinite amounts of contributors, manpower and time to do all this - we don't. We depend on the generosity of coders to help the project forward, and we can't exactly rely on programs like Google Summer of Code either.",1,0,msr
2073,"> I opened the issues for about, oh 1 year? Possibly more. You yourself even mentioned 'uh maybe we shouldn't do this guys' on one of the worst examples and then someone marked the bug as 'minor'.
> #7455 (comment)
>So no, i'm not going to do your features for you.
Stop being abrasive and show some respect. Last warning before I get Github moderators involved in here. Any kind of passive aggressive nonsense you can leave at the door from now on, keep it strictly focused on the factual and behave yourself. Nobody has time for this nonsense and this is supposed to be a drama-free developer zone. If you're not a developer, your ability to contribute to this project is limited to say the least, and walls of text don't amount to any code anybody can do anything with.
You say ""I'm not going to do your features for you"". The way I see it, YOU want us to do all this for you when I have indicated to you already it's not a fact of us wanting to do it or not, it's a matter of not having the necessary manpower for it and resources already being spread thin. Once again, stay polite while you're posting in these threads and don't cause a scene.
Being developers on open source projects is already enough of a burden and thankless job without having to deal with toxic personalities like the one you're portraying right now. Don't make our jobs even harder.",0,1,msr
2074,"Here's the facts:
RA is using serials, which lead to duplicates. It's also ditching certain emulator specific configurations of roms, lke MAME split sets (which could be supported by just taking the crc of the main game archive and pretend the others are not important). So the project decided to 'canonize' some particular sets, sometimes, and fuzzify most times.
This is still not enough to both fuzzily 'identify' certain games (games are missing), so people are asking for even more fuzzy filenames. I disagree with this because if the 'game is missing' even with serial, there is something else seriously wrong (what? Probably the scanner is dying on a untested hill somewhere on a set. Misidentifying sets of a console for another is my guess, that whole mess of scanner looking at a cd dump 'magic bytes' to find out what type of console it's supposed to be, which is not very resistant to different types of fileformat).
It's also *misidentifying* game hacks, because naturally they don't bother changing serials.
Finally, RA after getting the first entry of a game list (or the last don't recall) puts its (possibly bogus) CRC on the playlist (not certain, but probably because there are features and code that needs a crc).
Any future feature that depends on crc for uniqueness (say, web data, autoconfigs, auto tweaks, netplay) is built on quicksand.",0,0,msr
2081,"We have the option to be less aggressive, so let's go ahead and do that. There's no need to comply with the idiosyncratic norms of IRC culture in the labelling of UI features.",0,0,msr
2088,"I understand you are unhappy with the change in commit https://github.com/gradle/gradle/commit/dbb85f50bb184cc128949d4eec039b9ed30fc170. The change is part of the stabilizing process that every [Incubating API](https://docs.gradle.org/current/userguide/feature_lifecycle.html#sec:incubating_state) goes through. The summary of the contract is when an API is tagged as incubating, the users should be expecting breaking change without notice. You are welcome to submit a PR for this breaking change.
On another note, it would be preferable for your next issue to follow the issue template as well as reading over the [Code of Conduct](https://gradle.org/conduct/).",0,0,msr
2097,"You can add custom fields attributes (⚠️ not relations ⚠️ ) by creating a custom hook.
Make sure to put run it first by updating the config in `config/hook.json`
```json
{
""timeout"": 3000,
""load"": {
""before"": [""custom-hook""],
""order"": [
""Define the hooks' load order by putting their names in this array in the right order""
],
""after"": []
}
}
```
`hooks/custom-hook/index.js`
```js
const _ = require('lodash');
module.exports = () => {
return {
initialize(cb) {
_.merge(strapi.admin.models.administrator, {
attributes: {
randomColum: {
type: 'string',
},
},
});
cb();
},
};
};
```
We either can allow modifiy the model via json like the other and in the interface but this will make the possbile errors too important in my opinion.
I would like to provide and fairly simple api to customize a model on startup.
example:
```js
strapi.models('administrator').extend({
atttributes: {
newField: {},
newRelation: {},
},
});
```",0,0,msr
2100,(or convert the whole admin package itself into a plugin),0,0,msr
2113,"This issue has been mentioned on **Strapi Community**. There might be relevant details there:
https://forum.strapi.io/t/extend-user-admin-model/713/1",0,0,msr
2116,Here's an official answer: https://github.com/libretro/RetroArch/pull/8910#issuecomment-502708016,1,0,msr
2117,"Thank you for responding.
In my opinion it's awful but like someone told me recently ""not my circus, not my monkeys""
I'll stick to the current codebase until things stabilize.",1,0,msr
2118,"It's how the original SSNES/RetroArch was like, with most of the code that was split into separate driver files inside one big runloop file. This allows for better restructuring of the codebase by getting most global state in one central place, and less passing around of state everywhere.
Anyway, it's an iterative process, and it's done to make the program last and become more maintainable in the long run. We cannot get there with all these separate files in their current form with all sorts of global state all over the place. And as I keep maintaining, the majority of the code outside of retroarch.c and libretro-common should become 80 to 90% tasks, thread-safe and all. Anything global has to go into one centralized place, so that we can make order out of chaos. Not to mention it's more optimal this way as well. The code outside retroarch.c should become a lot more pure, no longer relying so much on global state everywhere, so that we can convert them easier into tasks. For this and other reasons, we need to do this drastic rebuilding.",0,0,msr
2120,"Branches are a non-issue, the refactors are already done piecemeal. Anyway, I'm the leader of this project, and I am making this decision to do it this way. The code more or less stayed the same anyway, it's a simple matter of the driver files being moved into one big file, and more functions turned static.
> Tasks are fine for certain things but are a huge headache when you need things to run in order.
Of course they are a headache, but it eventually has to happen anyway. CPUs are only getting more cores instead of faster clock speeds so allowing for stuff to be put on multiple threads at the same time is a very definite help (for things that can be done out of sequence), the main UI cannot block, many of these tasks on the main thread do block, which is bad. It's bad for Android, iOS, it's bad for the A/V synchronization timer, etc. It's bad for platforms like UWP where synchronous file I/O is very heavily penalized. It's a simple case of dealing with the headache because this is the only way forward.",0,0,msr
2121,"It was also an opportunity of getting rid of all these ctl functions (camera_driver_ctl, location_driver_ctl, audio_driver_ctl, etc). The ones that haven't already been gotten rid of, will be soon. There should be only one such function for all the driver stuff in retroarch.c, and that should be rarch_ctl. It was a bad design decision I made a few years ago to go for that approach, and themaister didn't like it either. So this is a way to correct that mistake.",0,0,msr
2130,"@adam-urbanczyk The problem with conda is that it pollutes the environment. I just removed it (again) because it was breaking everything else python-related on my machine in very pervert ways.
Docker might be A solution to this. It's should be much easier to use for non-python developers, too.",0,0,msr
2131,"I'm working on getting a nix environment with cadquery and cq-editor going. Don't have it working yet though.
@adam-urbanczyk A docker image would really help I guess. But it just seems really to extreme to resort to package managers which take over your entire system or having to run an entire second system just to be able to run a piece of software.",0,0,msr
2146,"> The advantage of Conda is that you can create different environments (cqgui in the case of CQ-Editor) with different package versions.
The disadvantage is that Anaconda never ever seems to work, whereas in my experience venv (managed by anything but conda), system interpreter en even pipenv work flawlessly every time. Example (this is me literally trying to get this to work on my system as I type this): Step 1: conda create -n cadquery ```
Fatal Python error: init_import_size: Failed to import the site module Python runtime state: initialized Traceback (most recent call last): File ""C:\Python39\lib\site.py"", line 73, in <module> import os File ""C:\Python39\lib\os.py"", line 29, in <module> from _collections_abc import _check_methods File ""C:\Python39\lib\_collections_abc.py"", line 12, in <module> GenericAlias = type(list[int]) TypeError: 'type' object is not subscriptable ``` Fixed the above problem. Let's try again:
```
Cannot set up a python SDK at Python 3.8 (dactyl-python) (C:\Users\xxx\.conda\envs\dactyl-python\python.exe).
The SDK seems invalid.
```
Wut? Why would it even...!? *uninstall and reinstall anaconda3 a few times to try again*
*uninstall anaconda3*
*try miniconda instead*
*manually set conda.exe env path, because that somehow didn't happen during install*
conda activate cadquery
``` CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'. ```
k? That's probably not supposed to happen *uninstalls miniconda*
*I realize environments are left behind in some directory, removes 6 GB of environments manually*
*install miniconda again. Installation mentions that it's recommended to start anaconda from start menu to set environment variables* *anaconda/miniconda does not appear in start menu*
*Google how to set miniconda in env path*
*Can't find conda.exe anywhere, reinstall minconda3 to let installation add miniconda to path, against installation recommendation yet according to documentation recommendation (they contract each other)*
Finally figured out what the problem was: I had Python 39 installed, but Conda can only handle up to Python 38 (which is unfortunate, because I would like to use dataclasses which are a Python 39 thing). Deleted all references to Python from environment variables (including Pythonhome/pythonpath), reinstalled minconda, chocolatey refreshenv, closed and reopened terminal and now the conda command works. On to the next challenge: set up Conda env in Pycharm and see if cadquery will work. Three hours later: can start coding. Day is over. Have to go to bed. Disclaimer, this is my personal opinion and I'm probably a n00b, but: I hate anaconda. Messing around with toolchains is my main frustration with coding. This is not the first time random conda crap makes my cry. I've made it work in the past, I've worked with it in the past, but I just can't get it to work now for the above reasons. It's unfortunate that this is not explained in docs that I found and even more unfortunate that Miniconda install doesn't detect that Python is already on the path and might cause problems (and I can't use system python now? I'm confused?). And this is even before all the potential environment and dependency mess. ugggggggggggg. I'm not trying to flame cadquery devs here, I love cadquery itself, it's awesome. My goal is to just give a report of ""a random day of a random dude who wants to use cadquery"" and the one and only massive struggle that I run into: anaconda.",0,1,msr
2150,That is the base env Python version. You can crate envs with 3.9 (using the above mentioned command).,0,0,msr
2155,"I think that @adam-urbanczyk wants to adapt the builds for Azure pipelines first, but that is the general plan.",0,0,msr
2159,There is also a Conda dependency with Spyder.,1,0,msr
2162,"> I'm using CD-editor for previewing my work but want to use a proper IDE (e.g. VS Code) for development - especially for code completion. Therefore I installed the pip package but now have realized that it's actually the wrong version...
> > For my specific use-case, having a pip package that doesn't include the binaries would already be pretty helpful!
@Hades32 you can use git directly with pip: `pip install git+https://github.com/CadQuery/cadquery.git`
> I also feel that Conda as a requirement is really hurting cadquery, and I know I'm not alone because as I've been reading up on CQ I find many people complaining about this. Conda is a niche tool hated by many, and the entire python ecosystem is based around pypi.
@rosejn let's agree to disagree about conda. From where I sit it is not niche. There are people complaining all the time, but somehow I haven't see anyone producing a working wheel yet. It is possible, the question is if it is practical and how much time will we spend on doing it and then supporting users in debugging a zillion of issues specific to different system configurations, incompatible compilation flags of OCCT etc. If you can contribute a build pipeline (preferably azure) for generating wheels for Win, Linux and OSX that work reliably then we'll be more than happy to merge it.
You can also consider building packages for your favorite distro - no blessing from the CQ team is needed to do that.
And BTW I hear that there is a deb package for miniconda nowadays. I haven't tried it myself, I always install to a local dir, never update `.bashrc` and I never had an issue. Same on Windows.",0,0,msr
2165,"@rosejn There are some build instructions in this [yaml file](https://github.com/CadQuery/OCP/blob/master/build-bindings-job.yml#L181), and there are a few discussions on the Google Group and GitHub on building OCP/Pywrap on FreeBSD, which might include some info that will be useful to you.
@roipoussiere Thanks for the link. I was not aware of your work on the flatpak.",0,0,msr
2166,"> Now I'd like to build OCP and CadQuery, but I suspect the build instructions on the README page haven't been updated in a while. pywrap doesn't seem to be an executable for one, and this doesn't seem like a typical cmake setup where you would configure and build from inside of a build directory. Could someone please update the README, or outline the build steps here? Or if there is an azure build pipeline log that you could share which shows the steps maybe that will help.
Azure runs https://github.com/CadQuery/OCP/blob/master/azure-pipelines.yml, which in turn runs https://github.com/CadQuery/OCP/blob/master/build-bindings-job.yml. [Here is a recent build log](https://dev.azure.com/cadquery/OCP/_build/results?buildId=1260&view=logs&jobId=bfbe1bfd-e8c3-5173-6d5d-4afb5d0fac7a&j=bfbe1bfd-e8c3-5173-6d5d-4afb5d0fac7a&t=4ddd68ef-aa5b-58d6-fcfa-706b6db49807), you can get them through the check mark next to each commit on the OCP page.
If you plan to operate outside of Conda, you'll probably have to build OCCT too. See here for the receipe: https://github.com/conda-forge/occt-feedstock
You'll get frequent segfaults if you just use a default build, you need those flags and patches.
Some additional help might come from @greyltc's AUR pkgbuild https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=python-ocp
Are you sure you don't want to try Conda-press first?",1,0,msr
2173,"> Do you believe that hosting bandwidth is somehow magically ""wasted away"" [..]
No magic involved. Just human nature. People tend to seed only what they use/watch/otherwise utilize now. Few seed something for the greater good. Even fewer do it from always-on server. Because of that some obscure and old packages would be hard or impossible to download. This is what I was talking about. This is not the place to discuss this initiative. CadQuery is not PyPI. If I'm wrong you probably should pitch this idea to pip maintainers at https://discuss.python.org/",0,0,msr
2175,"FWIW, it doesn't matter to me whether cadquery hosts on PyPI/supports pip install at all or not. I think every project should support the tooling that works best for them and their users. However I just wanted to clarify/point out a few things as a PyPI maintainer/admin!
Easiest/happiest thing first.
> * [_API to get dependencies without full download_](https://github.com/pypa/warehouse/issues/474).
This is coming, we've had a rough idea of what to do for awhile now, and we were just missing someone with the free time to go through and write it up as a proper PEP. That happened and [PEP 643](https://www.python.org/dev/peps/pep-0643/) is now accepted and is currently being implemented.
> * [_Distributing large binary payloads as separate downloads_](https://github.com/pypa/warehouse/issues/7852).
This is still a problem. In the linked thread under ""heated discussion"" (which I don't think was actually heated! but that's neither here nor there) Dustin (one of the PyPI maintainers/admins with me) laid out the problem with several possible solutions. We don't have a solution to this, but I'm pretty sure that everyone involved wants to solve this problem. Right now it's stuck in the ""discuss possible ideas"" phase, largely because nobody has stepped up to write, submit, and champion a PEP for a possible solution (not that that PEP would be guaranteed to be accepted, but that's how those projects make decisions).
The Venn diagram of people who have the ""large binary"" problem, people who care/want to work on pip/PyPI, and people who have the time to work on the above is pretty small, so it'll likely languish until someone gets more time, it bubbles up to be a much larger problem, or someone external champions an idea that gets successfully accepted.",1,0,msr
2177,"FYI: Last few days I've been intermittently working on my own attempt at creating **Nix flakes** for **pywrap** and **OCP** all that while learning how to do it properly. I know that there were some tries to do it but all links in this issue I've opened so far led to repositories almost 2 years old + attempts in **nixpkgs** repo seem to also have been abandoned. Please correct me if I'm wrong.
Btw. my goals are following:
- pywrap, ocp and cadquery must be easily buildable locally from source code. No hackary like repackaging conda packages! Also use normal OCCT package (of a properly tested fixed version) from *nixkgs* as a source of headers not the ones added/hardcoded into the repos as submodule.
- Package **libclang**'s missing python bindings and provide them as an **nixpkgs** overlay in a flake until it's in **nixpkgs**.
- Each project should have nix flake inside its repo. One could then directly install them using their github urls. CQ maintainers would have to be OK with it of course.
- For each project also provide a development shell inside its flake so one can just fire up VSCode/ium inside it and work on the code.
- Finally provide an exemplary devShell where CadQuery is installed so one can open open VSCode/ium or whatever editor inside it and start drawing immediately.
- Also package CQ-editor and provide an exemplary devShell with both CadQuery and CQ-editor.
So far it's going good but I'm going to have to add a parameter into **pywrap** through which one can fully specify all clang's parameters. `--include` is not sufficient since Nix does not use UNIX-like filesystem hierarchy with global libraries and everything from sysroot to libcxx include paths (`-isystem` not `-I`) have to be specified. Not to mention a possibility of crosscompilation. Those flag are calculated automatically by nixpkgs's buiild infrastructure but have to be passed in when not using clang's wrapper shell script.",0,0,msr
2181,"@jchidley > To increase adoption of cadquery, a key requirement is easy installation of it. That means that either installation should be a single operation or that there should be concise and clear documentation to follow
What you say is misleading this is definitely provided but it involve conda. Anyway I see several solutions : 1. Install miniconda, this avoid most of the burden of Anaconda while retaining the easy install of binary package anaconda provides ( You can also look at mamba and micromamba that can also be a good replacement)
2. CQ-editor and jupyter-cadquery allow you to use an external editor, CQ-editor can refresh the view when you save a file (that can be outside the editor) jupyter-cadquery as a external viewer that run in your browser and gets updated when you call the show function in your script.
3. You can build OCP and the others cq dependencies from source but that would be a pain
PS : Using WSL is actually not very complicated",1,0,msr
2182,"@Jojain I guess we have different ideas of what is easy. Anything that involves `conda` (or `miniconda` or `WSL`) isn't easy for the average user. If you want to grow the user base of this product beyond power users, it has to be trivially easy to install.
Having said that https://github.com/bernhard-42/jupyter-cadquery on Windows works well when using VS Code and the `jcv` viewer. I only found out about this option today.
As far as MacOS/Apple Silicon the only working solutions appear to be the binary download of CQ-editor and the Docker image for jupyter-cadquery. Or are there some instructions that I have missed?",0,0,msr
2186,"I had the same problems from within Code OSS. Cadquery installation went well with conda but I had to select the conda python interpreter to make it work from within VScode.
Just to rule this out :P",1,0,msr
2191,"I'd like to address some of practical points brought up in the rant(s) above, because they don't really make sense to me. It feels more like people arguing over vim & emacs rather than trying to come to a good solution to empower a community. I'm curious about a couple practical matters brought up here.
First, regarding the distribution of pre-compiled binaries. Who cares about making OCCT/OCP being available as a low-level dependency to other projects? That's not what the goal is here. Just package it up and make it part of cadquery, and if people want to use it like a library they can always depend on cadquery. I just checked, and the cadquery source itself is less than 600k, so this isn't a big deal at all. Many other python bindings to C (and C++) projects do exactly this, like tensorflow. As an alternative, the OCCT+OCP component could be packaged as a tarball, .deb, homebrew, etc., and then this would be installed separately before pip installing cadquery. In the same way that we need to download and install CUDA and CUDNN from nvidia before using tensorflow, this is a pattern that works just fine.
Second, the file size issue... is this really an issue? I just built opencascade 7.5 and the compiled libs + include headers are about 100 MB. I'm curious how this could become 1.5 GB with the addition of ""thin"" python wrappers, and whether all of the modules are even required. Maybe some of the challenge can be simplified by pruning the build output so that the end result is more manageable?
Last, as a final option, the NLTK example given above which requires a download() call (which means the size is irrelevant because it can just download from a github release link) sounds quite practical. What are the drawbacks of this approach for users? Clearly by the many people who care about this issue Conda is a constant source of pain, and in some cases just isn't an option. That download call can just look in .local/lib/CadQuery to see if it's installed already, and if not download it. This is simple, easy to verify, and not error prone. CadQuery is primarily a user tool, not something that will be deployed, but even if it is that could be setup in a dockerfile and done once on build, which is how most deployments happen these days anyways.
My prediction is that once cadquery is easily installable via pip so people can read a blog post and try it out in a moment of curiosity the user base, interest, projects and tools built using cadquery will all increase.",0,0,msr
2197,"@rowanG077 We have instructions in the README now for installing CadQuery via pip.
https://github.com/CadQuery/cadquery#cadquery-installation-via-pip
This addresses the OP's original request and so I am closing this issue. If there are other salient points in this issue, they can be broken out into new issues. To be clear, we are NOT dropping conda support for PyPI. Both will exist in parallel because conda has become an important part of our CI pipeline, and because some users will want/need to use conda and some will want/need to use PyPI.",1,0,msr
2198,"<!-- For general questions about how to use Locust, use either the Slack link provided in the Readme or [ask a question on Stack Overflow](https://stackoverflow.com/questions/ask) tagged Locust.-->
### Description of issue
I have 1 large box to perform load testing. I have all of its file descriptors and memory to perform load testing. But locust never exceeds 100 rps.
### Expected behavior
I want to push locust to run at 10000 rps.
### Actual behavior
I can only do 100 rps. Why?
### Environment settings
- OS: RedHat 7.
- Python version: 2.7.12
- Locust version: 0.11.0
### Steps to reproduce (for bug reports)
```
# locustfile.py
class MainTaskSet(TaskSet):
@task
def get_root(self):
with self.client.get('/', catch_response=True, verify=False) as response:
try:
body = json.loads(response.content)
if len(body) <= 0:
response.failure(self.bad_json_message(response))
except Exception:
response.failure(self.bad_json_message(response))
class LocustTests(HttpLocust):
task_set = MainTaskSet
min_wait = 0
max_wait = 0
# bash
locust -f locustfile.py --no-web --host=http://remote.example.com -c 10000 -r 10000 --run-time 10m
```",0,0,msr
2206,"<!-- *Before creating an issue please make sure you are using the latest version of mongoose -->
**Do you want to request a *feature* or report a *bug*?**
> *feature*
**What is the current behavior?**
> `findOneAndUpdate` query `toJSON` doesn't return the aliased keys. Rather it returns the original keys.
**If the current behavior is a bug, please provide the steps to reproduce.**
<!-- If you can, provide a standalone script / gist to reproduce your issue -->
**What is the expected behavior?**
`toObject` or `toJSON` should return the aliased keys instead of the raw keys. Or a custom function like [`reverseTranslateAliases`](https://mongoosejs.com/docs/api.html#model_Model.translateAliases) can be implemented.
Related to #5184
**What are the versions of Node.js, Mongoose and MongoDB you are using? Note that ""latest"" is not a version.**
mongoose: 5.6.2
node: 12.0.0
npm: 6.9.0
<!-- You can print `mongoose.version` to get your current version of Mongoose: https://mongoosejs.com/docs/api.html#mongoose_Mongoose-version -->",1,0,msr
2217,"This doesn't really make sense to me. I have a perfectly working app and this change breaks it. It's a BUG. I'm not concerned with the fact that sometimes using negative padding can result in no rows being returned, in fact I would say that is the EXPECTED behavior, and that is perfectly normal when you use it *improperly*. I don't see why a warning would not suffice here.
I don't understand why you INSIST that my app must HALT AND STOP WORKING COMPLETELY because I used a negative number you didn't like. Good developers should give their users MORE options, instead of TAKING AWAY options for usability because they went slightly outside how the developer anticipated the software being used.
Why would I submit a PR to FIX your BROKEN pagination tool, after you've already said you won't accept it, for some reason which makes no sense to me.",0,1,msr
2219,"@Tectract Your comments are offensive and disrespectful. Caution, I may block you to keep a safe place next time.",0,1,msr
2222,"Imho, halting errors should not be raised unless there really is a completely breaking situation that cannot be resolved. A ""best guess"" and ""forgiving"" approach should be used instead.",1,0,msr
2225,"Hmmmmm, I'm not sure how to fix this PR, or why it failed. I just put in one output to stderr statement.",0,0,msr
2227,"I disagree with your decision to REMOVE a widely used feature, and BREAK peoples applications, for no good reason. Why can't you just allow this to be a warning instead? Why does your app need to raise errors, at all? It's a completely unecessary BREAKING change, that removes a FEATURE that your users are begging you to keep! Why are you doing this???",0,1,msr
2231,"Sadly I had to block this user Tectract entirely from the kaminari org. This user isn't just aggressive to us, but also has a history of ranting on other repos, most notably on [Diaspora](https://github.com/diaspora/diaspora/issues/7866#issuecomment-421593824), [React](https://github.com/facebook/react/issues/15432#issuecomment-484290354), [Rails](https://github.com/rails/rails/issues/33009#issuecomment-392961139), and [Ruby](https://github.com/ruby/bigdecimal/issues/98). I have reported this user to GitHub as well.",0,1,msr
2238,"FYI, clearing var/cache/dev brought it down from 5 seconds to 1.",1,0,msr
2239,"@eventhorizonpl It seems you missed the point again. We don't use github issues for support questions which should be very clear if you take the time to read https://github.com/symfony/symfony/issues/new/choose
And even as a support question, the report is not given any information that would be required to help. If you are better at answering this support question, why did you provide any help at all?",0,1,msr
2246,"Hello,
I will try to start if we need this feature :)",0,0,msr
2247,How would this feature work? Because there's no reliable way of determining where to send back to as the referrer header is not foolproof,0,0,msr
2252,"the session is not working for that either: if you open 2 tabs, they will share the same server session, but not the same browser history.
Note that Rails (suggested as inspiration for the feature) implements it based on the Referer header, with a fallback location and an optional filtering of the allowed host for the Referer (using the fallback location if the filter rejects the referer).",0,0,msr
2259,"Currently it is not possible to mount a filesystem to a mount point that contains blanks.
There are many problems in the ``states.mount.mounted`` code path, e.g. the mount command arguments are not quoted in the ``modules.mount.mount`` function, see https://github.com/saltstack/salt/blob/develop/salt/modules/mount.py#L1237. Code should look like this IMO:
```
cmd = 'mount {0} {1} {2} '.format(args, device, shlex.quote(name))
```
But this will fix only a small piece of the whole problem.
Another one is that ``states.mount.mounted`` does not detect correctly that the filesystem might be mounted already, i think it's because the key in the active table is not unquoted, so a comparison between
``/srv/dev-disk-by-label-My\040Passport\040Blue`` and the specified ``/srv/dev-disk-by-label-My Passport Blue`` fails.
To me it looks like the whole mount state and module is not able to handle blanks in device names and mount points properly.
Example SLS:
```
mount_fs_with_label:
mount.mounted:
- name: ""/srv/dev-disk-by-label-My Passport Blue""
- device: ""/dev/disk/by-label/My\\x20Passport\\x20Blue""
- fstype: ext4
- mkmnt: True
- persist: False
- mount: True
```
Result:
```
ID: mount_fs_with_label
Function: mount.mounted
Name: /srv/dev-disk-by-label-My Passport Blue
Result: False
Comment: mount: bad usage
Try 'mount --help' for more information.
Started: 08:31:10.286521
Duration: 181.307 ms
Changes: ```
```
# salt-call mount.active
...
/srv/dev-disk-by-label-My\040Passport\040Blue:
----------
alt_device:
None
device:
/dev/sda1
fstype:
ext4
opts:
- rw
- noexec
- relatime
- jqfmt=vfsv0
- usrjquota=aquota.user
- grpjquota=aquota.group
...
```
```
# ls -alh /dev/disk/by-label/
total 0
drwxr-xr-x 2 root root 60 Sep 17 08:29 .
drwxr-xr-x 7 root root 140 Sep 17 08:29 ..
lrwxrwxrwx 1 root root 10 Sep 17 08:29 'My\x20Passport\x20Blue' -> ../../sda1
```
```
# ls -alh /srv
total 28K
drwxr-xr-x 7 root root 4.0K Sep 17 08:07 .
drwxr-xr-x 21 root root 4.0K Sep 16 16:07 ..
drwxr-xr-x 4 root root 4.0K Sep 13 13:40 dev-disk-by-id-scsi-0QEMU_QEMU_HARDDISK_drive-scsi0-0-2-part1
drwxr-xr-x 2 root root 4.0K Sep 16 16:11 'dev-disk-by-label-My Passport Blue'
drwxr-xr-x 2 ftp nogroup 4.0K Sep 10 14:23 ftp
drwxr-xr-x 3 root root 4.0K Sep 16 16:07 pillar
drwxr-xr-x 5 root root 4.0K Sep 16 16:07 salt
```
```
# cat /etc/fstab
proc /proc proc defaults 0 0
UUID=90ee6298-385f-4841-bfdc-8b1e0e0ae5c1 / ext4 errors=remount-ro 0 1
# >>> [openmediavault]
/dev/disk/by-label/My\x20Passport\x20Blue	/srv/dev-disk-by-label-My\040Passport\040Blue	ext4	defaults,nofail,user_xattr,noexec,usrjquota=aquota.user,grpjquota=aquota.group,jqfmt=vfsv0,acl	0 2
# <<< [openmediavault]
```
```
# cat /proc/self/mountinfo
...
265 25 8:1 / /srv/dev-disk-by-label-My\040Passport\040Blue rw,noexec,relatime shared:148 - ext4 /dev/sda1 rw,jqfmt=vfsv0,usrjquota=aquota.user,grpjquota=aquota.group
...
```",0,0,msr
2260,"Hello, @votdev I am trying to build myself home NAS with old Atom mini-ITX board... So i install OMV5, i plug in dad's old NTFS drive... and here we go...
Frankly, i wish Salt guys put the comments inside this source, listing all the bugs related to this module. So any hacker which for whatever reason would change it - would be instantly notifie on old pending bugs.
Salt seems extremely fragile here, probably no one else except for OMV5 uses it for partitions. Maybe OMV6 could do it outside Salt? Like good old UDEV rules or anything. I mean, before Salt porject might decide to drop this functionality that almost no one use, instead of burden of maintaining it for OMV5 alone....
Well, ranting aside, i am rather puzzled with your _device: ""/dev/disk/by-label/My\\x20Passport\\x20Blue""_
Where do you even get this hex substitution from???
Thing is, the whole mounting escaping is one uber-ancient legacy mess. Putting it here so maybe someone would use it. I spent like 3 hours googling around and experimenting with Python that i never used before. Tryied to google some standard about Posix/Linux/bash filename mangling/escaping.... and then Python module to undo it. To no avail.
Okay, so, to document it down.
- mtab/fstab and friends is one-of-a-kind ancient mess.
- it started with ancient BSD (not FreeBSD) function strunvis, which behaviour not documented. Probably that was OS-specific function (a la virtual methods). http://manpages.org/strunvis/3
- when Linux was mimicking good old BSd it only made ad hoc substitutions for 4 specific chars. There is no any systematic/generic pattern at all.
```
static inline void mangle(struct seq_file *m, const char *s)
{
seq_escape(m, s, "" \t\n\\"");
}
```
https://elixir.bootlin.com/linux/latest/source/fs/proc_namespace.c#L84
```
R(""\\"", '\\'),
R(""011"", '\t'),
R(""012"", '\n'),
R(""040"", ' '),
R(""134"", '\\')
```
https://sources.debian.org/src/sysvinit/2.96-6/src/fstab-decode.c/
So, whatever comes from Linux mounts information - should be de-mangled for those four special cases.
Every single space-separated column of every single line.
Ugly, and undocumented, but that is what it is. And, frankly, it is not that hard...
BUT, why do you want to compare with some arbitrary hex-escaped string? what can be a real use-case for that???
Linux kernel just does not have hex-escaping code for disk mounts.
Now, to be frank, even this would NOT be enough, because i can have multiple disks with the same partition label. Like many USB thumb drives with ""DATA"" partition. I can even have several partitions with the same name on singe disk!
Again, it can be fixed by detecting collisions and adding extra data, like counters or GUID or whatever, but...
What gonna OMV do if OMV's user has two drives with partitions having same labels, and then he hotplugs one disk, or another, or both in any order? Is it race condition now? Is it okay for OMV to have race condition?
Seems whatever use cases Salt imagined for them here is very different from what OMV users might face.",1,0,msr
2261,"Output from Linux's mount
`/dev/sdb1 on /media/U:NTFS Disk type fuseblk (rw,relatime,user_id=0,group_id=0,allow_other,blksize=4096)`
Spaces are NOT escaped there!
Dunno how it is done on BSD/Darwin
And then we have this...
```
# salt-call mount.list_mounts
local:
----------
/:
/dev/sda1
......
/media/U:NTFS:
/dev/sdb1
/proc:
proc
........
```",0,0,msr
2262,"@votdev re: escaping names for calling `mount` - i think that is what was intended to do so:
`""device"": device_name.replace(""\\040"", ""\\ ""),` inside `def _active_mountinfo()`
but that was only called when from `def active(extended=False)` then Extended is set to True, if ever
And similar code inside `def _resolve_user_group_names(opts):`
So it seems Salt prefers to keep space-containing names mangled, but mangled differently.
So, no escaping when calling `mount` or `umount` is needed,
---
I am not even sure that de-escaping mount point likes `xxx\040yyy` in Salt would be correct way to go.
There can be a point: since that module serves as abstraction layer and should hide UNIX-likes peculiarities from generic Salt modules, all IDs better be unmangled. But not sure. However IF to do this de-mangling, then quoting arguments for calling `mount` becomes required indeed.
But anyway, this line i believe should not had ended in /etc/fstab and whoever added it was at fault...
```
# >>> [openmediavault]
/dev/disk/by-label/My\x20Passport\x20Blue	```",0,0,msr
2269,"@votdev > OMV already workarounds this issue
by failing to mount the disk? because i can not mount disk in OMV5 or i would never learn about this issue.
failing to mount disk does not look like work-around at all.
let's think what we can do to make space-containing partitions mounted by OMV. It seems to be a kind of ""communication breakdown"" between Salt and OMV5, they expect and provide for mututally incompatible things.
here is minimally patched /usr/lib/python3/dist-packages/salt/modules/mount.py [mount.py.gz](https://github.com/saltstack/salt/files/6217180/mount.py.gz)
it makes space-containing mount point visible. If there still is something not working - i can not see what it is and how could i test it using `salt-call` scripts
```
# salt-call mount.list_mounts
local:
----------
/:
/dev/sda1
/dev:
udev
/dev/hugepages:
hugetlbfs
/dev/mqueue:
mqueue
/dev/pts:
devpts
/dev/shm:
tmpfs
/media/U:NTFS\040Disk:
/dev/sdb1
/proc:
proc
......
# salt-call mount.active
....
/media/U:NTFS\040Disk:
----------
alt_device:
/dev/sdb1
device:
/dev/sdb1
fstype:
fuseblk
opts:
- rw
- relatime
- user_id=0
- group_id=0
- allow_other
- blksize=4096
..........
# salt-call mount.active extended=true
......
/media/U:NTFS\040Disk:
----------
alt_device:
/dev/sdb1
device:
/dev/sdb1
device_label:
U - Arch-2 Hitachi_2Tb_7200
device_uuid:
c6705d84705d7bdd
fstype:
fuseblk
major:
8
minor:
17
mountid:
427
opts:
- rw
- relatime
parentid:
26
root:
/
superopts:
- rw
- user_id=0
- group_id=0
- allow_other
- blksize=4096
```
and also
```
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS Disk""
local:
False
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS\ Disk""
local:
False
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS\040Disk""local:
True
```",0,0,msr
2271,"@votdev it is sad how fast you were to say Salt is all wrong and how protective you fet about OMV.
You still try to push Salt to adhere to OMV data format, while common sense says it should be otherwise.
Salt users would not suffer from it. OMV users would.
Demanding PR from OMV users like me is funny when you did not make any PR to Salt, or maybe i am wrong and you did.
So, back to:
https://github.com/openmediavault/openmediavault/issues/566#issuecomment-809541057
The intention was and is to make OMV work with disks users insert. Without forcing them to go ssh sudo. So simple.
You make it look that making OMV ""just work"" is bad goal. > Why the hell should escapeshellarg be called here?
Because that woul be consistent with bash/Salt data format. But i alreeady said it was kneejerk impulse, so you eems to be crashing through door wide open.
> The function is doing exactly what you are suggesting, keep data raw/verbatim/unescaped within OMV
Some we are on the same page here. You blaze of ego is called for.
Since eysterday i was asking you to show me at the se
ems between OMV and Salt, the exact borderleines, didn't i?
I am glad you seem to did so above, https://github.com/openmediavault/openmediavault/issues/566#issuecomment-809529126
And when i showed those links, i commented upon them.
`Salt is based on Python, not PHP. The code you're ranting about never runs in the Salt context.`
I never said so. Both Salt and OMV are ""black boxes"" with some data exchange. And i was asking you to point me to the raw places of exchange and raw data being exchanged, didn't i?
Yesterday i spent hours looking into Salt code and patching it along your suggestions.
First i took your suggestions as correct and thought through. And just followed them. An then had to undo it all.
Now you imply it was your time wasted not mine.
That `Example SLS:` - many times from yesterday i asked you how can i reproduice this activity from bash command line.
For example above - https://github.com/saltstack/salt/issues/54508#issuecomment-808875885
You kind of answered by showing PHP code for SLS generation - after many requests and hours.
But you still not answered how to trigger that action from bash.
I asked you yesterday how to make OMV code trigger that action of Salt, allegedly buggy Salt.
And you refused to help me doing it.
https://github.com/openmediavault/openmediavault/issues/566#issuecomment-808955077
```
What can i patch in OMV5 to make this notification gone?
Why do you want to know that? What do you expect to improve?
```
You made me look into Linux kernel i am not familiar with, at the same time you are not very willing to point me to specific OMV code and Salt commands you are familiar with.
You are blocking any attempt to debug OMV and Salt interaction - and you demand perfectly polished PRs. It is not consistent. And it is would not help anyone. Not me, not you, not OMV users.",0,1,msr
2275,"Summary of the investigation at this point:
* When the client restarts, the network hook's `Prerun` fires and tries to recreate the network and setup the iptables via CNI.
* This fails because the netns already exists, as expected. So we tear down the task and start over.
* But in the next pass when we setup the iptables via CNI, we collide with the iptables left behind.
* To fix this we need the network namespace path (which is used by CNI as part of the handle for [`go-cni#Network.Remove`](https://godoc.org/github.com/containerd/go-cni#Network.Remove)).
* In the non-Docker case, Nomad controls the path to the netns and it's derived from the alloc ID, but in the Docker case (which includes all cases with Connect integration b/c of the Envoy container), Docker owns that path and derives it from the pause container name. So we can't use a deterministic name as a handle to clean up. But we can't get it from Docker either because at the point we need it that container has already been removed.
I've verified the following more common failure modes are handled correctly:
* Tasks recover fully when the client restarts (after a few PRs we landed in the current 0.10.0 release branch)
* There's no resource leak when the client restarts if the containers aren't removed.
* There's no resource leak when the client restarts as part of a node (machine) reboot.
Status:
* We could try to fix this by threading state about the network namespace from the allocation runner back into the state store, similar to how we deal with deployment health state. But this will always be subject to races between client failures and state syncs.
* We already have a PR open for 0.10.x to reconcile and GC Docker containers. Because all the rules we're creating are tagged with the string ""nomad"" and Nomad's alloc IDs, we can make a similar loop for iptable GC.
* Because I've verified that this leak doesn't happen in the common failure modes of a client or node reboot, we're not going to block the 0.10.0 release on this. We'll work up a PR for an out-of-band reconcile loop for 0.10.x.
Moving this issue out of the 0.10.0 milestone.",1,0,msr
2277,"We have a similar issue - iptables is a mess after some Nomad restarts.
We're on Nomad 1.0.4",0,0,msr
2280,"I've just re-read the upgrade guide for (in preparation for 1.2.0), and I think the changes in that 1.1.0 to append the CNI rules rather than insert at the top of the chain is what made this issue more noticeable (https://github.com/hashicorp/nomad/pull/10181). Previously, had transient rules been persisted, the next time an alloc was started, the new iptables rules would be inserted above the stale ones and thus take precedence. Now they are added below the stale rules, so traffic is matched and blackholed by the stale rules.",0,0,msr
2282,"This is becoming a major security and stability issue as we are seeing allocations try to forward from ports that already have rules in iptables, and requests bound for them are getting forwarded based on the stale iptables rule.
Is there anything we can do to ensure this gets prioritized? Or can someone share a cleanup script they have been using?
Here is the error log around the time it fails to cleanup the stale allocation:
<details>
<summary>Logs</summary>
```
containerd[3148]: time=""2022-05-24T07:05:43.326040572Z"" level=info msg=""shim disconnected"" id=d13e150a783b3a72482e859901590f7d002b71c43c36ffe2f0d46aecca64e794
containerd[3148]: time=""2022-05-24T07:05:43.326107477Z"" level=error msg=""copy shim log"" error=""read /proc/self/fd/17: file already closed""
dockerd[3364]: time=""2022-05-24T07:05:43.326090953Z"" level=info msg=""ignoring event"" container=d13e150a783b3a72482e859901590f7d002b71c43c36ffe2f0d46aecca64e794 module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
consul[3318]: 2022-05-24T07:05:43.342Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.342Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.349Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.349Z [WARN] agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.380Z [WARN] agent: Failed to deregister service: service=_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083-sidecar-proxy error=""Service ""_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>
consul[3318]: 2022-05-24T07:05:43.380Z [WARN] agent: Failed to deregister service: service=_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083-sidecar-proxy error=""Service ""_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>
kernel: docker0: port 1(vethc6029f5) entered disabled state
kernel: vethb5b69c3: renamed from eth0
kernel: docker0: port 1(vethc6029f5) entered disabled state
kernel: device vethc6029f5 left promiscuous mode
kernel: docker0: port 1(vethc6029f5) entered disabled state
systemd[1]: Stopping Nomad...
```
</details>",0,0,msr
2283,"Hit this same issue today. Performed some manual iptables clean up on a problem client. Here are my notes in case this helps anyone else:
* Seems likely it was caused by some combination of quick nomad job allocation stop and re-deployment and/or nomad systemd service restarts, possibly before cleanup of a stopped allocation could be cleaned up.
* Symptom first noticed indicitive of a problem was of failed consul healthcheck, where the port that should be the listener on the host level and is properly bridged into the container as seen from the Nomad UI and nomad job config just isn't working.
* Tcpdump on the client machine to the port listener which doesn't appear to work shows SYN packet sent to a nomad bridge IP (`[S]`) and a RESET packet immediately returned (`[R.]`)
* Looking at the tcpdump output shows that the packet is actually being sent to the wrong nomad bridge IP, and further looking at the IP tables, shows that there are duplicate (or more) rules set up for port forwarding the listener into the Nomad bridge due to unclean allocation handling of an old allocation.
* On our OS, cleaning up the iptables can be done with a client reboot, but cleaning up by hand in these occurrences can be also be done.
* General procedure -- check iptables (command refs below). There will be iptables entries which show comments of allocation ID association. Check for non-existent allocation IDs being present and/or conflicting with existing allocation iptables rules. If such allocation IDs are seen, they will also be seen to be associated with user defined iptables chains starting with CNI-[a-f0-9] and CNI-DN-[a-f0-9]. These can all be purged with the example cmds below:
```
# In this case, old rules from a nomad bridge IP with no active allocation superseded the correct rules to a nomad bridge IP with an active allocation listener.
# Find references to obsolete iptables rules with missing allocation IDs (first cmds contains all info, addnl cmds are a bit more verbose)
iptables-save
iptables -t filter -L -v -n
iptables -t nat -L -v -n
# Delete rules specific to the bad allocation IP from the filter and NAT tables
iptables -t filter -D CNI-FORWARD -d 172.26.66.2/32 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
iptables -t filter -D CNI-FORWARD -s 172.26.66.2/32 -j ACCEPT
# Delete references to the user defined chains of the non-existent allocation from the filter and NAT tables
iptables -t nat -D POSTROUTING -s 172.26.66.2/32 -m comment --comment ""name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -j CNI-fe647aec064bf60036a312df
iptables -t nat -D CNI-HOSTPORT-DNAT -p tcp -m comment --comment ""dnat name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -m multiport --dports 8008,5432 -j CNI-DN-fe647aec064bf60036a31
iptables -t nat -D CNI-HOSTPORT-DNAT -p udp -m comment --comment ""dnat name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -m multiport --dports 8008,5432 -j CNI-DN-fe647aec064bf60036a31
# Delete rules from user defined chains of the non-existent allocation
iptables -t nat -F CNI-DN-fe647aec064bf60036a31
iptables -t nat -F CNI-fe647aec064bf60036a312df
# Delete the user defined chains from the non-existent allocation
iptables -t nat -X CNI-DN-fe647aec064bf60036a31
iptables -t nat -X CNI-fe647aec064bf60036a312df
```",0,0,msr
2290,@johnalotoski has posted a process above; if you were to run that as a periodic task (or just a cron job) that'd clean up the iptables.,1,0,msr
2294,"Questions belong on the freeradius-users mailing list. As the github template makes clear, github issues are only for actual issues.",0,0,msr
2295,"Dear Alan, Thank you for your response all duly noted. I would like to put my point
of view whilst accepting that you can run your team and policy exactly
as you wish. For me I have to decide whether or not to join in. I am a
software developer with around 50 years of experience in small and large
projects. I am currently working on a secure email system which will
have a radius server as part of its back office services. For me, I
thought we had started a dialogue with my first problem, I acted on your
advice, reloaded linux, installed myql, tried to build freeradius and
there was a different kind of failure. In my view a failure of this kind
is usually a failure in the software, either configuration or build. As
such it was right to refer it back to the development team rather than
post to a users mailing list. I will not be doing this. I will fix the
problem myself or go to a different package. I will detach now. Best wishes with your excellent freeradius which I first found out about
in the O'Reilly book ""RADIUS"". Now on with my life. Bev
On 2019-10-07 12:25, Alan DeKok wrote:
> Questions belong on the freeradius-users mailing list. As the github template makes clear, github issues are only for actual issues. > > --
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub [1], or mute the thread [2].
Links:
------
[1]
https://github.com/FreeRADIUS/freeradius-server/issues/3029?email_source=notifications&amp;email_token=AHVY34RFLLE3KIJGSYBGJELQNMMEPA5CNFSM4I6ATUL2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAP6RDA#issuecomment-538962060
[2]
https://github.com/notifications/unsubscribe-auth/AHVY34XM44M2ULDOXSP5CFLQNMMEPANCNFSM4I6ATULQ",1,0,msr
2309,"> That's got to be a bug in WPF. There shouldn't be a goal to copy that functionality over to Avalonia when it just doesn't make sense.
Yes. Not to be rude but I have stopped all of my app development with Avalonia simply because there is basic functionality that remains broken for years because new features or ""compatibility"" is favored over deprecating or changing things for the better. I like Microsoft, and their acquisition of things usually makes things better, but this workflow I'm describing benefits nobody and it just stalls development for everyone for years.
Most of my bugs were filed in 2017-2019 and they still remain for the most part. I didn't even make reports for everything because they are so infuriating, or making a reproduction is just so time-consuming, or the issue is SO BASIC (ComboBoxes drive me insane in this) that I'm like ""surely they will find and fix this without my report"" so I just gave up. Whenever I NEED to update my stuff I end up spending the entire day diagnosing things that broke when updating (so the compatibility argument makes literally no sense since shit breaks anyway EVERY update without fail).
I have janky extension methods and fixes everywhere in every app because again, basic functionality just does not work and I'm waiting for things to get fixed, or events fire in different orders between updates. It's just impossible to justify my day doing this literally every time. I'm glad there is a ContainsExclusive now as I suggested in my PR but it should be used where possible ASAP. Not in 8 months. Then I can remove my extension method that does that in PointerPressed.
And people using my apps understand I have no motivation because of this. They see it's open source and want to help, then attempt to and go ""oh Avalonia with their 1000+ issues"" and try to port my stuff to GTK or something else since they also have issues with Avalonia. Again I don't want to be mean here but this is actual feedback you should consider seriously. You guys do hard work but the workflow is just not there where it matters. It's the hard truth and I wish it weren't since Avalonia can be so great, and should've been great a long time ago. But when ComboBoxes for example STILL don't perform as expected after 5+ years it really does hurt your users.
I pull every commit locally since I first found Avalonia, even to this day, and I read them all. But I see only new features and macOS low-level window fixes. Nothing that matters to me personally, and I get disappointed every now and then since I see the priorities. Anyway, that's just my rant and my opinion. I hope this new generation of users can develop with less headache than I did.",0,1,msr
2320,It baffles me this hasn't just been added yet. Just give people the flexibility instead of point towards web host configs.,1,1,msr
2322,"Updated from v92 to v110 and have flicker on the edges. I use PlaneBufferGeometry with null z values to represent unknown elevations(holes). The sparse grid shows a flicker where the nulls' edges are. Was not seen with flatShading: true or v92:
![image](https://user-images.githubusercontent.com/18248938/68339808-812af700-00a2-11ea-8f01-ec761d009580.png)
I know you are going to say NaN are not supported in geometries, but any chance you could fix this or point me in proper direction?
##### Three.js version
- [ ] Dev
- [x ] r110
- [ ] ...
##### Browser
- [x] All of them
- [ ] Chrome
- [ ] Firefox
- [ ] Internet Explorer
##### OS
- [ ] All of them
- [x] Windows
- [ ] macOS
- [ ] Linux
- [ ] Android
- [ ] iOS
##### Hardware Requirements (graphics card, VR Device, ...)",1,0,msr
2332,Seems very odd that separate instances of three are causing it. Even though unsupported seems like it could be a canary for a scoping issue?,0,0,msr
2333,"why not just map NaNs to some big but finite values to move the triangles out of the view. that would mean removing the index attribute as well, however (rough idea: https://jsfiddle.net/uxwqyjvm/ )",0,0,msr
2334,"In any event, I don't see a `three.js` bug here. I suppose this happens also with pure WebGL and with any other 3D engine so I think it's more correct to close the issue and move the dicussion to stackoverflow or the forum.",1,0,msr
2336,"So there were two shader changes in the revision in the build. EDIT
single change then adding that change in so this is messing it up.
sorry my github skills suck , we are svn:
it is here in light_pars_begin lines 1-40:
```
export default /* glsl */`
uniform vec3 ambientLightColor;
uniform vec3 lightProbe[ 9 ];
// get the irradiance (radiance convolved with cosine lobe) at the point 'normal' on the unit sphere
// source: https://graphics.stanford.edu/papers/envmap/envmap.pdf
vec3 shGetIrradianceAt( in vec3 normal, in vec3 shCoefficients[ 9 ] ) {
// normal is assumed to have unit length
float x = normal.x, y = normal.y, z = normal.z;
// band 0
vec3 result = shCoefficients[ 0 ] * 0.886227;
// band 1
result += shCoefficients[ 1 ] * 2.0 * 0.511664 * y;
result += shCoefficients[ 2 ] * 2.0 * 0.511664 * z;
result += shCoefficients[ 3 ] * 2.0 * 0.511664 * x;
// band 2
result += shCoefficients[ 4 ] * 2.0 * 0.429043 * x * y;
result += shCoefficients[ 5 ] * 2.0 * 0.429043 * y * z;
result += shCoefficients[ 6 ] * ( 0.743125 * z * z - 0.247708 );
result += shCoefficients[ 7 ] * 2.0 * 0.429043 * x * z;
result += shCoefficients[ 8 ] * 0.429043 * ( x * x - y * y );
return result;
}
vec3 getLightProbeIrradiance( const in vec3 lightProbe[ 9 ], const in GeometricContext geometry ) {
vec3 worldNormal = inverseTransformDirection( geometry.normal, viewMatrix );
vec3 irradiance = shGetIrradianceAt( worldNormal, lightProbe );
return irradiance;
}
```",1,0,msr
2351,"@kpetrow > Just a simple test for isNaN would resolve most these. Why is there such push back and not a desire to resolve this issue?
You may have more luck by doing a PR with that ""simple test"" rather than asking us to do it for you.",0,1,msr
2367,"> Folks - Markus was rather clear in his comment; that decision has not changed.
> Repeatedly asking for this or `me too`ing an issue is simply noise and unproductive.
> > > [We don't have plans to support language over quality at this time](https://github.com/Sonarr/Sonarr/issues/3394#issuecomment-556944433)
> > There's 1 item to do from this GHI:
> > * preventing downgrades of the language in order to upgrade the quality.
> > If you want a workaround then group all your qualities together and handle quality preferences as preferred words. This would effectively remove quality from the equation as they'd all be equal.
> https://wiki.servarr.com/sonarr/faq#how-are-possible-downloads-compared
Hi,
if there is really a working solution... could you please ellobrate it a little bit more and maybe show how to do it? maybe some screenshots? mabye add a faq or wiki speciall for this? Would help all non english speaking ppl in the world..",1,0,msr
2376,"- [x] **I have created my request on the Product Board before I submitted this issue**
- [X] **I have looked at all the other requests on the Product Board before I submitted this issue**
About the GraphQL plugin:
If a model has a unique field defined in it, we should be able to filter for that field.
In this moment only IDs can be used for filtering
Example:
model Article with title, content and slug. Slug is required and unique
I would like to run this query
`
query {
article(slug: ""my-awesome-article"") {
title
id
content
}
}
`",1,0,msr
2377,Thank you for this feedback!,0,0,msr
2381,"> Hi @nicecatch great suggestion. You can achieve that by overridding the query and the action to handle it quickly on your own :)
@alexandrebodin Where I can override the query? Thanks!",0,0,msr
2384,"Here is a working example. You need to await your service call and only pass the right params
```js
const { sanitizeEntity } = require('strapi-utils');
module.exports = {
query: `
articleBySlug(id: ID name: String): Article
`,
resolver: {
Query: {
articleBySlug: {
resolverOf: 'Article.findOne',
async resolver(_, { slug }) {
const entity = await strapi.services.article.findOne({ slug });
return sanitizeEntity(entity, { model: strapi.models.article });
},
},
},
},
};
```",0,0,msr
2387,"This can be done via customization, marking as closed. We have filters that would work just as easily also.",0,0,msr
2389,The test failure is legit; could you please fix it?,0,0,msr
2401,"Thanks everyone, further input will not be needed at this point in time. Please let @derickr review this change.",0,0,msr
2418,"@fragglet Thanks for taking the time to explain this further, I do really appreciate it. Though unfortunately I still don't feel any closer to really understanding the rationale to let one optional vanilla break exist but not another. Well, besides the *very* thin 'keeping Chocolate closer to Vanilla by not introducing more stuff'.
I don't believe I've made any contradictions, or at least not in the way you were explaining I did. Maybe I didn't write it out adequately enough before, so I'll try to re-iterate it against the points you made to hopefully help clarify my point. The feature I'm suggesting would have absolutely no affect at all on the vanilla experience, outside of getting to actually *have the experience*. I definitely wouldn't side with any of the counter changes you suggested to mine. Increased frame rate, increased resolution, changed networking... those type of changes would affect the vanilla experience and definitely would be 100% against the philosophy document. Chocolate should only have changes that can give greater access and range to the vanilla experience, which I feel experiencing these otherwise vanilla compliant maps is perfectly acceptable; but *only* as something optionally! It *shouldn't* be the default behavior to accommodate limit breaking maps and I would fully agree with that. But I feel there is no reason there can't be an option to allow for using these maps so people can get the 'vanilla experience' with them. That's sort of what I feel is the entire point of Chocolate - bringing the vanilla experience into the present.
I'm not quite sure why you feel the simpler approach I suggested for the limit raise would suddenly make the vanilla compatibility muddled for Chocolate. An array is still an array. An array created by a variable with the value of 127 would be exactly the same size as an array was created by a #define with the value of 127. So if the array sizes are defaulted to vanilla limits, I can't see how that muddles things in any way at all. Of course, if you explicitly tell it to muddle the limits, then obviously that would - akin to the demo and save limits. Though the more complicated approach of auto setting the limits I suggested, while still possible to do as arrays and get precisely the same vanilla behavior with defaults, would introduce a lot more non vanilla code; so I can understand that being a potential thumbs down at the very least.
But, it seems at this point though that we will likely just have to agree to disagree. It is only opinion vs. opinion now, and we appear to be unable to see eye to eye on the topic. Likely we will only waste each other's time trying to continue on. I'll just have to take some time over a weekend to make a tiny hack in a fork, and that'll be that. It is unfortunate though that the fork is even necessary for something so incredibly minor. But at least this way everyone else who also requested this feature can still use Chocolate Doom's exceptional vanilla experience and have the option play otherwise fully compatible maps as intended.
I'll just close with thanking you again for taking the time to try and explain this further. And thank you and everyone else who worked on and created Chocolate. It really is quite exceptional and amazing software, even despite this hold back it insists to carry. I have used Chocolate as my choice Doom port for years, and is why I was particularly interested to try and understand why something I feel would fit it perfectly as a non-disruptive option, apparently cannot.",0,1,msr
2421,"@fabiangreffrath Again, then why do you allow the vanilla breaking extended save file and demo formats still? Those *don't replicate the behavior of the DOOM.EXE whatsoever*. It's very hypocritical to try and justify not adding the *option* to increase the level limits for this reason, when you *already* provide options that *don't* replicate DOOM.EXE's behavior.",1,1,msr
2428,"Same here ☹️ I have to refactor all my `useResultCache()` calls with an ugly `if`s:
```
👌 Nice and smooth in ORM 2:
$res = $entityManager
->createQuery($dql)
->useResultCache($cache_enabled, $ttl)
->getResult();
----
🤮 Ugly in ORM 3:
$q = $entityManager->createQuery($dql);
if ($cache_enabled) {
$q->enableResultCache($ttl);
}
$res = $q->getResult();
```",0,0,msr
2435,@Ocramius @greg0ire Lets say you want to implement an api that should handle GET parameter “nocache” that will make it hit the DB bypassing the cache. In this case replacing the cache is not so good. I think it’ll cost you nothing to add a proxy method useResultCache() that will call one of the two new methods and everyone will be happy.,0,1,msr
2445,"Hi @ostrolucky - I would like to remind you that we are a small team of maintainers, and it has been less than or barely 24 hours since I responded to your issue yesterday. There are a lot of other folks that we are supporting at the same time as you, and we are responding as fast as we can. Please know that we aren't ignoring you, nor are we happy to close your issues and never respond. Please have patience with us while we try to assist you and others. Thanks :heart: I'm happy to make a temporary fix to support the two parameter method for this third party plugin using an internal library to Vagrant, but as I mentioned yesterday you will likely get more traction and a quicker fix if you open an issue on the plugin side and fix it there. Our Vagrant release cadence isn't too frequent, and there are lots of other issues and features that we're trying to get in for each release.
If you wish to get this moving faster, there is already a pull request on the plugin: https://github.com/smerrill/vagrant-gatling-rsync/pull/37 I recommend pinging the maintainer on there.
For fixing this today on your side, you should still be able to use the built in rsync synced folder feature, as this is the recommended way to use rsync and Vagrant. Alternatively, downgrading to Vagrant 2.2.5 and using that plugin until a fix has been met is also a possibility. You could also bring in the changes locally and run and install the plugin from source.
We can leave this open to track the issue for now in case others find it. If our next release time comes around and it's still not fixed on their side, I'll apply a patch that fixes this so that it works in 2.2.7. Thanks!",0,0,msr
2451,"Bad C code is generated with the following Nim code:
### Example
```nim
import locks
var l: Lock
echo l
```
### Current Output
```
CC: ltest.nim
Error: execution of an external compiler program 'gcc -c -w -I/home/tay/.choosenim/toolchains/nim-1.0.4/lib -I/home/tay -o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c.o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c' failed with exit code: 1
/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c: In function ‘dollar___3Dq1hcGH7a2iNlnh4hnPbg’:
/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c:219:50: error: ‘pthread_mutex_t’ {aka ‘union <anonymous>’} has no member named ‘abi’
219 | addQuoted__N56rmvBL9a3xqnKq09cKdb3A((&result), x.abi);
| ^
```
### Expected Output
Compiler either compiles properly or errors before C code gen
### Possible Solution
* It looks like there might be an issue with the implementation of `$` for the Lock type
### Additional Information
* Fedora 31: `Linux localhost.localdomain 5.3.11-300.fc31.x86_64 #1 SMP Tue Nov 12 19:08:07 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux`
```
$ nim -v
Nim Compiler Version 1.0.4 [Linux: amd64]
Compiled at 2019-11-27
Copyright (c) 2006-2019 by Andreas Rumpf
git hash: c8998c498f5e2a0874846eb31309e1d1630faca6
active boot switches: -d:release
```",1,0,msr
2454,"@3n-k1 that's not how it works in open-source in general - it just means that nobody worked on the issue yet, but as long as the bug is still there it should be open",0,1,msr
2458,Just brigading so I can get my promised block. I certainly would not like you to use anything I contribute on.,0,1,msr
2466,"Hello, TSS edit on Details/Metric tab is not working (running V3.5-RC2X) although it was working properly in previous version. A number can be edited but when clicked outside of the box it goes to zero automatically. I made a search but did not find a similar topic.
Thanks.",0,0,msr
2471,"For new users it is created automatically, existing users need to add it using the (+) button and the arrows to order.",1,0,msr
2475,"I tried this method a few weeks ago and could still not get it to work properly,does not give you a tick box next to bikestress in the edit metrics tab and does not overwrite imported details because of this. Have tried deleting the TSS field and just leaving bikestress there and that does not work either",0,0,msr
2477,"Hi VS Code dev here :wave:
Open a python file, invoke a context menu in the editor and notice the following:
1) Run Current Test File command is there. When I execute it it says that I do not have testing enabled. I suggest to add this command only if testing is enabled in my workspace
2) Sort Imports -> shouldn't this be a part of the format document. I do not see why this command needs to be so promiment to take space in the context menu. Also there is already `Source Action...` command which shows the Sort Imports so this feels like duplication.
3) ""Run python file in terminal"" and ""Run selection line in python terminal"". I do not understand why one command says ""in terminal"" and the other ""in python terminal"". When I exectute them they just go to the terminal for me. Can we have some consistency in titles, or is it possible to drop one of those actions. Reason being, user can select the whole file and do Run selection, to cover the python file case
4) Run in Interactive window: I suggest to show this only if I have Jupter or whatever needed setup
5) Most of these actions do not make sense while you are debugging. I would also consider to hide them while the user is debugging (since the menu grows then additonally with vscode debug commands)
![Screenshot 2019-12-24 at 12 48 24](https://user-images.githubusercontent.com/1926584/71411983-d66b9980-264b-11ea-822c-a3375166d30c.png)
I believe VS Code plans to introduce context sub menus and then python could add all the actions to the submenu potentially.
fyi @qubitron @DonJayamanne",0,0,msr
2481,"> @isidorn Yeah, the python/jupyter team has absolutely no regard for these issues.
> > In particular, the [context menu bloat](https://github.com/microsoft/vscode-python/issues/15834) is even worse these days, and all the items are contributed by the jupyter extension.
> > The summary of microsoft's view is what @claudiaregio said:
> > > Just like any other VS Code menu item, _users do not have to use or click on any options/features made available to them if they do not wish to use them._
> > As I [said](https://github.com/microsoft/vscode/issues/9285#issuecomment-812277371), it's like saying:
> > ## If you don't want candy crush, just don't click in the candy crush icon.
It's ""disruptive"" because it's true.",0,1,msr
2492,"You can try installing Jinja2 2.11.0rc1 from https://test.pypi.org/project/Jinja2/2.11.0rc1/:
```
pip install -i https://test.pypi.org/simple/ --pre Jinja2
```
You should get Jinja 2.11.0rc1 pulled in as a dependency and both `jinja` and `jinja2` should be importable.",0,0,msr
2493,We actually discussed about this on our Discord a few days ago and the rename will be 3.0 and not a point release as initially planned.,1,0,msr
2498,"Since I update to the latest version of Strapi, even the simple fetch doesn't return the avatar object.
I read the documentation about the customization concept but there is no `schema.graphql` in the extension folder. Grateful if you could share some hints.",0,0,msr
2502,Can you please share the content of the `schema.graphql` file.,0,0,msr
2503,"Just one line added, the ""name: String"" in line 25, but name cannot be queried through the me modek in graphql
```
const _ = require('lodash');
const { ApolloError } = require('apollo-server-koa');
/**
* Throws an ApolloError if context body contains a bad request
* @param contextBody - body of the context object given to the resolver
* @throws ApolloError if the body is a bad request
*/
function checkBadRequest(contextBody) {
if (_.get(contextBody, 'output.payload.statusCode', 200) !== 200) {
const statusCode = _.get(contextBody, 'output.payload.statusCode', 400);
const message = _.get(contextBody, 'output.payload.message', 'Bad Request');
throw new ApolloError(message, statusCode, _.omit(contextBody, ['output']));
}
}
module.exports = {
type: {
UsersPermissionsPermission: false, // Make this type NOT queriable.
},
definition: `
type UsersPermissionsMe {
id: ID!
username: String!
name: String
email: String!
confirmed: Boolean
blocked: Boolean
role: UsersPermissionsMeRole
}
type UsersPermissionsMeRole {
id: ID!
name: String!
description: String
type: String
}
input UsersPermissionsLoginInput {
identifier: String!
password: String!
provider: String = ""local""
}
type UsersPermissionsLoginPayload {
jwt: String!
user: UsersPermissionsMe!
}
`,
query: `
me: UsersPermissionsMe
`,
mutation: `
login(input: UsersPermissionsLoginInput!): UsersPermissionsLoginPayload!
register(input: UserInput!): UsersPermissionsLoginPayload!
`,
resolver: {
Query: {
me: {
resolverOf: 'User.me',
resolver: {
plugin: 'users-permissions',
handler: 'User.me',
},
},
role: {
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.getRole',
resolver: async (obj, options, { context }) => {
context.params = {...context.params, ...options.input};
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.getRole(context);
return context.body.role;
},
},
roles: {
description: `Retrieve all the existing roles. You can't apply filters on this query.`,
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.getRoles', // Apply the `getRoles` permissions on the resolver.
resolver: async (obj, options, { context }) => {
context.params = {...context.params, ...options.input};
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.getRoles(context);
return context.body.roles;
},
},
},
Mutation: {
createRole: {
description: 'Create a new role',
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.createRole',
resolver: async (obj, options, { context }) => {
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.createRole(context);
return { ok: true };
},
},
updateRole: {
description: 'Update an existing role',
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.updateRole',
resolver: async (obj, options, { context }) => {
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.updateRole(
context.params,
context.body
);
return { ok: true };
},
},
deleteRole: {
description: 'Delete an existing role',
plugin: 'users-permissions',
resolverOf: 'UsersPermissions.deleteRole',
resolver: async (obj, options, { context }) => {
await strapi.plugins[
'users-permissions'
].controllers.userspermissions.deleteRole(context);
return { ok: true };
},
},
createUser: {
description: 'Create a new user',
plugin: 'users-permissions',
resolverOf: 'User.create',
resolver: async (obj, options, { context }) => {
context.params = _.toPlainObject(options.input.where);
context.request.body = _.toPlainObject(options.input.data);
await strapi.plugins['users-permissions'].controllers.user.create(
context
);
return {
user: context.body.toJSON ? context.body.toJSON() : context.body,
};
},
},
updateUser: {
description: 'Update an existing user',
plugin: 'users-permissions',
resolverOf: 'User.update',
resolver: async (obj, options, { context }) => {
context.params = _.toPlainObject(options.input.where);
context.request.body = _.toPlainObject(options.input.data);
await strapi.plugins['users-permissions'].controllers.user.update(
context
);
return {
user: context.body.toJSON ? context.body.toJSON() : context.body,
};
},
},
deleteUser: {
description: 'Delete an existing user',
plugin: 'users-permissions',
resolverOf: 'User.destroy',
resolver: async (obj, options, { context }) => {
// Set parameters to context.
context.params = _.toPlainObject(options.input.where);
context.request.body = _.toPlainObject(options.input.data);
// Retrieve user to be able to return it because
// Bookshelf doesn't return the row once deleted.
await strapi.plugins['users-permissions'].controllers.user.findOne(
context
);
// Assign result to user.
const user = context.body.toJSON
? context.body.toJSON()
: context.body;
// Run destroy query.
await strapi.plugins['users-permissions'].controllers.user.destroy(
context
);
return {
user,
};
}
},
register: {
description: 'Register a user',
plugin: 'users-permissions',
resolverOf: 'Auth.register',
resolver: async (obj, options, {context}) => {
context.request.body = _.toPlainObject(options.input);
await strapi.plugins['users-permissions'].controllers.auth.register(context);
let output = context.body.toJSON ? context.body.toJSON() : context.body;
checkBadRequest(output);
return {
user: output.user || output, jwt: output.jwt
};
}
},
login: {
resolverOf: 'Auth.callback',
plugin: 'users-permissions',
resolver: async (obj, options, {context}) => {
context.params = {...context.params, provider: options.input.provider};
context.request.body = _.toPlainObject(options.input);
await strapi.plugins['users-permissions'].controllers.auth.callback(context);
let output = context.body.toJSON ? context.body.toJSON() : context.body;
checkBadRequest(output);
return {
user: output.user || output, jwt: output.jwt
};
}
}
}
}
};
```",0,0,msr
2525,"so i've done everything as described in the link and also reinstalled GC.
It is still not working, so it must be a false code!!!!",0,1,msr
2526,"I just tried it and it works. Authorise with Strava via add account, then sync (make sure you set the date range to a range that includes some activity).
If the code was broken there would be a lot more reports of issues, believe me. We have a large user base. Make sure you are using the latest dev build from here:
https://github.com/GoldenCheetah/GoldenCheetah/releases/tag/V3.5-RC2X
Also, the full release is due in a week or so.",0,0,msr
2527,"So i tried it again, nothing works. After login in strava via Facebook and allow Golden cheetah, the fault ""socket Operation (4)"" appears.",1,0,msr
2529,"If you post in the users forum may be other user with the same problem can explain what he did i to solve it.
GitHub issues are used to track bugs and features only, as a FOSS we don’t have the resources to give case by case support here.",0,0,msr
2532,"![Imgur](https://i.imgur.com/2DkGNXk.jpg)
Real path: D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin\buffer_history.vim
and repository in
D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin",1,0,msr
2544,"<!--
######################################################################
WARNING!
IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE
######################################################################
-->
## Checklist
<!--
Carefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:
- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2020.01.15. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.
- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.
- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.
- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.
- Finally, put x into all relevant boxes (like this [x])
-->
- [x] I'm reporting a broken site support
- [x] I've verified that I'm running youtube-dl version **2020.01.15**
- [x] I've checked that all provided URLs are alive and playable in a browser
- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped
- [x] I've searched the bugtracker for similar issues including closed ones
## Verbose log
<!--
Provide the complete verbose output of youtube-dl that clearly demonstrates the problem.
Add the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2020.01.15
[debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
<more lines>
-->
```
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: ['--verbose', '-j', 'https://www.youtube.com/playlist?list=OLAK5uy_kj9o0LqqeGu3wJf_G1JqOJ-YHzexqptlM']
[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 3.8.1 (CPython) - Linux-5.5.0-1-MANJARO-x86_64-with-glibc2.2.5
[debug] exe versions: ffmpeg 4.2.2, ffprobe 4.2.2, rtmpdump 2.4
[debug] Proxy map: {}
ERROR: Signature extraction failed: Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(""Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/YoutubeDL.py"", line 796, in extract_info
ie_result = ie.extract(url)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/common.py"", line 530, in extract
ie_result = self._real_extract(url)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 2046, in _real_extract
signature = self._decrypt_signature(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1393, in _decrypt_signature
raise ExtractorError(
youtube_dl.utils.ExtractorError: Signature extraction failed: Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
func = self._extract_signature_function(
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
right_val = self.interpret_expression(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
x, abort = self.interpret_statement(
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(""Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see https://yt-dl.org/update on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
```
## Description
<!--
Provide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.
If work on your issue requires account credentials please provide them or explain how one can obtain them.
-->
Trying to dump the Json information of a playlist seem to be broken for youtube.",0,0,msr
2545,"I can confirm the the issue on many videos.
```
./youtube-dl -4 -v ""https://www.youtube.com/watch?v=pqIv3e5eBeo""
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: [u'-4', u'-v', u'https://www.youtube.com/watch?v=pqIv3e5eBeo']
[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 2.7.9 (CPython) - Linux-4.9.182-xxxx-std-ipv6-64-x86_64-with-Debian-7
[debug] exe versions: avconv 11.12-6, avprobe 11.12-6, ffmpeg N-48007-g62f8d27ef1-static
[debug] Proxy map: {}
[youtube] pqIv3e5eBeo: Downloading webpage
[youtube] pqIv3e5eBeo: Downloading video info webpage
[youtube] {18} signature length 109, html5 player vfl_PLd61
[youtube] pqIv3e5eBeo: Downloading player https://www.youtube.com/yts/jsbin/player_ias-vfl_PLd61/en_US/base.js
ERROR: Signature extraction failed: Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/YoutubeDL.py"", line 796, in extract_info
ie_result = ie.extract(url)
File ""./youtube-dl/youtube_dl/extractor/common.py"", line 530, in extract
ie_result = self._real_extract(url)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 2047, in _real_extract
encrypted_sig, video_id, player_url, age_gate)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1394, in _decrypt_signature
'Signature extraction failed: ' + tb, cause=e)
ExtractorError: Signature extraction failed: Traceback (most recent call last):
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
video_id, player_url, s
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
cache_res = res(test_string)
File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
return lambda s: initial_function([s])
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
res, abort = self.interpret_statement(stmt, local_vars)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
m.group('expr'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
m.group('x'), local_vars, allow_recursion - 1)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
v = self.interpret_expression(expr, local_vars, allow_recursion)
File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
(caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type youtube-dl -U to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
```",1,0,msr
2549,"Your test helpers should live in `test/` not in `lib/`. `lib` is not autoloaded, which is why `require 'lib/test/authenticated_test_helper'` doesn't work.
You are right though the docs are confusing. I believe they wrote a typo and meant write to store your helpers in `test/lib`, I usually put them in `test/test_helper/`. I have fixed the documentation in e7514128a6a5ca11385abfa7f699cc4fd9ceefd1 and 9082609a33531cf671c62231712d9a21a7facef6. The guides will be updated on the next release of Rails.
---
As an aside, you have more than one issue open on this repo that feels a bit like you're yelling at us. I get that you're new to Rails and are frustrated, but most of the folks here are volunteers and no one is purposefully making your life hard. ❤️ We appreciate bug reports, and will fix things as soon as we can, but we're all human and we make mistakes.",0,0,msr
2550,@eileencodes what's the point of the lib folder if I can't require files from it? that has always been the standard folder to place code that doesn't belong in `app`,0,1,msr
2554,"I wasn't upset even in the slightest until you decided to take this discussion off-topic over one capital word. I have *never* assumed someone was angry at me for trying to report a bug in any of the libraries I maintain, so I get irked when someones else's first response to someone trying to help improve the library is ""pull requests welcome"" (no shit it's open source. we know, we're just trying to make you aware of the issue) or the *assumption* that because we found an error we're mad about it or expect a free library to be perfect",0,1,msr
2558,"What options are supported by brewfile? I can't find anything after extensive googling. Even the main README mentions `unless` without describing it....
Can you point me to docs on the syntax of your DSL?",1,0,msr
2563,"Here is the output of `brew bundle --help` in case you were curious:
```
Usage: brew bundle subcommand
Bundler for non-Ruby dependencies from Homebrew, Homebrew Cask and the Mac App
Store.
--file= Read the Brewfile from this file. Use
--file=- to pipe to stdin/stdout.
--global Read the Brewfile from ~/.Brewfile.
brew bundle [install] [-v|--verbose] [--no-upgrade]
[--file=path|--global]
Install or upgrade all dependencies in a Brewfile.
-v, --verbose Print the output from commands as they are
run.
--no-upgrade Don't run brew upgrade on outdated
dependencies. Note they may still be
upgraded by brew install if needed.
brew bundle dump [--force] [--describe] [--no-restart]
[--file=path|--global]
Write all installed casks/formulae/taps into a Brewfile.
--force Overwrite an existing Brewfile.
--describe Include a description comment above each
line, unless the dependency does not have a
description.
--no-restart Do not add restart_service to formula
lines.
brew bundle cleanup [--force] [--zap] [--file=path|--global]
Uninstall all dependencies not listed in a Brewfile.
--force Actually perform the cleanup operations.
--zap Remove casks using the zap command
instead of uninstall.
brew bundle check [--verbose] [--no-upgrade] [--file=path|--global]
Check if all dependencies are installed in a Brewfile.
-v, --verbose Print and check for all missing
dependencies.
--no-upgrade Ignore outdated dependencies.
brew bundle exec command
Run an external command in an isolated build environment.
brew bundle list [--all|--brews|--casks|--taps|--mas]
[--file=path|--global]
List all dependencies present in a Brewfile. By default, only Homebrew
dependencies are listed.
--all List all dependencies.
--brews List Homebrew dependencies.
--casks List Homebrew Cask dependencies.
--taps List tap dependencies.
--mas List Mac App Store dependencies.
```
No mention of the DSL, not even the subcommands allowed. I'm a big fan of brew-bundle, but it's not super useful for complex cases without docs. It's especially confusing because there are a number of outdated docs on the net for old iterations of brew bundle.",1,0,msr
2568,"I came here with an issue that you've acknowledged exists. I've done nothing but try to work with you to try to resolve that problem, respectfully and openly.
I will leave now. Good luck with whatever's bugging you.",0,1,msr
2569,"> I'm sorry if I have.
I appreciate the apology.
> I've done nothing but try to work with you to try to resolve that problem, respectfully and openly.
I have not found your communication to be respectful. For example:
> Good luck with whatever's bugging you.
Yes, I agree leaving is probably for the best, sorry.",0,1,msr
2578,"The sample apps' styleguide page is intended to be a live reference of usage examples that you can actually run. See the link example:
https://github.com/Sitecore/jss/blob/dev/samples/react/src/components/Styleguide-FieldUsage-Link/index.js",0,0,msr
2580,"I agree that an API reference will be helpful.
However, I think the following can help you in the meantime. > I have no way of knowing what I can import from the package, or what props might be supported on each component, without diving into its (minified!) sourcecode. Never mind what everything does 😱
You don’t have to try to decipher minified code. The JSS modules are written in TypeScript and are heavily commented. Just clone this repo locally and all the typed function signatures and props models will be available for you to reference.",0,0,msr
2585,"It would also have helped if the issue title and description had more
information. I certainly could not tell what this PR was for just by
looking at this among all of the other PRs and issues.
On Tue, Mar 17, 2020 at 8:37 PM Thomas A Caswell <notifications@github.com>
wrote:
> The tests are still not passing and none of the issues brought up in
> #16631 <https://github.com/matplotlib/matplotlib/issues/16631> (the need
> to document the API changes, an attempt to move to the simpler fix, or a
> clear argument as to why IndexFormatter should not be deprecated). From my
> point of view we have been waiting on you.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/matplotlib/matplotlib/pull/16634#issuecomment-600366626>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AACHF6DM2ANDPG462BOB3BLRIAJUTANCNFSM4K73BSUA>
> .
>",1,1,msr
2586,"> The tests are still not passing
I will repeat for the last time : THE TEST REQUIRES THE BUG TO BE PRESENT.
The bug manifests itself by outputting ``Label 0`` for -1 < indexes < 0 instead of outputting an empty string ``""""``. The test is passed when the bug is present, that is when``Label 0`` is wrongly outputted. The test fails when the bug is NOT present, that is when ``IndexFormatter`` returns an empty string, ``""""``, AS IT SHOULD.
The solution --- which consists only of rounding the index value, ``round(i)`` instead of truncating the index value, ``int(i + 0.5)`` --- returns an empty string, ``""""``. The solution fails the test because the test is WRONG. The test is by itself A BUG.
> From my point of view we have been waiting on you.
I CANNOT CORRECT THE TEST. IT'S YOUR RESPONSIBILITY AND PREROGATIVE.
> the need to document the API changes
If you adopt the solution, THERE NO NEED TO CHANGE THE DOCUMENTATION NOR THE API
> why IndexFormatter should not be deprecated)
Because, WITH THE SOLUTION instead a deprecation :
1. IndexFormatter works. It simply works, as intended, without any bug.
2. ``IndexFormatter(x_labels)`` is far simpler than proposed convoluted alternatives like ``FuncFormatter(lambda x, _: dict(zip(range(len(x_labels)), x_labels)).get(x, """"))`` to achieve the same exact end result.
3. The solution doesn't break back compatibility in the middle of minor revision. The users don't have to change their programs. They don't have to find an alternative. 4. It follows the precepts of semantic versioning instead of adopting post hoc rationalisations (like ""others deprecated functionalities in the middle of minor revision so lets pretend it's okay for us to do the same"")
5. You don't have to spend time and energy changing the documentation. You don't have to teach the users alternative methods that are necessarily harder to implement and explain.
6. Apparently, the DEPRECATION is based, at least partially, on the false assumption that a decade of zero complaints from the users somehow means the users are unhappy with IndexFormatter and therefore don't use it. 7. THE DEPRECATION misses entirely the point of avoiding the vicious cycle of complexity. Increasing complexity to achieving the same exact functionality is a huge waste of your time. It only make you busier and busier, writing more convoluted code and spending more time managing harder to debug bugs.
I have wasted enough of my time trying to convince you. So be it.",0,1,msr
2588,"@Wlodarski We expect everyone to engage in a professional manner consistent with the PSF code of conduct (https://www.python.org/psf/conduct/). I do not think your comments in this thread (or in #16631) reach that bar, please be more considerate of your words in the future.
I am going to close and lock this thread as I do not think this discussion will be productive if continued here. If anyone wants to pick up this work please open a new PR where we can start fresh.
-----
@ksunden Thank you for your kind and considered words :)",0,0,msr
2592,"There is a simple issue here I was going to fix on merge, but didn't get a chance to this weekend with what is going on in my country. After work today I will at least comment on it so anyone can make the change if I cannot.",0,0,msr
2595,"Thanks @gireeshpunathil ! As we discussed on the TC meeting, it should be in the format as the current commit on `master` and our guide: the message on the first line, a blank line, a reference to the PR in the form ""`closes #PR`"". I can make the said changes if you need. LMK",0,0,msr
2597,"Ok... It still does not seem to be in the form from my comment... ? I can just update if you need, let me know. If I can better clarify how the message should be constructed let me know as well. I believe it was clear, and provided an example of a previous comment message, but if I can provide better guidance for the format, I certainly can.
I am circling around on this because I would like this to get merged, but I recall that you wanted to do the commit editing so I could just do the fast-forward merge, but it's not in the correct state for that. I can make those edits if you like, but don't want to force push on your branch without you being aware as per our previous conversation :)",1,0,msr
2599,"1. The subsystem is ""docs"" and not ""doc""
2. The closes should be `closes #4210`
3. We do not include co-author fields, but we can if desired to amend our guidelines, in which this commit would wait for that to happen
It should just be something like
```
docs: add project captains to contribution
closes #4210
```
Then as discussed in the TC meeting, the committer on the commit would need to be the person performing the merge itself (myself unless you would like to wait for another to do that).",0,0,msr
2617,"1.6.2 released with fix: https://github.com/memcached/memcached/wiki/ReleaseNotes162
specifically: https://github.com/memcached/memcached/commit/02c6a2b62ddcb6fa4569a591d3461a156a636305
and fwiw, I've been responsive to security reports (or even report them myself) and give credit happily when due for over ten years. Don't waste my good will, please.",0,0,msr
2619,"I can't reproduce this.
```
λ curl -k --socks5-hostname localhost:1080 https://self-signed.badssl.com/
```
works fine. Here's how I invoke mitmproxy:
```
λ mitmdump --mode socks5 --listen-port 1080 --ssl-insecure
Proxy server listening at http://*:1080
127.0.0.1:52598: clientconnect
::ffff:127.0.0.1:52598: Certificate verification error for self-signed.badssl.com: self signed certificate (errno: 18, d
epth: 0)
::ffff:127.0.0.1:52598: Ignoring server verification error, continuing with connection
127.0.0.1:52598: GET https://self-signed.badssl.com/
<< 200 OK 502b
127.0.0.1:52598: clientdisconnect
```",0,0,msr
2628,"OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.",0,0,msr
2632,"Ok, I think that is enough.",0,0,msr
2638,@Pliner @sungam3r i added the sync behavior of the InternalConsumer Dispose. But we should consider bumping the package to netstandard 2.1 and use IAsyncDisposable instead or implement our own async await AutoResetEvent and create an Async Dispose :),1,0,msr
2650,"@cybersupernova this module is not here to resolve ""disputes"" -- it is here to provide a aggragate json database of the types as defined from the three sources at the top of the readme.",0,0,msr
2654,"The RFC-to-be's current IESG status is ""Approved-announcement to be sent"". The draft has been formally approved, but there are some administrative things left to take care of before it gets officially published as an RFC.
I don't think we should wait for that to happen to take action here, though.",0,0,msr
2655,My 2 cents is not to jump the gun... drafts are drafts for a reason... it's something that can be achieved when it's actually published. While it would be nice to have it more standardized patience would be a good thing to make sure the draft doesn't expire at its expiry date without a resolution.,1,0,msr
2659,"Hi,
you can use JWT tokens without requesting sessions via `_/session` API.",0,0,msr
2663,"> How can I set this _client request header_ on the server?
Which server do you mean, the CouchDB server?",0,0,msr
2664,"No the web server. In my case Node.js, or PHP, any web server.",0,0,msr
2682,"RFC 6750, § 5.3,
> Don't store bearer tokens in cookies: Implementations MUST NOT store
bearer tokens within cookies that can be sent in the clear (which
is the default transmission mode for cookies). Implementations
that do store bearer tokens in cookies MUST take precautions
against cross-site request forgery.",0,0,msr
2694,"It order to make such things work in an IDE you will probably need to teach it about the emcc compiler. Things like what its default include paths are.
You might have more luck with Visual Stdio Code, its seems a lot more flexible. But you will still need to configure it yourself. Perhaps write the mailing to the ""emscripten-discuss"" mailing list to see if anyone else has done this yet. If you end up with a working solution I'm sure we would be happy to add the configuration instructions to the docs.",0,0,msr
2697,"This thread reminded me of [an excellent article that Rich Hickey wrote a couple of years ago about open source development interaction around the Clojure programming language and community](https://gist.github.com/richhickey/1563cddea1002958f96e7ba9519972d9). Old, but seems to resonate quite well with this conversation thread.
When we write that maybe you'd like to champion such support, we are dead serious. There are currently no Emscripten developers that would be spending their free time implementing Visual Studio integration, and there are unfortunately no companies that would have this close at heart either, so someone will have to step up for that to happen.",0,0,msr
2705,"> This doesn't look like a bug to me. `singularize()` expects a plural, `pluralize()` expects a singular
Exactly. That's how the methods are specified. The component uses heuristics to determine singular/plural forms __under the assumption that the input is correct__.
NLP is hard and I don't think this component aims to solve NLP problems. :wink:",1,0,msr
2707,"We show the movie how the user problem converts into *we already have best quality, dont be stupid* as a result some lalala and issue gone, noone understand, noone try. Ok no problem",1,1,msr
2710,"Please note that human language is very difficult to grasp in programming logic. Developers usually are no language experts. I think it's quite amazing that there is programming logic that is able to singularize/pluralize words.
If you think it's possible to detect if a word is singular and plural, please change the code and create a pull request. That's the greatness of open source: combining the knowledge of the every developer that is happy to share their knowledge.
At last, I want to make you aware of the [Symfony Code of Conduct](https://symfony.com/coc). That's how the Symfony community has decided to communicate with eachother (as every developer lives in very different cultures, it's important to have one standard code of conduct for the community).",1,0,msr
2720,"@pedrobmarin Apologies if I didn't use the right terminology, I'm not so experienced with the project yet. The behavior I see (and others reported in the google group as well) is when the room was created with the guestPolicy=ALWAYS_ACCEPT , then you can promote any user as moderator. But if you created the room with guestPolicy=ASK_MODERATOR , then the menu option is shown but the user is not promoted. In both cases, the users are VIEWERS. Not sure if the code you are showing is connected to this issue.",0,1,msr
2731,![medecin-famille-masque_23-2148168455](https://user-images.githubusercontent.com/6266038/83276270-a44cfd80-a1d0-11ea-93d2-dc5004f4d889.jpg),0,1,msr
2739,ERCC currently suffers from #18232,0,1,msr
2740,Please also include screenshots that show how the harvester clips through solid walls and the roof for a fair comparison.,1,0,msr
2743,"One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO.",1,1,msr
2753,"I am officially drawing a line under this request. ERCC will never be merged upstream while I remain involved with OpenRA. At this point it has nothing to do with any of the in-game aspects, it is entirely about the behaviour of the people championing it.
While most of the competitive RA community are good people, there is a rotten core of toxic entitlement that manifests as abuse and belittlement, often focused around ERCC, and usually focused personally against me. This behaviour has forever soured ERCC, and integrating it into the upstream RA mod now would validate that behaviour as an effective strategy for influencing OpenRA's development.
Why now? Two recent incidents targeting me (from N/A in the community discord, and Longely/Widow in the competitive discord - the two main personalities behind ERCC) were incredibly offensive and belittling, and are the straw that has broken the camels back when added on top of many other generally toxic events this year. People in the competitive community were having fun meming about ""us vs the devs"" earlier this year, but at that time I was seriously considering quitting OpenRA completely due to how unhappy and embarrased the behaviour of some people in the community (and the general tolerance of that behaviour as acceptable by everybody else) was making me. Ultimately, I decided that I enjoy working on the project, and making things better for everybody else in the community. I *do not* enjoy interacting with the assholes, and intend to be increasingly blunt in dismissing their toxic behaviour. This is the first concrete action towards that.
I can certainly appreciate the desire for solving directional map imbalance, to the point where I fully implemented the code for one solution (rotatable structures) and developed a working prototype for a second (allowing harvesters to dock from any direction, visually jumping docking point). I think there is scope for some ERCC alternative, but I suggest that the competitive community as a whole put some effort into solving its attitude problem before they try to revisit such ideas here.",1,1,msr
2756,"<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->
<!--- Provide a general summary of your changes in the Title above -->
<!---
Documentation on ZFS Buildbot options can be found at
https://openzfs.github.io/openzfs-docs/Developer%20Resources/Buildbot%20Options.html
-->
### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
The horrible effects of human slavery continue to impact society. The
casual use of the term ""slave"" in computer software is an unnecessary
reference to a painful human experience.
### Description
<!--- Describe your changes in detail -->
This commit removes all possible references to the term ""slave"".
Implementation notes:
The zpool.d/slaves script is renamed to dm-deps, which uses the same
terminology as `dmsetup deps`.
References to the `/sys/class/block/$dev/slaves` directory remain. This
directory name is determined by the Linux kernel. Although
`dmsetup deps` provides the same information, it unfortunately requires
elevated privileges, whereas the `/sys/...` directory is world-readable.
### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->
Manual testing of `dm-deps` script and manpage.
Ran the test suite.
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [x] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [x] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).",1,0,msr
2759,This is not a technically difficult task (https://www.hanselman.com/blog/EasilyRenameYourGitDefaultBranchFromMasterToMain.aspx) but it might break some things. I definitely think we should try to change it.,1,0,msr
2761,"This is getting silly already. No way it makes any sense.
UPD: How is this offtopic? Please stop bringing politics into development world. That's just mindblowingly silly.",1,1,msr
2763,"@ronag i don't think that cluster was brought up in this thread, though it has been discussed on other occasions.",1,0,msr
2770,"> To let tsc make a call on this request.
As far as I understand it that's not how our governance works. Any collaborator may add the `tsc-agenda` label for an issue so it gets TSC eyes on it - but that should only be done if consensus seeking fails. It's an escape hatch for when we need to _force a vote_ which is pretty rare. At least that is my understanding of [the process](https://github.com/nodejs/node/blob/master/GOVERNANCE.md).
So far everyone here seems to be pretty in sync (no one is opposed but no one is particularly in favor). We're not even in disagreement 😅",1,0,msr
2772,"Bear in mind that URLs linking to `master` will not redirect to the new default branch name (and for repos where it matters, which isn't this one, github-pages only works on the default branch when it's named `master`). It may be worth waiting for Github to fix these discrepancies before making the switch.
(to be clear; i'm in favor of making the change, and ""main"" seems as good as anything else, but the disruption caused by Github's incomplete support for a non-master default branch are significant)",1,0,msr
2776,"Ok, it looks like there are people in favor in the org who feel strongly that we should change this. So far no objections and everyone in the conversation is +0 -0 or +1.
Does anyone object to changing this (just the main branch name from `master` to `main`)?
It looks like it's [not particularly hard](https://github.com/nodejs/node/issues/33864#issuecomment-643656288) technically (+ an update to the collaborator guide and policy). We probably need to address [the links](https://github.com/nodejs/node/issues/33864#issuecomment-643672588) as well.",0,0,msr
2784,Putting this on the tsc agenda for discussion,0,0,msr
2785,-1 on renaming the branch,0,0,msr
2786,"I would recommend we escalate this (to a vote for example) when:
- We have a clear plan on how to make this change addressing the raised issues (GH links, build infrastructure etc).
- We have an individual or group willing to champion those changes.
Can either of the collaborators -1ing (@gireeshpunathil / @mscdex ) speak up regarding what in particular they are objecting to? (The process of changing it? the name `main` itself?)",1,0,msr
2791,There are definitely some very real and difficult technical challenges to address here. My suggestion would be to make this a strategic initiative with the first step being to establish precisely how to make this change with the least amount of disruption.,0,0,msr
2792,"As a community that has pledged for inclusivity, it is natural to be sensitive and sympathetic to the current situations. However, I guess taking a step back and looking at things from a more wider perspective, I would ask these questions myself, in order:
- Is Node.js community well represented by race, gender, ...? any process change required for the membership criteria for better inclusion of under-represented groups?
- Is Node.js leadership well represented by race, gender, ...? again, any process change required?
- Is Node.js process and practices designed to be inclusive, and is it working well? a retrospective session with commitment to acting on the results?
- Is Node.js nomenclature, words used etc. free from offensive terms? (items such as this)
Doing the naming change first - easy, but causes turbulence in the eco-system, plus gives an impression that we are good with the rest (it is so possible that we are doing good there, but we haven't inspected). Addressing the other things - difficult and time consuming, but addresses the root of the issue from systemic perspectives.
I am willing, and eager to taking part, and / or driving such initiatives, if there is a consensus on `Let us do these things first, and attack this later!!!`",0,0,msr
2797,"> It seems like you are saying that because making this change will be easier than other changes will imply there is no other work to be done?
@MylesBorins - that was an addendum. My main point is that the other items that I listed are more tangible, personal and direct to the people / group who are subjected to the theme in question, and hence present a natural order to address.
I now see your submission in collab summit, and acknowledge it as part of a constant attempt towards improving our community, in the context of diversity and inclusivity! thank you!!",0,0,msr
2805,"GitHub has released a repo with official guidance
https://github.com/github/renaming
It mentions a new redirect feature that launched today
> Now: supporting early movers 🚚
Some projects on GitHub have already renamed their default branch. As a result, any links to those projects that contained the old branch name would previously have been broken.
>
>So, our first change, shipped on July 17th, updates GitHub.com to redirect links that contain a deleted branch name to the corresponding link in the repository's default branch.
>
>This change supports projects that have already moved. If you haven’t moved yet, we recommend not moving right now, and waiting until later this year. We’re investing in tools to make the renaming the default branch of an existing repository a seamless experience for both maintainers and contributors.",1,0,msr
2814,see also: https://github.com/nodejs/admin/issues/543 regarding changing the default for new repos,1,0,msr
2816,"List of non-archived repos, along with whether they need to be updated or not.
Generated with:
```JavaScript
const repos = require('./noderepos.json');
for (let repo of repos) {
if (!repo.archived) {
let done = ' ';
if (repo.default_branch !== 'master') {
done = 'x';
}
console.log('- [' + done + '] ' + repo.name + ':' + repo.default_branch);
}
}
```
on output from
`npx repos nodejs noderepos.json`
- [x] Gzemnid:master
- [x] Release:master
- [x] TSC:main
- [x] abi-stable-node:doc
- [x] admin:master - PR - https://github.com/nodejs/admin/issues/589
- [x] badges:master
- [x] bot-love:master
- [x] branch-diff:master
- [ ] build:master - https://github.com/nodejs/build/issues/2761
- [x] build-toolchain-next:main
- [x] changelog-maker:master
- [x] ci-config-github-actions:master
- [x] ci-config-travis:master
- [x] citgm:main
- [x] code-and-learn:master
- [x] commit-stream:master
- [x] community-committee:main
- [x] core-validate-commit:master
- [x] corepack:main
- [x] create-node-meeting-artifacts:master
- [x] diagnostics:main
- [x] docker-node:master
- [x] education:master
- [x] email:master
- [x] examples:main
- [x] getting-started:master
- [x] github-bot:master
- [x] gyp-next:master
- [x] hardware:master
- [x] help:master
- [x] http-next:main
- [x] http-parser:master
- [x] i18n:master - https://github.com/nodejs/i18n/issues/502
- [x] installer:master
- [x] llhttp:master - https://github.com/nodejs/llhttp/issues/115
- [x] llnode:master
- [x] llparse:master
- [x] llparse-test-fixture:master
- [x] lts-schedule:master
- [x] make-node-meeting:master
- [x] meeting-picker:master
- [x] mentorship:master
- [x] modules:main
- [x] nan:master - https://github.com/nodejs/nan/issues/920
- [x] napi-test-suite:master
- [x] next-10:master
- [ ] node:master
- [x] node-addon-api:main
- [x] node-addon-examples:main
- [x] node-api-headers:main
- [x] node-auto-test:master
- [x] node-code-ide-configs:master
- [x] node-core-utils:master
- [ ] node-gyp:master - https://github.com/nodejs/node-gyp/issues/2495
- [x] node-inspect:master
- [x] node-meeting-agenda:master
- [x] node-report:master
- [x] node-review:master
- [x] node-v8:canary
- [x] node-version-jenkins-plugin:master
- [x] node.js.org:gh-pages
- [x] nodejs-ar:master
- [x] nodejs-bg:master
- [x] nodejs-bn:master
- [x] nodejs-collection:master
- [x] nodejs-cs:master
- [x] nodejs-da:master
- [x] nodejs-de:master
- [x] nodejs-dist-indexer:master
- [x] nodejs-el:master
- [x] nodejs-es:master
- [x] nodejs-fa:master
- [x] nodejs-fi:master (archived)
- [x] nodejs-fr:master
- [x] nodejs-he:master (archived)
- [x] nodejs-hi:master
- [x] nodejs-hu:master
- [x] nodejs-id:master
- [x] nodejs-it:master
- [x] nodejs-ja:master
- [x] nodejs-ka:master
- [x] nodejs-ko:master
- [x] nodejs-latest-linker:master
- [x] nodejs-nightly-builder:master
- [x] nodejs-nl:master
- [x] nodejs-pl:master
- [x] nodejs-pt:master
- [x] nodejs-ru:master
- [x] nodejs-sv:master
- [x] nodejs-sw:master
- [x] nodejs-ta:master
- [x] nodejs-tr:master
- [x] nodejs-uk:master
- [x] nodejs-vi:master
- [x] nodejs-zh-CN:gh-pages
- [x] nodejs-zh-TW:master
- [x] nodejs.dev:master
- [X] nodejs.org:master - https://github.com/nodejs/nodejs.org/issues/3761
- [x] nodetogether:master
- [X] official-images:master - https://github.com/nodejs/docker-node/issues/1563
- [x] outreach:master
- [x] package-compliant:master
- [x] package-maintenance:main
- [x] post-mortem:master
- [x] promises-debugging:master
- [x] readable-stream:master - https://github.com/nodejs/readable-stream/issues/461
- [x] reliability:master
- [x] remark-preset-lint-node:master
- [x] repl:master
- [x] security-advisories:master
- [x] security-wg:master
- [x] snap:master - https://github.com/nodejs/snap/pull/17
- [x] social-media-delegates:master
- [x] string_decoder:master
- [x] tap2junit:master
- [x] tooling:master
- [x] tweet:main
- [x] undici:master
- [x] unofficial-builds:master - https://github.com/nodejs/unofficial-builds/issues/35
- [x] uvwasi:master
- [x] version-management:master
- [x] web-server-frameworks:master
- [x] webidl-napi:main
- [x] whatwg-stream:master
- [X] ~~worker:master~~ archived instead",0,0,msr
2817,"> post-mortem:master
Suprised this one hasn't been archived.",1,0,msr
2823,Updated a few of the inactive/historical CommComm initiative repos and checked them off.,0,0,msr
2826,"@mhdawson I already did education, apparently clicking the checkbox didn't take",0,0,msr
2842,"> There are concerns about repos like nodejs/node where we land PRs manually, that after the rename, collaborators can still push to master by mistake and recreate the branch by doing so.
>
> We can mitigate this with a branch protection rule that prevents pushes, but according to @Trott's tests, the rules do not prevent branch creation (first push after the rename).
Maybe it would work if we push an empty master branch and protect it? I don't know what would happen with GitHub automatic redirects in this case...
IIUC the concern is only until the migration happens (i.e. until when both `master` and `main` exist and are in sync in nodejs/node), correct? We can plan a time for the migration (similarly to what we do with security releases and CI upgrades), communicate it, and then during that time we restrict push to this repo to those involved in the migration. Once the migration is completed, we restore push to `@nodejs/collaborators`, and ensure the branch protections for `master` only let the bot push to it. I _think_ we can even add a git hook for push to give collaborators a message telling them to use `main` instead of `master` when pushing (not entirely sure).
Another idea I have (which I've been thinking for a different repository/organization, is to use the `post-checkout` git hook to communicate every time someone checks out to master (and try to identify if it was ran on our CI etc). That would help us identify when usage of master branch drops enough for us to safely discontinue it.",0,0,msr
2845,"@mmarchini I'm not sure, but I've not heard of any issues after switching over any of the other repos.
I do think treating it along the lines of a security release make makes sense. A pre-planned block of time where we restrict access makes sense.",0,0,msr
2848,@targos and I guess a loud FYI for people to update as well.,0,0,msr
2859,"I worked through most of the repositories so that we now have all but 6 out of over 100 moved over - see https://github.com/nodejs/node/issues/33864#issuecomment-773576351 for the checklist.
The remaining ones are more complicated in that there are external dependencies. I know that @richardlau has volunteered to look at some of them.
I've not had any cycles lately to loop back and see where we have made process/try to unblock. @bnb that is where a champion who can take a look at the overall picture, do want's need for the remaining repos or find volunteers for the remaining 6 could really help to close it out.",1,0,msr
2862,"https://github.blog/changelog/2022-05-05-block-creation-of-branches-that-have-matching-names/
🎉 (2)",0,0,msr
2874,"Re-ran steps to check all repos in the node.js org above (https://github.com/nodejs/node/issues/33864#issuecomment-773576351) and they look good:
[midawson@midawson rename]$ node doit.js - [x] .github:main
- [x] Gzemnid:main
- [x] Release:main
- [x] TSC:main
- [x] abi-stable-node:doc
- [x] admin:main
- [x] bot-love:main
- [x] branch-diff:main
- [x] build:main
- [x] build-toolchain-next:main
- [x] changelog-maker:main
- [x] ci-config-github-actions:main
- [x] ci-config-travis:main
- [x] citgm:main
- [x] cjs-module-lexer:main
- [x] code-and-learn:main
- [x] commit-stream:main
- [x] core-validate-commit:main
- [x] corepack:main
- [x] create-node-meeting-artifacts:main
- [x] devcontainer:main
- [x] diagnostics:main
- [x] docker-node:main
- [x] email:main
- [x] eslint-plugin-nodejs-internal:main
- [x] examples:main
- [x] full-icu-npm:main
- [x] full-icu-test:main
- [x] getting-started:main
- [x] github-bot:main
- [x] gyp-next:main
- [x] hardware:main
- [x] help:main
- [x] http-next:main
- [x] http-parser:main
- [x] i18n:main
- [x] icu4c-data-npm:main
- [x] js-native-api-test:main
- [x] llhttp:main
- [x] llnode:main
- [x] llparse:main
- [x] llparse-test-fixture:main
- [x] loaders:main
- [x] loaders-test:main
- [x] lts-schedule:main
- [x] make-node-meeting:main
- [x] meeting-picker:main
- [x] modules:main
- [x] nan:main
- [x] next-10:main
- [x] node:main
- [x] node-addon-api:main
- [x] node-addon-examples:main
- [x] node-api-headers:main
- [x] node-auto-test:main
- [x] node-code-ide-configs:main
- [x] node-core-test:main
- [x] node-core-utils:main
- [x] node-gyp:main
- [x] node-meeting-agenda:main
- [x] node-pr-labeler:main
- [x] node-review:main
- [x] node-v8:main
- [x] node-version-jenkins-plugin:main
- [x] node.js.org:gh-pages
- [x] nodejs-collection:main
- [x] nodejs-dist-indexer:main
- [x] nodejs-ko:main
- [x] nodejs-latest-linker:main
- [x] nodejs-nightly-builder:main
- [x] nodejs.dev:main
- [x] nodejs.org:main
- [x] official-images:main
- [x] package-maintenance:main
- [x] post-mortem:main
- [x] promises-debugging:main
- [x] readable-stream:main
- [x] release-keys:main
- [x] reliability:main
- [x] remark-preset-lint-node:main
- [x] repl:main
- [x] security-wg:main
- [x] snap:main
- [x] social-media-delegates:main
- [x] social-team:main
- [x] string_decoder:main
- [x] tap2junit:main
- [x] tooling:main
- [x] tweet:main
- [x] undici:main
- [x] unofficial-builds:main
- [x] uvwasi:main
- [x] version-management:main
- [x] web-server-frameworks:main
- [x] webidl-napi:main
[midawson@midawson rename]$",0,0,msr
2880,"We've completed running some tests and all looks ok at this point. We are not aware of any issues at this point.
This was a long an incremental effort (almost 2 years) and I'd like to thank @richardlau and @sxa who helped me do some of the tricker/more complicated repos. It was easier to do those as a group than it would have been for any one person on their own. Also thank to all of the other collaborators who supported in ways from finding a way to prevent accidental pushes of the old branch to quickly responding to issues suggesting the rename in repos across the nodejs org.
I'd also like to thank Red Hat management who early in the larger discussion on renaming primary repos across the GitHub ecosystem not only supported but asked teams to help open source communities make the change.",0,0,msr
2888,"Who put this garbage into your head? Should we also ban traffic lights because there are red and yellow lights (sorry, I can't even guess anything for green)?",1,1,msr
2891,"<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->
<!--- Provide a general summary of your changes in the Title above -->
<!---
Documentation on ZFS Buildbot options can be found at
https://openzfs.github.io/openzfs-docs/Developer%20Resources/Buildbot%20Options.html
-->
### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
These terms reinforce the incorrect notion that black is bad and white
is good.
### Description
<!--- Describe your changes in detail -->
Replace this language with more specific terms which are also more clear
and don't rely on metaphor. Specifically:
* When vdevs are specified on the command line, they are the ""selected""
vdevs.
* Entries in /dev/ which should not be considered as possible disks are
""excluded"" devices.
### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->
compiles
### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)
### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [ ] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [x] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).",0,0,msr
2892,"as I just posted an issue as a person of colour asking to have my thoughts considered, it was closed to discussion and locked as **too heated**.
this is a pretty blatant indicator that you didn't care about race relations at all.
please stop locking all the comment sections for these IMPORTANT ISSUES you supposedly want to further and support.",0,1,msr
2894,"As I indicated in https://github.com/openzfs/zfs/issues/10458, I'm open to a discussion on this. However, we have seen that when comments are open to anyone, the discussion quickly becomes dominated by people who are not involved in ZFS development, and too heated to be productive. (e.g. reddit, ars technica).
I believe that as the OP you should be able to continue to post to locked issues. Members of the OpenZFS github organization can also continue to post to locked issues.",0,0,msr
2895,"I determined that even the OP can't post on locked issues (unless they are a member of the organization). I didn't hear from you privately, so I'm going to unlock the other issue and continue the discussion there.",1,0,msr
2899,"What is an alt? Do you mean the german word for `old`? You're name doesn't seem very old, it seems like a minecraft name.",0,1,msr
2911,your ranting is misplaced. You should know this is not the place of such.,0,1,msr
2921,"> Chill out men!! kids under 10 can use retroarch confirmed, aside from that, your attitude it's so bad, you could read retroarch docs to how to make a plesant UI, also you could use kiosk mode to hide advanced settings, you can customize retroarch to your own flavor, but have respect to the devs, also you could make a PR to contribute, instead of just spittings scorpions from your keyboard, this is an open source project, help them, not sink them.
Hmm. I do not need 'pleasant ui'
Project exist 10 years. Classic emulator like nestopia snes9x was created with very small team, or with only one person
Or, for example, there is zsnes it has its own 'driver' like retroarch does, but its intuitive
![image](https://user-images.githubusercontent.com/61594968/85352867-eff37e00-b4d4-11ea-8f27-e45e43a1a67b.png)
only one 1 file, size is 580kb
It just works. Its not 'ideal', but its very easy to understand.
Ntsc 'shader' exists too
![image](https://user-images.githubusercontent.com/61594968/85353582-95f3b800-b4d6-11ea-854a-7c129d5f45e1.png)
Simple sliders
![image](https://user-images.githubusercontent.com/61594968/85353636-ba4f9480-b4d6-11ea-9f52-2611cbb05f7c.png)
Retroarch has **54** crt shader presets. No preview. No remember position. How does they work... Usually they are very strong or complex, some of them can lag nes on intel hd4000. No sliders or any visualisation
![image](https://user-images.githubusercontent.com/61594968/85353837-30ec9200-b4d7-11ea-914f-5bfb68c2a67c.png)
After 10 years of development with huge team...
For me as 'normal' user retroarch has only 1 advantage, i can calibrate image
I dont know all these ntsc crt things, just some colors changing, noise, blur...",1,1,msr
2932,"Oh some guys make core. There is a lot of cores for snes for example
In old days people who made core - they make interface too. Its art work too, by the way. Snes9x has its own interface, zsnes, nestopia
Its nothing wrong if people who make cores make usable interface. Or they can share retroarch build with included core
People actually have their own taste. If somebody likes classic emulators, he never make build with such huge scale for pc and dont assign function on alphabet keys. Or something like F1 calls menu
Its possible to make build for yourself or your friend",1,0,msr
2940,Pre-emptively locking the issue. I'm letting people know i'll be making this change. The change itself is not up for debate.,0,1,msr
2944,"@CodeLiam I guess my `Masters Degree` now has to renamed to `Main Degree` as well.. What people don't grasp is that in this context, the Master is like the Master in an audio recording. It's THE version. That's the context and the only context. I can't believe the insanity that's going on.",0,1,msr
2949,"Currently IPython uses `~/.ipython` which just clutters people's home directories and makes things like backing up configuration files much more hassle than it should be. This was last discussed more than five years ago and should be discussed again. Points made at that point in time frankly, don't really hold up. It is more consistent to follow platform specs, people learn where to look first. Secondly, for support reasons, you don't really have to ask what platform they're on, just mention the three paths in one sentence, it's actually rather easy.",0,0,msr
2950,"For those looking for the previous discussion, it can be found [here](https://github.com/ipython/ipython/pull/4457).",0,0,msr
2956,"There is not one place where the term blacklist was used in this project code! Only once in a comment (how to change the comment???). Also a quick search for the term ""sanity"", ""dummy"" and so on did not result in any code line affected. Yeah sure... rename your ""master"" branch to ""primary"" or so... The lazyness from OP stinks as it is easier to . Close this thread, as this is not stopping supposed racism but some person feeling better by yakshaving instead of actually solving issues.",0,1,msr
2959,@Uzlopak Your comments are not constructive and the tone is not very respectful. Please read our [Contributor Covenant Code of Conduct](https://github.com/sinonjs/sinon/blob/master/CODE_OF_CONDUCT.md).,0,1,msr
2963,@rgeerts thank you for your contribution 💯,0,0,msr
2968,"No reason to open a duplicate. You will need to provide more info if you want to see this fixed. I also suggest toning down your language a bit, most people working on Rust do this in their free time, and it isn't exactly motivating to have people complain like that.",1,1,msr
2971,"@storeilly, @kraney, @maallyn I updated the code on the feature_singlemixserver branch with the following functionality:
- The master ""director"" client defines the mode, i.e. if mono/stereo and the audio quality.
- The slave clients have to use the same mono/stereo and audio quality settings as the director, otherwise their client will show ""TRYING TO CONNECT"" all the time until it automatically disconnects after a while.
- Only the master ""director"" sees the full audio mixer board faders and can control the audio mix. All slave clients will see an empty audio mixer board.
It would be great to get feedback from you:
- Is the source code stable?
- Does it work according to the above specification?
- How many clients does a fast server PC can serve in that mode (I would recommend to use Mono mode to get the most number of clients)?",0,0,msr
2975,"Is there any way what's done could be tuned to having each participant (apart from the director) see only their own channel control? This wouldn't save server side processing - you'd still have n mixes to create. But it retains simplicity in the UI whilst preserving control.
The levels would be set by the director, apart from a participant's own level, which they could set for themselves. Perhaps it would still need a ""balance"" so you could raise your own level against the rest of the mix, or in this mode, your own slider becomes ""balance"" -- at the bottom, you only hear everyone else, at the top, you only hear yourself, in the middle you hear the director's mix.
(I'd also think a ""list participants"" option would be needed. Otherwise I'd be wondering ""who's here?"" all the time.)
----
Of course, my other question is -- how does this affect the jam recorder? Is the emit AudioFrame still done before the mix? This seems like it would answer one of my questions on the thread for recording a pre-mixed file - the director's mix is what you'd record. (Though I'd still advise a new signal/slot, it needs the filename handled properly and the file not included in the projects.)",1,0,msr
2989,"See here:
- https://github.com/corrados/jamulus/issues/547#issuecomment-692877664
- https://github.com/corrados/jamulus/issues/547#issuecomment-695171942
- https://github.com/corrados/jamulus/pull/535#issuecomment-679365410
A simple approach with only little changes in the Jamulus server code is wanted. Specification:
Adding a new command line argument to the server like --singlemix:
- No multithreading (since we only have one encoding and mixing so no multithreading needed)
- Only 128 sample frame size support
- Only Mono support (gives us the most possible number of connected clients which is what this modification is all about)
- The first connected client on that server is the ""director"", all other clients which connect afterwards get his mix. So you just have to make sure that the director is already connecting to the server before your session begins (this requirement should be very easily to be fulfilled).
There is a vecvecbyCodedData buffer which is used for both, encoding and decoding. I'll introduce a separate buffer so that I can re-use the output buffer of the first client for all other clients. So instead of calling MixEncodeTransmitData for all the other clients, they simply get vecChannels[iCurChanID].PrepAndSendPacket ( &Socket, vecvecbyCodedData[iDirectorID], iCeltNumCodedBytes );. I just did a quick hack: If I modify CreateChannelList so that no client is added, the audio mixer panel is just empty. This would be the case for the slave clients. But then they do not see how many clients are currently connected which is not a big issue. If ""--singlemix"" is given, ""-F"" and ""-T"" is deactivated and a warning is shown that these cannot be combined. In the OnNetTranspPropsReceived function we can check that the client uses 128 samples, if not, refuse to connect.
There is a new branch were the implementation is done:
https://github.com/corrados/jamulus/tree/feature_singlemixserver",1,0,msr
2993,"> They can control their own input level - no one else has control over that, just like in Jamulus.
Presumably you mean their own *output* level, i.e. singing volume?",0,0,msr
3001,"> Was there a change in the audio protocol, which makes Version 3.6.2 somehow incompatible with 3.5.12 (but only for receiving signals)?
No, there wasn't. I don't see a reason why the 3.5.12 server should be incompatible to 3.6.2. There are way older Jamulus servers still in use, see, e.g., https://explorer.jamulus.io. There you can see the versions of the servers.
Maybe there is some other issue in your setup which prevents this problematic client not to have any audio.",1,0,msr
3002,"That was my hope, but I didn't find an issue in my setup.
The strange thing is, if Client3 connects to the idle server, then I hear myself, but if I'm the second or later, I don't hear anything.
The special thing with this branch is that the first client becomes the ""director"", which defines the mode (mono/stereo and the audio quality). If Client3 is in ""director"" mode, audio is present, otherwise not.
If the ""director"" is a 3.6.0 client (Client1), Client3 audio is received, in case of 3.6.2 no audio at Client3.
I got the same issue with 3.6.0 when ""director"" mode settings are different to Client2/3. settings.
Maybe that additional info helps ...
Do you need any additional info's? Is there a possibility to verify the mode settings (is it sent over the protocol)? Debug via wireshark?",0,0,msr
3004,"That is, what I try to explain, same hardware setup, same client config, the only difference: if the ""director"" is version 3.6.0, Client3 works as expected, if version 3.6.2, then I receive no audio on Client3 :-(
Is it possible to enable some ""detailed"" logging to see, which mode is transferred to the server?
I'd like to dig into more detail, but maybe you can give me some hints. Thanks.",1,0,msr
3007,"Hello and thanks for Jamulus in general!
We are ""ramping up"" Jamulus in our 23-people choire, too. We had a test run with 5 clients today, and already experienced vast differences in levels (& delays).
I like the ""delegation"" idea a lot, should that become a separate ticket? In my dreams, this would include downstreaming the slider positions, so I can derive my personal from the conductor's mix. (edited:) I just found that is just another step from there:
#756 > Either all of your clients are < 3.6.0 or all your clients must be >= 3.6.0.
All clients including or excluding the serverinstance?",0,0,msr
3008,"> Or do you see a chance to rebase this branch to the master?
I have just merge the latest code into the feature_singlemixserver. Can you please test if it now works fine for you?",0,0,msr
3013,"The problem is that the ""official"" rule is to listen to the signal from the server. What about adding a Parameter to the single mix Parameter which enables/disables the own signal for all singers?",1,0,msr
3019,"Hi, Sorry for the maintenance noise here. I’m just into triaging issues.
Since this issue is locked, and another discussion is linked by gilgongo it doesn't make sense to leave this issue open. Therefore, I'll close it.
Please continue any related discussion in the other discussion.
Note to the topic: See the feature branch: https://github.com/cwerling/jamulus-mastermix/tree/feature_singlemix_reloaded which any interested person can take up if needed. Please open a new issue if you plan to improve it.",1,0,msr
3020,"@marciseli Which version are you using? which errors are you facing? Other than that, have you read the issue template? https://github.com/FreeRADIUS/freeradius-server/blob/master/.github/issue_template.md",1,0,msr
3027,"Core apps really shouldn't trigger deprecation messages, but get fixed (arguably prior to a release).
Or other way round: if the implementation of files_sharing is considered stable and future proof, NC shouldn't mark it as deprecated.",1,0,msr
3031,"> As a user
So as a user you're complaining about a *debug log output*. Seriously, why do you worry about this so much? You enable debug mode for a few secs to gather some info, then you disable it again.
Now tell me: would you rather have a silent log where we, the maintainers, can't help you solve an issue due to the lack of information?",0,1,msr
3032,"> you're complaining about a _debug log output_
No, actually I'm not complaining about a debug log output, and I've already conceded that deprecation notices might be useful for developers. What I'm specifically stating is that **deprecation notices** at a rate of 3/second is a **useless thing in the UI**.
> Now tell me: would you rather have a silent log where we, the maintainers, can't help you solve an issue due to the lack of information?
I thought deprecation notices are more of a reminder to later tackle the phased out code. Am I wrong?",0,1,msr
3035,"In particular, if you're using the Logging app with the auto-refresh option checked in the web UI, the requests generated by the auto-refresh itself will trigger this very frequently. I ran into this myself recently and if you turn that off and then tail the actual log file on the command line, you'll see it settles down.",1,0,msr
3042,"It seems, that the logger itself is responisble for the endless production of deprication warnings in the logfile. If this could be avoided the problem would be smaller and acceptable. As I am not familiar with the core development, I have no real proposal for a solution.",1,0,msr
3046,"Wth are you talking about
On Fri, Jan 22, 2021, 1:36 AM Bjoern Franke <notifications@github.com>
wrote:
> We have loglevel 1 and the log is spammed with deprecation warnings, 25GB
> log in the last days.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/nextcloud/server/issues/23046#issuecomment-765235684>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARULPN57G2EOV52LAFLOQFDS3E2HVANCNFSM4RZQC2IQ>
> .
>",0,1,msr
3047,"> Wth are you talking about
Some entries above @ChristophWurst says users should not complain about debug output when setting loglevel to `debug`. I changed loglevel 0 to 1 expecting `info` messages, but no deprecation warnings. And the logfile grew from 2GB to 25 GB in 4 days, full of deprecation warnings.",0,0,msr
3049,"Why aren't repeated duplicate log messages rate limited (the approach the Linux kernel takes is good where if a message is repeated a lot of times in `dmesg` it'll just stop showing it and print a message saying the previous message was repeated X times)?
![Screenshot_2021-01-22 Settings - Nextcloud](https://user-images.githubusercontent.com/3309784/105509550-85eac380-5cc5-11eb-8666-77f5d66a4039.png)",0,0,msr
3052,One step closer to a more reasonable debug log: https://github.com/nextcloud/server/pull/25497 :v:,1,0,msr
3055,"> files_sharing
Be my guest and help finish what I was unable to finish https://github.com/nextcloud/server/pull/25267 :wink: > contacts
https://github.com/nextcloud/contacts/issues/1831
> suspicious_login
https://github.com/nextcloud/suspicious_login/issues/499",0,0,msr
3069,@morozov I am very disappointed as allowing CI to run on push in very and industry standard practice across broad spectrum of open-source projects. Please see https://github.com/doctrine/dbal/pull/4349#discussion_r506721571,0,1,msr
3073,"> Many of them just create a branch, push, and want to see the CI result.
Isn't that what draft PRs are for?",0,0,msr
3078,"I am saying, that there a cases, when PR is not needed. I think, we can agree, that every change/experiment does not need a push to PR nor needs an attention from others.
To be objective, I have provided links to popular repos - in https://github.com/doctrine/dbal/pull/4349#discussion_r506721571 - that this is ""very common practice"".",0,0,msr
3083,@morozov please review the discussing with @greg0ire here. I find this PR very helpfull as any push runs a CI test which is very helpfull information for developer,0,0,msr
3085,"I'm locking this conversation as too heated and not bringing any value to the project. @mvorisek if you don't stop pushing your agenda, harassing maintainers, and creating an unnecessary sense of urgency, you will be banned from this project.",0,1,msr
3087,"- Please paste the whole log messages here, not parts of ones. It should start with `Initializing LaTeX Workshop`. It is very important to identify problems.
![スクリーンショット 2020-02-14 20 41 25](https://user-images.githubusercontent.com/10665499/74528024-87b2ef80-4f6a-11ea-9c5d-4f9feff25524.png)
When all of the above is done, we will reopen this issue.",0,0,msr
3091,"Thank you for your explanation.
If I have understood correctely what tab group means, PDF tab is on the same group of my other code file. When I type Ctrl+Alt+V (View PDF Latex File), it opens on a different group, as you can see here
![image](https://user-images.githubusercontent.com/22801918/96390803-aff38d80-118c-11eb-9f5d-3e49c1006c80.png)
Then I append manually the PDF tab to my previously tab group, like that
![image](https://user-images.githubusercontent.com/22801918/96390838-d0234c80-118c-11eb-9f8a-f40d90fafcb1.png)
In this way, I can pass through all tabs with the keybing `alt+n`. However, when I go to PDF tab, this keybind doesn't work anymore...",0,0,msr
3092,"See `Go` > `Switch Group` on the menu.
This issue is not related to LaTeX Workshop. Please ask https://stackoverflow.com/ on the primary usage of VS Code.",1,0,msr
3097,Reported microsoft/vscode/issues/109147.,0,0,msr
3111,"It'd be very nice to be able to watch lbry videos on NewPipe. There is already support for SoundCloud, MidiaCCC and FramaTube. I'm all for it.",0,0,msr
3119,Yes please. Also don't pull that bitchute bullshit on us this time please!,0,1,msr
3125,"> I hope you guys are aware of the difference between closing a feature request that took little effort and rejecting a pull request that took a lot of effort.
@DraconicNEO Doesn't matter. If you read the contribution guidelines, you will see that we strongly advise contributors to get in touch with us _before_ doing anything major, _precisely_ to avoid wasted effort. If a contributor ignores that, it is on them.
BTW, there was no such pull request. There have only been issues asking for support. You might be thinking of another project or repo.",0,0,msr
3133,"> Did the developers of NewPipe **actually** say that they won't implement service X or Y because of ideological reasons?
Yes they did, here: https://github.com/TeamNewPipe/NewPipeExtractor/issues/434#issuecomment-1255038576
They went so far as to argue that you saying something they disagree with is a *violation of their rights*. Pure politics.",0,1,msr
3134,"Right. Lots of speculation, assumptions and accusations in the comments. And some whining, but that's par for the course.
Firstly, there have been no comments by any team member either accepting or rejecting the service. Nor has there ever been a PR adding support that could be accepted or rejected. So any assumptions or accusations based on that are wrong. Some voices of reason questioned this, so thank you for that. Your comments were still marked off-topic because that's just the case on Github. Feel free to go off-topic galore in our IRC channel.
Secondly, just like Bravenewpipe supports 2 additional services that Newpipe doesn't have, you can go all out and add whatever services you like to your own forks. Because FOSS. Or, you could _politely_ request the maintainers of a fork (there are at least 2) to add support. Because FOSS. But complaining about the supposed ""political views"" of team members won't suddenly make them 'see the light' or the 'error of their ways' and get them to drop everything to spend hours/days/weeks to add support for [insert your personal bandwagon service here] for free. Because FOSS. So there is no point in doing that, in this repo or in any FOSS project.
Finally, Odysee unfortunately caters to misinformation: https://www.reuters.com/investigates/special-report/usa-media-misinformation/
If you genuinely think that [insert your favourite content creator here] is not part of the misinformation camp and doesn't deserve to get blocked out merely because they use LBRY, consider persuading them to try Peertube. Not only will it potentially increase their viewers, but it would help them avoid the stigma associated with LBRY. And of course, you can add their instance, or one their chosen instance federates to, to Newpipe. (Just to be clear, this is not about superior tech, merely about exploring a platform that could help such creators out.)
And now, some food for thought: https://en.wikipedia.org/wiki/Paradox_of_tolerance
Service rejected. If, in the future, it turns out to be the case that a vast majority of LBRY content creators are genuine and the misinformation camp has been reduced to a tiny minority, we could revisit this service request.",0,0,msr
3135,"> Many people don't install to the default directory, but to the user directory (--user-install), in fact, it's the default on many distributions.
I don't think so. The users who are using the default package provided many of Linux distributions are minority. also see https://rails-hosting.com/2020/#ruby-rails-version-updates
I'm Ruby programmer over the 20+ years. I and my company also didn't use it. We use the custom runtime build by like ruby-build.
> No satisfactory resolution from the development team.
This comment is a little offensive.",0,1,msr
3136,"@hsbt > I don't think so. The users who are using the default package provided many of Linux distributions are minority.
Really? Is that why 64% of Arch Linux users have [installed ruby gems](https://pkgstats.archlinux.de/packages#query=rubygems)? Is that also why Debian packages rubygems directly into the `ruby` package?
> https://rails-hosting.com/2020/#ruby-rails-version-updates
Rails developers != Ruby users.
> This comment is a little offensive.
You think reality is offensive?",1,1,msr
3139,"Right, it _does_ feel like a hack, since these `gemrc` options are meant as permanent flags to the `gem` CLI commands. Not sure what the best solution is, need to think about it, but I don't like adding yet another environment variable.",0,0,msr
3140,"@deivid-rodriguez I've thought about it and tried many things. The environment variable is the only clean solution I see.
I don't think the configurations of `gem` and `bundler` were thought through.",0,0,msr
3143,"> I don't think the configurations of `gem` and `bundler` were thought through.
These libraries are like 15 years old. While I work on improving them I try to not make any judgement on past decisions, because te context at the time was completely different from the context now. To give you an example, when `--user-install` was added, maybe `bundler` didn't exist yet. Statements like the above feel quite arrogant to me, to be honest.",0,1,msr
3146,"> Be honest, respectable, and collaborative.
I am.
- Honest: I am speaking the truth, that's called **honesty**.
- Respectable: I am **not disrespecting** anybody. Ideas on the other hand don't have feelings and are meant to be criticized.
- Collaborative: I've sent multiple patches and addressed the feedback on those patches. That is called **collaboration**.
> This is the final notice.
Or what? You are going to willingly harm the users of rubygems that have been reporting these issues for more than **ten years**?
What you do is up to you. If you fix this, great, if not, I will take this ticket as evidence and suggest a hacky solution downstream to my distribution maintainers, since you leave us no other choice.
What you are doing is called [tone policing](https://en.wikipedia.org/wiki/Tone_policing), and it distracts from the issue at hand. It would be much more productive if you concentrated on **the bug**, rather than my tone, but that's your choice.",0,1,msr
3156,"@enoch85 If you're familiar with Open Source, then you should be familiar with the concept that ""we don't work for you"". You should also be familiar with the idea of ""you have zero right to demand that we do what you want"". You should also be familiar with the idea of ""please supply patches, as we can't do everything"".
We are _very_ sensitive to the all-too common practice of ""you do what I want, because I can't be bothered to do it myself"". We have very little sympathy for such poor attitude.
We don't expect everyone to be experts in FreeRADIUS. We do expect that people's contributions will be productive, and polite.
The current release (3.0.21) does not have Focal support because Focal was released after we released 3.0.21. As Matthew points out, Focal is supported in the v3.0.x branch. Which will become 3.0.22 any time now.
To close this out, we're sorry that the software was imperfect. But your approach is to take offence at straightforward, factual, comments. We suggest this is entirely the wrong approach.",1,1,msr
3157,"@mcnewton > as long as the correct build dependencies have been installed.
I don't remember the exact name now but some libxxx-c2 and libxxx-c3 weren't possible to install from Ubuntu Focals own repos (clean ISO from their site), hence `make deb` failed. I did `apt-get install libxxx*` (1500 MB of packages) just to don't miss out on anything, but still - no success.
If the docker solution works, then great!
@alandekok > ""please supply patches, as we can't do everything"".
I'm sure familiar with that, and I understand 100% - but putting things IN CAPITAL LETTERS when you could do **bold** or *cursive* instead just seemed offensive to me. > ""you do what I want, because I can't be bothered to do it myself""
Well, at least I tried. I hope you didn't found my efforts (though small) as not bringing anything to the table.
> We suggest this is entirely the wrong approach.
Noted, and sorry for that.",1,1,msr
3162,"After hours of struggling with getting this right, I thought I would post this config as the default one since the current one is wrong, or at least doesn't work.
When building from source, the paths are wrong. Also, systemd PID shouldn't be run in `/var/run` (`usr/local/var/run`), it should be run in `/run/folder`.
This PR solves all that so that the paths are correct, and the actual PID is run from the correct directory.
Also added logging for systemd instead of logging to syslog.",0,0,msr
3163,"Seems completely backwards to me. Perfectly OK from a MIT license PoV: your package, my freely provided code.
Discussion also at https://github.com/suimarco/proxy-manager/pull/1, but overall, instead of helping maintainers with their OSS work, forks are being done, and dependency graphs are turned into a mess.
As I mentioned there, there's a way to support the upstream work @ https://github.com/Ocramius/ProxyManager/tree/f65ae0f9dcbdd9d6ad3abb721a9e09c3d7d868a4#ocramiusproxy-manager-for-enterprise, and bugfixes for critical stuff are being backported anyway :shrug:",1,1,msr
3164,"And btw, thanks to the symfony ecosystem for ruining the mood again: was going to work on PHP 8 support this weekend, but yet again I find myself being nervous, angry and frustrated at people treating weekend projects (**LITERALLY DOING IT IN THE WEEKEND**) like paid-for products.",1,1,msr
3166,"There's no reason to create a organiation like `suimarco`. It's a really **toxic** movement and totally apart of the technical implementation. Personal discussions should live apart. Authors can do whatever they want with their packages, if it's that critical and needs to be versioned differently then let's move it inside symfony organisation to follow same policies and maintain it there. I understand the reasons, I don't like how. Stay away from personal attacks.",1,1,msr
3167,"As other have said, the naming here of the org/user is severely aggressive and makes it a _personal attack_ on @Ocramius . Call it `nicolas-grekas/proxy-manager` or something, and _allow issues to be reported_ on it. I don't want to pile on, but this is really working _against_ the PHP community by making aggressive moves like this.
@nicolas-grekas you might not personally like @Ocramius, but this is really a personal attack and is 100% out of order coming from a Symfony representative.",0,1,msr
3174,"For more information - I have developed a linter plugin for this issue, just trying to figure out how to contribute it.
https://github.com/computamike/remark-link-escape
Do I close this issue, or is that something that the team needs to do?",0,0,msr
3175,Let's leave it open; hopefully it will be addressed together with #5031,0,0,msr
3176,"It seems that the | in the link breaks the rendering
![C#](https://user-images.githubusercontent.com/464876/99858549-b73ff980-2b85-11eb-81bc-0cd2dcb04e77.png)
Renders correctly when viewing the page within GitHub editing tools, but renders incorrectly on the GH Pages site.",0,0,msr
3179,"> Took me awhile to find the relevant page: https://ebookfoundation.github.io/free-programming-books/casts/free-podcasts-screencasts-en.html
> > Appears to be rendering fine now
We know it. See the fix #5177. The vertical bar is escaped",0,0,msr
3181,"It also happens in Arabic courses file #6715 | Environment | Screenshot |
|---|---|
| Github File Preview :+1: | ![image](https://user-images.githubusercontent.com/3125580/153564450-5db31da1-beb6-458f-b783-734f06e277f8.png) |
| GitHub Pages :-1: | ![image](https://user-images.githubusercontent.com/3125580/153564725-14fdd447-9871-4d26-b2e4-ed0213fb1f57.png) ![image](https://user-images.githubusercontent.com/3125580/153564985-80c9f555-6218-44d0-bf89-27a7371274fd.png) |
Is wellknown that pipes is the table markdown token. Seems that Kramdown don't deal well with it",0,0,msr
3182,"Needs linter replug in to definitelly solve it. See https://github.com/EbookFoundation/free-programming-books/issues/5176#issuecomment-890391432
Notified to kramdown engine at https://github.com/kramdown/parser-gfm/issues/35",1,0,msr
3184,"I believe that most people following this repo will also be in the know from other sources, but passing this on as an update to my previous comment, the tag related to slash commands from the discord.py server:
> slash commands are a new way to make commands right within discord, discord.py will probably not support them as they are lacking features and requires a major rewrite to handle
>
> bad command handler:
> - no default argument system - the argument isnt passed if you dont pass it making handling harder
> - no Union/Optional system like dpy
> - cant handle arguments yourself - required to use there parser
> - no way to invoke group command - you can only invoke the subcommands
> - no command aliases
>
> limitations:
> - 15 mins max per command - a token lasts 15 mins
> - only one message can be hidden (the first one)
> - hidden messages cannot have embeds
> - without a bot token you cant do much apart from send messages and etc - no api interaction
> - you only get ids for the parameters - if the parameter is a user you only get there id
> - if using the webhook based intergrations you cannot get any other events so you are limited to commands
>
> if you do want to use them there is a half maintianed fork that has support for them ( we will not support them here ) if you wish to use them.
> https://github.com/Zomatree/discord.py/tree/slashcommands - no docs because im lazy
TL;DR - Unfortunately, it appears discord.py proper is unlikely to support slash commands due to some drawbacks in comparison to ext.commands. There is a non-supported fork which one may use, if you want the new features such as the command autocomplete, hidden messages, and context system messages. The above linked option still seems viable.",1,0,msr
3189,"@Znunu someone spamming some comments about the (current) usability of slash commands, don't worry about it",0,0,msr
3192,"What really has me sold about the slash commands that we've got lately is how user-friendly they've become with the UI to help guide users to using commands. In the past, we've had to type a help command or look at some personal bot website to understand the context of how they function, but now, that's all changed merely to a description for each option, each choice, and etc.
Idealistically, the only thing that slashes commands should be different than the commands we can already create and implement into our own discord.py bots should be the UI, everything should in my eyes come natively. Guild/role permissions, requirements, no limit to choices for options, and etc. are many things that we are still waiting to seek for slash commands to receive, either via. un-official 3rd party support or by the developers at Discord themselves.
Because of this, I continue to believe that we should seek to be able to code slash commands as an actual usable thing for the discord.py library, not because they're something new and trendy, but simply because this library is an API wrapper in and of itself. The main purpose it overall serves is to help make the Bot API easier to use with Python, and by those means, we shouldn't limit ourselves from implementing slash commands as well. They may not be used as much as regular commands, for sure, but in the future, we won't have to worry about having to code it later than doing it now. This is at least my consensus on it, and I think others might agree.",0,0,msr
3200,The linked issue microsoft/vscode/issues/110450 is the one. It has been reopened.,0,0,msr
3217,"> As a temporary fix for anyone just trying to get things working again:
> > ```
> pip install jedi==0.17.2
> ```
> > It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> > Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> > Still wish you a Merry Christmas!
Thank you for your solution.
I got the same issue and IPython worked after I installed Jedi",0,0,msr
3239,"Releasing a new Jedi version might be easier than waiting for the next IPython release.
@davidhalter, how about making a one time exception in this case and reintroducing the deprecated features, the lack of which breaks IPython (and a few more packages) in a Jedi 0.18.1?",1,0,msr
3246,"> Am I missing something here?
Not you, but conda's dependency resolver. This is a problem and has been for years. (Sorry for the off-topic, but I could not resist)",0,1,msr
3248,"Welp, spoke too soon, conda-forge has the same problem (basically, the higher build number seems to lose against having one dependency have a higher version, which is not how it _should_ work. A possible solution would be backporting https://github.com/ipython/ipython/commit/dcd9dc90aee7e4c5c52ce44c18e7518934790612 to the feedstock, but that really should happen here first).
> Not you, but conda's dependency resolver. This is a problem and has been for years. (Sorry for the off-topic, but I could not resist)
Have you heard of [mamba](https://github.com/mamba-org/mamba) already? It has a rewritten resolver but is fully compatible with the conda-ecosystem. Not only does it solve the biggest problem of the conda-solver (speed), but it also correctly sets up the environment here:
```
>mamba create -n test python=3.8 ipython
[...]
ipython 7.19.0 py38hc5df569_2 conda-forge/win-64 1 MB
ipython_genutils 0.2.0 py_1 conda-forge/noarch 21 KB
jedi 0.17.2 py38haa244fe_1 conda-forge/win-64 944 KB
[...]
```",0,1,msr
3256,Nice! I just confirmed this on my end. I guess I'll keep those callout boxes up on PLYMI for a bit in case folks had already installed anaconda.,0,0,msr
3260,"@Carreau: dude, you rock!",0,1,msr
3263,"@mostealth Obviously you have a different perspective on that matter. That's fine. However, it seems rather unproductive to dissipate the time and energy of productive people like @Carreau with pointless discussions. If you have deeper insights I see the following options: You can fork the project, file a PR or just sate your improvement suggestion in a way that is arguable, maybe in a dedicated issue.
For the rest of us I would suggest to basically ignore distracting comments which contain neither helpful information nor constructive criticism, but instead spread insults and negative emotion. Reactinv with :-1: should be enough attention for such comments.",0,0,msr
3264,"@Carreau Thanks a lot! I guess the next release is going to be 1.0 anyway, so it shouldn't happen again. We might then think about pinning `jedi < 2.0.0` (which is probably years away).
In general for all people like @mostealth: If you are worried about older versions breaking: **Pin your dependencies!** The Python packaging system is still in a bit of a problematic state. The devs are doing a good job trying to turn it around, but there is a lot of technical debt from years ago. Keep in mind they want to keep backwards compatibility with a lot of old and new things. Just a enumerating a few things to show how complicated it is: eggs, wheels, explicit namespace packages, setuptools, distutils, import hooks, import meta paths, importlib, easy_install, venvs, virtualenvs, zip imports, freezed packages, source packages for C/C++ libraries and a lot of other things that I don't even know or remember anymore.
Most of you guys complaining don't really understand how hard certain things are if you consider all consequences. Read about Chesterton's Fence and try to understand why you shouldn't want something changed unless you understand why it is that way in the first place.",1,0,msr
3267,"<!-- This is the repository for IPython command line, if you can try to make sure this question/bug/feature belong here and not on one of the Jupyter repositories. If it's a generic Python/Jupyter question, try other forums or discourse.jupyter.org.
If you are unsure, it's ok to post here, though, there are few maintainer so you might not get a fast response. Ability of maintainers to spend time and resources on project like IPython is heavily influenced by US politics, and the current government policies have been harmful to the IPython Maintainers and Community. If you are on the fence on who to vote for or wether to vote, please cast your vote in for the democrat party in the US.
-->
Relevant traceback reads as follows: ```
File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete
completions = self._jedi_matches(
File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches
interpreter = jedi.Interpreter(
File ""../venv/lib/python3.8/site-packages/jedi/api/__init__.py"", line 725, in __init__
super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'
```
sys info: ```
{'commit_hash': '62779a198',
'commit_source': 'installation',
'default_encoding': 'utf-8',
'ipython_path': '../venv/lib/python3.8/site-packages/IPython',
'ipython_version': '7.18.0',
'os_name': 'posix',
'platform': 'Linux-4.15.0-128-generic-x86_64-with-glibc2.17',
'sys_executable': '../venv/bin/python',
'sys_platform': 'linux',
'sys_version': '3.8.5 (default, Jul 20 2020, 19:50:14) \n[GCC 5.4.0 20160609]'}
```
same reported in jedi repo too",1,0,msr
3269,"there still is autodeletion, just slower.
used disk space depends on the ratio between new build and time until deletion.
in past this ratio was such, that all builds were deleted. no build at all remained for download.
i suggest to go with the dirty solution of longer deletion-time and adjust it if disk space should become a problem. i can also look into deleting based on file names or dates and only keep a certain amount of builds.",1,0,msr
3276,"do not delete so soon.
relevant forum thread: https://springrts.com/phpbb/viewtopic.php?f=1&t=40040",0,0,msr
3277,"> Does nobody review changes, docs, and release notes anymore prior to the release? It looks this way.
this is not a friendly way to ask for things.
`DataFrame.lookup` has seen NO love at all since its initial implemention. It is a duplicative and unmaintained function. This is also polluting the namespace and is not in any way integrated to all of the other indexers. We do not need N ways to do the same thing.",0,1,msr
3278,https://github.com/pandas-dev/pandas/pull/35224 is the PR and https://github.com/pandas-dev/pandas/issues/18262 is the issue,0,0,msr
3286,"@impredicative you are not comparing apples to apples here. try with indexing operators, we *already* have `.loc`, `.iloc`, `.at`, and `.iat`, not to mention partial slicing and of course `[]`. which one shall we cut to keep `.lookup`????
I in fact have an issue to remove `.iat` and `.at`. We simply do not need this much choice.",0,1,msr
3290,"> @erfannariman
> > > out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background.
> > I learned about the deprecation of `lookup` when answering [this SO question](https://stackoverflow.com/q/65892265/4238408). And [this](https://github.com/pandas-dev/pandas/pull/35224#discussion_r564046976) is my thought on the proposed alternative for `lookup`. Of course, if current `lookup` is a `for` loop, this method would be faster for small dataset. But for bigger dataset, things would change due to potentially larger memory allocation, especially when column names are long strings and data to be looked up are homogeneous.
Sure I would be happy to make a PR with a new proposed more efficient method and there we can discuss into more details and the core devs can share their thoughts as well. What would be your suggestion, the answer you gave on SO? Could you maybe share a reproducible example in a new ticket and mention this issue? @quanghm",0,0,msr
3294,"Thanks for the extensive example and speedtests @quanghm . I did a quick check and seems like `lookup` wasn't a for loop actually, only for columns with mixed types. So the in terms of speed it looks okay. Only thing is that I remember it being quite slow for certain cases, but those must have been mixed type columns in my cases I think. See here for source code https://github.com/pandas-dev/pandas/blob/b5958ee1999e9aead1938c0bba2b674378807b3d/pandas/core/frame.py#L3848-L3861
For your approach, I agree that it's more efficient both in speed and memory allocation, but I remember writing multiple methods to replace `lookup` and I decided to go with melt since it was most ""readable"", without making it too complex for the documentation.
So I think it all boils down to @jreback and the argument that `DataFrame.lookup` has not been used as much, which is being discussed by @impredicative",0,0,msr
3300,"> Third - @impredicative please consider your tone
My tone is justifiable. I have had enough of Pandas being unprofessionally developed. Panda works only for simple manipulations of small dataframes, and scales extremely poorly or not at all. I have spent weeks and weeks trying to work around its various issues, and the best thing for me to do right now is to move to a real package like Dask, PySpark, etc. I'm sure that many other users here feel similarly.
Fourth - @MarcoGorelli please consider how Pandas is actually developed.",1,1,msr
3307,"This is a meta-issue to track reproducible reports of jank in Flutter apps.
If you are experiencing jank in your app:
1. Try to reproduce the problem in a test app. Either run `flutter create janktest` and recreate the situation you are experiencing in that app, or clone your app and delete code until you have the jank reproducing with a single .dart file.
2. [File a bug](https://github.com/flutter/flutter/issues/new?assignees=&labels=created+via+performance+template&template=5_performance_speed.md&title=) and include your .dart file demonstrating the problem. If you need more than just a .dart file (for example, assets are needed to reproduce the issue, or plugins/packages are needed to reproduce the issue) then create a GitHub repository and upload the app there.
Make sure to include the `flutter doctor -v` output and any logs from `flutter run` and `flutter analyze`.
3. Switch flutter to master channel and run this app on a physical device using profile mode with Skia tracing enabled, as follows:
`flutter channel master`
`flutter run --profile --trace-skia`
The bleeding edge master channel is encouraged here because Flutter is constantly fixing bugs and improving its performance. Your problem in an older Flutter version may have already been solved in the master channel.
4. Record a video of the performance issue using another phone so we can have an intuitive understanding of what happened. Don’t use ""adb screenrecord"", as that affects the performance of the profile run. Attach the video to your bug.
5. Open Observatory and save a timeline trace of the performance issue so we know which functions might be causing it. See ""How to Collect and Read Timeline Traces"" on this blog post:
https://medium.com/flutter/profiling-flutter-applications-using-the-timeline-a1a434964af3#a499
Make sure that the performance overlay is turned OFF while recording the trace.
Attach the JSON file containing your trace to your bug. You may also wish to include a screenshot of the part of the trace showing the problem you are seeing, just so that people can see at a glance what kind of performance issue the bug is about.
6. Mention _this_ bug in your bug, so that GitHub includes a link to it here.
Please avoid commenting on this bug. Keep each issue separate so that we can examine each specific problem individually. Having one issue that contains comments about multiple problems make the issue intractable.",0,1,msr
3311,"It has helped us when triaging performance issues to attach the timeline trace with the triage response and it also acts as a nice guide to point to authors of the issue and everyone else. Some sample comments where it has helped us
https://github.com/flutter/flutter/issues/104709#issuecomment-1139451006
https://github.com/flutter/flutter/issues/87811#issuecomment-900019236 (I got to know about this issue here)",1,0,msr
3316,"> The reason I'm asking this is because thousands of people are still actively using these projects. If the project is losing momentum (which it seems) we have to move towards an alternative choice.
You don't necessarily have to move. You could also incentivise contributors and maintainers by supporting their efforts either financially or through active participation.
> Developing a product with a dying project can result in serious financial losses!
The fact that there was a release roughly a month ago (2020-12-04) should tell you that the project is not inactive. The work on an upcoming 2.9.x release could also help indicate at some activity. Not to mention the activity in sister projects like dbal and the releases there.
If you consider this project vital it might make sense to let the maintainers work on it instead of wasting their time with frivolous claims or requests of a risk assessment for your projects or whatever this is supposed to be.
If you have an alternative that you feel is more stable and a better choice for your project, feel free to switch to it. Otherwise I suggest you put your energy in supporting the dependencies that you feel are vital to your projects so they don't become ""dying projects"".",0,1,msr
3317,Let me close this as a way to show you that we do have a look at issues.,0,0,msr
3320,"@mdogancay Doctrine's code base is over 10 years old and heavily matured. While we do have 1000 open issues, when I worked through 100 of them in December only a small number was actually bugs, most of them are help requests and users wrongly using or misconfiguring. A lot of issues are feature requests that we tend to be conserative about and not consider.
The complexity of Doctrine often makes it very time consuming finding the ultimate problem of the user. We provide free software, not free support. Yes, not much is happening code wise, but that also means not a lot of stuff is breaking or requires your constant attention, getting to the new APIs.
There isn't an ORM in PHP that comes even close to features that Doctrine ORM has. Due to the nature of ORMs being leaky abstraction, it is my firm believe working on ORMs for 15 years now, that this feature richness comes at the price of complex and sometimes ""fragile"" internals (you can trade this off for better internals at low performance). This proves to be a high hurdle for new contributors and it takes a while to understand the code to understand the side effects of a change. As such you should be happy that we are keeping it working as is.",1,0,msr
3324,"@ronblum Yes, this came about when adding database support. The dependency should not be necessary on the desktop, but due to how we deal with dependencies a shared component that depends on this library is pulled in by both server and desktop. This requirement could be removed in a future release.",0,0,msr
3328,I'm attempting to install rstudio-2021.09.1-372 and am using R 4.1.0. Is there really no way to get around the postgres requirement? We support a shared high performance computing environment and do not install databases. Is there a way to install just the libraries necessary to get the rstudio install to complete? Any advice is greatly appreciated!,0,0,msr
3331,"> Note: I believe we're requiring an installation of `libpq5`, but not all of PostgreSQL, when installing RStudio Desktop in a Linux environment.
WHY? I have used RStudio desktop for years, and it never had this requirement before version 1.4.
1. RStudio desktop should not require this library.
2. Database connectivity should remain in the odbc package.
3. Typical desktop installations don't have this library (including mine - I would have to compile the entire PostgreSQL to upgrade to RStudio 1.4).
4. If you really feel that RStudio desktop absolutely **has** to have libpq, then why don't you provide it in the package? The installation package includes libraries for Qt and icu, which virtually every Linux desktop has. It makes absolutely no sense whatsoever to include those libraries with RStudio, yet demand users produce their own libpq library.",0,1,msr
3336,"@mikebessuille > @dsajdak are you installing desktop, or server? What platform? I'm not understanding why this is a concern; you shouldn't have to install a database to get it to work.
I'm attempting to install desktop version 2021.09.1+372, for CentOS 7, from source. We install everything to a shared mounted filesystem and can not install using OS packages. I spent 2 days getting all the dependencies installed in the shared filesystem only to have the rstudio 'cmake' fail with:
`CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
Please set them or make sure they are set and tested correctly in the CMake files:
/shared/directory/.../PQ_LIB
linked by target ""rstudio-core-tests"" in directory /shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
linked by target ""rstudio-core"" in directory/shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
SOCI_POSTGRESQL_LIB
linked by target ""rstudio-core-tests"" in directory/shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
linked by target ""rstudio-core"" in directory /shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
`
I was hoping someone would have a suggestion as to how to get around this without a full Postgres install.",0,0,msr
3340,"> The `libpq` library has quite a few dependencies itself, so I think bundling them all and shipping them with RStudio is error-prone.
Hard for me to believe that bundling those exceeds the complexity of bundling `libQt5WebEngineCore`. :)",1,1,msr
3345,"Your perspective is much appreciated; please maintain a professional tone.
Coding decisions including dependency decisions are not made frivolously. Sometimes, design choices are made to minimize development cost, maximize maintainability, reduce testing cost, or to have common code between the many versions of our product (server, desktop, pro, open source). As such, this issue is not a bug. Nor is it an ""idiotic"" design choice. We will certainly consider improving it such that it doesn't affect the case you describe, keeping in mind that most of our users are unaffected by this, as they install the IDE from installers (which take care of the appropriate dependencies), rather than by building from source or by unpacking a tarball and manually installing.",1,1,msr
3350,"Hi!
As an example Node.js and https://nodejs.org/api/fs.html is starting to recommend promise-based versions foremost. (compare with earlier versions that has the promised based versions last: https://nodejs.org/docs/latest-v14.x/api/fs.html).
Express 5.0 has added automatic handling in routes of async/await routes that will automatically invoke the Express error handler if a an async function throws. There are current workarounds to create a `wrapAsync()` function around routes, but in bigger projects some users miss this and it's also not-so-nice-looking (and confusing for some).
I am merely wondering if Express 5.0 could be released, and current items on https://github.com/expressjs/express/milestone/11 could be moved to a 6.0 release? I am very respectful of the fact that you voluntarily work on this, and these kind of questions are frowned upon. https://github.com/expressjs/express/pull/2237 was created 7 years ago now, so I at least think it should be ok to ask for this without in any way not being grateful of this project already and its release cadence.
I guess what I am saying is that there are some nice things already on the main branch for Express 5.0 that works good with latest versions of Node.js 15, so a release would be amazing.",1,0,msr
3352,"Hi @thernstig I'm sorry for one of the comments in the thread; I went ahead a hid it for now.
As for your idea, that is pretty much what the TC decided, even though there is still a lot of flac from the community about deferring things like http/2 to express 6. Really, there is only one outstanding issues for express 5, and it is related to async/await in the router. There is an open PR to address this, but the author has kind of faded away. Ideally that's really all that needs to be fixed if the idea is for express 5's main goal to be async/await.",0,0,msr
3357,"@dougwilson, could you share the only issue holding off the release?",0,0,msr
3367,"quest backing up likely unintended feature is not a good proof of intention since quests get rewritten a lot. Previously tool forge took MV to get due to 3 alum screws so i still dont see the point, we made tool forge just fine back then by either waiting or foraging for aluminium ingots",0,0,msr
3416,"Xamarin as a name might be a history but MAUI is not. As MAUI use native controls (native SDKs) to render UI that means it will not go out of support and native dev will be available -> Android/iOS sdks will have to be updated.
MvvmCross been heavily supported over many years, I think I've been using it since 2015? Mobile development (and the enormous pace SDK's evolve - in opposite to ex. backend/database development) - means breaking changes here are something expected and not unusual.",0,0,msr
3418,"You clearly don't understand what you are talking about :)
Tomasz said he will release .NET 6.0 support (which is required to build ""new"" MAUI toolset project/app). If he won't release that, I will pick and complete that. Therefore, native development will still be available. MAUI uses native SDKs and native tooling to produce builds.
MAUI-XAML-multiplatform support is a different story. If you want to go full xaml/full MAUI-views (as it was with Forms) then there is no really need/much benefit of using MvvmCross as framework. I (and many other devs) will keep building native apps anyway - I plan to create a bridge where I will consume some MAUI/XAML views in addition inside MvvmCross/native app. Right now, it's too early to bother, MAUI is RC, not production ready.",0,1,msr
3436,"Hey,
Thank you for all those suggestions. I'm gonna keep an eye on those.
It is better to split your suggestions in order to have one suggestion per issue. I need to split it that way because it is easier to produce a changelog, for example:
## v1.2.5 includes
- issue #9876
- issue #9765
- issue #1269
For example, I implemented your request ie this comment https://github.com/binogure-studio/city-game-studio-i18n/issues/587#issuecomment-819647849 . I cannot put a link like that in the roadmap: https://github.com/binogure-studio/city-game-studio-i18n/projects/2 because if the ticket contains more than one suggestion and I implemented only one _comment_ then I cannot close the issue and I cannot publish a new version without breaking everything.
So I'm gonna lock this issue, because I won't be able to track it correctly.",0,0,msr
3437,"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
**Describe the solution you'd like**
A clear and concise description of what you want to happen.
1-Is it possible to add ""Filter by name"" or Search button in sales report right hand side menu?
When you have so many games it takes too much time to find the game that you want and apply discount or update.**_
2-The bottom panels is too small now while the older version was better; but the left and right hand side panels are okey now because in older version they cover too much space.
3-The fans evolving threshold may perhaps be revised.Of course reality is important but in fact this is a game.When you open lots of studios and produce thousand of games you obviously reach limits quicker than it was foreseen,and it is not pleasant to see losing fans at this point while doing very good games...
4-The auto-furnish option's desk efficiency is very low: in 750m2 studio it places max 47 desks while you can manually place 70 desks easily with 100% happiness.In auto furnish mode you have to make adjustments as well because 100% happiness is not guaranteed.
5-The latest consoles of the game(each material latest technology) can have a cost greater than 1K $ while the game limits the selling price to 1K $ so it's a deficit in debut.
**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.
**Additional context**
Add any other context or screenshots about the feature request here.",1,0,msr
3445,This was discussed in the Node-API team meeting today and and consensus was we don't think we want/will change at this point. Some of the team members could not comment directly since it was locked.,0,0,msr
3449,[Works for me](https://plnkr.co/edit/mgmTP9xh7dBfMJUF).,1,0,msr
3452,"@IvanSanchez No it does not:
```
dragend event parameter 0 {distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, …}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {…}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, …}
type: ""dragend""
__proto__: Object
map_pins.js:229 calling function ƒ (marker) {
//console.log('pin was dragged to', marker.latLng.lat(), marker.latLng.lng());
this.coords.lat = marker.target._latlng.lat;
this.coords.lng = marker.target._lat…
map_pins.js:230 using parameter 0 for aforementioned function {distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, …}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {…}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, …}
type: ""dragend""
__proto__: Object
map_pins.js:232 function ended
map_pins.js:233 map.panTo() fails here, map object: i {options: {…}, _handlers: Array(6), _layers: {…}, _zoomBoundLayers: {…}, _sizeChanged: false, …}
attributionControl: i {options: {…}, _attributions: {…}, _initHooksCalled: true, _map: i, _container: div.leaflet-control-attribution.leaflet-control}
boxZoom: i {_map: i, _container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-dra…, _pane: div.leaflet-pane.leaflet-overlay-pane, _resetStateTimeout: 0, _initHooksCalled: true, …}
doubleClickZoom: i {_map: i, _initHooksCalled: true, _enabled: true}
dragging: i {_map: i, _initHooksCalled: true, _enabled: true, _draggable: i, _positions: Array(0), …}
keyboard: i {_map: i, _panKeys: {…}, _zoomKeys: {…}, _initHooksCalled: true, _enabled: true, …}
options: {}
scrollWheelZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 98, _delta: 0, …}
touchZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 99}
zoomControl: i {options: {…}, _initHooksCalled: true, _map: i, _leaflet_id: 94, _zoomInButton: a.leaflet-control-zoom-in, …}
_animateToCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_animateToZoom: 10
_animatingZoom: false
_container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-drag.leaflet-touch-zoom
_containerId: 92
_controlContainer: div.leaflet-control-container
_controlCorners: {topleft: div.leaflet-top.leaflet-left, topright: div.leaflet-top.leaflet-right, bottomleft: div.leaflet-bottom.leaflet-left, bottomright: div.leaflet-bottom.leaflet-right}
_events: {moveend: Array(3), zoomend: Array(1), zoomlevelschange: Array(1), unload: Array(4), dblclick: Array(1), …}
_fadeAnimated: true
_firingCount: 0
_handlers: (6) [i, i, i, i, i, i]
_initHooksCalled: true
_lastCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_layers: {100: i, 117: i}
_layersMaxZoom: 20
_layersMinZoom: 0
_leaflet_id: 91
_loaded: true
_mapPane: div.leaflet-pane.leaflet-map-pane
_onResize: ƒ ()
_paneRenderers: {}
_panes: {mapPane: div.leaflet-pane.leaflet-map-pane, tilePane: div.leaflet-pane.leaflet-tile-pane, shadowPane: div.leaflet-pane.leaflet-shadow-pane, overlayPane: div.leaflet-pane.leaflet-overlay-pane, markerPane: div.leaflet-pane.leaflet-marker-pane, …}
_pixelOrigin: k {x: 53035, y: 99811}
_proxy: div.leaflet-proxy.leaflet-zoom-animated
_resizeRequest: 113
_size: k {x: 270, y: 1000}
_sizeChanged: false
_targets: {92: i, 118: i}
_zoom: 10
_zoomAnimated: true
_zoomBoundLayers: {100: i}
__proto__: i
```",0,0,msr
3454,At the map size shown.,0,0,msr
3456,"Here is this.pin
```
this.pin {obj: {…}, info: div, draggable: true, marker: i, dragCb: ƒ}
dragCb: ƒ ()
draggable: true
info: div
marker: i
dragging: i {_marker: i, _initHooksCalled: true, _enabled: true, _draggable: i}
options: {title: ""af-40mp-625rmr"", icon: i, zIndexOffset: 1, draggable: true}
_events: {remove: Array(2), dragend: Array(1), click: Array(1)}
_firingCount: 0
_icon: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_initHooksCalled: true
_latlng: D {lat: 39.001823432304754, lng: -106.97038027457896}
_leaflet_id: 103
_map: i {options: {…}, _handlers: Array(6), _layers: {…}, _zoomBoundLayers: {…}, _sizeChanged: false, …}
_mapToAdd: i {options: {…}, _handlers: Array(6), _layers: {…}, _zoomBoundLayers: {…}, _sizeChanged: false, …}
_shadow: null
_zIndex: 501
_zoomAnimated: true
__proto__: i
obj:
alertDisabled: 0
channel: 0
clientInfo: ""ispapp-snmp-relay-0.1""
createdAt: 1620056840
fw: null
fwVersion: null
groupId: ""5f88e6a24ba991445d8ec738""
hardwareCpuInfo: null
hardwareMake: null
hardwareModel: null
hardwareModelNumber: null
hardwareSerialNumber: null
key: ""asdfasdfasdf""
lastConfigRequest: 1621359482
lastUpdate: 1621379634
latitude: 38.86706476159009
login: ""af-40mp-625rmr""
longitude: -106.98185137007387
name: ""af-40mp-625rmr""
notes: ""testing snmp relay""
os: null
osBuildDate: null
osVersion: ""null""
outsideIp: (8) [""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14""]
outsideIpChangeTsSeconds: 1620146969
outsideIpChanges: 1
reboot: 0
ssid: ""undefined""
uptime: 1809293
usingWebSocket: true
vlan: ""undefined""
wanIp: ""63.151.94.152""
wds: ""undefined""
wirelessBeaconInt: 0
wirelessChannel: 0
wirelessConfigs: []
wirelessMode: ""ap_bridge""
zoneId: ""5f88e6954ba991445d8ec736""
_id: ""60901b075223241f3a472497""
__proto__: Object
__proto__: Object
```",0,0,msr
3459,Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.,0,1,msr
3462,"Everything is there and readable. If you cannot explain the reasoning as to why that logic would not prevail then the code is flawed. Thank You,
Andrew Hodel
> On May 18, 2021, at 6:50 PM, Andrew Hodel ***@***.***> wrote:
> > Why? You should be able to plug in the parameters to each function easily. That is why I did it. > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 6:48 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> >> >> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
>> >> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",0,0,msr
3470,"You could write it to a file or with a prefix and pass that to a library test that automatically assured everyone of validity in application code or error in library code. I do not have understand your reasoning for “playinbrowserjs” or “required bug fields” when you have a library this pervasive and this solution is that much better. Thank You,
Andrew Hodel
> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> > The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> > Why you wouldn’t is beyond understanding. > > I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. > > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> >> >> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> >> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",0,0,msr
3471,"I would also like an apology or a reason as to why the smart way isn’t a debug option in the library with a prefix on the console output, as you want me to stop and read a book rather than paste console output to a test that verifies code in library and the application. Thank You,
Andrew Hodel
> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> > The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> > Why you wouldn’t is beyond understanding. > > I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. > > > > > Thank You,
> Andrew Hodel
> >>> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> >> >> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> >> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.",0,1,msr
3472,"OK, so now that we've all had some time to cool down, I'll say this:
Andrew: you come here to a turf that is not your turf, you ignore the templates for relevant information, you disrespect the etiquette, you post comment after comment making it difficult to follow your train of thought, and all that while having a holier-than-thou attitude towards bug triage techniques.
So while I *could* explain why we ask for live reproducible examples, I feel that doing that would be a waste of my time.
I will, however, ask you to work on your empathy and on your communication skills.",0,1,msr
3473,"`map.panTo()` within bounds that are already shown does not center the map on the point sent to `map.panTo()`.
1.7.1",1,0,msr
3475,"> [...] your obviously angry issue handlers.
Antagonizing the maintainers won't help.",0,1,msr
3477,"Hi @petkaantonov! Thanks for opening this feature request.
The current behavior is reflecting the long-standing behavior that Terraform does still detect and incorporate remote objects into the state, but then while producing a plan it ignores differences _between the configuration and the state_. The difference reported in your example is a difference between the prior state and the current remote object, and `ignore_changes` has never affected that situation but that fact was less visible before because Terraform just silently updated the state rather than reporting it.
While it might seem immaterial whether Terraform updates the state or not here, it _can_ result in a change in behavior of your configuration if any other expressions in the module refer to that value. To be specific, if you had any reference to `aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus` elsewhere in your module then they would return `2` rather than `0` after detecting this change, and so that is what Terraform is reporting here, in case that ends up being useful context for understanding which actions Terraform proposes (or doesn't propose) in the plan.
I assume that in your case this doesn't really matter much, because you _don't_ have any references to `aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus` elsewhere in your module and so you don't actually care what's reported in the state. One potential improvement we could consider for this mechanism is for Terraform to try to only report changes to resources that have other expressions referring to them, since changes to a value that nothing refers to can't possibly affect the behavior of the configuration.
However, such a rule is easier to say than to implement because what we've done here is just expose in the UI some long-standing Terraform behavior that was previously invisible, and so changing that behavior at this late state will likely require a lot of research to make sure that the changes don't break use-cases we're not currently aware of. The current output is an honest and correct account of Terraform's behavior, and so I think we need to consider here whether the right thing to do is change Terraform's behavior (which, for something this fundamental, would be challenging to do at this late stage in Terraform's life) or to change the UI to fudge the details a little so that it leaves hidden some details that surface inconvenient truths that don't actually affect configuration behavior.
We won't be able to make any significant changes in this regard in the near future, because the scope of this project was just to be more explicit about what Terraform was already doing rather than to change how Terraform behaves, but we'll use this issue to represent the need and consider what we might change in future releases.
Thanks again!",1,0,msr
3487,"Just adding another resource type that now always appears in my terraform output - aws_efs_file_system. (from https://github.com/hashicorp/terraform/issues/28845 which I will mark as a duplicate). EFS file systems are *supposed* to grow and change, as many other types of resource they are explicitly designed that way.
With no way to ignore, the plan/apply output is now so noisy I've gone back to terraform 0.15.3 as wading through tens of modified resources on every single plan/apply just to catch the one that might be important is not a workable pattern for me.",0,0,msr
3492,"I think it is fine if Terraform now reports drifts explicitly but it will be great if we could have a way to hide it because we might not be interested in changes happening outside of Terraform in certain use cases.
One way we can implement this is to add a `--ignore-drifts` flag to the relevant `terraform xxx` subcommands. This is the approach I'm most in favour with because it allows this to be decided at runtime and to be varied flexibly across different use cases (i.e. running locally vs running in CI/CD platforms, etc.). It also provides users a simple way to revert to the old representation of the output.
```console
$ terraform plan --ignore-drifts
$ terraform apply --ignore-drifts
```
In addition, we could also implement this at a resource/module level using the `ignore_drifts` field. This may be helpful for some use cases where we might be interested drifts in general but not all fields.
```terraform
resource ""aws_instance"" ""example"" {
# ...
lifecycle {
ignore_drifts = [
etag, # Ignore drifts for specific attributes
]
}
}
```
```terraform
resource ""aws_instance"" ""example"" {
# ...
lifecycle {
ignore_drifts = all # Ignore drifts for all attributes
}
}
```",1,0,msr
3494,"To add to the discussion here, I'm also seeing that (for example) the Principals list in a role always shows up as ""has been changed outside of Terraform"", even if I attempt to manually update the principals list via the AWS web console.
``` ~ assume_role_policy = jsonencode(
~ {
~ Statement = [
~ {
~ Principal = {
~ Service = [
+ ""lambda.amazonaws.com"",
+ ""ec2.amazonaws.com"",
""states.us-east-1.amazonaws.com"",
# (1 unchanged element hidden)
""cloudwatch.amazonaws.com"",
- ""ec2.amazonaws.com"",
- ""lambda.amazonaws.com"",
]
}
# (3 unchanged elements hidden)
},
]
# (1 unchanged element hidden)
}
)
```
Each time I edit it via the web console to reflect what Terraform thinks it ought to be, AWS saves the list in a random order (not alphabetical), so Terraform always thinks that there's been a change. Even when I force an apply, or taint and import the resource, this warning persists.
Outside of this warning, the plan results in the message ""No changes. Your infrastructure matches the configuration."".
This, to me, doesn't make sense because if Terraform thinks that there is drift, an apply should be able to rectify that drift. This does not appear to be the case, which makes the whole drift warning kind of pointless and noisy.",0,0,msr
3495,"Hi guys, as everyone else, this change is quite impacting for us.
On azure, it appears that every apply resulting in changes in other resources will be seen as a drift.
So far, I encountered to cases:
- creating a new subnet using the azurerm_subnet resource:
The plan/apply goes well, but on the next plan, even if there are no changes, Terraform informs me that my VNet has changed, because it wants to reflect the newly added subnet in the VNet attributes. But this change was actually made by Terraform, in the previous run.
- creating a virtual machine, as the MAC Address and VM id are known after deployment, Terraform detects changes on the NIC on the next plan, because now, the NIC has indeed a MAC and a VM id.
```hcl
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.Azadds_mgmt_VM.azurerm_network_interface.TerraVM-nic0 has been changed
~ resource ""azurerm_network_interface"" ""TerraVM-nic0"" {
id = ""/subscriptions/<subscription_id>/resourceGroups/myRG/providers/Microsoft.Network/networkInterfaces/myvm-nic0""
+ mac_address = ""00-xx-xx-xx-xx-xx""
name = ""myvm-nic0""
+ tags = {}
+ virtual_machine_id = ""/subscriptions/<subscription_id>/resourceGroups/myRG/providers/Microsoft.Compute/virtualMachines/myvm""
# (9 unchanged attributes hidden)
# (1 unchanged block hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions to undo or respond to these changes.
```
I think we can all agree the Terraform team is doing a tremendous job at bringing the product where it is today, but this particular change although attempting to bring more visibility, brings, in my opinion, a lot of confusion.
I think I will roll back to 0.15.3 until some improvements are made about this feature.",1,0,msr
3502,"I think it's clear that opinions are split, and while a part of the community is happy about this change, the other part doesn't have a way to opt out.
For us it's a blocker and prevents us from upgrading to 0.15 (1.0).",1,0,msr
3507,"@ryanking hmm this is quite the antithesis of what @jbardin has claimed recently: https://github.com/hashicorp/terraform/issues/28911#issuecomment-857647716
If there is really no way for providers to work with this tf-core feature in mind, then we should definitely have a disable flag asap",1,0,msr
3510,"I have some similar error:
```
panic: Error reading level state: strconv.ParseInt: parsing ""84850493440"": value out of range
```
This is the attribute output by terrafom of efs system. There're some wy to evict this error without delete efs system from tfstate?
Thanks",0,0,msr
3511,"I also just ran into this upgrading from `0.14.11`. My feedback would be NOT a command line flag I have to provide to ignore changes outside of Terraform but a new attribute that behaves like `lifecycle { ignore_changes = [] }`.
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.fusionauth.aws_db_instance.rds has been changed
~ resource ""aws_db_instance"" ""rds"" {
id = ""fusionauth""
~ latest_restorable_time = ""2021-07-06T16:42:02Z"" -> ""2021-07-06T16:57:02Z""
name = ""fusionauth""
tags = {
""Env"" = ""stage""
""Name"" = ""fusionauth""
}
# (52 unchanged attributes hidden)
}
# module.postgres.aws_db_instance.rds has been changed
~ resource ""aws_db_instance"" ""rds"" {
id = ""postgres""
~ latest_restorable_time = ""2021-07-06T16:39:08Z"" -> ""2021-07-06T16:59:09Z""
name = ""postgres""
tags = {
""Env"" = ""stage""
""Name"" = ""postgres""
}
# (52 unchanged attributes hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions
to undo or respond to these changes.
```",0,0,msr
3522,"> If you refer to the terraform documentation, you can see that this command is deprecated.
@Tazminia The command `terraform refresh` is deprecated, but they clearly state that it is an alias to `terraform apply -refresh-only -auto-approve`, which they WANT people to know about and understand because they introduced the `-refresh-only` flag in `0.15.4`; these new flags are important to interacting with this new output.
It's definitely been a bit of a transition to `0.15.4+`, and we would also like the option to show/hide this output, but overall so happy for the Terraform team to hit `1.0+`! Thanks for all your hard work!",0,0,msr
3527,"After upgrading to 0.15.4 terraform reports changes that are ignored. It is exactly like commented here: https://github.com/hashicorp/terraform/issues/28776#issuecomment-846547594
### Terraform Version
```
Terraform v0.15.4
on darwin_amd64
+ provider registry.terraform.io/hashicorp/aws v3.42.0
+ provider registry.terraform.io/hashicorp/template v2.2.0
```
### Terraform Configuration Files
<!--
Paste the relevant parts of your Terraform configuration between the ``` marks below.
For Terraform configs larger than a few resources, or that involve multiple files, please make a GitHub repository that we can clone, rather than copy-pasting multiple files in here. For security, you can also encrypt the files using our GPG public key at https://www.hashicorp.com/security.
-->
```terraform
resource ""aws_batch_compute_environment"" ""batch_compute"" {
lifecycle {
ignore_changes = [compute_resources[0].desired_vcpus]
}
...
compute_resources {
...
}
}
resource ""aws_db_instance"" ""postgres_db"" {
...
lifecycle {
prevent_destroy = true
ignore_changes = [latest_restorable_time]
}
}
```
### Output
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.db.aws_db_instance.postgres_db has been changed
~ resource ""aws_db_instance"" ""postgres_db"" {
id = ""db""
~ latest_restorable_time = ""2021-05-25T10:24:14Z"" -> ""2021-05-25T10:29:14Z""
name = ""db""
tags = {
""Name"" = ""DatabaseServer""
}
# (47 unchanged attributes hidden)
# (1 unchanged block hidden)
}
# module.batch_processor_dot_backend.aws_batch_compute_environment.batch_compute has been changed
~ resource ""aws_batch_compute_environment"" ""batch_compute"" {
id = ""batch-compute""
tags = {}
# (9 unchanged attributes hidden)
~ compute_resources {
~ desired_vcpus = 0 -> 2
tags = {}
# (9 unchanged attributes hidden)
}
}
```
### Expected Behavior
No changes should be reported because they are listed in ignored changes.
### Actual Behavior
Changes are reported.
### Steps to Reproduce
Change any resource outside of terraform and see that `terraform apply` reports changed even when they should be ignored.
### Additional Context
### References
- https://github.com/hashicorp/terraform/issues/28776
- https://github.com/hashicorp/terraform/issues/28776#issuecomment-846547594
- https://github.com/hashicorp/terraform/pull/28634#issuecomment-845934989",0,0,msr
3530,"Issue has been open for three months, is there an intention to fix? Who owns the issue.",1,1,msr
3532,"I'll add one more: Amplify app autogenerates a thumbnail for production branch and on refresh returns a different (huge) signed URL _each time_... and, of course, both old and new URLs are printed!
```terraform
# aws_amplify_app.client_app has been changed
~ resource ""aws_amplify_app"" ""client_app"" {
id = ""d23rt6p25fb7vl""
name = ""some-client-app""
~ production_branch = [
~ {
~ thumbnail_url = ""https://aws-amplify-prod-us-east-1-artifacts.s3.amazonaws.com/d23rt6n25fb7vl/production/SCREENSHOTS/thumbnail.png?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCye8iFXchHrCWyEqiLbSkPxDmw%2FxmpHpSEvSE1o%2FmJ2wIgEUFPLw9Eewz2UgnPnoSLXeEGGgoyZCQiPRdMq79jfhYqxAIIw%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNzM2NTMxNzE1NzYiDJxI6qx84Pesb%2BdZJiqYAoTfP9nvhSYLQKxZjK625h8ZsedKZQDKJ65vJp30wSAQbNqVNl1nZNMobB9zPpR0%2F6X78cemCcl%2FVM0o%2FkKFoHXSrQJMnHFNXnMEXI5fUV%2Bh%2BMnLbgDQQpg%2FreRP276R8WFWwKlZW0%2BYx29gcSOHpFXZ5AwEeJMXeVhDbAknCtAZNTpSL7BSFTZ1hZ%2FnEEzjvbANCyeTeM8hzipZILjOYTufWhQE7UlEx6XujoY%2F2gR2AuW3zcQYd1waKIFxnADCDsD6qR2TwP%2Ba75Scrtj7XHBnk0VfduCmkI3HnzoBLhDNs3vCTDvnmhZDKTmxNFNqHE7tsAqWlL%2FC4QADk9QH9tsD4j5kVAhuVuB06oN78At8EUN5zaGJJFIwk%2BzKiAY6mgFtcDPJJgyUsmj%2FWXvEL5r0U2yV1%2BNhvcJVB2n5hPhFMCNw%2BtZSIOply%2B9niNSz5LYxLl%2FgyYGqCRn6%2BBkbHQxbHmxP2GR45DIaro09ZZfg6XzvFBJh1cYgt2NRJiq6uHo%2FcsrrEs4qta%2BV%2FCp2tIPMvpz6szGhDZB7ZOyoDFpFN8WZokAIL11IZk5cH0XAtfhO8Gzsh9V%2Bof1R&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210710T182343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=xxxxxxxxPVOQEF%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0997f37xxxxxxxxxxxd2265a8a7fd07aedc647e869cab95931a0cf16f3a3bb"" -> ""https://aws-amplify-prod-us-east-1-artifacts.s3.amazonaws.com/d23rt6n25fb7vl/production/SCREENSHOTS/thumbnail.png?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCye8iFXchHrCWyEqiLbSkPxDmw%2FxmpHpSEvSE1o%2FmJ2wIgEUFPLw9Eewz2UgnPnoSLXeEGGgoyZCQiPRdMq79jfhYqxAIIw%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNzM2NTMxNzE1NzYiDJxI6qx84Pesb%2BdZJiqYAoTfP9nvhSYLQKxZjK625h8ZsedKZQDKJ65vJp30wSAQbNqVNl1nZNMobB9zPpR0%2F6X78cemCcl%2FVM0o%2FkKFoHXSrQJMnHFNXnMEXI5fUV%2Bh%2BMnLbgDQQpg%2FreRP276R8WFWwKlZW0%2BYx29gcSOHpFXZ5AwEeJMXeVhDbAknCtAZNTpSL7BSFTZ1hZ%2FnEEzjvbANCyeTeM8hzipZILjOYTufWhQE7UlEx6XujoY%2F2gR2AuW3zcQYd1waKIFxnADCDsD6qR2TwP%2Ba75Scrtj7XHBnk0VfduCmkI3HnzoBLhDNs3vCTDvnmhZDKTmxNFNqHE7tsAqWlL%2FC4QADk9QH9tsD4j5kVAhuVuB06oN78At8EUN5zaGJJFIwk%2BzKiAY6mgFtcDPJJgyUsmj%2FWXvEL5r0U2yV1%2BNhvcJVB2n5hPhFMCNw%2BtZSIOply%2B9niNSz5LYxLl%2FgyYGqCRn6%2BBkbHQxbHmxP2GR45DIaro09ZZfg6XzvFBJh1cYgt2NRJiq6uHo%2FcsrrEs4qta%2BV%2FCp2tIPMvpz6szGhDZB7ZOyoDFpFN8WZokAIL11IZk5cH0XAtfhO8Gzsh9V%2Bof1R&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210710T182343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=xxxxxxxxPVOQEF%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0997f37xxxxxxxxxxxd2265a8a7fd07aedc647e869cab95931a0cf16f3a3bb""
# (3 unchanged elements hidden)
},
]
tags = {}
# (16 unchanged attributes hidden)
# (3 unchanged blocks hidden)
}
```",0,0,msr
3539,"It is September, Do we have any update on this? It is still super annoying!!!
We ran into some BIG issue with a very buggy provider when running Terraform refresh just to avoid the infinite list of changes. The Issue caused some resources to be deleted in the state file while they were actually there as mentioned a few comments back.
We really need to have this removed it is such a big issue for us, since most of the things are changed by other workloads like Spinnaker. And having to see 300 lines of changes that are not our changes in Atlantis is very hard to spot issues like a faulty provider.",1,1,msr
3542,"I would consider this a regression (bug) in the terraform 0.15.4+ plan output, not a feature (enhancement).
Resource attributes like `aws_db_instance` -> `latest_restorable_time` are expected to change constantly (every 5 minutes), and make the terraform plan output very noisy. It shouldn't become necessary to add explicit resource lifecycle configuration blocks to ignore these drifting.
We're upgrading from terraform 0.12 -> 0.13 -> 0.14 -> 1.0, and we need to stop and evaluate if this is an upgrade blocker, and perhaps we should stay on 0.14 until this is fixed?
EDIT: This is a [terraform 0.15.4 change](https://github.com/hashicorp/terraform/blob/v0.15/CHANGELOG.md#0154-may-19-2021), so I think the best workaround is to stick with 0.15.3 until this issue is resolved.",0,0,msr
3543,"Some people here mistake few different issues:
* Principals in policies having not consistent order (https://github.com/hashicorp/terraform-provider-aws/issues/11801)
* Outside changes reported in plan - real drift in configuration made by some 3rd party action. And some just want to ignore it for unknown reason.
* Dynamic fields with constantly changing data being reported as outside change - it exists in various providers for various resources, and will ALWAYS show up in plan.
With this GH Issue we want to address the third type. Which only obstructs plan and will happen on each and every run, even when there are not actual changes made.
Some of us use Terragrunt and have hundreds of tfstates to manage and having those false-positives is very frustrating and time consuming.",1,0,msr
3544,"the size attribute of azuredevops_git_repository is another of these dynamic fields that i really don't need to see every time i plan/apply. was disappointed ignore didn't just silent it, makes sense cause someone somewhere has some TF that does something based on size changing i'm sure...thinking something like a new lifecycle prop like
```
lifecycle {
use_current = [
size
]
}
```
that tells TF to always update state from current value without reporting it as a difference.
could also see it being driven by providers, since there are cases where someone may specify a size for an azdo repo, so azdo provider could report size as this new class of property that doesn't need to indicate change when it's say set to null but should if it's set to a non null value (course the latter case is redundant since you'll see that in the plan)",0,0,msr
3548,"Yet another example:
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the
last ""terraform apply"":
# aws_elb.snip has been changed
~ resource ""aws_elb"" ""snip"" {
id = ""snip""
~ instances = [
- ""i-0245934821285edc7"",
- ""i-054d85efadb1d4c0a"",
- ""i-0cdda3b2595fe3281"",
# (3 unchanged elements hidden)
]
name = ""snip""
tags = {
""Name"" = ""snip""
}
# (13 unchanged attributes hidden)
# (2 unchanged blocks hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the
relevant attributes using ignore_changes, the following plan may include
actions to undo or respond to these changes.
```
The following plan will 100% not ever include actions to undo or respond to those changes, because we've explicitly and deliberately in our configuration told Terraform not to manage the instances in the ELB. (Sometimes other things need to take instances in and out, e.g. Ansible while deploying things.)
It says right there in our configs that Terraform should 100% not ever care what instances are in the ELB. Our configuration is being ignored, and Terraform is now caring (albeit only at the level of a warning) about things that we explicitly and deliberately told it not to care about.
This is not detecting drift.
This is a bug.",1,1,msr
3556,"Same problem, updating from 0.13.5 to 1.0.9.
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":
# module.lambda_exec_role.aws_iam_role.iam_role has been changed
~ resource ""aws_iam_role"" ""iam_role"" {
~ assume_role_policy = jsonencode(
~ {
~ Statement = [
~ {
~ Principal = {
~ Service = [
- ""s3.amazonaws.com"",
- ""ssm.amazonaws.com"",
""lambda.amazonaws.com"",
+ ""ssm.amazonaws.com"",
""edgelambda.amazonaws.com"",
+ ""s3.amazonaws.com"",
]
}
# (2 unchanged elements hidden)
},
]
# (1 unchanged element hidden)
}
)
id = ""lambda_exec_role""
name = ""lambda_exec_role""
tags = {
""env"" = ""test""
}
# (8 unchanged attributes hidden)
# (1 unchanged block hidden)
}
# aws_lambda_layer_version.pkgupdates has been changed
~ resource ""aws_lambda_layer_version"" ""pkgupdates"" {
+ compatible_architectures = []
id = ""arn:aws:lambda:eu-west-1:497776581864:layer:pkgupdates:23""
# (9 unchanged attributes hidden)
}
Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions to undo or respond to these changes.
```",1,0,msr
3567,"Hi all! Thanks for all the feedback here.
It's clear that there are lots of opportunities to improve the signal to noise ratio of the current change detection. As I mentioned right back at the top of this discussion, this is Terraform making explicit some behaviors that were previously implicit and thus a common cause for confusion. However, it's also clear that various particular provider features, along with the historical confusing design of `ignore_changes`, have led to the new behavior being confusing in quite a different way.
Two specific things that the Terraform team is researching to improve this feature are:
* A new feature that does what everyone _thinks_ `ignore_changes` does: declare that a particular resource attribute is not relevant to the current configuration and thus not interesting to track.
What `ignore_changes` _actually_ does is tell Terraform to ignore changes _to the configuration_, which unfortunately means that it explicitly _doesn't_ ignore changes in the remote system (which is the source of record). We can't change the behavior of `ignore_changes` due to [the v1.0 compatibility promises](https://www.terraform.io/docs/language/v1-compatibility-promises.html), but we _could_ introduce a new feature that makes a stronger statement, which would include making Terraform consider changes to it to be totally immaterial, and thus not mention them.
* Heuristics to detect better whether any of the _proposed_ changes (the result of the plan) seem likely to have been caused by one of the _detected_ changes (the result of refreshing), and thus automatically effectively infer by static analysis which changes are relevant to report.
This would effectively be an automatic version of the explicit feature I described in the previous point, avoiding the need to explicitly annotate everything and instead saving explicit annotations only for situations that are too complicated for Terraform's static analysis to understand automatically.
Both of these features are cross-cutting and therefore not something we can just casually implement without careful design first. However, we are indeed actively investigating both and hope to have improvements to share in a forthcoming release.
Since this discussion has got quite heated and it seems like we've already gathered sufficient feedback, I'm going to lock this discussion for now in order to reduce the noise for the many folks who are following this issue. We'll unlock it again when either we have more news to share or if we need some specific feedback in order to shape solutions like what I described above.",1,0,msr
3570,"FYI: CTRL-C typically sends `SIGTERM` from most console emulators on Linux, which I would _expect_ to generally terminate whatever is running. Only bring this up because you mentioned ""Linux systems"".",0,0,msr
3571,"Ctrl+C isn't intended for clearing the line at all. That said, some REPLs handle SIGTERM by stopping the currently running code instead of exiting the REPL (which has the side effect of just cancelling the command if you didn't enter it yet). That does make sense for REPLs, but I'm not sure it makes sense in this case, as there's no user-inputted code to cancel.",0,0,msr
3576,"CTRL-D typically sends EOF, making it a logical sane default to exit a REPL.
I would also suggest, if feasible, CTRL-C twice should as well. In other words CTRL-C, when the readline is already empty, should terminate.",0,0,msr
3583,"[![Coverage Status](https://coveralls.io/builds/40046053/badge)](https://coveralls.io/builds/40046053)
Coverage decreased (-0.003%) to 61.193% when pulling **a5c2580a4aff0905563016c58f202d3c2ba34ce2 on noahjacob:item_so_variant_fix** into **477a90e2ac839201a40ea00c7467caafe06db644 on frappe:version-13-hotfix**.",0,0,msr
3588,"This change is idiotic.
The whole point of having variant items is that you must create variant BOMs to match. Fetching a BOM linked to a parent template does not make sense at all. The whole point of a template is that *IT DOESN'T EXIST* and that variants of it are *DIFFERENT TO EACH OTHER AND HENCE HAVE DIFFERENT SUBITEMS*
In the gif example, you are making a RED item with a BOM that does not have a colour. So how are you supposed to order, transfer stock and make the item from the RED raw material?
This also completely misses the point that the linked BOM is saved in the Sales Order Item.
I truly give up with this development team. Too many people contributing too many ill-thought out features, badly.
This is just one example of a complete break in logic.
@ankush @noahjacob @rmehta @rohitwaghchaure",0,1,msr
3590,"> such language is completely uncalled for. >
> > I truly give up with this development team. Too many people contributing too many ill-thought out features, badly.
> > Feel free to share flaws, but don't make personal, judgemental comments.
1. There was no person mentioned in the comment, therefore not personal in the slightest. You are aware your contributors and development team are made up of people?
2. Plenty of evidence from the enormous volume of issues and bug PRs I and everyone else have submitted, so is completely evidential and objective, and therefore not judgemental in the slightest
You can try to hide behind overly-emotive excuses and stalebot if you want, but being accountable for mistakes just might improve your projects.",0,1,msr
3602,"Wow, the hostility in this thread is stunning.",0,1,msr
3603,"> Wow, the hostility in this thread is stunning.
Polkadot did lie by referring to this situation as a ""compiler bug"" in [their tweet](https://twitter.com/Polkadot/status/1396936062266716166) on the subject. To be honest, it might be a good idea to preemptively lock this thread, before it gets any more attention.",1,0,msr
3616,"> I really wanna see his face once someone links him to all the work I did for the Vita on networking and netplay.
Sorry... What? Where's the proof!? As I've clearly missed something here, as I only got a email about this today and I see nothing apart from ""oh, is it alright to close this issue?""",1,0,msr
3629,Are you not going to merge this?,0,1,msr
3634,"So stupid… And obviously not interested in solving problems…Am 06.06.2023 um 14:53 schrieb Luca Boccassi ***@***.***>:﻿no permission given to distribute under MPL2, the relicensing is complete so this cannot be merged, closing
—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>",0,1,msr
3638,"It is impolite to assume that each user opening an issue is stupid and lazy.
Of course I search the issue tracker. I assume I used the wrong keywords.
But it is also impolite to point to other Issues without giving the links.
It is a feature of [HyperText](https://de.wikipedia.org/wiki/Hypertext) to link your words to content. ;)
If you want contributers to pandas you should work on your attitude.
And as a ""pandas member"" you should know how to use GitHubs Issue tracker.
if there is a duplicate Issue. Link to the origin Issue and close the duplicate.
Anything else is waste of ressources.",0,1,msr
3644,"There is a provided issue template when you create an issue, please follow it.
Does this issue happen in software mode? (Pressing F9 or changing the renderer in gsdx)
Issue template:
**Describe the bug**
<!-- A clear and concise description of what the bug is. -->
**To Reproduce**
<!-- Steps to reproduce the behavior. -->
**Expected behavior**
<!-- A clear and concise description of what you expected to happen. -->
**GS Settings**
<!-- Any non-default settings for GS. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window (including hw hacks if enabled). -->
**Emulation Settings**
<!-- Any non-default core settings. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window. -->
<!-- Please note that the safe preset works for most games. -->
<!-- MTVU can have some compatibility issues so please disable it before making a report. -->
<!-- If you need to modify the settings manually because a game requires you to do so to work, please state that explicitly. -->
**GS Window Screenshots**
<!-- If your issue is graphical in nature and you think screenshots will help illustrate your issue, you may do that here. -->
**System Info (please complete the following information):**
- PCSX2 Revision: <!-- e.g. dev-525 -->
- OS: <!-- e.g. Windows 10 -->
- CPU: <!-- e.g. i5-7600 -->
- GPU: <!-- e.g. GTX 1070 -->
**Logs and Dumps**",0,0,msr
3652,"> I'm just using 1.7 build 1286, the bars I get are because my window isn't exactly 4:3, but if I correct it I get the below.
> > ![image](https://user-images.githubusercontent.com/6278726/121198659-366af580-c86a-11eb-96be-be2fc05d6163.png)
> > Just to note, mine is the PAL version
@refractionpcsx2 I have only the NTSC version of both GoW1 and 2. And from your screenshot I can see the internal resolution is different. iirc Sony Santa Monica rendered the FMV's in ddifferent resolutions for each region, 480 for NTSC and 576i for PAL. Which is why I believe you dont get the same bug I get.
This is what observed so far: on the my NTSC version
1.6.0 / 1.7.0 r1286:
GoW1 Has the Black border bug
GoW1 game runs at 512x448 inerlaced and 640x448 w/ progressive scan on
GoW1 FMV's run at 640x480 regardless
GoW1 FMV files are PSS files rendered at 640x480
on 1.4.0 the resolutions are the same, except that the bug happenes when I enable progressive scan.
GoW2 doesnt have this issue at all because the FMVs are at 640x448
Ibelieve thats the reason you dont experience the same bug because PAL FMV files are rendered at a higher res than NTSC
I extracted the files and examined them for both games to exmine them",1,0,msr
3664,"Which upscaling bug? It isn't related to this issue, but instead of complaining about bugs which haven't been reported yet, maybe consider reporting them, then we can look in to them.
However the only upscaling bug I know is the green/purple fringing, which we probably can't do a lot about easily, it's due to how the PS2 renders effects and it expects the original resolution, so upscaling it causes problems.",1,0,msr
3665,"Well, that's just the nature of upscaling PS2 games, I'm afraid. The console kinda sucks for that and our life is a lot harder than say the Gamecube (and most definitely the PS3)",0,0,msr
3669,Mean guys trying to get this bug fixed unlike rpcs3 team so just close problem up and mark invalid even bug happened on Rpcs3 god of war hd so hope guys do get bug fixed soon.,0,0,msr
3674,"> That's not where Matplotlib looks for `matplotlibrc`; you have some custom patches.
I've been trying to pass path to that file in $MATPLOTLIBRC encv variable but seems it does not work.
So it is not possible to generate documentation without installing matplotlib??",1,0,msr
3675,Matplotlib must be installed to build the docs. We need to be able to import Matplotlib to extract the docstrings and to run the examples / generate all of the output images.,0,0,msr
3676,And how to that building matplotlib fron non-root account???,1,1,msr
3683,"Looks like something is missing and probably some defailt location of the matplotlibrc file is used. This is causeing tha standard way of generarting sphinx documentation fails.
```console
[tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=PYTHONPATH=$PWD/build/$(cd build; ls -1d lib*) /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
Edit setup.cfg to change the build options; suppress output with --quiet.
BUILDING MATPLOTLIB
matplotlib: yes [3.4.2]
python: yes [3.8.9 (default, Apr 7 2021, 13:42:48) [GCC 11.0.1 20210324
(Red Hat 11.0.1-0)]]
platform: yes [linux]
tests: yes [installing]
macosx: no [Mac OS-X only]
running build_sphinx
Running Sphinx v4.0.2
Configuration error:
There is a programmable error in your configuration file:
Traceback (most recent call last):
File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 323, in eval_config_file
exec(code, namespace)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/conf.py"", line 19, in <module>
import matplotlib
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 825, in <module>
rcParamsDefault = _rc_params_in_file(
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 730, in _rc_params_in_file
with _open_file_or_url(fname) as fd:
File ""/usr/lib64/python3.8/contextlib.py"", line 113, in __enter__
return next(self.gen)
File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 708, in _open_file_or_url
with open(fname, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
```",0,1,msr
3684,"INT 21h AH=2Bh CX=4442h DX=2D58h AL=0x00..0x0F breaks your setup?
I suppose that was a bad idea, yes. I'll remove it.
Programs that want to be DOSBox-X aware can use the Integration Device at I/O ports 28h-2Bh if enabled by the user anyway, if that is enabled in dosbox.conf, regardless of native DOS environment or guest OS.",0,0,msr
3686,"Yes I updated my comment for you I am a TOP Assembler programmer in FACT I develop the DARPA.NET AI (You call it internet/TCP) probably what most people don't know is ONLY TCP components can be on internet Windows for example is build from my TCP component set (so is android and ubuntu, everything online must be built with these components)
The AI RIPS components from other programs in ANY language (Converted to assembler) and converts Assembler to Delphi TCP components if you write apps in dos, the AI will also take that assembler and build it into Delphi/TCP components. It's so effective I do most my work in DOS using ASIC (It's almost basic) and NASM. You can get those POWERFUL features no one else has like that.
FREE PASCAL, they stole that years ago I actually named it FAST PASCAL back then. Was part of the DARPA.NET AI
It wasn't supposed to be free(or even public) products where copyrighted and intended for personal use.
Someone stole my floppies long time ago, when backup options where very limited and any storage expensive. They ended up with Bill Gates, thank god the DARPA.NET AI was compiled and not in source and it included the certificate systems. So I'm the product-owner and eventually fired Bill @Microsoft
I can live with Pascal being free, that idea was ripped from Borland anyways. (I have my own custom version anyways)
What is so cool is that TCP components communicate with each other and the AI learns new components from that communication, that are not programmed by humans. Anything written in a dosbox will become a Delphi/TCP component also.
I have the development suite, but this functionality is default in any OS/SOFTWARE built form TCP components
Writing DOS programs might change details (or a lot if big program) in Windows/Ubuntu/Android or any other DARPA.NET connected product. GLOBALLY and instantly. The difference is, I get the components on my IDE toolbar since I own the product.
JW, CIA",0,1,msr
3699,"What we build is the blueprint for new hardware as hardware is designed to support the software. You might see your virtual prototypes become real, like this keyboard, that prototype was made (by me) 20 years ago as a touchscreen keyboard-prototype now it's real hardware. That works the same and anyone can buy. - https://drive.google.com/file/d/1JIyjBXKIcS8fnVbdwKer9m-zTmD36yTY/view?usp=sharing",0,0,msr
3702,"The MATRIX must be TURING-COMPLETE since all computer systems can exist in reality. The idea of the braces experiment was prove reality to be some sort of simulation. Humans can't understand everything what GOD made. But we can understand light and produce it. I just set the injection point (of my IDE) on a LED with Assembler and pointed it on a light source in reality. Anyway it worked proving reality is some kind of simulation too, the one made by GOD. I am religious, all good logic is based on that. The MATRIX even has system restore. LOL - https://youtu.be/i18uwmek9UE
Anyway humans can now work on MATRIX reality, as far as we can understand. Not as if we can make animals or something, but an aircraft engine is just simple enough for humans to change inside the MATRIX database with code-injection.
Who know what we will learn it's all EXTREMELY experimental even if we have this tech since '96 and some of the AI even uses it.",0,0,msr
3706,"**ALL worked very will in version 83.13**
**In PC-DOS 7.1 (IBM PC-DOS 2000)**
-The mouse acts REALLY strange in various applications and in Windows 3.11 (Win 3.11 crashes after strange mouse movements)
-Z (ZED) Game Setsound and ZED.EXE hang
-The mouse acts strange on the title-bar of the dosbox window
-Many applications and my own COM applications have problems
**SURE the new ""TALK to DOSBOX"" functionality cause this,**
YOU CAN'T JUST ADD/CHANGE ASSEMBLER INTERRUPTS (especially not before the bootstrap, if loading a custom image)
OSes and TSR Software may assign these interrupts. (I usually assign interrupts automatically with a driver installer routine for unhandled interrupts, the diver executable is then added to the next free interrupt)
**DosBox-X version 0.83.15 TOTALLY USELESS!** **IT IS JUST NOT 100% COMPATIBLE WITH x86 ASSEMBER IF YOU ADD CUSTOM INTERRUPTS,**
**THAT MIGHT BE FINE IF YOU WRITE A CUSTOM OS (NOT DOS COMPATIBLE) WITH A CUSTOM INTERRUPT TABLE.**
**NOT IF YOU NEED TO RUN 16 BIT DOS COMPATIBLE SOFTWARE (STANDARD x86 ASSEMBLER) , THAT IS THE WHOLE POINT OF DOSBOX, SO THIS VERSION IS TOTALLY USELES, SOME PROGRAMS WILL RUN BUT IT IS MORE LUCK THAN WISDOM. I EXPECT LOADS OF PROBLEMS IN MANY DIFFERENT APPLICATIONS.**
**Also this ""TALK to DOSBOX"" functionality destroys the SAND-BOX** An assembler debug run-point can now run from a DOS program into the Window of DosBox,, not handy since Python and CUDA reverse engineer applications on runtime while the machine AI is learning.
**This version should be deleted**
You should put the old version back until you make a release without the ""TALK to DOSBOX"" NEVER TALK TO THE SANDBOX. Your assembler run-point may go from the SANDBOX to the DOS bootstrap, but NEVER from DOS back in the sandbox. (Might cause strange infinite loops) Especially if you have Python and CUDA installed you can get VERY strange effects. (EVEN WHEN NOT RUNNING, due to CUDA/Python working on it)
Maybe add a version history to the website this will also be good for machine learning (I use that a lot, it can be used for Pythagoras predictive code, best is if you have a complete version history, except versions like this, should not be in such a history list, (causes only problems) ONLY good versions, with NO bugs but added or changed features, that work.
**The whole IDEA of SANDBOX is you can't talk to the Window (You should do it like this)**
WHAT YOU DID IS AN ARCHITECTURE FLAW, AND ALSO A BUG.
To get the same effect you could let the Windows application POL the DOS Memory page for example let the Window react on that. memory content. (Like PEEK/POKE You could turn the option OFF in the Windows and SANDBOX integrity is not changed ALSO compatibility is maintained.
**Helpful tool for doing it correctly**
It becomes super easy to program that if you make a memory viewer so you can browse LIVE trough your running DOS memory pages. A viewer can be made very advanced too and you can do things PETER-NORTON would be amazed by.
Jimmy Walter, CIA",0,1,msr
3713,"Hey David, I'm currently chipping away at the backlog of open PRs to see what can be landed before the next release.
> Anything blocking it that someone else could work?
Mostly reviews, there's a few thousands of lines of changed code that needs to be looked through. I'm getting up-to-speed on the project as well.
-----
However, something that _would_ be sweet is if we could get the dependencies updated, as it's been a hot minute. There were some patches doing so from a year ago, but if that could be done I'd be pretty happy. While we're at it, tackling modern compiler warnings and clippy lints will be important as well.",0,1,msr
3719,I disagree and please stop bikeshedding.,0,1,msr
3721,"I explained in the first comment that I already setup caching of the sccache build. But that's not a good solution; any time the cache misses, there's a 10 minute cost of rebuilding sccache which negates the benefit of using sccache for that build. Moreover, I just shouldn't need to do that in the first place.",0,0,msr
3725,"I'd like to add a little more perspective as well here, if it helps things.
> I encourage you not to hold back releases for ""one more little thing"", especially for not for trivial code cleanup. I've been waiting on a release for around a year now. I don't care about Clippy warnings.
Heh, it _is_ an indirect benefit. Either way, this is a red herring: the _real_ blocker isn't clippy warnings, it's that I'm getting up-to-speed with the project. Sure, there's a few commits that are sitting in `main` ready-for-release, but there's also a significant number of PRs, some of which are ready for landings and some of which need some polish first. I don't know which is which, so my current priority is to work through them and find out.
Keep in mind that it isn't specifically clippy warnings that are blocking the release - I raised this work as something that the community could potentially contribute assistance with while I tackle the other blockers.
> There is no need to review every open PR, fix every warning, or update every dependency to publish a new release.
True, one option that exists here is to push out a release based on what's in `main`, then handle the pending PRs and do a follow-up release afterwards. I'm leaning against this because I don't know the stability state of `main`, and I don't have enough local infrastructure to raise that confidence. Yes, we have some automated testing, but I'd rather have a more methodical release than to push something out with unknown risk.
By addressing PRs, catching up some issues, and doing some local work, I can build my mental model of the project, which will enable making more confident decisions around releases.",0,0,msr
3728,"[An issue has crept up during benchmarking](https://github.com/mozilla/sccache/issues/1150), and I'm away tomorrow, so the release will happen next week at the earliest.",1,0,msr
3729,[`v0.3.0` is out](https://github.com/mozilla/sccache/releases/tag/v0.3.0),0,0,msr
3731,Where can I buy a Raspberry Pi for free? :),0,0,msr
3734,I am waiting for new M1 release.,1,0,msr
3739,@hmsjy2017 Debian.,0,0,msr
3740,"I tried to build RustDesk on Raspberry Pi4(Ubuntu-ARM64) and Raspberry Pi3(Raspbian OS-ARM32).
Unfortunately,both failed.
For Pi4(test on Linux ARM64):
From the code and README:
> Desktop versions use **sciter** for GUI......
sciter-sdk only provides the _libsciter-gtk.so_ build on x64 / ARM32 , they didn't release arm64 for linux yet.
[libsciter-gtk.so doesn't support in ARM64 platform #210](https://github.com/c-smile/sciter-sdk/issues/210)
We can't build on ARM64 unless sciter-sdk provides the .so someday.
For Pi3(test on Linux ARM32):
_libsciter-gtk.so_ maybe fine.
but I can't build success vcpkg on Raspbian OS-ARM32.
So I build the three libs _libvpx libyuv opus_ from those offical source code,then I put them in vcpkg/installed/arm-linux/ and _cargo build_
......a very long build time...... all rust crates and lib passed the compilation,but finally failed on maybe some gcc/link problem:
![2021-12-21 20-44-39屏幕截图](https://user-images.githubusercontent.com/62206297/146932228-c9e36c20-2496-41e3-8004-51b7c5e1c594.png)
...
![2021-12-21 20-45-29屏幕截图](https://user-images.githubusercontent.com/62206297/146932277-e88fd7a2-eb20-48c6-9415-be20761fcf34.png)
Im not good at gcc compiling and linking,I cant understand these problem...
I think it's not ready to support Raspberry Pi(Linux ARM) yet.",0,0,msr
3749,"There does not exist a ubuntu 16 desktop arm iso, for this case you should go for debian, as it has less overhead
![image](https://user-images.githubusercontent.com/7207103/166691451-45780da3-266a-48cf-a56b-d6fbddc298aa.png)",0,0,msr
3757,"> I have successfully compiled ...
If possible, could you please write a tutorial on https://github.com/rustdesk/doc.rustdesk.com?",0,0,msr
3761,"Thanks, English also please. :)",0,0,msr
3762,"> Thanks, English also please. :)
Your translation is good!",0,0,msr
3765,"Ok, you persuaded me.",0,0,msr
3773,"Hello barbedknot
Thank you for your concerns, I will assist by addressing them one by one.
> Chatterino dropped support for Windows 7 on version 2.1.0
The dropped support was as a result of Microsoft themselves no longer offering support for Windows 7. To users that are on Windows 7, it is highly suggest you upgrade to Windows 10.
> I was actually able to install the latest version for Windows normally a few weeks ago, despite my operating system being Windows 7
Yes, support has been dropped for Windows 7. You may be able to install it, however we have no plans to provide support for bugs/issues on Windows 7 devices.
> * A chatterino7 release that is discrepant from the chatterino2 philosophy of not allowing non-logged in users to receive ""notify when online"" notifications (among other features)
As already mentioned, we are trying to move away from Kraken as a result of the API being decommissioned. Helix endpoints have a strict requirement for OAuth when trying to use any endpoint as per the API docs: https://dev.twitch.tv/docs/api/reference. This is inclusive of knowing when a stream is online or not. Hence why you are unable to see if a stream is live (and do/see many other things) if you are not logged in.
> * A chatterino7 release that supports Windows 7 (my understanding was that 2.1.0 was the last version to support Windows 7, but at some point that appears to've changed, so perhaps this is just a given)
As aforementioned, Microsoft has dropped all support for Windows 7. Why would we support Windows 7 if the developer doesn't themselves? It seems absolutely preposterous to me.
> Is a Windows 7 user still able to have a functioning Chatterino above version 2.1.0?
Sure, it may work. We still don't provide support for it.
> Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.
Chatterino is a community-made, open source project. Nobody gets paid to work on it. I'll put it quite simply; Kraken is not being added into Chatterino, just so users are able to see when a streamer is live in anonymous mode.
Your whole issue revolves around complaining that there's no Windows 7 or Kraken support. Both of which have had all support dropped by their main developers. So I ask you; why would we add support for said feature, if the devs themselves do not support it?
Kind Regards,
ALazyMeme",1,1,msr
3785,Here's one more link for you to read: https://en.wikipedia.org/wiki/XY_problem,0,1,msr
3789,"> to go out of your way
Why would I do that? First, I need to understand what you're actually trying to do, before showing you the direction: otherwise the direction I show would be completely wrong. But you're refusing to answer my questions! Again, please read about the XY problem I linked above.
> understand the blind spots I have in my understanding
How am I supposed to know what parts of protocols you know and what parts you don't know? Do you expect me to teach you the whole computer science from ""2+2=4"", or? The two facts ""And SSL handshake happens even before"" and ""So no, ""Unable to connect"" can't be done."" imply that you obviously can't go back in time. Or I should expect that you don't know physics and also teach you that?
> you didn’t even try to guide me towards the parts of ZNC that would need to be patched
You think the correct answer always is a patch? Wrong!
The first step is to figure out what you actually want. Then show you the existing configuration option (or e.g. a module) to do what you want. Then, if there's no such opiton, a patch, and not just on your side, but in upstream ZNC, so that it benefits everyone, not just you. Of course, if this is useful at all.",0,1,msr
3791,"Hi !
I’ve been wanting to remove the `webadmin` module and fully disable ZNC from being able to respond anything meaningful over HTTP, because it listens to the same port as the IRC protocol one and so I cannot just block the port. Either this is a behaviour not achievable or I am missing a setting somewhere after searching the wiki for a solution.
I tried setting `AllowWeb` to `false` by hand on my only listener but it doesn’t work, as ZNC still serves an html page saying that `Web Access have is not enabled.`
```
<Listener listener0>
AllowIRC = true
AllowWeb = false
IPv4 = true
IPv6 = true
Port = ****
SSL = true
URIPrefix = /
</Listener>
```
![名称未設定](https://user-images.githubusercontent.com/37584624/130321317-f36e1f7e-10d6-4e93-a206-ce85d7e7d056.png)
[From this changelog](https://wiki.znc.in/ChangeLog/0.090), I can see that the http server got embedded in ZNC so that modules other than webadmin can serve pages over http. Is there a way to tell ZNC to not load it?
Serving a page *isn’t* that big of a deal so I’m okay with this for now. But from a rigorous standpoint, if I don’t want my bouncer to communicate over HTTP I wish I could tell it to not do it, at all.
I am on Debian Bullseye, using their package of ZNC. I took a quick glance over the other issues but I have to say there was too much of them to see if this was an issue already raised.
```
$ znc --version
ZNC 1.8.2+deb2+b1 - https://znc.in
IPv6: yes, SSL: yes, DNS: threads, charset: yes, i18n: yes, build: cmake
```
Thanks in advance for your time,",0,0,msr
3794,"All of the basic built-in Markdown elements are built as internal plugins. They rely on etree. If we allow custom parsers, how do we avoid the inevitable questions of ""I implemented my own parser now all of the basic Markdown things are broken?"". Or ""Everything isn't broken but with lxml this edge case is broken for Admontions, can we fix that so it isn't broken?"".
This is what we don't want to deal with. I don't really know what you are doing with lxml and at what stage in the process, but I imagine there are ways to do what you need to without lxml. I realize that lxml is a preferred option for you, but I can envision all sorts of issues if we went this route.
And this isn't really a completely abstracted approach as there may be non-etree like parsers out there that someone would want to use. This really would only work for etree-like parsers, particularly lxml. What if I wanted to use html5lib? I don't think this would work. You'd have to completely abstract the interface. But again, you'd still have quirks maybe.
This is the bare minimum of abstraction needed to get lxml (and probably only lxml) shoehorned in, but not enough to abstract this for all custom parsers and it may introduce all sorts of quirks for anyone interfacing with the built-in Markdown parsing when using their own custom parser.",0,0,msr
3797,"Regarding the [earlier pull request](https://github.com/Python-Markdown/markdown/pull/1177) I propose a _custom parser interface_ for better integration with third party projects using e.g. _lxml_.
### Purpose:
Help developers of third party projects to easily change the internal parser (etree) if their custom extensions do have need for extended functionality of e.g. _lxml_.
### Usage:
```python
import markdown
import lxml.etree
markdown.etree.set_parser(lxml.etree)
html = markdown.markdown(""# Hello extended XPath World"")
```
### Criticism:
Compatibility issues of third party parsers do not have to be targeted by Python-Markdown development team, as projects like _lxml.etree_ claim to be compatible with _xml.etree.ElementTree_
### Licensing:
There should be no legal conflicts (BSD/MIT) in case of lxml. :-D
**I hope you'll accept my idea to make my life a bit easier beyond python-markdown update cycles.**
Cheers
Dom",1,0,msr
3799,"Issue labels are indeed useful but it still seems like a reasonable idea for the project to have its own repository. :smiley: There is a lot of unrelated activity in this repository such as running the full Gutenberg test suite on every pull request, even when it just relates to wp-env. ![Peek 2021-08-26 13-58](https://user-images.githubusercontent.com/17307/130951178-370010bf-a76d-4f76-879b-583b160d464a.gif)
Also, developers have to check out all of the Gutenberg code when wanting to only make a change to wp-env.
![image](https://user-images.githubusercontent.com/17307/130949152-155f62d1-543b-4ec1-92a7-0bd66881dfec.png)",1,0,msr
3802,"Relatedly, a single-line pull request for `wp-env` documentation is [currently blocked by failing Gutenberg end-to-end tests](https://github.com/WordPress/gutenberg/pull/34322#issuecomment-914055014).
I still believe that moving `wp-env` to a separate project repository would make development more simple and clear. In effect, `wp-env` is a useful tool for general WordPress (PHP) developers as well as Gutenberg developers, so would benefit from neutral positioning and streamlined :racing_car: repository structure and development workflow. :smiley:",0,0,msr
3804,"I'm on the opinion that a separate repository is just going to require more maintenance work for no real gains. The setup to publish npm is already in place here, there are already folks working and following this repo. A separate repo would require building new workflows, a new team dynamic that I don't think is worth it personally. I'm also of the opinion that all WP npm packages should just be in this repository. The fact that e2e tests from Gutenberg can block PRs for wp-env look like a good thing to me, they're validating that wp-env changes work properly as well. Intermittent failures are an issue of course but it's a constant priority for us.",0,0,msr
3817,"> Env, Scripts, Babel-Preset, Browserllist-Config, Prettier-Config, Stylelint-Config, Eslint-Plugin and any other NPM-published package that's allegedly supposed to be useful to plugin and theme authors beyond just gutenberg, should all be separated out of this ""monorepo.""
The fact that all these packages are dependent on each other and evolve together most time is a great indication that the mono repo is the best path forward. You shouldn't be forced to have multiple PRs in parallel in multiple repos to make a change, or do a waterfall kind of flow. The whole open source community is evolving in this direction for these reasons. Smaller PRs for specific packages exist but more often, a PR touches multiple packages at the same time.",1,0,msr
3819,"@nerrad I think the problem is that `wp-env` if it's supposed to be for plugin and theme development really shouldn't be held hostage by the Gutenberg plugin. All of what's mentioned makes me question that if the mono repo is such a good thing then why isn't Gutenberg and all of the packages put in WordPress Core. From what I've seen the reason is exactly the opposite of your cases on why `wp-env` should stay in this mono repo, keeping Gutenberg separate means that it can be developed faster than WordPress Core releases. Which also allows people to install this plugin which overrides the Gutenberg that is included in WordPress Core.",1,0,msr
3824,"That's quite a strong reaction. Look at it on the other hand. A small group of contributors are only willing to discuss the idea of moving the package out of the monorepo, and unwilling to consider any other solution. This approach to diplomacy will only end in a stalemate. Nothing actually productive will come out of it. The issue I'm seeing is that you have identified problems but are only willing to consider a single solution and no other options. I don't really see how you expect such inflexibility to receive a positive response.",1,0,msr
3828,"Agree with your points Noah.
I think it's time to close this issue given this is clearly not going to happen, but also as the conversation became toxic and aggressive.
I did start one new pull request to help improve things based on one of the (now deleted) comments - https://github.com/WordPress/gutenberg/pull/38122.
I'm happy to also work on other ways to improve the contributing experience. Lets focus on making individual issues/PRs though for those things.",0,0,msr
3829,"Thank you for your friendly request. The feature that you're looking for is called `json_encode()`, see https://www.php.net/json_encode
If you need more sophisticated serialization solutions, there's Symfony Serializer, JMS Serializer and other high quality open source libraries. I don't think that Doctrine has to invent its own solution.",1,0,msr
3837,"Well then, I pointed you to the documentation repository where you're free to suggest improvements to the documentation. There's nothing I can do for you here because we're only tracking bugs and feature requests on this tracker.
If you're looking for support, feel free to open a discussion here: https://github.com/symfony/symfony/discussions",0,0,msr
3842,"~~AFAIK there's no way to resume compressed data like that which is why you see bad header.~~ The data is needed from the start to decompress. We should probably warn about it or stop it entirely.
slight edit: There is a way transfer encoding but it's hardly supported. curl could ask for the range of resource and then that range of resource is compressed (as opposed to requesting it compressed first then the requesting the range of that)",0,0,msr
3848,"To Badger: please, look at combination of options «-JO» and main page of your site:
curl.exe -J -O https://curl.se/windows/
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
0 0 0 0 0 0 0 0 --:--:-- 0:00:02 --:--:-- 0Warning: Remote filename has no length!
11 7914 11 928 0 0 360 0 0:00:21 0:00:02 0:00:19 360
curl: (23) Failure writing output to destination
But the «size_download» NOT equals to zero. Downloaded, but NOT SAVED. What is the point?
And I can not get the warning «Remote filename has no length!» from the script. Only the «errormsg» is available.",0,0,msr
3857,"Inspired by the steam forum topic ""Convoluted"" by Chelle, and a prompt by Developer Gadsby to post feedback,
this is a Quality Assurance Write-up Complaint about everything bad, unintuitive, convoluted, and otherwise
in need of serious QA (Quality Assurance) love. Retroarch NEEDS to up their QA to be able to act as a platform
for emulators. This post is meant to be intentionally hyper-critical and nitpicky on first world problems, because the experience is just that annoying. I hope to point out to readers many problems with the readability of Retroarch, and it's common user experience. I also hope if taken seriously, i get a contributor interested enough by my points, to add me and talk to me on discord, so we can work together on bringing retroarch up to a level people expect it to be at, and help retroarch hit console or platform level quality it is so desperately missing.
==================Chapter 0: I Didn't Even Launch It Yet And I Have Complaints=================
Note: If you are only contributor level and not developer level, scroll down and skip to chapter 1.
I'm going to start by looking at retroarchs website and steam page, and boy, before we even launch retroarch
for the first time, there a lot to talk about already. Infact, to even do this write up, or rather to attempt to
talk about it on the issues section on github i was pointed to, it says this is for bug reports only.
Currently, i'm under the assumption i'm already somehow breaking the rules by doing what i was told.
Not a good start, but not the worst. With that out of the way that i'm posting here on request, and not out
of ignorance for the warning github gives new posts, lets move on.
Lets look at the Website and steam page front pages. I kind of want to talk about them both at once.
I'll start with the opening description on both. It's to technical. Retroarch as i see it, is looking to be
a platform, like steam, to access things. Like the playstation XMB, or switch home screen. For a description,
on the site we get ""RetroArch is a frontend for emulators, game engines and media players."" and on steam ""RetroArch is an open source and cross platform frontend/framework for emulators, game engines, video games, media players and other applications""
This actually tells a user very little about what retro arch is, and isn't. Even people emulator savvy won't
understand the rest of what is being thrown at them. a 'frontend' for emulators? lets google frontend. ""frontend can refer to any hardware that optimizes or protects network traffic."" People aren't playing these
online, or it's not their first thought. They don't need protection from the net when playing mario. Okay,
maybe a smart person will think it has multiple definitions, and google more accurately. lets google Frontend Program
""A Front End developer (dev) works with designers and Back End devs to create a website"" Well, retroarch
isn't a website (Well technically it is but you know what i mean), it's a program itself. Already, using the
term frontend is to much. If you want retroarch to really succeed, to shine, you need to make it marketable and understandable. The optics matter, if people don't know what it is, they won't know why they should care
about being interested in it. when people say it's convoluted, you shouldn't need an 30 minutes of research
to even understand your opening sentences.
it also says 'game engines, and media players'. Why? Lets be honest, retroarch runs emulators. Thats what it
does. Why do you need to complicate this? Does it run other things sure, but nobody is seriously wanting it as their number 1 music player or picture taker. why can't you speak english? the steam page says emulators,
so someone knew that word, why aren't these two sources on the same page for their advertising? Unfortunately
the steam page then says game engines, video games, media players, and other apps.
What fucking goon is downloading retroarch just to play music. This is not a good use of advertising space.
The human being looking at your steam page can see the music menu in your advertising images. Use your text
space better, seriously!!! Why is this so hard! Also, it says emulators, then game engines, then video games.
pick ONE! these are all synonyms in practicality. People also don't need to know it's open source
in your intro, you can say that later. From what i've later come to understand, the devs want to use the term 'Core'. They want this because they believe in what retroarch does aside from emulators. Sure, except...the opening advertisement doesn't say 'Core'. Here is a good opener. ""RetroArch is a all in one platorm for Emulation that enhances the games you play
and other Cores. From retro era to modern games, you can even play online with friends!"" This is a fantastic, easy to digest, explanatory, to the point advertisement that tells people what
retroarch is, what it DOES (Enhances emulator games), and avoid words like ""nintendo"" or ""platstation"" for
copywrite while still getting the ""old to current"" idea across. Make your text readable to people! If they want more
they can hit up a FAQ. Note: Going forward, i will be referring to a reasonable way to read something, or
a problem with text that is to jargin-driven as ""Readability"" as a unbrella term. This will come up A LOT
and will be a serious problem with retroarch in nearly every part of anyones experience. Please remember this.
Lets move on. in the websites next section, it says you can run original game discs, but steam says the feature isn't out yet. These are literally giving conflicting information. Seriously? ITS YOUR FRONT PAGE!
Next, lets look at some visuals here. For the website, i want to look at the ""A polished Interface"" Slide.
This makes it look like playstation, cool. Except this isn't what it looks like in retroarch by default.
Make it clear in advertising this is a skin. Also, ironically, the interface is excessively NOT polished, but
i will be getting to that as a major complaint in another chapter. But right now, it's a skin, say it's
a skin. The very idea their not saying its not the default skin here, implies someone knew it was terrible and
tried to hide it. Maybe ask why they needed to hide your real interface? We will get to that later...
Moving back to steam, lets look at the slides/image roll. First is the trailers. All three are really fucking bad.
Trailer 1: its a fun logo (sure, fine) then shows the menu with garbage readability that makes a person
want to flee just looking at it, followed by showing what has GOT to be one of the WORST POSSIBLE game examples.
like, you CANNOT be serious. the website slides show mario, and this trailer has a level ID called zero / megaman129. Show us a actual video game. Make the trailer show like 8+ games of different genres. Make it CLEAR that retroarch plays VIDEO GAMES, and not, whatever the fuck that is. Bad advertising affects
end user readability to understand what retroarch is and is not. and this is contributing to why you get topics
saying people are confused about retroarch.
Trailer 2: Literally a logo. Why is this here? Remove it.
Trailer 3: This feels like a meme. Seriously, this is cruel, it hurts. It's like someone took what makes a good intro video and butchered it to the max. No BGM, no person explaining things, no examples
of games being played, it's super fast (Epilepsy educing even?). So fast you can't even read all the text
without pausing the video. Please make a real trailer. Look at like, any nintendo direct. Do that, but for retroarch. Show off and explain lots. I'll help QA it if you need someone. We can improve this.
Then the steam slides, their mostly the exact same menu. Why? Why not Show off various parts of retroarch? Actually make people interested. Your telling me you create all of retroarch, and the only part
you want people to know about is 2 screens? wtf? Now i can go on, but the pre-retroarch experience is getting comically long at this point.
I'll move onto actual retroarch, but rest assured, i can REALLY go on about whats wrong before even downloading.
In summary
1: Very poor readability
2: Poor advertising
3: Bad use of space
4: inconsistent between steam and website
5: Doesn't actually say what retroarch does in words people can understand let alone google. ===============Chapter 1: ""A Polished Interface""? Actually It's Really Fucking Garbage======================
Note: I have gotten some responses to points in this chapter from users using custom alternate skins for
RetroArch that change how the menu actually functions and displays information, and not just the visuals.
This chapter is about a first time experience, and as such uses the default skin. Before i actually talk about the REAL first impression, i want to point out i downloaded website and steam
both, to check for differences. Guess what? The first impression is diffrent. Inconsistent again? Ugh.
From website, it launched windowed, and you lack the cool cursor, and get the ultra low quality windows
menu that feels like a 2005 program with file/command/windows despite that retroarch has it's own in-app menus.
This doesn't happen on steam, it launches in fullscreen on first boot. The difference here is very important.
Not everyone knows you can Alt+Enter programs to fullscreen them. Infact, this is semi-rare info. This is made worst by F11 being the windows default to fullscreen, but boy do i wish complaining ended here.
If a non-steam user tries to fullscreen by hitting F11, retroarch hotkeys this to swapping between the windows
and custom cursor. Now, the custom one looks good, but it locks you into the windows. Surely you can
make your cursor not get locked, come on. But even worse, is when someone wants out and presses F11 again. I CANNOT BELIEVE how bad this is. If you are multi-monitor, your cursor is now locked to the monitor
retroach was in itself, untill you move retroarch. Clickin on it, pressing hotkeys ect won't fix this.
I don't mean using the retro cursor, i mean normal windows cursor, you can't leave your moniter. what the actual fuck? what a joke! I haven't even talked about retroarch contents directly, and
i can barely type up this QA review because retroarch literally won't let my cursor go click on notepad. >:(
it gets worse. If your fullscreen and press F11, your cursor doesn't change, but your mouse still can't
leave the moniter. The user would have no idea why and think retroarch is just a bad program. How can
you have this many errors and i'm not even talking about retroarch itself yet?????
OKAY, actual retroarch time. You can argue before this, thats a unintended bug, sure. Lets looks at whats NOT a bug.
So, we boot into the main menu that is...lacking readability at best. ""Load Core""? Why? these things are called emulators, call them emulators. ""Load Emulator"" is perfectly reasonable English. It's my understanding the devs want the term
Cores because it also uses something called Cores in addition to emulaitors. Sure, but you want to be
a marketable, understandable program? speak in words people actually understand. it's my understanding the team personally wants 'Load Core / Content' as they want to do more then emulators. In counter to this, i have 2 suggestions. Either split the menu AND load between Emulators (Load Emulaitor / Game), and other Cores (Load Core / Content), or make the load
button hidden untill a emulaitor or core is ready, then have it show Load Game or Load Content based on what Emu/Core
is loaded! Easy! Also, this menu is bloated.
So, 1: Split the cores menu, between ""Cores"", and a new one called ""Emulators""
2-1: In addition to Load Content, add a new menu called Load Game right under Emulators.
2-2: Or, make load content/game hidden until a emulator or core is loaded in already.
3: Also change Esc defaulting to exiting the program, to a back button. This is semi-common, and straight up default
in nearly every PC game. If your a platform on a PC for people gaming on a PC, use a PC control scheme. wtf?
there is a quit button if needed, and Alt+F4 or Alt+Enter, we don't need to troll PC users with a comically
bad default control scheme thats normally a menu button on almost every pc video game ever released.
""Bloated"" i'm going to define, as to many things being in one menu, or things being in the wrong location.
Here, we have the main menu showing config, info, and help. But we have a left side bar. whats the difference between config and settings? this is already a readability problem, they can be seen as synonyms. Now, i'm gonna be going all over unfocused here, because there is so many problems their all interwoven.
The Config file menu, why is this a menu? 1: Delete the Config menu entry
2: Move the contents of the config menu, to the top of the settings menu. 3: Why is help a menu with only 1 content? Literally delete this menu.
4: Move Menu Controls and it's icon to the bottom left of retroarch below the white line. This menu is first of all stupid and should probably be removed entirely, however...
4-2: Rename Menu Controls to Controller and have this lead directly to the input menu. If you really want,
include the Menu controls in Controller, but really it's common sense, and people can customize this,
so the stated controls in Menu Controls can literally lie to the user, this is dumb.
5-1: on first startup, maybe do a first time setup that asks the user what menu icons they would like.
let them pick from switch, xbox, playstation, ect. Its also kinda bad to display Switch by default, but
use american playstation input scheme, and not having a switches A button actually be A. Readability issues!!!!!
5-2: If that is a copywrite problem, alternatively let people customize the on-screen symbols with letters. In this way,
people can recreate their console of choice setup while working around looking like it clones a competitor, because
it's a customization option of a user. And don't tell me customization options aren't fair game when your straight up
stealing the playstation home menu theme >_>
Now that thats done, we have a nice (enough) looking home. Onward to settings. The entire thing is bloated. There is also a lot of 2-menus required trolls. There are so many times you look in one menu and need to google to find another menu, it's not even funny. 1: First up, just like the white line at the bottom of retroarch, i recommend a new break line in the middle
of settings. Things below it will be advanced settings, and title the section Advanced. Alternatively, if
this won't translate well to alternate skins, instead have a menu in here called Advanced at the bottom of settings.
Rename Saving to Saving Games (Readability, this seems like save Retroarch settings when it's here like this!)
2: Throw logging, File Browser, On-Screen Display, AI Service, Latency, into Advanced.
3: Delete the entire power management menu. wtf is this? It's literally empty. LITERALLY, USELESS. WHO????
4: Rename Network to Online, or Online Multiplayer. (Readability!)
5: Rename Input to Controllers
6-1: On the left bar, Rename Import Content to ""Create Playlist"" (Because that is literally what it does)
6-2: Move the playlists menu into the now named ""Create Playlist"" menu, de-bloat settings, so only people who care about the playlist menus are forced to see and scroll past this option. 6-3: Add a button named ""More Information on Playlists"" or maybe ""Playlists Tutorial"" or similar as well
to explain what is a 'Playlist', how to set them up, needing cores/emulators, ect.
6-4: if a Scan to create a playlist in a directory comes up empty, have a pop up explaining why. (Missing cores, ect)
7: I cannot believe you have a menu called configuration here, when i earlier talked about configuration in
the main menu. Why do you have 2 config menus, and a settings menu? HELLO? This is so troll, the
config options in settings/config effect your stuff in main menu/config. Why the fuck are they not together?
Who designed this? *Phew* now that thats done.....
7-1: In this submenu, move 'Use Global Core Options File' to the 'Load Core' Menu we are renaming to 'Load Emulator'.
7-2: We are also renaming this option ""Use Global Emulator Options"".
Readability is very important! 7-3: Finally, If (hopefully) Main menu config is moving to settings, either delete this Configuration menu, and move its only remaining option (Save Config on Quit) to settings, or move the config stuff from main menu, inside this config sub-menu. 8: AGAIN with the 2 menus needed trolls. Inside Directory we have a lot of playlist stuff, why?
8-1: In playlist menu (that we are moving to the menu in point 6 we are calling ""Create Playlist"" Menu) create a new menu called Playlist Directories
8-2: Move everything playlist related from settings/Directory to Music/Playlist Directories. This is to de-bloat menus with unnecessary information overflow, and put settings in menus they are actually
used in, like any other emulaitor (PCSX2/RPCS3/Citra/Cemu/Yuzu/ect)
9: Speaking of multiple same named menus, we have achievements, and a users achievements. STOP DOING THIS!!!!
why do you want people to need to google one menu doesn't work without the other menu! THIS IS SO TROLL, STOP IT!
9-1: Rename User to Account Settings & Achievements
9-2: Move the achievements menu into this newly named menu to de-bloat settings, and put this in a reasonable spot.
9-3: The Accessibility menu only has 1 option in it. Stop having troll 1 option menus. Delete this menu
9-4: Rename the Sub-menu ""Accessibility Enable"" To ""Text to Speech""
9-5: Move ""Text to Speech"" to the Account Settings & Achievements, so one user can use it, and
another different user doesn't have to deal with troll text to speech in their ears. 10: Continuing from 9, we have streaming options in account, and them in recording. WHY.
10-1: Rename ""Recording"" to ""Recording & Live Streams""
10-2: move the the content of the now ""Account Settings & Achievements"" / Account / Retroachievements to
the Achievements menu that was moved here in part 9. 10-3: The Youtube/Twitch/Facebook Gaming sub-menus only have 1 content, STOOOP FUCKINGGG DOOOING THIIIIS. >:((((
we are deleteing these sub-menus. 10-3: Move the menus inside the Youtube/Twitch/Facebook Gaming sub-menu stream key options from here, to the now named ""Recording & Live Streams"" menu. 10-4: in the ""Recording & Live Streams"" menu/Streaming Quality, give one of those grey sub-descriptions.
Something like ""Suitable for 720P Streams"" ""For 1080P Streams"" Ect. 10-5: Also rename the menu to Stream Quality.
Oh boy, onto input (that we said we are renaming to Controller)
11-1: Move Maximum Users to...somewhere. Wtf even is this menu? If this actually means users, move it to the
relevant online menu. If it means locally, rename this to ""Maximum Controllers"". 11-2: If the later, Create one of those Advanced Options line breaks again at the bottom, and move this
down there, along with every other option in this menu untill we get to Menu Controls. 11-3: MENU CONTROLLS? HELLO? Why does main menu get help on menu controlls, BUT WE HAVE A ACTUAL MENU CONTROLS MENU? PUT RELEVANT MENUS IN RELEVANT PLACES THANK YOU.
11-4: Move the Automatic Mouse Grab option to the Video menu in Settings. This is something usually in that
menu on other programs, stop being unintuitive!!! How can you make so many mistakes!!!
Speaking of the Video menu, lets tackle that next.
12-1:in Fullscreen Mode, rename Windowed Fullscreen to ""Borderless Fullscreen"" or just ""Borderless"",
and change the description appropriately.
12-2: Rename Fullscreen Width & Height to ""Custom Width"" (& Height). It's already in the fullscreen sub-menu, you don't need to remind them what menu they are in, it's at the top of the screen!
12-3: Move CRTSwitchRes and Output below the Fullscreen and Windowed sub-menus. Order matters! Most important
things go on top!
I can keep going, but i'll stop here. This is a Serious Checklist of Readability improvements across the entire settings menu. However, if i reconsider, i may suggest adding Controller to the left bar as a major
menu instead, as it's reasonab;y so important, it should be a major menu, rather then some garbage like Music.
=====================Chater 2: A Smoothe Gameplay Experience? Or A Troubleshooting Nightmare?=====================
Before i get into further details on games, i strongly recommend a new setting somewhere, on by default, that
makes it so when you load a game, and pick a emulaitor (Remember, we are calling these Games and Emulaitors, NOT
Cores and Content) that it associates that game to that emulaitor by default. In the future, you should be
able to Load game -> Pick the game -> it just boots up without needing to pick a emulaitor. I...HOPE i don't have to explain how bad it is that steam retroarch is missing a download button. I will skip
over the easy wall of text on this and move forward.
1: For steam users, there should still be a download emulaitor button, but as a submenu. 1-1: In here, There should be a link to your website with a list of emulaitors to download.
1-2: There should be a setting here, or in the Install Emulaitor menus to set the directory for cores.
This is because this options is only useful & relevant right at this moment, and never again, so it
does not need to be in the directroies menu elsewhere.
2: When in-game, move the Command/Menu button to the top of the list. I also feel a menu rename is in order, but can't think of one right now, and want to move on.
3-1: Add save state and load state to the Command menu. Yes i know they are in the F1 menu. Make them accessable.
3-2: In the F1 menu, move State slot below load state
And oh boy, lets get into the F1 menu. In no particular order...
4-0: Make it WAY more obvious to assign a button or combo to open this menu from controller.
probably include it in the quick menu directly at the bottom, even if the option exists somewhere else,
just duplicate it. 4-00: Also, Escape should open the quick menu by default
4-1: Originally i said Rename Information to Game Information, but i forgot i was still allowing for cores/content menus before.
i now suggest making this menus name be contextual, based on if a emulator, or core is loaded. This way people only using
emulators can have it reasonably say 'Game Information' and those who use cores still get 'Information' so it
doesn't clash with any development team desires.
4-2: Move the Controller menu to right below Close Content
4-3: Also rename that to Close Game
4-4: Adding something to favorites and you can't remove it? WHY? Make this menu change to Remove from Favorites if it's a favorite >_>
4-5: Remove the descriptions for Resume, Restart, and Close game. This screen has lots the player wants
to see and not enough screen space even when in fullscreen. 4-6: Also remove descriptions on save state, load state, Take Screenshot, Cheats, Achievements, and Rewind.
4-7: (Someone gave a good point against this, nevermind)
4-8: Make it so if the player presses cancel in the F1 to go to Main Menu, Then cancel again, it resumes the game.
4-9: move Show Desktop Menu from Main Menu while ingame to somewhere else (Probably settings).
With that out of the way, once ingame, there is no menu option to go back!
Add a button in windowed mode to open back the retroarch menu. If you tell me it's a hotkey, i don't care.
thats not intuitive! Make a button to go back to the retroarch menu!!!
Finally, this SHOULD be obvious, but it's not, so i will say it here.
I assume this is a major feature, but really, let people access that emulaitors settings when using it.
if we can't do that, retroarch is just worse then every emulaitor, because they have settings you NEED to even
run some games. If this option DOES exist, it's EXTREMELY hidden!
Moving on, post gameplay, your recent games don't show up in load content / load game.
Why isn't there a recent games list? And why can't you set a content / game directory in directories? If you can, that option should 100% be in the load content / Load Game menu, as this is where
it's actually relevant!!!
=======================Ending==================
I'll stop here, it's been 3 hours of complaints, but retroarch is riddles with terrible quality.
Why would anyone want to use this? 1: An unfun troubleshooting experience in exchange for playing games in the worst quality possible without access to emulators settings. 2: Some games are literally unplayable in retroarch because you can't access emu settings
3: The menus are bloated, and lie / decieves / give half truths by hiding relevant other options in other menus
for no other reason then to troll you. The overall feeling is Retroarch is giving you the run-around, asking you
to do something in some menu you don't know where it is, and when you find it, wants another setting in
another menu you don't know where it is. 4: it's a pain to setup even for simple emulators
5: you can't even understand what retroarch is or isn't from inconsistent and contradictory messages,
to near moon-rune levels of advertising.
6: unintuitive menus and lacking default settings mean you need to go over everything before even attempting
to test a single title.
7: Using it via steam is a whole extra nightmare that could be made easy if anyone actually fucking cared.
It's easy to see why people don't recommend recroarch. The real question is if anyone is going to actually
do anything about it, or delete this post / no reply / no one for me to directly work with in a timely fashon /
stick their head in the clouds and ignore why people don't enjoy their product. My discord is Dawnbomb#3408, and i'm open to any actual developer contacting me and working hand in hand for
some actual quality assurance before the steam deck launches, but considering how obvious, easy, and
near intensionally-troll the levels of design are here, i expect no contact and noone willing to work to make
retroarch not be complete fucking bullshit bloatware on a PC, and i only scratched the surface of this QA nightmare.
====================Chapter 3: Extra add-ons i thought of later=========================
Note: Some of the things here delve closer to more serious changes to retroarchs codeing.
Still, their good changes.
1: Controller backround input should be in the input (Hopefully named Controller) Menu, not in another menu (User Interface).
Setting up controller input in your controllers, then having if the actual input happens be decided in a separate menu, is another weird location problem.
2: While im at it, Turbo Fire should be a setting INSIDE a controllers port controls. This would allow each controller to have separate turbo settings per player. 3: Altho highly unusual, some players enjoy playing games with two people one controller mode. Some people even go so far as to speedrun like this. Allowing multiple controllers assign buttons to the same button on the same port of a input would go a long way in support for those people.",0,1,msr
3862,"Release v2.10.14 of Joda-Time uses [global-tz](https://github.com/JodaOrg/global-tz), which is derived from the IANA TZDB. The global-tz project reinstates all the data that has been systematically removed from IANA TZDB over the past few years.",0,0,msr
3870,"> I want only to say that as long that markdown test suite will be not working I would not be able to do that or in this exact case I would be doing that only with part of the test suite.
I understand you have a vision and you have a desire to collect testing data, but I'm going to be very blunt about this, if your whole system depends on every python package you use to support your specific testing framework, you are building a very fragile system. Cornering other projects into using one specific testing framework is not a good idea, and it takes away the freedom of that project to maintain their project in a way that they find most suitable to them.
What if some other framework comes out that a project finds more enticing than pytest? Are they not allowed to take advantage of that testing framework as to not break your pytest dependent system?
I'm not seeing an advantage for us to switch to pytest, and I definitely can't see a reason to set up and test an additional framework that will test the exact same tests.
Again, I have nothing against pytest, and I do use it myself. There are features pytest has that make testing easier in some cases, but this project doesn't currently have a scenario that we **need** anything beyond what is already offered by Python's unittest module.",0,1,msr
3876,@hmdne Thanks,0,0,msr
3878,"Excuse me, I was just trying to help you and is this how you thank me. By marking it spam. If you dont need it then just close the request.",1,1,msr
3895,the bot didnt opened since 19 hours... Something is wrong with the world,0,0,msr
3900,Who will win this war 👀,0,0,msr
3904,This has no limit,0,0,msr
3906,"> You have no power ower this 😂
What if..... We made another bot to conquer this issue 😎",0,0,msr
3919,"> > this issue will still exist even after years, even when this repo dies out, god bless this immortal renovate bot🤝
> > Not after the release of v5
🙏🏼",0,0,msr
3921,"This issue lists Renovate updates and detected dependencies. Read the [Dependency Dashboard](https://docs.renovatebot.com/key-concepts/dashboard/) docs to learn more.
## Rate-Limited
These updates are currently rate-limited. Click on a checkbox below to force their creation now.
- [ ] <!-- unlimit-branch=renovate/node-17.x -->chore(deps): update dependency @types/node to v17.0.45
- [ ] <!-- unlimit-branch=renovate/prettier-3.x -->chore(deps): update dependency prettier to v3
- [ ] <!-- unlimit-branch=renovate/typescript-5.x -->chore(deps): update dependency typescript to v5
- [ ] <!-- unlimit-branch=renovate/major-nextjs-monorepo -->chore(deps): update nextjs monorepo to v14 (major) (`eslint-config-next`, `next`)
- [ ] <!-- unlimit-branch=renovate/node-20.x -->chore(deps): update node.js to v20 (`node`, `@types/node`)
- [ ] <!-- unlimit-branch=renovate/node-21.x -->chore(deps): update node.js to v21
- [ ] <!-- unlimit-branch=renovate/major-yarn-monorepo -->chore(deps): update yarn to v4
- [ ] <!-- unlimit-branch=renovate/discordjs-rest-2.x -->fix(deps): update dependency @discordjs/rest to v2
- [ ] <!-- unlimit-branch=renovate/nextui-org-react-2.x -->fix(deps): update dependency @nextui-org/react to v2
- [ ] <!-- unlimit-branch=renovate/axios-1.x -->fix(deps): update dependency axios to v1
- [ ] <!-- unlimit-branch=renovate/express-rate-limit-7.x -->fix(deps): update dependency express-rate-limit to v7
- [ ] <!-- unlimit-branch=renovate/pretty-ms-8.x -->fix(deps): update dependency pretty-ms to v8
- [ ] <!-- create-all-rate-limited-prs -->🔐 **Create all rate-limited PRs at once** 🔐
## Open
These updates have all been created already. Click a checkbox below to force a retry/rebase of any.
- [ ] <!-- rebase-branch=renovate/nextui-org-react-1.x -->[fix(deps): update dependency @nextui-org/react to v1.0.0-beta.9-dbg2](../pull/1176)
- [ ] <!-- rebase-branch=renovate/better-erela.js-spotify-1.x -->[fix(deps): update dependency better-erela.js-spotify to v1.3.11](../pull/1178)
- [ ] <!-- rebase-branch=renovate/discord-api-types-0.x -->[fix(deps): update dependency discord-api-types to v0.37.63](../pull/1179)
- [ ] <!-- rebase-branch=renovate/react-monorepo -->[chore(deps): update dependency @types/react to v18.2.37](../pull/1177)
- [ ] <!-- rebase-branch=renovate/eslint-8.x -->[chore(deps): update dependency eslint to v8.53.0](../pull/1184)
- [ ] <!-- rebase-branch=renovate/prettier-2.x -->[chore(deps): update dependency prettier to v2.8.8](../pull/1185)
- [ ] <!-- rebase-branch=renovate/node-16.x -->[chore(deps): update node.js to v16.20.2](../pull/1181)
- [ ] <!-- rebase-branch=renovate/yarn-monorepo -->[chore(deps): update yarn to v3.6.4](../pull/1182)
- [ ] <!-- rebase-branch=renovate/colors-1.x -->[fix(deps): update dependency colors to v1.4.0](../pull/1183)
- [ ] <!-- rebase-branch=renovate/node-fetch-2.x -->[fix(deps): update dependency node-fetch to v2.7.0](../pull/1180)
- [ ] <!-- rebase-all-open-prs -->**Click on this checkbox to rebase all open PRs at once**
## Ignored or Blocked
These are blocked by an existing closed PR and will not be recreated unless you click a checkbox below.
- [ ] <!-- recreate-branch=renovate/typescript-4.x -->[chore(deps): update dependency typescript to v4.9.5](../pull/1076)
- [ ] <!-- recreate-branch=renovate/nextjs-monorepo -->[chore(deps): update nextjs monorepo to v12.3.4](../pull/1082) (`eslint-config-next`, `next`)
- [ ] <!-- recreate-branch=renovate/better-erela.js-apple-1.x -->[fix(deps): update dependency better-erela.js-apple to v1](../pull/708)
- [ ] <!-- recreate-branch=renovate/discord.js-14.x -->[fix(deps): update dependency discord.js to v14](../pull/1006)
- [ ] <!-- recreate-branch=renovate/node-fetch-3.x -->[fix(deps): update dependency node-fetch to v3](../pull/576)
## Detected dependencies
<details><summary>docker-compose</summary>
<blockquote>
<details><summary>docker-compose.yml</summary>
</details>
</blockquote>
</details>
<details><summary>dockerfile</summary>
<blockquote>
<details><summary>Dockerfile</summary>
- `node 17.9.1-alpine`
</details>
</blockquote>
</details>
<details><summary>npm</summary>
<blockquote>
<details><summary>dashboard/package.json</summary>
- `@emotion/react ^11.9.3`
- `@emotion/styled ^11.9.3`
- `@mui/icons-material ^5.8.4`
- `@mui/material ^5.8.4`
- `@nextui-org/react 1.0.0-beta.9`
- `next 12.2.4`
- `react 18.2.0`
- `react-dom 18.2.0`
- `@types/node 17.0.41`
- `@types/react 18.0.16`
- `eslint 8.19.0`
- `eslint-config-next 12.2.4`
- `typescript 4.7.4`
</details>
<details><summary>package.json</summary>
- `@discordjs/builders ^1.4.0`
- `@discordjs/rest ^1.5.0`
- `axios ^0.27.0`
- `better-erela.js-apple ^0.1.0`
- `better-erela.js-spotify 1.3.9`
- `colors 1.3.3`
- `discord-api-types 0.37.1`
- `discord-together ^1.3.25`
- `discord.js ^13.14.0`
- `dotenv ^16.0.1`
- `ejs ^3.1.6`
- `erela.js ^2.3.3`
- `erela.js-deezer ^1.0.7`
- `erela.js-facebook ^1.0.4`
- `erela.js-filters ^1.2.6`
- `express ^4.17.1`
- `express-rate-limit ^6.2.0`
- `express-session ^1.17.3`
- `express-ws ^5.0.2`
- `js-yaml ^4.1.0`
- `jsoning ^0.13.0`
- `lodash ^4.17.21`
- `moment ^2.29.1`
- `moment-duration-format ^2.3.2`
- `node-fetch 2.6.7`
- `os ^0.1.2`
- `passport ^0.6.0`
- `passport-discord ^0.1.4`
- `pretty-ms ^7.0.1`
- `rlyrics ^2.0.1`
- `systeminformation ^5.9.12`
- `winston ^3.3.3`
- `youtube-sr ^4.3.4`
- `prettier 2.6.2`
- `node >=16.x <=16.16`
- `node 16.15.1`
- `yarn 3.3.0`
</details>
</blockquote>
</details>
---
- [ ] <!-- manual job -->Check this box to trigger a request for Renovate to run again on this repository",0,0,msr
3923,"Wait, how to close issue?",1,0,msr
3926,Legend says this issue will still exist even on the end of mankind,0,1,msr
3935,"> I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.
Giving the user the _option_ to grant the application permission to take future screenshots/screencasts without further user permission would be OK. We just don't want the application to be able to give itself this permission, or to be able to force the user to grant this permission in order to use it just once.
So let's forget about additional metadata. I would retitle this issue from ""Screenshot portal without prompt"" to ""Screenshot portal should have toggle for user to disable future prompts for this application.""",1,0,msr
3940,"Can I express how powerless this issue makes me feel in relation to Gnome development. The tone from the Gnome folks here: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970 indicates to me this issue will not be resolved/is not considered an issue. Creating a Gnome related bug report is akin to shouting into the void, isn't it?",0,1,msr
3943,"@kurobeats @DamirPorobic
The Gnome developers made this decision (don't allow external apps to use gnome's private API) for protecting the privacy of users and forbidding API abuse. They are also doing a redesign for the screenshot UI of gnome: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1954 The Words accusing the developers of not caring about something are not true, and totally unconstructive.",1,0,msr
3944,"@VitalyAnkh I've spoken with Gnome developers (also same with KDE developers but they haven't disabled the private dbus yet) on few occasions about this topic, on tickets and on IRC and my impression was that this is for them a minor inconvenience. One of them told me even that this is a fair trad off, few clicks more but you get more security. So I don't see it as ""not true"" and even the ""totally unconstructive"" is debatable, user requirement sets the prio for features, if no one speaks up the developers that don't use this feature won't get the user pain. Not sure what the screenshot UI redesign to do with this issue, our problem is that we need to give permission for every screenshot instead of once like most people are used to like from mobile phone apps. @mcatanzaro is right, there is a suggestions that, when implemented, would fix our issues, I don't see any further discussion here.",0,1,msr
3945,"> Honestly, your expectations are misplaced. Maybe so but the action taken has now created a negative user experience. Forgetting my comments, I'm a voice in the crowd, think about the ""regular user"", they are going to see this behaviour and be confused by it because it's not something they have come to expect.
>Bypassing the screenshot portal is unacceptable, as it defeats the point of having the portal in the first place. Applications should not be able to screenshot your desktop without your permission. Removing the backdoor should not be controversial.
Let's keep in mind we are talking about a screenshotting tool here not a trojan (which I imagine we are trying to defend against here). Flameshot, for example, doesn't have to overcome hurdles to screenshot on Windows or MacOS.
> If you read up in this issue, we already have agreement on the path forward to enhance the screenshot portal. It's just waiting for a motivated developer to tackle it.
I'm very happy to hear it, but wouldn't it have been pragmatic to implement the solution before we do unexpected things to the user base?",0,0,msr
3946,"Author of flameshot here, actually on MacOS you do have to grant a one time permission to record the the screen.
While I was annoyed this changed in gnome before we adjust the portal to only ask a single time, overall this will be a great compromise between ease of use and security. It also will be exactly the same as MacOS. I think my users will also find the one time prompt acceptable.",0,0,msr
3947,"> Yeah, I think just a dialog that asks for that permissions with an option to make it permanent would in that case be the best, without any other selection option. Bonus points for additional parameter in the API call like ""Include cursor"" and ""Include window decoration"".
> > > That wouldn't apply the same to screencasts, but those are a separate portal.
> > Yes, I think it was a different API but I can imagine that they have similar issues if they haven't been fixed already.
Yeah I fully agree!!
I understand the security concerns from the Gnome team, but it's also about giving users options. Give users the ability to give certain applications permission to do this and remember their choice.
Certain programs like Screenshot apps and screen capture/recording programs need this to work. And giving those programs permission every time gets tiring and not user friendly. (from an UX point of view)",0,0,msr
3950,"Hi Team,
Can I request if this issue is being addressed? Totally fine if it isn't deemed an issue, I can revert to Xorg.",0,0,msr
3958,"https://github.com/flatpak/xdg-desktop-portal/pull/853 exists, now someone with proper authority have to review and eventually merge that",0,0,msr
3964,"I'm sorry if I missed something, but I wasn't able to find any guidence on how to update xdg-desktop-portal in order for gnome not to ask if I want to share my screenshot with Flameshot every time. Could you please point me to how I can do this? I'm using ubuntu 22.04 with gnome 42.",0,0,msr
3971,"Thanks a lot, devs! :D expecially @GeorgesStavracas",1,0,msr
3977,"### Key information
""joda-time"" uses Kiev (incorrect) instead of Kyiv (correct) spelling.
Kyiv is a capital of Ukraine.
According to the Ukrainian local and international law, correct name of the ""Kiev"" is ""Kyiv"".
Reference materials:
- Local: Resolution No 55 of 27.01.2010 of the Cabinet of Ministers of Ukraine ""On regulation of transliteration of Ukrainian alphabet by means of Latin alphabet "".
- Global: https://unstats.un.org/UNSD/geoinfo/UNGEGN/docs/10th-uncsgn-docs/econf/E_CONF.101_85_Standardization%20of%20Geographical%20Names%20_eng.pdf
- Wikipedia: https://en.wikipedia.org/wiki/KyivNotKiev
### Problem description
Incorrect spelling of Kyiv is used all across this project:
```
$ grep -r ""Kiev""
src/site/xdoc/timezones.xml:<tr><td align=""left"" valign=""top"">+02:00</td><td align=""left"" valign=""top"">Europe/Kiev</td><td align=""left"" valign=""top""></td></tr>
src/main/java/org/joda/time/tz/src/europe:# from Kiev to Moscow time sometime after the January 1994 elections.
src/main/java/org/joda/time/tz/src/europe:# From IATA SSIM (1994/1997), which also says that Kerch is still like Kiev.
src/main/java/org/joda/time/tz/src/europe:# ""Time in Ukraine is set to second timezone (Kiev time). Each last Sunday
src/main/java/org/joda/time/tz/src/europe:# (Europe/Kiev) to introduce permanent daylight saving time (similar
src/main/java/org/joda/time/tz/src/europe:# From Vladimir in Moscow via Alois Treindl re Kiev time 1991/2 (2014-02-28):
src/main/java/org/joda/time/tz/src/europe:# For example, tzdb uses Europe/Kiev, as ""Kiev"" is the most common spelling in
src/main/java/org/joda/time/tz/src/europe:# ""Praha"". (""Kiev"" came from old Slavic via Russian to English, and ""Prague""
src/main/java/org/joda/time/tz/src/europe:# English; in the meantime, stick with the traditional English ""Kiev"" as that
src/main/java/org/joda/time/tz/src/europe:# This represents most of Ukraine. See above for the spelling of ""Kiev"".
src/main/java/org/joda/time/tz/src/europe:Zone Europe/Kiev	2:02:04 -	LMT	1880
src/main/java/org/joda/time/tz/src/europe:	2:02:04	-	KMT	1924 May 2 # Kiev Mean Time
```
This issue should be fixed.
Instead of ""Kiev"" spelling ""Kyiv"" should be used instead.",1,0,msr
3983,"> I would not be surprised if this database is maintained by the Russians, and the servers are located in the Russian-occupied Crimea.
I'm pretty sure that's not the case. In addition, there is a comprehensible answer as to why this wish was not realised at the time.
https://mm.icann.org/pipermail/tz/2020-August/029203.html",0,0,msr
3988,"### Key information
""joda-time"" uses Kiev (incorrect) instead of Kyiv (correct) spelling.
Kyiv is a capital of Ukraine.
According to the Ukrainian local and international law, correct name of the ""Kiev"" is ""Kyiv"".
Reference materials:
- Local: Resolution No 55 of 27.01.2010 of the Cabinet of Ministers of Ukraine ""On regulation of transliteration of Ukrainian alphabet by means of Latin alphabet "".
- Global: https://unstats.un.org/UNSD/geoinfo/UNGEGN/docs/10th-uncsgn-docs/econf/E_CONF.101_85_Standardization%20of%20Geographical%20Names%20_eng.pdf
- Wikipedia: https://en.wikipedia.org/wiki/KyivNotKiev
### Problem description
Incorrect spelling of Kyiv is used all across this project:
```
$ grep -r ""Kiev""
src/site/xdoc/timezones.xml:<tr><td align=""left"" valign=""top"">+02:00</td><td align=""left"" valign=""top"">Europe/Kiev</td><td align=""left"" valign=""top""></td></tr>
src/main/java/org/joda/time/tz/src/europe:# from Kiev to Moscow time sometime after the January 1994 elections.
src/main/java/org/joda/time/tz/src/europe:# From IATA SSIM (1994/1997), which also says that Kerch is still like Kiev.
src/main/java/org/joda/time/tz/src/europe:# ""Time in Ukraine is set to second timezone (Kiev time). Each last Sunday
src/main/java/org/joda/time/tz/src/europe:# (Europe/Kiev) to introduce permanent daylight saving time (similar
src/main/java/org/joda/time/tz/src/europe:# From Vladimir in Moscow via Alois Treindl re Kiev time 1991/2 (2014-02-28):
src/main/java/org/joda/time/tz/src/europe:# For example, tzdb uses Europe/Kiev, as ""Kiev"" is the most common spelling in
src/main/java/org/joda/time/tz/src/europe:# ""Praha"". (""Kiev"" came from old Slavic via Russian to English, and ""Prague""
src/main/java/org/joda/time/tz/src/europe:# English; in the meantime, stick with the traditional English ""Kiev"" as that
src/main/java/org/joda/time/tz/src/europe:# This represents most of Ukraine. See above for the spelling of ""Kiev"".
src/main/java/org/joda/time/tz/src/europe:Zone Europe/Kiev	2:02:04 -	LMT	1880
src/main/java/org/joda/time/tz/src/europe:	2:02:04	-	KMT	1924 May 2 # Kiev Mean Time
```
This issue should be fixed.
Instead of ""Kiev"" spelling ""Kyiv"" should be used instead.",1,0,msr
3997,"> I'll prepare a PR that
> > * moves all the instructions files into a docs directory (is that the best name?)
> * removes crosslinks from each translated file and adds them to a docs index page so new files only need to add a link in one place.
See this note in chore posted by you: https://github.com/EbookFoundation/free-programming-books/issues/6164#issuecomment-940222595",0,0,msr
4003,"> Just checking, you tested in the GitHub Mobile App?
> https://play.google.com/store/apps/details?id=com.github.android&hl=en_US&gl=US
yes. it works as well.
| Closed | Open |
|---|---|
| ![Screenshot_20220215-152254.png](https://user-images.githubusercontent.com/3125580/154081554-0255f8dc-97fa-4f83-97fb-276be29d739f.png) | ![Screenshot_20220215-152337.png](https://user-images.githubusercontent.com/3125580/154081630-f5667cfd-8dd2-4da4-9a45-c538ba1fcc11.png) |",0,0,msr
4005,"- Homogenize markdown format across files. Solve some linter faults
- [x] #6625
- [x] #6724
- [x] #6698
- [x] Sort links according to english text seen in README.md. **Addressed in #6164**
### Local branch
- [x] Reintegrate #5837
- [x] ~Reintegrate stalled conflictive #5752 to complete translation~. Addressed by: #6719
- [x] Reintegrate #6429
- [x] Reintegrate #5831
- [x] Reintegrate #6567",1,0,msr
4019,"> $ ./test.js
> Illegal instruction (core dumped)
`$ bun run test.js` ?",0,0,msr
4024,"This is test.js
```
#!/usr/bin/env -S ./bun console.log('Hello, World!');
```",1,0,msr
4028,"> and thats not gonna work for you...
Evidently. That is why I filed https://github.com/Jarred-Sumner/bun/issues/504.
Cf. QuickJS and Node.js, respectively
```
#!/usr/bin/env -S ./qjs
console.log('Hello, World!');
```
```
$ ./test.js
Hello, World!
```
```
#!/usr/bin/env -S ./node
console.log('Hello, World!');
```
```
$ ./test.js
Hello, World!
```",0,0,msr
4038,"This issue is cause by an instruction set that is not present on older CPUs. I believe that the contributors are all running on modern CPUs, so they didn't run into this issue before the release of v.0.1.0.",0,0,msr
4045,No Answer to Fix this Problem?????,0,1,msr
4064,"Yeah, It will be a nice addition to Retroarch as it allows us to emulate **Nintendo Switch Games**. > ### Yuzu Emulator Core Request
> [So, i thinking a new core as Nintendo Switch called Yuzu, so what do you think?]
> > Note: this iis an request, if u add it its a pleasure♡♡",1,0,msr
4077,"Hi @GeorgeZan , let me rephrase the issue that you have described and please correct me if I have not understood it correctly.
The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ?",0,0,msr
4078,"> The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ?
In general, any classifier can never predict a class that was not part of the training set. I am not sure what ""break down"" means in this context.
> But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.
Can you please craft a standalone reproducible code snipet example that generates its own toy dataset so that we can just execute ? I am afraid the English language is ambiguous while Python is not.
Please also report the result you observe and the output you would have expected.",0,0,msr
4081,"Hello @GeorgeZan , for the case 3 as you have mentioned:
If I keep the categorical feature in the model and the test set instance provided has a unknown/unseen value at the categorical feature then the model returns the following prediction:
B: 0.5
A: 0.3
C: 0.2
So, to which class do you expect this example to belong to ? Because I'm confused here when you mentioned - - ' (iii) it returns essentially random predictions.
It does not make sense to me that if you just receive an unseen value your prediction changes so so much.
I would expect (iii) to be more like (ii).'",0,0,msr
4083,"I don't think that there is anything wrong but I would not be able to be sure until we have an example showing the wrong behaviour and the expected behaviour.
Since `SGDClassifier` is a linear model, when it comes to categorical data, the type of encoding is rather important. So did you check the coefficients of your model? Did you use an `OneHotEncoder` for encoding your categories? Basically, the value of the ""unseen categories"" will have an impact depending on the coefficient of the associated variable.",0,0,msr
4095,"### Env info
```
$ sw_vers
ProductName:	Mac OS X
ProductVersion:	10.14.6
BuildVersion:	18G103
$ clang --version Apple clang version 11.0.0 (clang-1100.0.33.17)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
$ ls -l /usr/local/lib | grep glog
-rw-r--r-- 1 neo admin 391864 Dec 9 15:02 libglog.a
$ mkdir -p /tmp/t && cp /usr/local/lib/libglog.a /tmp/t && ar x /tmp/t/libglog.a
$ ls -1 /tmp/t/*.o | xargs nm | grep MakeCheckOpValueString | cut -d' ' -f2- | sort -u
T __ZN6google22MakeCheckOpValueStringIaEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIcEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIhEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIiEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
$ ls -1 /tmp/t/*.o | xargs nm | grep MakeCheckOpValueString | cut -d' ' -f3 | sort -u | c++filt
void google::MakeCheckOpValueString<signed char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, signed char const&)
void google::MakeCheckOpValueString<char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, char const&)
void google::MakeCheckOpValueString<unsigned char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, unsigned char const&)
void google::MakeCheckOpValueString<int>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, int const&)
# Build error msg
Undefined symbols for architecture x86_64:
""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in foo.cpp.o
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in bar.cpp.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```",1,0,msr
4103,"My simple program links to the `glog` library, but it seems cannot find some symbols:
```
...
Undefined symbols for architecture x86_64:
""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in foo.cpp.o
std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in bar.cpp.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```
### HOW I compile glog
<details>
```bash
git clone --depth=1 https://github.com/google/glog.git
pushd glog
cmake -S . -B build -DBUILD_SHARED_LIBS=OFF
cmake --build build --target install
popd
```
```
Cloning into 'glog'...
remote: Enumerating objects: 97, done.
remote: Counting objects: 100% (97/97), done.
remote: Compressing objects: 100% (86/86), done.
remote: Total 97 (delta 25), reused 38 (delta 6), pack-reused 0
Unpacking objects: 100% (97/97), done.
-- The CXX compiler identification is AppleClang 11.0.0.11000033
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Could NOT find GTest (missing: GTest_DIR)
CMake Warning at CMakeLists.txt:90 (find_package):
By not providing ""Findgflags.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""gflags"", but
CMake did not find one.
Could not find a package configuration file provided by ""gflags"" (requested
version 2.2.0) with any of the following names:
gflagsConfig.cmake
gflags-config.cmake
Add the installation prefix of ""gflags"" to CMAKE_PREFIX_PATH or set
""gflags_DIR"" to a directory containing one of the above files. If ""gflags""
provides a separate development package or SDK, be sure it has been
installed.
-- Looking for C++ include pthread.h
-- Looking for C++ include pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE -- Could NOT find Unwind (missing: Unwind_LIBRARY Unwind_PLATFORM_LIBRARY) -- Looking for C++ include unwind.h
-- Looking for C++ include unwind.h - found
-- Looking for C++ include dlfcn.h
-- Looking for C++ include dlfcn.h - found
-- Looking for C++ include execinfo.h
-- Looking for C++ include execinfo.h - found
-- Looking for C++ include glob.h
-- Looking for C++ include glob.h - found
-- Looking for C++ include inttypes.h
-- Looking for C++ include inttypes.h - found
-- Looking for C++ include memory.h
-- Looking for C++ include memory.h - found
-- Looking for C++ include pwd.h
-- Looking for C++ include pwd.h - found
-- Looking for C++ include stdint.h
-- Looking for C++ include stdint.h - found
-- Looking for C++ include strings.h
-- Looking for C++ include strings.h - found
-- Looking for C++ include sys/stat.h
-- Looking for C++ include sys/stat.h - found
-- Looking for C++ include sys/syscall.h
-- Looking for C++ include sys/syscall.h - found
-- Looking for C++ include sys/time.h
-- Looking for C++ include sys/time.h - found
-- Looking for C++ include sys/types.h
-- Looking for C++ include sys/types.h - found
-- Looking for C++ include sys/utsname.h
-- Looking for C++ include sys/utsname.h - found
-- Looking for C++ include sys/wait.h
-- Looking for C++ include sys/wait.h - found
-- Looking for C++ include syscall.h
-- Looking for C++ include syscall.h - not found
-- Looking for C++ include syslog.h
-- Looking for C++ include syslog.h - found
-- Looking for C++ include ucontext.h
-- Looking for C++ include ucontext.h - not found
-- Looking for C++ include unistd.h
-- Looking for C++ include unistd.h - found
-- Looking for C++ include ext/hash_map
-- Looking for C++ include ext/hash_map - found
-- Looking for C++ include ext/hash_set
-- Looking for C++ include ext/hash_set - found
-- Looking for C++ include ext/slist
-- Looking for C++ include ext/slist - not found
-- Looking for C++ include tr1/unordered_map
-- Looking for C++ include tr1/unordered_map - not found
-- Looking for C++ include tr1/unordered_set
-- Looking for C++ include tr1/unordered_set - not found
-- Looking for C++ include unordered_map
-- Looking for C++ include unordered_map - found
-- Looking for C++ include unordered_set
-- Looking for C++ include unordered_set - found
-- Looking for C++ include stddef.h
-- Looking for C++ include stddef.h - found
-- Check size of unsigned __int16
-- Check size of unsigned __int16 - failed
-- Check size of u_int16_t
-- Check size of u_int16_t - done
-- Check size of uint16_t
-- Check size of uint16_t - done
-- Looking for dladdr
-- Looking for dladdr - found
-- Looking for fcntl
-- Looking for fcntl - found
-- Looking for pread
-- Looking for pread - found
-- Looking for pwrite
-- Looking for pwrite - found
-- Looking for sigaction
-- Looking for sigaction - found
-- Looking for sigaltstack
-- Looking for sigaltstack - found
-- Performing Test HAVE_NO_DEPRECATED
-- Performing Test HAVE_NO_DEPRECATED - Success
-- Performing Test HAVE_NO_UNNAMED_TYPE_TEMPLATE_ARGS
-- Performing Test HAVE_NO_UNNAMED_TYPE_TEMPLATE_ARGS - Success
-- Looking for pthread_threadid_np
-- Looking for pthread_threadid_np - found
-- Looking for snprintf
-- Looking for snprintf - found
-- Looking for UnDecorateSymbolName in dbghelp
-- Looking for UnDecorateSymbolName in dbghelp - not found
-- Performing Test HAVE___ATTRIBUTE__
-- Performing Test HAVE___ATTRIBUTE__ - Success
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_DEFAULT
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_DEFAULT - Success
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_HIDDEN
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_HIDDEN - Success
-- Performing Test HAVE___BUILTIN_EXPECT
-- Performing Test HAVE___BUILTIN_EXPECT - Success
-- Performing Test HAVE___SYNC_VAL_COMPARE_AND_SWAP
-- Performing Test HAVE___SYNC_VAL_COMPARE_AND_SWAP - Success
-- Performing Test HAVE_RWLOCK
-- Performing Test HAVE_RWLOCK - Success
-- Performing Test HAVE___DECLSPEC
-- Performing Test HAVE___DECLSPEC - Failed
-- Performing Test STL_NO_NAMESPACE
-- Performing Test STL_NO_NAMESPACE - Failed
-- Performing Test STL_STD_NAMESPACE
-- Performing Test STL_STD_NAMESPACE - Success
-- Performing Test HAVE_USING_OPERATOR
-- Performing Test HAVE_USING_OPERATOR - Success
-- Performing Test HAVE_NAMESPACES
-- Performing Test HAVE_NAMESPACES - Success
-- Performing Test HAVE_GCC_TLS
-- Performing Test HAVE_GCC_TLS - Success
-- Performing Test HAVE_MSVC_TLS
-- Performing Test HAVE_MSVC_TLS - Failed
-- Performing Test HAVE_CXX11_TLS
-- Performing Test HAVE_CXX11_TLS - Failed
-- Performing Test HAVE_ALIGNED_STORAGE
-- Performing Test HAVE_ALIGNED_STORAGE - Failed
-- Performing Test HAVE_CXX11_ATOMIC
-- Performing Test HAVE_CXX11_ATOMIC - Success
-- Performing Test HAVE_CXX11_CONSTEXPR
-- Performing Test HAVE_CXX11_CONSTEXPR - Failed
-- Performing Test HAVE_CXX11_CHRONO
-- Performing Test HAVE_CXX11_CHRONO - Success
-- Performing Test HAVE_CXX11_NULLPTR_T
-- Performing Test HAVE_CXX11_NULLPTR_T - Success
-- Performing Test HAVE_LOCALTIME_R
-- Performing Test HAVE_LOCALTIME_R - Success
-- Performing Test COMPILER_HAS_HIDDEN_VISIBILITY
-- Performing Test COMPILER_HAS_HIDDEN_VISIBILITY - Success
-- Performing Test COMPILER_HAS_HIDDEN_INLINE_VISIBILITY
-- Performing Test COMPILER_HAS_HIDDEN_INLINE_VISIBILITY - Success
-- Performing Test COMPILER_HAS_DEPRECATED_ATTR
-- Performing Test COMPILER_HAS_DEPRECATED_ATTR - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /Users/neo/work/build/glog/build
[ 3%] Building CXX object CMakeFiles/glogbase.dir/src/demangle.cc.o
[ 7%] Building CXX object CMakeFiles/glogbase.dir/src/logging.cc.o
[ 11%] Building CXX object CMakeFiles/glogbase.dir/src/raw_logging.cc.o
[ 14%] Building CXX object CMakeFiles/glogbase.dir/src/symbolize.cc.o
[ 18%] Building CXX object CMakeFiles/glogbase.dir/src/utilities.cc.o
[ 22%] Building CXX object CMakeFiles/glogbase.dir/src/vlog_is_on.cc.o
[ 25%] Building CXX object CMakeFiles/glogbase.dir/src/signalhandler.cc.o
[ 25%] Built target glogbase
[ 29%] Linking CXX static library libglog.a
[ 29%] Built target glog
[ 33%] Linking CXX static library libglogtest.a
[ 33%] Built target glogtest
[ 37%] Building CXX object CMakeFiles/logging_unittest.dir/src/logging_unittest.cc.o
[ 40%] Linking CXX executable logging_unittest
[ 40%] Built target logging_unittest
[ 44%] Building CXX object CMakeFiles/stl_logging_unittest.dir/src/stl_logging_unittest.cc.o
[ 48%] Linking CXX executable stl_logging_unittest
[ 48%] Built target stl_logging_unittest
[ 51%] Building CXX object CMakeFiles/symbolize_unittest.dir/src/symbolize_unittest.cc.o
[ 55%] Linking CXX executable symbolize_unittest
[ 55%] Built target symbolize_unittest
[ 59%] Building CXX object CMakeFiles/demangle_unittest.dir/src/demangle_unittest.cc.o
[ 62%] Linking CXX executable demangle_unittest
[ 62%] Built target demangle_unittest
[ 66%] Building CXX object CMakeFiles/stacktrace_unittest.dir/src/stacktrace_unittest.cc.o
[ 70%] Linking CXX executable stacktrace_unittest
[ 70%] Built target stacktrace_unittest
[ 74%] Building CXX object CMakeFiles/utilities_unittest.dir/src/utilities_unittest.cc.o
[ 77%] Linking CXX executable utilities_unittest
[ 77%] Built target utilities_unittest
[ 81%] Building CXX object CMakeFiles/signalhandler_unittest.dir/src/signalhandler_unittest.cc.o
[ 85%] Linking CXX executable signalhandler_unittest
[ 85%] Built target signalhandler_unittest
[ 88%] Building CXX object CMakeFiles/cleanup_immediately_unittest.dir/src/cleanup_immediately_unittest.cc.o
[ 92%] Linking CXX executable cleanup_immediately_unittest
[ 92%] Built target cleanup_immediately_unittest
[ 96%] Building CXX object CMakeFiles/cleanup_with_prefix_unittest.dir/src/cleanup_with_prefix_unittest.cc.o
[100%] Linking CXX executable cleanup_with_prefix_unittest
[100%] Built target cleanup_with_prefix_unittest
Install the project...
-- Install configuration: """"
-- Installing: /usr/local/lib/libglog.a
-- Installing: /usr/local/include/glog/export.h
-- Installing: /usr/local/include/glog/logging.h
-- Installing: /usr/local/include/glog/raw_logging.h
-- Installing: /usr/local/include/glog/stl_logging.h
-- Installing: /usr/local/include/glog/vlog_is_on.h
-- Installing: /usr/local/include/glog/log_severity.h
-- Installing: /usr/local/include/glog/platform.h
-- Installing: /usr/local/lib/pkgconfig/libglog.pc
-- Installing: /usr/local/lib/cmake/glog/glog-modules.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-config.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-config-version.cmake
-- Old export file ""/usr/local/lib/cmake/glog/glog-targets.cmake"" will be replaced. Removing files [/usr/local/lib/cmake/glog/glog-targets-noconfig.cmake].
-- Installing: /usr/local/lib/cmake/glog/glog-targets.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-targets-noconfig.cmake
```
</details>",0,0,msr
4127,"WXKG11LM Rev 2
Home Assistant 2021.12.5
deCONZ Current version: 6.11.1
FW 26720700
Same Problem, worked before. Cant add Sensor with Phoscon again.",0,0,msr
4129,"## Describe the bug
The Aqara Smart Wireless Switch (WXKG11LM), which was easy to add in previous version, can not be added anymore via Phoscon (see also #5585).
## Steps to reproduce the behavior
1. Go to 'Switches""
2. Click on 'Add Switch'
3. Click on 'Others'
4. Press the reset button of the switch for ~ 5 seconds
In deconz a new node was created, but in Phoscon nothing happens.
## Expected behavior
Pairing should work in Phoscon as it worked in the previous 2.12.06 version
## Screenshots
in Phoscon
![image](https://user-images.githubusercontent.com/38468371/147098979-0671d26a-1c23-4828-86dd-f5481d9f9cfb.png)
in deconz
![image](https://user-images.githubusercontent.com/38468371/147095448-16bd2ddb-f408-4306-9aa9-2b5aefa415d3.png)
![image](https://user-images.githubusercontent.com/38468371/147095473-ac74a48c-7e00-457d-8fe6-c76f4ce8ef21.png)
## Environment
- Host system: Fedora 35 (Raspberry Pi 4)
- Running method: deconz-community/deconz-docker podman container
- Firmware version: (26720700)
- deCONZ version: (2.13.04)
- Device: Conbee 2
- Do you use an USB extension cable: yey
- Is there any other USB or serial devices connected to the host system? If so: Which? - HardDisk
## deCONZ Logs
```
13:02:05:413 send permit join, duration: 65
13:02:19:315 0xFCFC nwk changed to 0xDEEA
13:02:19:316 device announce 0x00158D0006D4DEF5 (0xDEEA) mac capabilities 0x80
13:02:19:317 set fast probe address to 0x00158D0006D4DEF5 (0xDEEA)
13:02:19:317 FP indication 0x0000 / 0x0013 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:317 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:318 device announce 0x00158D0006D4DEF5 (0xDEEA) mac capabilities 0x80
13:02:19:336 DEV found DDF for 0x00158D0006D4DEF5, path: 13:02:19:344 FP indication 0x0104 / 0x0000 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:345 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:345 ZCL attribute report 0x00158D0006D4DEF5 for cluster: 0x0000, ep: 0x01, frame control: 0x18, mfcode: 0x0000 13:02:19:534 FP indication 0x0104 / 0x0000 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:535 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:19:536 Remember Xiaomi special for 0x00158D0006D4DEF5
13:02:19:537 ZCL attribute report 0x00158D0006D4DEF5 for cluster: 0x0000, ep: 0x01, frame control: 0x1C, mfcode: 0x115F 13:02:19:537 0x00158D0006D4DEF5 extract Xiaomi special attribute 0xFF01
13:02:19:538 01 battery 3145 (0x0C49)
13:02:19:539 03 Device temperature 29 °C
13:02:19:539 04 unknown 424 (0x01A8)
13:02:19:540 05 RSSI dB (?) 108 (0x006C)
13:02:19:540 06 LQI (?) 4294967296 (0x0100000000)
13:02:19:541 0a Parent NWK 60247 (0xEB57)
13:02:20:213 [1] get node descriptor for 0x00158d0006d4def5
13:02:20:214 ZDP get node descriptor for 0xDEEA
13:02:24:414 saved node state in 0 ms
13:02:24:415 sync() in 0 ms
13:02:24:889 FP indication 0x0000 / 0x8002 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:24:889 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:24:890 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) cluster 0x8002
13:02:24:890 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) clear timeout on cluster 0x8002
13:02:24:905 [2] get active endpoints for 0x00158d0006d4def5
13:02:24:906 ZDP get active endpoints for 0xDEEA
13:02:25:654 ZCL attribute report 0x00158D000774335F for cluster: 0x0201, ep: 0x01, frame control: 0x1C, mfcode: 0x1037 13:02:28:101 FP indication 0x0000 / 0x8005 (0x00158D0006D4DEF5 / 0xDEEA)
13:02:28:102 ... (0x00158D0006D4DEF5 / 0xDEEA)
13:02:28:102 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) cluster 0x8005
13:02:28:103 ZDP indication search sensors 0x00158D0006D4DEF5 (0xDEEA) clear timeout on cluster 0x8005
13:02:28:115 [4] Skipping additional attribute read - Model starts with 'lumi.'
13:02:31:683 Bind response success for 0x00158d000774335f ep: 0x01 cluster: 0x0001
```",1,0,msr
4131,i need some help right here,0,0,msr
4133,"this
`0 verbose cli [
0 verbose cli 'C:\\Program Files\\nodejs\\node.exe',
0 verbose cli 'C:\\Users\\10\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js',
0 verbose cli 'config',
0 verbose cli 'mvs_version'
0 verbose cli ]
1 info using npm@7.13.0
2 info using node@v17.1.0
3 timing config:load:defaults Completed in 2ms
4 timing config:load:file:C:\Users\10\AppData\Roaming\npm\node_modules\npm\npmrc Completed in 0ms
5 timing config:load:builtin Completed in 1ms
6 timing config:load:cli Completed in 20ms
7 timing config:load:env Completed in 1ms
8 timing config:load:file:C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-3\midori-desktop-v-1.1.5-beta-3\.npmrc Completed in 0ms
9 timing config:load:project Completed in 1ms
10 timing config:load:file:C:\Users\10\.npmrc Completed in 1ms
11 timing config:load:user Completed in 1ms
12 timing config:load:file:C:\Program Files\nodejs\etc\npmrc Completed in 0ms
13 timing config:load:global Completed in 0ms
14 timing config:load:cafile Completed in 1ms
15 timing config:load:validate Completed in 0ms
16 timing config:load:setUserAgent Completed in 1ms
17 timing config:load:setEnvs Completed in 1ms
18 timing config:load Completed in 29ms
19 verbose npm-session 0bdec25efc6d5ac6
20 timing npm:load Completed in 180ms
21 timing command:config Completed in 2ms
22 verbose stack Error: npm config set <key>=<value> [<key>=<value> ...]
22 verbose stack npm config get [<key> [<key> ...]]
22 verbose stack npm config delete <key> [<key> ...]
22 verbose stack npm config list [--json]
22 verbose stack npm config edit
22 verbose stack npm set <key>=<value> [<key>=<value> ...]
22 verbose stack npm get [<key> [<key> ...]]
22 verbose stack
22 verbose stack alias: c
22 verbose stack at UsageError (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\config.js:64:17)
22 verbose stack at config (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\config.js:89:15)
22 verbose stack at cmd (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\config.js:27:27)
22 verbose stack at Object.[_runCmd] (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\npm.js:119:7)
22 verbose stack at fn (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\npm.js:46:40)
22 verbose stack at Object.<anonymous> (C:\Users\10\AppData\Roaming\npm\node_modules\npm\lib\cli.js:58:7)
23 verbose cwd C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-3\midori-desktop-v-1.1.5-beta-3
24 verbose Windows_NT 10.0.19043
25 verbose argv ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\10\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js"" ""config"" ""mvs_version""
26 verbose node v17.1.0
27 verbose npm v7.13.0
28 error code EUSAGE
29 error npm config set <key>=<value> [<key>=<value> ...]
29 error npm config get [<key> [<key> ...]]
29 error npm config delete <key> [<key> ...]
29 error npm config list [--json]
29 error npm config edit
29 error npm set <key>=<value> [<key>=<value> `
is the log of the error of the compilation of midori ""beta-3""",0,0,msr
4134,"see this thing
× Rebuild Failed
An unhandled error occurred inside electron-rebuild
gyp info it worked if it ends with ok
gyp info using node-gyp@6.1.0
gyp info using node@17.1.0 | win32 | x64
gyp info find Python using Python version 3.9.5 found at ""C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe""
gyp http GET https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http 200 https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http GET https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp info find VS using VS2019 (16.11.32002.261) found at:
gyp info find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools""
gyp info find VS run with --verbose for detailed information
(node:9472) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.
(Use \node --trace-deprecation ...` to show where the warning was created)`
gyp info spawn C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe
gyp info spawn args [
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\gyp\\gyp_main.py',
gyp info spawn args 'binding.gyp',
gyp info spawn args '-f',
gyp info spawn args 'msvs',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build\\config.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\addon.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\.electron-gyp\\9.3.5\\include\\node\\common.gypi',
gyp info spawn args '-Dlibrary=shared_library',
gyp info spawn args '-Dvisibility=default',
gyp info spawn args '-Dnode_root_dir=C:\\Users\\10\\.electron-gyp\\9.3.5',
gyp info spawn args '-Dnode_gyp_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp',
gyp info spawn args '-Dnode_lib_file=C:\\\\Users\\\\10\\\\.electron-gyp\\\\9.3.5\\\\<(target_arch)\\\\node.lib',
gyp info spawn args '-Dmodule_root_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar',
gyp info spawn args '-Dnode_engine=v8',
gyp info spawn args '--depth=.',
gyp info spawn args '--no-parallel',
gyp info spawn args '--generator-output',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build',
gyp info spawn args '-Goutput_dir=.'
gyp info spawn args ]
gyp: name 'openssl_fips' is not defined while evaluating condition 'openssl_fips != """"' in binding.gyp while trying to load binding.gypgyp ERR! configure error
gyp ERR! stack Error: \gyp` failed with exit code: 1`
gyp ERR! stack at ChildProcess.onCpExit (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\node-gyp\lib\configure.js:351:16)
gyp ERR! stack at ChildProcess.emit (node:events:390:28)
gyp ERR! stack at Process.ChildProcess._handle.onexit (node:internal/child_process:290:12)
gyp ERR! System Windows_NT 10.0.19043
gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""rebuild"" ""--target=9.3.5"" ""--arch=x64"" ""--dist-url=https://www.electronjs.org/headers"" ""--build-from-source""
gyp ERR! cwd C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\keytar
gyp ERR! node -v v17.1.0
gyp ERR! node-gyp -v v6.1.0
gyp ERR! not ok
Failed with exit code: 1
Error: gyp info it worked if it ends with ok
gyp info using node-gyp@6.1.0
gyp info using node@17.1.0 | win32 | x64
gyp info find Python using Python version 3.9.5 found at ""C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe""
gyp http GET https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http 200 https://www.electronjs.org/headers/v9.3.5/node-v9.3.5-headers.tar.gz
gyp http GET https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http GET https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-x86/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/win-arm64/node.lib
gyp http 200 https://www.electronjs.org/headers/v9.3.5/SHASUMS256.txt
gyp info find VS using VS2019 (16.11.32002.261) found at:
gyp info find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools""
gyp info find VS run with --verbose for detailed information
(node:9472) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.
(Use \node --trace-deprecation ...` to show where the warning was created)`
gyp info spawn C:\Users\10\AppData\Local\Programs\Python\Python39\python.exe
gyp info spawn args [
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\gyp\\gyp_main.py',
gyp info spawn args 'binding.gyp',
gyp info spawn args '-f',
gyp info spawn args 'msvs',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build\\config.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\addon.gypi',
gyp info spawn args '-I',
gyp info spawn args 'C:\\Users\\10\\.electron-gyp\\9.3.5\\include\\node\\common.gypi',
gyp info spawn args '-Dlibrary=shared_library',
gyp info spawn args '-Dvisibility=default',
gyp info spawn args '-Dnode_root_dir=C:\\Users\\10\\.electron-gyp\\9.3.5',
gyp info spawn args '-Dnode_gyp_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp',
gyp info spawn args '-Dnode_lib_file=C:\\\\Users\\\\10\\\\.electron-gyp\\\\9.3.5\\\\<(target_arch)\\\\node.lib',
gyp info spawn args '-Dmodule_root_dir=C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar',
gyp info spawn args '-Dnode_engine=v8',
gyp info spawn args '--depth=.',
gyp info spawn args '--no-parallel',
gyp info spawn args '--generator-output',
gyp info spawn args 'C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\keytar\\build',
gyp info spawn args '-Goutput_dir=.'
gyp info spawn args ]
gyp: name 'openssl_fips' is not defined while evaluating condition 'openssl_fips != """"' in binding.gyp while trying to load binding.gypgyp ERR! configure error
gyp ERR! stack Error: \gyp` failed with exit code: 1`
gyp ERR! stack at ChildProcess.onCpExit (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\node-gyp\lib\configure.js:351:16)
gyp ERR! stack at ChildProcess.emit (node:events:390:28)
gyp ERR! stack at Process.ChildProcess._handle.onexit (node:internal/child_process:290:12)
gyp ERR! System Windows_NT 10.0.19043
gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\10\\Desktop\\midori-desktop-v-1.1.5-beta-2\\midori-desktop-v-1.1.5-beta-2\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""rebuild"" ""--target=9.3.5"" ""--arch=x64"" ""--dist-url=https://www.electronjs.org/headers"" ""--build-from-source""
gyp ERR! cwd C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\keytar
gyp ERR! node -v v17.1.0
gyp ERR! node-gyp -v v6.1.0
gyp ERR! not ok
Failed with exit code: 1
at SafeSubscriber._error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\spawn-rx\lib\src\index.js:267:84)
at SafeSubscriber.__tryOrUnsub (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:205:16)
at SafeSubscriber.error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:156:26)
at Subscriber._error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:92:26)
at Subscriber.error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:72:18)
at MapSubscriber.Subscriber._error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:92:26)
at MapSubscriber.Subscriber.error (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:72:18)
at SafeSubscriber._next (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\spawn-rx\lib\src\index.js:242:65)
at SafeSubscriber.__tryOrUnsub (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:205:16)
at SafeSubscriber.next (C:\Users\10\Desktop\midori-desktop-v-1.1.5-beta-2\midori-desktop-v-1.1.5-beta-2\node_modules\rxjs\internal\Subscriber.js:143:22)",0,0,msr
4138,"Also https://github.com/doitsujin/dxvk/issues/2412 This not are piracy.
Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb
Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content
So you can only go to ff7remake_.exe to taket the log file
https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts
And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.",0,0,msr
4141,"> > What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?
> > ![image](https://user-images.githubusercontent.com/29872453/147506479-a6768826-bbd8-474e-9bd7-5f1bcdcba37b.png) I'm sorry that my English is bad and I can't express it well, just like this post
![image](https://user-images.githubusercontent.com/29872453/147506776-e14819bf-8389-4339-a251-ad07c652c95c.png)",0,0,msr
4152,"DXVK does not leak memory, it reuses its internal allocations as it always has. It's certainly not the best memory mangement in the world but I doubt it has anything to do with the issue at hand.",0,0,msr
4157,"In Final Fantasy VII-Remake Intergrade, if you continue to play, the video memory will increase sharply, and then finally the game will crash.
### Software information
Final Fantasy VII-Remake Intergrade, Material: High Shadow: Low Number of Characters: 8.
### System information
- GPU:3080TI
- Driver:497.29
- Wine version: - DXVK version:1.9.2
### Log files
- d3d9.log:N/A
- d3d11.log:
[ff7remake__d3d11.log](https://github.com/doitsujin/dxvk/files/7771234/ff7remake__d3d11.log)
- dxgi.log:
[ff7remake__dxgi.zip](https://github.com/doitsujin/dxvk/files/7771235/ff7remake__dxgi.zip)",0,0,msr
4163,"> 你们有一个误区：认为自己的想法一定是正确的，然后强加给不支持这种想法的人！
这句话适用于两种人：
1. 支持LGBT+且强制要求所有人支持LGBT+的人
2. 不支持LGBT+且强制要求所有人不支持LGBT+，甚至觉得支持/作为LGBT+就该消失的人
本 org 没有强制任何人支持 LGBTQ+，任何人可以选择在此 repo 署名表达支持，任何人也可以选择直接忽略。同样的，我想一个人走到一个团体中然后大喊这就是个笑话然后被讨厌是正常的。现在，这个 issue 里，是谁在《认为自己的想法一定是正确的，然后强加给不支持这种想法的人》？",0,0,msr
4167,"[Magic comments](https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#magic-comments) disabled by default since v8.23.0. The feature can be a security risk for users who don't know the feature.
If you want to use the feature, please enable it by setting:
```
latex-workshop.latex.build.forceRecipeUsage: false
```
- https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#latex-workshoplatexbuildforceRecipeUsage",0,0,msr
4173,Tracking WAN on a manual loopback device? I've seen before this doesn't work.,0,0,msr
4174,"> Tracking WAN on a manual loopback device? I've seen before this doesn't work.
I have not knowingly configured anything regarding the loopback interface. All I did configure was the VLAN interfaces and that's it.",0,0,msr
4177,"Not sure what is going on, but radvd is empty so maybe no tracking enabled? It should have said in the initial post.",1,0,msr
4181,"Yep, legacy service integration does not use rc.d… we only started using it after forking.",0,0,msr
4182,So what do I need to do to fix this?,0,0,msr
4183,"I’m unsure what the goalpost is. You keep giving no further information on what you actually expect other than radvd starting, which is irrelevant without configuration and ISP considerations.",0,0,msr
4184,"radvd starts and runs, like it did (without changing anything) before updating from 21.7 to 22.1.
My setup is very simple and straightforward. It's using a 6in4 tunnel with static IPs on each VLAN interface.
![Screenshot_4](https://user-images.githubusercontent.com/1129902/148674407-b8e6a79d-f5a8-4593-9c01-2fb7f260023c.jpg)
![Screenshot_5](https://user-images.githubusercontent.com/1129902/148674411-52439f3f-a1a2-4ed0-be1d-aad973435d86.jpg)
```
root@inet-fw:~ # pluginctl -s radvd start
Service `radvd' has been started.
root@inet-fw:~ # ps -auxwww | grep radvd
root 25618 0.0 0.1 12740 2204 0 S+ 03:37 0:00.00 grep radvd
```
I don't see a verbose flag for pluginctl. How do you see what is going on?",0,0,msr
4185,"Radvd only starts when you configure tracking Interfaces, but then you need a prefix lager than /64 anyway on your tunnel. Despite radvd not starting what sort of functionality have you lost from radvd not starting?",0,0,msr
4186,"That makes absolutely no sense. Who broke things?
A network that doesn't work. Even if I didn't care about SLAAC (which I do) RA is required.",1,1,msr
4187,I’m sorry to say this is a waste of both of our time.,0,1,msr
4188,"I had the same issue of radvd not starting up after the update from 21.7.8 to 22.1.
I have multiple interfaces configured with static IPv6 + DHCP6 + managed RA, so in my understanding it definitely needs to be run.
On the old version it was still running, after the update not running anymore.
On the old version, DHCP6 was still working, after the update DHCP6 and therefore IPv6 on these managed networks was completely broken (apart from the statically configured IP6s).
Can't say anything about SLAAC since we don't use it on any network.
Trying to start radvd manually didn't help.
Log entries nowhere to be found unfortunately.
Curiously, `/var/etc/radvd.conf` was completely empty which can't be correct.
I've found a workaround though, which I wanted to share here:
If you go to **Services -> Router Advertisements -> Some Interface** and just click on save, the config will be rewritten and radvd will start up again.
In my case, this fixed the DHCP6 problems.
I did the save thing for any interface that was shown in the RA menu there to be sure.
The fix seems to persist across reboots as well, so I think it only needs to be applied once after the update to fix things.
So I guess this is some kind of problem where the config gets lost during the upgrade and doesn't get rewritten afterwards.
And btw:
> Radvd only starts when you configure tracking Interfaces
Either I'm misunderstanding something or this claim is false, since I don't have any tracking interfaces (or is DHCP6 only managed interface also tracking?) and still radvd is needed, so it needs to be started (and did start in prior versions, and now starts again after applying the workaround).",0,0,msr
4189,"> I’m sorry to say this is a waste of both of our time.
@fichtner: I’m sorry to say this is a waste of my time... and many more people who have broken IPv6 after update, because you released broken version, while this was known issue for a month. Sorry, but I simply don't understand your dismissive attitude towards @brad0. It is not his job to debug and fix the issue.
Anyway, back to the issue. `/var/etc/radvd.conf` is cleared during the update, even tho in the web gui configuration looks ok, one need to change something, save and change back to regenerate `radvd.conf`. And @megmug seems to confirm that. Once you start digging it is easy to fix, but I guess you can agree that desynchronized settings in gui/system is not something that is immediately seen as a solution. And not everyone like @brad0 have to know how to fix that.
> That makes absolutely no sense. Who broke things?
@brad0: They did... You did nothing wrong. Upgrade process broke your settings.",1,1,msr
4190,Closing for heated discussion.,0,0,msr
4191,"I upgraded to 22.1 and radvd no longer starts up.
The web UI mentioned an error..
PHP Warning: Invalid argument supplied for foreach() in /usr/local/www/services_router_advertisements.php on line 334
OPNsense 22.1.b_141-amd64",0,0,msr
4192,"From https://github.com/James-Yu/LaTeX-Workshop/issues/3032#issuecomment-1003920519
We should clearly log the commands and their argument separately, see https://github.com/tamuratak/LaTeX-Workshop/commit/cf8ba0e1894f437974aadd332d53ed6722038e77",0,0,msr
4193,"`child_process` in node deals with this issue by [sorting the variables in lexicographic order first, and then trimming duplicates](https://github.com/nodejs/node/blob/master/lib/child_process.js#L610), if on windows.
`cross-spawn`, which is what is currently being used here, doesn't do that, and seems to always take `Path` over `PATH` if it exists.
The solution is just simply checking if windows, and then just get rid of duplicate entries in a case insensitive manner, while having full control over which path value to take: https://github.com/James-Yu/LaTeX-Workshop/pull/3040/files#diff-84955e6048d64ce6603b6b0039e04152911577b7123438041a8bee7664a3c343R257",0,0,msr
4194,"- Fill out all the information required in [the issue template](https://github.com/James-Yu/LaTeX-Workshop/blob/master/.github/ISSUE_TEMPLATE/bug-report.md). - Please paste the whole log messages here, not parts of ones. It should start with `Initializing LaTeX Workshop`. It is very important to identify problems.
![スクリーンショット 2020-02-14 20 41 25](https://user-images.githubusercontent.com/10665499/74528024-87b2ef80-4f6a-11ea-9c5d-4f9feff25524.png)
When all of the above is done, we will reopen this issue if needed.",0,0,msr
4195,"### LaTeX Workshop Output [Required]
```
[04:09:09] Initializing LaTeX Workshop.
[04:09:09] Extension root: D:\Documents\Visual Studio Code\Extensions\james-yu.latex-workshop-8.23.0
[04:09:09] $PATH: C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\OpenSSH\;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
[04:09:09] $SHELL: undefined
[04:09:09] vscode.env.appName: Visual Studio Code
[04:09:09] vscode.env.remoteName: undefined
[04:09:09] vscode.env.uiKind: 1
[04:09:09] editor.acceptSuggestionOnEnter: ""on""
[04:09:09] latex-workshop.bind.enter.key: true
[04:09:09] latex-workshop.docker.enabled: false
[04:09:09] latex-workshop.docker.image.latex: """"
[04:09:09] latex-workshop.hover.preview.mathjax.extensions: []
[04:09:09] latex-workshop.intellisense.package.enabled: true
[04:09:09] latex-workshop.intellisense.update.aggressive.enabled: false
[04:09:09] latex-workshop.intellisense.update.delay: 1000
[04:09:09] latex-workshop.latex.autoBuild.run: ""onFileChange""
[04:09:09] latex-workshop.latex.outDir: ""%DIR%""
[04:09:09] latex-workshop.latex.recipes: [
{
""name"": ""latexmk 🔃"",
""tools"": [
""latexmk""
]
},
{
""name"": ""latexmk (latexmkrc)"",
""tools"": [
""latexmk_rconly""
]
},
{
""name"": ""latexmk (lualatex)"",
""tools"": [
""lualatexmk""
]
},
{
""name"": ""pdflatex ➞ bibtex ➞ pdflatex × 2"",
""tools"": [
""pdflatex"",
""bibtex"",
""pdflatex"",
""pdflatex""
]
},
{
""name"": ""Compile Rnw files"",
""tools"": [
""rnw2tex"",
""latexmk""
]
},
{
""name"": ""Compile Jnw files"",
""tools"": [
""jnw2tex"",
""latexmk""
]
},
{
""name"": ""tectonic"",
""tools"": [
""tectonic""
]
}
]
[04:09:09] latex-workshop.latex.tools: [
{
""name"": ""latexmk"",
""command"": ""latexmk"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""-pdf"",
""-outdir=%OUTDIR%"",
""%DOC%""
],
""env"": {}
},
{
""name"": ""lualatexmk"",
""command"": ""latexmk"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""-lualatex"",
""-outdir=%OUTDIR%"",
""%DOC%""
],
""env"": {}
},
{
""name"": ""latexmk_rconly"",
""command"": ""latexmk"",
""args"": [
""%DOC%""
],
""env"": {}
},
{
""name"": ""pdflatex"",
""command"": ""pdflatex"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""%DOC%""
],
""env"": {}
},
{
""name"": ""bibtex"",
""command"": ""bibtex"",
""args"": [
""%DOCFILE%""
],
""env"": {}
},
{
""name"": ""rnw2tex"",
""command"": ""Rscript"",
""args"": [
""-e"",
""knitr::opts_knit$set(concordance = TRUE); knitr::knit('%DOCFILE_EXT%')""
],
""env"": {}
},
{
""name"": ""jnw2tex"",
""command"": ""julia"",
""args"": [
""-e"",
""using Weave; weave(\""%DOC_EXT%\"", doctype=\""tex\"")""
],
""env"": {}
},
{
""name"": ""jnw2texmintex"",
""command"": ""julia"",
""args"": [
""-e"",
""using Weave; weave(\""%DOC_EXT%\"", doctype=\""texminted\"")""
],
""env"": {}
},
{
""name"": ""tectonic"",
""command"": ""tectonic"",
""args"": [
""--synctex"",
""--keep-logs"",
""%DOC%.tex""
],
""env"": {}
}
]
[04:09:09] latex-workshop.viewer.pdf.internal.keyboardEvent: ""auto""
[04:09:09] Creating a new file watcher.
[04:09:09] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:09:09] Creating PDF file watcher.
[04:09:09] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{}}
[04:09:09] Creating Bib file watcher.
[04:09:09] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:09:09] Set $LATEXWORKSHOP_DOCKER_LATEX: """"
[04:09:09] Cannot run pdflatex to determine if we are using MiKTeX
[04:09:09] [Server] Creating LaTeX Workshop http and websocket server.
[04:09:09] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:09:09] LaTeX Workshop initialized.
```
### Developer Tools Console [Required]
```
(node:15712) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.(Use `Code --trace-deprecation ...` to show where the warning was created)
```
## Additional questions
### Are you using VSCodium?
No
### Are you using the Snap or Flatpack versions of VS Code?
No
### Are you using LaTeX Workshop with VS Code Remote?
No",1,0,msr
4196,"Please paste the whole log messages when compiling LaTeX files with your recipe:
```
""latex-workshop.latex.tools"": [
{
""command"": ""latexmk"",
...
""env"": {
""PATH"": ""foo""
}
}
```",1,0,msr
4197,"> Please paste the whole log messages when compiling LaTeX files with your recipe:
> > ```
> ""latex-workshop.latex.tools"": [
> {
> ""command"": ""latexmk"",
> ...
> ""env"": {
> ""PATH"": ""foo""
> }
> }
> ```
```[04:19:58] Initializing LaTeX Workshop.
[04:19:58] Extension root: D:\Documents\Visual Studio Code\Extensions\james-yu.latex-workshop-8.23.0
[04:19:58] $PATH: C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\OpenSSH\;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
[04:19:58] $SHELL: undefined
[04:19:58] vscode.env.appName: Visual Studio Code
[04:19:58] vscode.env.remoteName: undefined
[04:19:58] vscode.env.uiKind: 1
[04:19:58] editor.acceptSuggestionOnEnter: ""on""
[04:19:58] latex-workshop.bind.enter.key: true
[04:19:58] latex-workshop.docker.enabled: false
[04:19:58] latex-workshop.docker.image.latex: """"
[04:19:58] latex-workshop.hover.preview.mathjax.extensions: []
[04:19:58] latex-workshop.intellisense.package.enabled: true
[04:19:58] latex-workshop.intellisense.update.aggressive.enabled: false
[04:19:58] latex-workshop.intellisense.update.delay: 1000
[04:19:58] latex-workshop.latex.autoBuild.run: ""onFileChange""
[04:19:58] latex-workshop.latex.outDir: ""%DIR%""
[04:19:58] latex-workshop.latex.recipes: [
{
""name"": ""latexmk 🔃"",
""tools"": [
""latexmk""
]
},
{
""name"": ""latexmk (latexmkrc)"",
""tools"": [
""latexmk_rconly""
]
},
{
""name"": ""latexmk (lualatex)"",
""tools"": [
""lualatexmk""
]
},
{
""name"": ""pdflatex ➞ bibtex ➞ pdflatex × 2"",
""tools"": [
""pdflatex"",
""bibtex"",
""pdflatex"",
""pdflatex""
]
},
{
""name"": ""Compile Rnw files"",
""tools"": [
""rnw2tex"",
""latexmk""
]
},
{
""name"": ""Compile Jnw files"",
""tools"": [
""jnw2tex"",
""latexmk""
]
},
{
""name"": ""tectonic"",
""tools"": [
""tectonic""
]
}
]
[04:19:58] latex-workshop.latex.tools: [
{
""name"": ""latexmk"",
""command"": ""latexmk"",
""args"": [
""-synctex=1"",
""-interaction=nonstopmode"",
""-file-line-error"",
""-pdf"",
""-outdir=%OUTDIR%"",
""%DOC%""
],
""env"": {
""PATH"": ""foo""
}
}
]
[04:19:58] latex-workshop.viewer.pdf.internal.keyboardEvent: ""auto""
[04:19:58] Creating a new file watcher.
[04:19:58] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:19:58] Creating PDF file watcher.
[04:19:58] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{}}
[04:19:58] Creating Bib file watcher.
[04:19:58] watcherOptions: {""useFsEvents"":false,""usePolling"":false,""interval"":300,""binaryInterval"":1000,""awaitWriteFinish"":{""stabilityThreshold"":250}}
[04:19:58] Set $LATEXWORKSHOP_DOCKER_LATEX: """"
[04:19:59] Cannot run pdflatex to determine if we are using MiKTeX
[04:19:59] [Server] Creating LaTeX Workshop http and websocket server.
[04:19:59] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:19:59] LaTeX Workshop initialized.
[04:19:59] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:19:59] Trigger characters for intellisense of LaTeX documents: [""\\"","","",""{""]
[04:19:59] Bibtex format config: {""tab"":"" "",""case"":""lowercase"",""left"":""{"",""right"":""}"",""trailingComma"":false,""sort"":[""key""],""alignOnEqual"":true,""sortFields"":false,""fieldsOrder"":[],""firstEntries"":[""string"",""xdata""]}
[04:19:59] Current workspace folders: undefined
[04:19:59] Current workspaceRootDir: [04:19:59] Found root file from active editor: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Root file changed: from undefined to d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Start to find all dependencies.
[04:19:59] Root file languageId: latex
[04:19:59] [Server] Server successfully started: {""address"":""127.0.0.1"",""family"":""IPv4"",""port"":57241}
[04:19:59] Reset file watcher.
[04:19:59] Parsing a file and its subfiles: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Parse fls file.
[04:19:59] Fls file found: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.fls
[04:19:59] Parse aux file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.aux
[04:19:59] Linter for root file started.
[04:19:59] Linter for root file running command D:\Applications\TeXLive\2021\bin\win32\chktex.exe with arguments [""-wall"",""-n22"",""-n30"",""-e16"",""-q"",""-f%f:%l:%c:%d:%k:%n:%m\n"",""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:19:59] Added to file watcher: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:19:59] Snippet data loaded.
[04:19:59] Checking for duplicate labels: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex.
[04:19:59] Linter for root file successfully finished in 0s 208ms
[04:19:59] The .chktexrc file not found.
[04:19:59] Linter log parsed with 0 messages.
[04:20:01] Manager.fileWatcher.getWatched: {""d:\\Documents\\Visual Studio Code\\Projects\\temp"":[""Untitled-1.tex""]}
[04:20:01] Manager.filesWatched: [""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:20:01] BibWatcher.bibWatcher.getWatched: {}
[04:20:01] BibWatcher.bibsWatched: []
[04:20:01] PdfWatcher.pdfWatcher.getWatched: {}
[04:20:01] PdfWatcher.pdfsWatched: []
[04:20:02] Current workspace folders: undefined
[04:20:02] Current workspaceRootDir: [04:20:02] Found root file from active editor: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:02] Keep using the same root file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:02] Linter for root file started.
[04:20:02] Linter for root file running command D:\Applications\TeXLive\2021\bin\win32\chktex.exe with arguments [""-wall"",""-n22"",""-n30"",""-e16"",""-q"",""-f%f:%l:%c:%d:%k:%n:%m\n"",""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:20:02] Linter for root file successfully finished in 0s 225ms
[04:20:02] The .chktexrc file not found.
[04:20:02] Linter log parsed with 0 messages.
[04:20:03] BUILD command invoked.
[04:20:03] The document of the active editor: file:///d:/Documents/Visual Studio Code/Projects/temp/Untitled-1.tex
[04:20:03] The languageId of the document: latex
[04:20:03] Current workspace folders: undefined
[04:20:03] Current workspaceRootDir: [04:20:03] Found root file from active editor: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] Keep using the same root file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] Building root file: d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] Build root file d:\Documents\Visual Studio Code\Projects\temp\Untitled-1.tex
[04:20:03] outDir: d:/Documents/Visual Studio Code/Projects/temp
[04:20:03] Recipe step 1: latexmk, -synctex=1,-interaction=nonstopmode,-file-line-error,-pdf,-outdir=d:/Documents/Visual Studio Code/Projects/temp,d:/Documents/Visual Studio Code/Projects/temp/Untitled-1
[04:20:03] Recipe step env: {""PATH"":""foo""}
[04:20:03] cwd: d:\Documents\Visual Studio Code\Projects\temp
[04:20:03] LaTeX build process spawned. PID: 5308.
[04:20:03] LaTeX fatal error: spawn latexmk ENOENT, 'latexmk' is not recognized as an internal or external command,
operable program or batch file.
. PID: 5308.
[04:20:03] Does the executable exist? $PATH: foo
[04:20:03] Does the executable exist? $Path: C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\OpenSSH\;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
[04:20:03] The environment variable $SHELL: undefined
[04:20:04] Manager.fileWatcher.getWatched: {""d:\\Documents\\Visual Studio Code\\Projects\\temp"":[""Untitled-1.tex""]}
[04:20:04] Manager.filesWatched: [""d:\\Documents\\Visual Studio Code\\Projects\\temp\\Untitled-1.tex""]
[04:20:04] BibWatcher.bibWatcher.getWatched: {}
[04:20:04] BibWatcher.bibsWatched: []
[04:20:04] PdfWatcher.pdfWatcher.getWatched: {}
[04:20:04] PdfWatcher.pdfsWatched: []
```",1,0,msr
4198,"`$PATH` is set to `foo`, and `$Path` remains the same environment variable, which is used in the `cs.spawn()` as I mentioned.
Also, @tamuratak may I know why #3040 is closed? If it's really that `${env:var}` that's the problem, I'll just remove that. I'm replying here since I cannot comment on that pull request since you closed it.",1,0,msr
4199,"Windows users might have to use `""Path""` instead of `""PATH""` on Windows to override the PATH environment variable since, on Windows, the PATH environment variable is typically set as `Path`.
- https://www.computerhope.com/issues/ch000549.htm
I have added a notice to wiki.",0,0,msr
4200,"## Preliminary questions [Required]
yes all
## Describe the bug [Required]
The `env` object in an entry of the configuration `latex-workshop.latex.tools` will not allow case insensitivity on windows, and leads to confusion on the end user.
Set the configuration
```js
""latex-workshop.latex.tools"": [
{
""command"": ""latexmk"",
...
""env"": {
""PATH"": ""foo""
}
}, ...
```
and build with `latexmk`.
- Expected behavior: `""foo""` is used to run `foo/latexmk`
- Actual behavior: `process.env.Path` is used instead
## Logs [Required]
Irrelevant. It's a problem with `process.env` and `cs.spawn()` on Windows.
## Desktop [Required]
- OS: Windows 10
- VS Code version: 1.63.2
- Extension version: 8.23",1,0,msr
4201,"Hi, ScreenToGif was never truly portable.
Older versions required .NET Framework 4.8, which was installed by default.
In order for it to be truly portable, the download size will go up to 78MB.
I could add that download option in future releases.",1,0,msr
4202,@NickeManarin That would be cool. I don't care much about portability - but bundling ScreenToGif with the needed .NET6 files insetad of requiring a systemwide .NET6 install would be very welcome. Other popular software (such as paint.net) already go this way.,0,0,msr
4203,"I am here to second that motion. Was quite surprised to see that I needed to install some additional .net software in order to use ScreenToGif. This is soo sad.
From a developer point of view, WHY do you need the newer .Net framework - What part of it, is it that you need ?",0,1,msr
4204,"Right. Let's use the right terms here, it's not sad, it's just inconvenient for you. There was always the requirement to have installed .NET Framework, you just already had it installed.
I can make it more convenient, but the file size will go to 140MB (78MB compressed) and the update system won't work the same way that it works now.
Also, I don't need to explain myself as to why I did something with my project (but it's easy to guess why or easy to search for .NET Framework to .NET 6 comparisons online).",0,1,msr
4205,"Next version (v2.36) will be distributed with a Full and Light package types.
The updater will know how to handle each installation mode.
Also, I'll distribute the app as an msix package.",0,0,msr
4206,"Hello,
I wanted to upgrade **ScreenToGif** (my favorite GIF recorder) but now it requires **NET**. I don't have admin rights on my laptop so I'm using **ScreenToGif** portable, but **NET** requires admin rights. That is sad.",0,0,msr
4207,Workaround described here: https://github.com/electron-userland/electron-forge/issues/2665#issuecomment-1008225325,0,0,msr
4208,This is a massive screw-up :(,0,1,msr
4209,":tada: This issue has been resolved in version 3.2.6 :tada:
The release is available on:
- [npm package (@latest dist-tag)](https://www.npmjs.com/package/electron-rebuild)
- [GitHub release](https://github.com/electron/electron-rebuild/releases/tag/v3.2.6)
Your **[semantic-release](https://github.com/semantic-release/semantic-release)** bot :package::rocket:",0,0,msr
4210,"I am using the electron 13.0.0 and node.js v14.8.2
the terminal is zsh, font is SauceCodePro Nerd.
![image](https://user-images.githubusercontent.com/47300853/148669010-1f3928f1-840e-42ef-965e-ddeadade2aae.png)",0,0,msr
4211,"I have confirmed that it was fixed:
Version: 1.64.0-insider (Universal)
Commit: 23664355e61f3fb614932a5cb16613b4d987cfb7
Date: 2022-01-21T13:27:12.081Z
Electron: 13.5.2
Chromium: 91.0.4472.164
Node.js: 14.16.0
V8: 9.1.269.39-electron.0
OS: Darwin arm64 20.6.0",0,0,msr
4212,"When the internal PDF viewer for the current document is not visible, executing forward SyncTeX moves the panel into the current editor group.
https://user-images.githubusercontent.com/10665499/148715374-f51335be-8074-41de-9966-394d60013588.mov
`panel.reveal` doesn't work well. Related to microsoft/vscode/issues/138914",1,1,msr
4213,"I would think it would, in order to maintain SOLID code. Also, as several other people pointed out previously, we are not always immune to managers etc. making us code to interfaces, do gratuitous TDD and automated-unit-test writing etc/mocking, and/or use IoC containers. Bosses like to make software engineers do things ""because I heard about it at a conference,"" or ""the insurance is making us,"" etc.
The interface just supporting the two methods is, in principle, okay -- just make sure that the static extension methods refer to the object through the interface and not the actual class.",0,0,msr
4214,"Brian, I understand the argument about managers, although in my 30 years of engineering I never encountered a single manager who was telling me how to write code... I mean, if you write your code against an interface it's one thing. But how some managers can tell a third-party library how to write _their_ code? `HttpClient` doesn't have any interfaces implemented and it seems to be fine, no?
Automated testing and mocking HTTP requests are totally possible with RestSharp in its current shape. I have shown an option in the docs.
I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).
The SOLID argument is not solid :) What do you mean by that, could you elaborate?
Then again, I am seriously not against adding that small interface. However, I would not be happy to add `IRestRequest` and `IRestResponse` back as those are glorified property bags.",0,0,msr
4215,"A typical method in a wrapper service for my HTTP calls does the following
1. Take some inputs to form a request
2. Execute
3. Handle response (maybe retry, return data)
If I can mock `IRestClient`, then I can verify the contents of the `RestRequest` given to it by my method. That lets me avoid splitting 1 into a separate method just for testing that I correctly crafted the web request (which I want because string formatting is tricky).
If I can mock `IRestClient`, then I can setup responses for 200, 404, 401, etc and don't need to split out 3 either.
Each method is already short enough that I _don't want_ to split it up just for test coverage. Example:
```csharp
public async Task<List<Something>> GetData(DateTimeOffset startEpoch, DateTimeOffset endEpoch)
{
var request = new RestRequest(""/url"", Method.GET);
await AddBearerToken(request);// other method which might make a request to authenticate or use cached token
request.AddQueryParameter(""start"", startEpoch.ToUnixTimeSeconds().ToString());
request.AddQueryParameter(""end"", endEpoch.ToUnixTimeSeconds().ToString());
var result = await _client.ExecuteAsync<ApiResponseModel>(request);
if (result.StatusCode != HttpStatusCode.OK)
throw new Exception($""Failed to fetch data. Status code: \""{(int)result.StatusCode}\"", message: \""{result.StatusDescription}\"""");
return result.Data.ThingICareAbout;
}
```",0,0,msr
4216,"@fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.",0,0,msr
4217,"> I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).
Can you tell a little bit more about your plans on DI support? Perhaps a word on DI in the docs could clarify many questions before the've to be asked.",0,0,msr
4218,"The DI registration heavily depends if you want to follow the same pattern as Microsoft does, using `IHttpClientFactory`, registering the client as a transient dependency.
For many use cases it's enough to register a typed client as a singleton:
```csharp
services.AddSingleton<IMyApiClient>(new MyApiClient(new RestClient(options)));
```
If I'd to provide some use of `IHttpClientFactory` registration extensions, I'd need to know if people want to configure the `HttpClient` instance used by `RestClient` on the outside. In this case, lots of options that RestSharp uses to configure the message handler won't be applicable.",0,0,msr
4219,"> @fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.
Thanks! I didn't know about that library. I've been using [Moq.Contrib.HttpClient](https://github.com/maxkagamine/Moq.Contrib.HttpClient) in places we used HttpClient rather than RestSharp.
This library using HttpClient is an implementation detail that I don't think library users should have to think about, unless they have specific requirements making the need for a custom HttpClient necessary. Having the interface makes testing easier for all users without having to add extra dependencies.",0,0,msr
4220,"The library you mentioned would work as well. As long as you can add a delegating handler, it will be possible to test RestSharp calls. All you need to do is to compose or override the message handler used by the wrapped HttpClient:
```csharp
var client = new RestClient(...) { ConfigureMessageHandler = _ => mockedHandler };
```
Basically, one of the benefits of moving to HttpClient (besides that it's obvious and `HttpWebRequest` is heavy legacy) is the ability to use delegating handlers and build message processing pipelines.
The point about testing real requests is obvious to me. If you look at StackOverflow questions about RestSharp, most of the issues are caused by incorrectly added parameters, content type overrides and other weird things. But, if people would test against a RestShar interface, they would have all their tests passing. But it won't work anyway, because the request is just not correctly formed. It's a great benefit to at least inspect the actual request and ensure that it looks exactly as it should, rather than inspecting the parameters collection on a `RestRequest` instance. The latter gives you zero idea how the actual request would look like.",1,0,msr
4221,This is not necessary if the existing RestClient methods are marked virtual. This will allow for mocking overrides of the public methods instead of needing a full interface. One of the two is necessary as the handler system is not able to simulate invalid http responses. We switched from Http client to RestSharp because RestSharp had an interface that allowed us to simulate the result of those responses as well as other error conditions not simulatable through the mock handler. Some of these result in StatusCode 0 or exceptions being thrown which require an interface or virtual method.,0,0,msr
4222,@sspates-starbucks why can't you simulate the error response with a testing handler like `MockHttp`? It provides a behaviour that is much closer to reality. It can return anything you want based on its configuration.,0,0,msr
4223,@maor-rosenfeld thanks for your extensive and elaborative feedback for this issue.,0,0,msr
4224,"If we keep discussing testing, maybe it's a good idea to post some test samples and check how these tests can be refactored. So far, the discussion goes quite theoretical imo.",0,0,msr
4225,"@alexeyzimarev we can argue all day about use cases of using the interface for tests, extension methods, tests on these extension methods and other reasons of why suddenly removing all interfaces from an existing infrastructure package is a bad idea, but at this point it's clear that you're here to tell us how to write code. I'm sorry if I choose not to align with your views, but at this point upgrading to the latest version of RestSharp is so painful that there's really no justification to even try. On a positive note, I'm completely with you regarding the separation of services and settings objects. I like `RestClientOptions`.",0,0,msr
4226,@maor-rosenfeld you definitely know how to motivate OSS maintainers to keep doing their community work for free.,0,1,msr
4227,"I've come here from a breaking scenario that I'm not sure I can fix other than backing out the upgrade to 107 due to missing IRestClient.
I updated one of my libraries to use 107. Why not right? I'm trying to be proactive with keeping libs up to date. It all seemed to work OK until some tests starting failing. This is because we use a third-party library and they reference IRestClient and v106 of the library. I could ask them to update but I don't see that they would prioritise that and it would then force all of their customers to update to v107 as well which might be a big deal. If I use an assembly redirect to 107, it won't work because it won't find IRestClient and if I use 106, my code won't work because the method signatures changed.
I'm not sure what I would have done differently but perhaps something as simple a a different nuget package with different namespaces and mark the old one as deprecated. Then we can run them side-by-side until everyone is on 107?
Not sure. Any suggestions?",0,0,msr
4228,"@lukos I don't think it would work in any scenario, interfaces or not. RestSharp v107 internals are completely different due to migration to `HttpClient` and ditching `HttpWebRequest`. It made it impossible to keep the API backwards compatible, and even with interfaces, the API scope of those interfaces won't ever be compatible. Lots of properties of `RestClient` and `RestRequest` are moved to `RestClientOptions` and there's no way to keep them where those properties were before.",0,0,msr
4229,"We are running into cases now where one library requires the interface and another requires the v107 structure, we can't upgrade either one due to the lack of backwards compatibility with IRestClient. Adding a compatibility wrapper that implements the interfaces might be a way to handle this better rather than implementing the interface on the current client.",0,0,msr
4230,"@sspates-starbucks Even if there's something to make things build-time compatible, these won't be compatible at runtime anyway. As I mentioned, most of the properties of those (`IRestClient` and `IRestRequest`) interfaces were (by necessity) moved to `RestClientOptions`, and it's something that simply cannot be undone due to the configuration of `HttpMessageHandler`.
I can only suggest opening issues for those libraries that still use RestSharp 106 and helping them with the migration. It's not a lot of work honestly.",0,0,msr
4231,"The new class having internal only options generates problems for dependency injection. I set up my DI container to hand out RestClient, but different consumers need different options. There is now now way to set the options after having an instantiated Client.",0,0,msr
4232,"@Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:
```csharp
services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
```
But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.",0,0,msr
4233,"> @Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:
> > ```cs
> services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
> ```
> > But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.
Is there a reason why (some) options can't be set after the constructor? Its a HUGE code change to say go make a new interface and options class for every location that is going to use restsharp, vs just setting the timeout and baseurl after the DI injection is done",0,0,msr
4234,"Every man and his dog has an opinion on software design/standards and the debates will rage on forever - they're usually just different mindsets/approaches. Those mindsets aren't even usually the mindset of the developer using your library, they're probably dictated by the organisation they work for and having a conversation to change those standards is either way above their pay grade or a battle that's not worth fighting. The problem here isn't one of who is right or wrong, it's one of practicality. The removal of the interfaces causes the amount of work some consumers have to do to significantly increase. Most devs really just need to deliver the work they had promised within the timeframe promised. I'm a huge fan of MockHttp but in my case, updating over 1000 tests to use it (which were written 5 years ago) is a huge amount of work. No matter how you plan your pipeline, the sizing of the work item is now completely invalid. The amount of work required to update your library has become inhibitive to doing so - it will either have to be picked up as a separate work item or it just won't ever get updated.
Irrespective of whether consumers are testing the right thing, using IoC correctly or just plain writing crap code, they no longer have the flexibility to choose like they used to. I think most devs would agree that the code changes make sense but at the end of the day we all need to deliver functionality to a business and I guarantee that those business users really don't care how well the code is written as long as it reliably does what they need. The best development tools out there make our lives easier. That's why we use them - they abstract us away from the detail/hard work we really don't need to care about and they provide flexibility to customise how we utilise them. They help us do more, faster. Removing these interfaces does the opposite of that, irrespective of whether it's ""better"" code or not.",1,1,msr
4235,"What everyone is saying is correct. Of course, we appreciate Alexey for making this library but the changes are very breaking.
It seems like the best way out of it would be to create a separate library with a different name/package name (and importantly, different namespaces) to contain your newer/better way of doing things and which should be used by people going forwards. Any serious bugs found in the old library will probably be fixed but otherwise it won't get any new features.
This way, we can have both packages alongside each other so if we can update our own code for the new library we can do that without forcing all of our dependencies to update all of their code at the same time. If they eventually do, then we can eventually uninstal the old package.
Not sure if you think that sounds fair or not?",0,0,msr
4236,"The question is why is there such a strong need to migrate to v107 if v106 works fine for most? Migration to `HttpClient` is a big change in itself, the `IRestRequest` (for example) is impossible to shape to the same form as it was before anyway, as well as `IRestClient` interface API surface cannot be recovered.",0,0,msr
4237,"Why update? People want the [stated benefits](https://restsharp.dev/v107/#presumably-solved-issues) that this update (_and future updates_) will bring. As a consumer, I shouldn't have to care what implementations are used internally as long as it works. This is a debate about the impact of the removal of interfaces and the consequences that has for those who use the library. (The fact that I'm even aware of what components are used internally screams leaky abstraction to me but that's just my opinion and somewhat off topic)
Changing the method signatures is something that's not overly complex to deal with (the compiler will help identify what needs changing and, in our case, this library is abstracted away from our code anyway) however, removing all the interfaces entirely fundamentally changes how to set up and work with the library and how easy/difficult that is given what's gone before. The work you've done is great - we're just providing feedback of the real world issues it causing us. Rightly or wrongly, people consume this library is a myriad of ways and for us, the migration path is so large that it's currently a barrier to entry.",0,1,msr
4238,"I don't know why, but my message is not getting through. I will try again:
- The API surface of `IRestClient` and `IRestRequest` is impossible to hold intact after the migration
- Removal of interfaces made the maintenance work much easier.",0,0,msr
4239,"*** EDITED ***
I really don't want to get bogged down in specifics as I don't think it's helpful
> The question now is if it makes sense to introduce an interface with those two functions?
For us, yes it does. There are probably many others who will find that interface useful too for many different reasons. Adding them in provides flexibility to choose and reduces the impact of the changes for many people especially given that this library has been around for such a long time. This isn't about what code/approach is better, it's about what's practical and helpful for existing consumers.",0,0,msr
4240,"Thanks very helpful @alexsaare, thank you.
The scope of refactoring, however, would be significant, both for me (all the extensions need to change) and for those who use the previous version of the library.
I am not against it, that's why I opened this issue. I still, though, wait for some sample test code, so I can understand the need better.",0,0,msr
4241,"As I mentioned before, I am trying to understand how people use interfaces in tests. Tests aren't PI, nor IP. Please, share your tests.",0,0,msr
4242,"To answer some questions @alexeyzimarev posed above : 1) Why upgrade if we don't need to? Because corporate policy requires us to be on recent versions
2) Though I personally agree with some of your motivations and reasoning regarding testing and interfaces, corporate policy also often doesn't care if a test provides value or not
But my biggest problem is the change to DI because of options. I have an app that connects to 10 different services. Currently I have them all getting an IRestClient via DI, set the options they need in their consumer's constructors, and move on.
You are asking me to create 10 new interfaces, move all that rest configuration logic into the DI configuration, change the signature of every consumer of the rest client etc. Further, if two services can currently share configuration they can share the injected interface. But if one of them needs a different bit of configuration in the future, now I have to create a new interface for that DI again, and change the signatures again, just to have a different timeout or base url. I can inject a vanilla RestClient instead of IRestClient just fine. But not being able to set options on it after I get it, is a deal breaker.",0,1,msr
4243,"@Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.",0,0,msr
4244,"@Terebi42 as per the latest version requirement, I don't think it is even feasible, although I am not aware of your particular case. Say, you have .NET 6 released in November, does the policy mean you have to upgrade all your applications to .NET 6, with all the breaking changes fixed? I never had such experience in any company.",0,1,msr
4245,"> @Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.
Why were the options moved from the request to the client? It was very convenient to be able to set those parameters differently per request. And if they do need to be on the client, why are they now read only? Being able to set them at the time of call would also be very convenient. A change like this is literally asking for dozens of new classes to be created to support the new config paradigm and DI. If we could even pass in a different options class per request, that would be great.",0,0,msr
4246,"Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.",0,0,msr
4247,"> Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.
hrm, well, at least I understand. Perhaps Ill make a restclient factory that I DI, and pass it options to get the old pattern back",0,0,msr
4248,@Terebi42 I opened an issue for that https://github.com/restsharp/RestSharp/issues/1791,0,0,msr
4249,"> * All the sync methods were removed. Those who need it can make extensions, wrapping up async calls with `.GetAwaiter().GetResult()`
Can you update this ticket to remove this bit? .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks. You need to use an async helper like the one in Rebus for this to work correctly in sync code:
https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs",1,0,msr
4250,"BTW, if you did bring back IRestClient, the only function that matters is all variations of ExecuteAsync(). There are good valid reasons to want to mock ExecuteAsync() to avoid an actual call as its high level enough to mock away a significant amount of lower level stuff. But it's also low level enough that I usually don't bother ;). Usually we mock at the level above that, which is our REST API client contract. After all the point of mocking is to be able to stub out lower level stuff as a black box and assume that black box works correctly. That assumption works as then you expect something else to validate that the actual client itself is properly tested (integration test etc).
And there is a good argument to be had that writing tests to ensure the client itself is actually working correctly is much better being done by mocking HttpClient as suggested. Something that was almost impossible to do with earlier versions of RestSharp, but is now much easier to do now it relies on HttpClient internally so you can swap it out for mocking purposes.",0,0,msr
4251,"> the only function that matters is all variations of ExecuteAsync().
The point here is that there's only one :) All other overloads are extensions. So, if the interface is back, all the extensions need to be on `this IRestClient` instead.
> .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks
I found this via SO: https://github.com/aspnet/AspNetIdentity/blob/main/src/Microsoft.AspNet.Identity.Core/AsyncHelper.cs",0,0,msr
4252,"I believe that works because it uses a separate task, so it's a similar pattern achieved in a different way to how Rebus did it. I am not sure which one is more efficient. We use Rebus in our code and I stole their code to solve the deadlock issues when we had it, and it worked. I have also solved in the past simply by running the code in a separate background thread similar to the AspNet approach and that works also. But I think it's higher overhead than the approach taken by Rebus and other libraries?
I am sure either approach works, but I know that just using GetAwaiter().GetResult() on an async function will cause deadlocks in web apps (or Windows Forms apps). We had tons of hung web requests when I was doing it without the task wrapper.",0,0,msr
4253,Updated,0,0,msr
4254,"Been a while since I looked and and debugged the Rebus version, but I did step through it with a debugger to understand it back then. I am pretty sure it's much lower overhead as it never triggers a separate background thread, so puts no extra pressure on the thread pool. Rather it swaps out the SynchronizationContext so that when the task returns, it comes back on something other than the original sync context which is where you get the deadlocks.",0,0,msr
4255,"I don't think there's an explicit new thread when using the task factory. I think it will use the available IO thread from the pool. Synchronization context, on the other hand, produces some overhead. I believe that doing something like `.ConfigureAwait(false).GetAwaiter().GetResult()` would work, but not in WinForms (for example). I also know that ASP.NET Core behaves differently compared with .NET Framework when it comes to `GetAwaiter().GetResult()`",0,0,msr
4256,"Yes, its possible it would work differently in ASP.NET Core. Porting our code to that is a massive undertaking which is still a work in progress :( But we also use RestSharp in WinForms apps (EasyPost and ShipEngine) so the AsyncHelper approach is what has worked for us.
I guess the only way to tell would be to benchmark the two approaches and see which is faster.",0,0,msr
4257,"Version 107 got several interfaces removed, which is one of the major breaking changes in that version.
It seems most of the interfaces were okay to remove, except `IRestClient`. My arguments are [described](https://restsharp.dev/v107/#motivation) in the docs.
This issue is a place where we should have a civilised discussion about it. I will not follow up on the interface issue anywhere else, except here.
Some details about the current state of the `RestClient` API signature.
- All the sync methods were removed. Those who need it can make extensions, wrapping up async calls in some [async helper](https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs).
- Most of the overloads for making requests are now extensions, as they just call each other, so they would never be part of any interface anyway.
- Most of the options that were previously on `IRestClient` and `IRestRequest` are now in `RestClientOptions`, which is a property bag and, therefore, won't have any interface
So, what's there in stock for `IRestClient`? Basically, it boils down to this signature:
```csharp
namespace RestSharp; public interface IRestClient {
RestClient AddDefaultParameter(Parameter parameter);
Task<RestResponse> ExecuteAsync(RestRequest request, CancellationToken cancellationToken = default);
}
```
The question now is if it makes sense to introduce an interface with those two functions? Please comment.",0,0,msr
4258,This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,0,0,msr
4259,⚠️ This issue has been marked wontfix and will be closed in 3 days,0,0,msr
4260,"From the explanation page;
> The best way to test HTTP calls is to make some, using the actual service you call. However, you might still want to check if your API client forms requests in a certain way. I have to disagree with this. I don't think calling actual services from unit tests is good practice. At that point they're functional tests. next line goes into that;
> As RestSharp uses HttpClient internally, it certainly uses HttpMessageHandler. Features like delegating handlers allow you to intersect the request pipeline, inspect the request, and substitute the response. You can do it yourself, or use a library like [MockHttp](https://github.com/richardszalay/mockhttp)
this requires me to actually know how you're making your calls inside your library that you publish to nuget. The main reason people wants to use external libraries is to not be concerned about the internals of that external code. It also assumes I know to use this MockHttp library (had no idea it existed, I didnt need to since I've used RestSharp to handle all ""HttpClient"" related things so far), with the assumption that, that package will also be maintained properly and will not be making breaking changes.
> Mocking an infrastructure component like RestSharp (or HttpClient) is not the best idea.
This statement is also wrong in my opinion. I think the opposite actually, making us test external library code in our _unit_ tests implicitly makes them not unit tests, because we are always testing more than a small ""unit"" of code, in addition to our code change, now we're testing your code.
> As RestSharp uses HttpClient internally
What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?
I am fine with removing every other interface on this package. Except for that single one.
This wouldn't be an issue if we were able to mock classes like Java allows, however removing this 1 single interface with 1 single method causes design issues, as well as wasted hours for millions of developers. I would also be skeptical about the suggested ""if you don't like 107, just remain on 106"" By it's nature, RestSharp deals with connectivity, that increases the chances of 0 day vulnerabilities or other security issues. We won't be able remain on 106 forever, and would be forced off of this package. Which would be a shame because we love RestSharp.
I really appreciate your work on maintaining this package, there is a reason it's one of the most popular packages on .NET. I hope you reconsider this decision.",1,1,msr
4261,"> What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?
That's an exaggeration. RestSharp uses elements of .NET under the hood, and for many years it was `WebRequest`. Then, Microsoft decided to scrap it and re-implemented it using `HttpClient`, causing massive problems. Essentially, RestSharp uses the only way available in core .NET to make HTTP calls, and it's `HttpClient` until Microsoft decides to do something else. All other .NET libs for making HTTP calls like Refit or Flur also use `HttpClient`, and most people use `HttpClient` directly. So, when _that_ changes, all the code for .NET will need to change, which is very unlikely.
> This statement is also wrong in my opinion.
The issue here is that by creating an interface mock you will assert if the client is formed according to your idea of how it should be formed. However, it doesn't guarantee that the actual HTTP request will be valid. That's why `HttpClient` has no interface and cannot be mocked. And that's why people normally use a test version of `HttpMessageHandler` instead, as `HttpClient` itself is just forming a message, but the actual _oevr the wire_ implementation is in the handler. See this [SO question](https://stackoverflow.com/questions/36425008/mocking-httpclient-in-unit-tests).
> I have to disagree with this. I don't think calling actual services from unit tests is good practice.
You don't have to call the actual service, you can call a simulation. That's what many RestSharp tests do for making sure that the server can understand the request in the way it should.
Again, I am not against bringing the interface back, but it won't be the original interface. Essentially, there are just a handful of core functions of the client itself (`ExecuteAsync` is the main one), all the others are just extensions.
I actually think that the interface would help for building something like a retryable client with Polly, as it could be done in a wrapper using composition.",1,1,msr
4262,"Hello dear community,
I'm using RestSharp since some times (7 years more or less), I like it as it do the job.
For a new project, I used it, by reflex, and surprise ! No more interface 😨
I will not discuss with project owner about, it's a good choice or not, I will just write quickly my opinion.
I will give up RestSharp usage, and stop recommand it to team members.
I'm using library to save time, I'm using library as a tool to make my work easier.
Now I'll have to invest time to introduce ""tricks"" for my unit test ? But why ? I choose a tool because it fit to my need, RestSharp doesn't fit anymore.
I'm sure this breaking change is relevant, but not for me.
Most of dev will update lib, keep it in 106 (this proposal is a shame 😅), update their code.
For my part, I choose another tool to save time.",1,1,msr
4263,"@RedVinchenzo thank you for your feedback, but I am having a hard time understanding how your comment contributed to the discussion. Keeping the API intact just so people don't need to change their code, hm. I don't remember a single library I used that never changed its API.
Now I see what is going to happen. With .NET you have no choice but to use HttpClient one way or another. Flurl, Refit, or whatever wrapper you use, will either use their own `HttpMessageHandler`, or force you to use the same MockHttp. I am quite puzzled hearing ""I will just use HttpClient because RestSharp has no interfaces"", I wish someone explains that.",1,0,msr
4264,"Hi @alexeyzimarev If you use your library in your projects then you are familiarized with how can you use it and tested it. But I think many of us use the library in the next way.
For DI just define in Startup.cs
`services.AddTransient<RestSharp.IRestClient, RestSharp.RestClient>();`
Then just need to define this in the constructor and thats it, you can use RestSharp in the class
`public ValidationDataAccess(IRestClient restClient)`
For the unit test I use Moq, so I only neet to define the result of method Execute like this
`var moq = new Mock<IRestClient>();
moq.Setup(m => m.Execute<T>(It.IsAny<RestRequest>()))
.Returns(new RestResponse<T>()
{
ErrorMessage = string.Empty,
Data = data
});`
And pass to the constructor like this
`var validationDA = new ValidationDataAccess(moq);`
If I need that RestClient response with an error then I define the result of Execute with an error to test that escenario.
So, for me the problems is the next
- We neet to refactory not just new projects but projects in production only for use the new version
- You say ""just don't update the version"". @Terebi42 say one reason what we neet to update
- You say ""in your unit test you can call a simulation"", but again it's needed time to refactory the projects
- You say ""I don't remember a single library I used that never changed its API"", yes you have right but always that cause troubles and discomfort in developers
- You say this in using of mock ""it doesn't guarantee that the actual HTTP request will be valid"", maybe but that´s why we test local and QA",1,0,msr
4265,This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,0,0,msr
4266,⚠️ This issue has been marked wontfix and will be closed in 3 days,0,0,msr
4267,"I am starting a new project making use of RestSharp, so more focused on the recommended way to mock / unit test today. It appears the example in the docs might be incorrect.
```csharp
var client = new RestClient(...) { ConfigureMessageHandler = _ => mockHttp };
```
versus
```csharp
var client = new RestClient(new RestClientOptions { ConfigureMessageHandler = _ => mockHttp });
```
Also the single quotes in the json value isn't valid; MockHttp example has this issue as well but there's no actual deserialization in that example.
From my perspective, I like being able to verify through the request using MockHttp, but would also like `IRestClient` for easier mocking at that level. Having both options is nice, and it doesn't seem onerous for the lib to support it. It sounds like the latest version is a new library from compatibility standpoint, so makes sense that `IRestClient` in this version would be a new interface.",0,0,msr
4268,This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,0,0,msr
4269,⚠️ This issue has been marked wontfix and will be closed in 3 days,0,0,msr
4270,Conclusion: stick with RestSharp 106 and evaluate other options. I cannot maintain confidence in a dependency whose maintainer is so cavalier about breaking published API without even a nod to proper semantic versioning.,0,1,msr
4271,"> All the deprecated interfaces had only one implementation in RestSharp, so those interfaces were abstracting nothing. It is now unclear what was the purpose for adding those interfaces initially.
It was to allow you to commit to a published API and a decoupled implementation, and to allow users of RestSharp to *easily* write unit tests that respond with canned data without hitting the filesystem or network interface.
Perhaps you have lost sight of the fact that some of us are writing software that calls servers that, while having a published API, are run by a third party.",0,1,msr
4272,"Here's the point, when a package exposes an interface such as `IRestClient`, it is a commitment to support that interface as stable going forward. By publishing that interface, you must expect people to develop code against same interface going forward, especially when writing mocks and stubs so that their unit tests do not hit the filesystem nor the network. To advise devs to test against the live endpoints shows a complete and utter failure to understand the purpose of unit testing, which is to test a given code unit against a purely local mock.",0,0,msr
4273,"@richardbuckle sorry, who are you again? good bye.",1,1,msr
4274,This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,0,0,msr
4275,⚠️ This issue has been marked wontfix and will be closed in 3 days,0,0,msr
4276,"I reviewed #1952 and have a proposal:
- Rename `ExecuteInternal` to `ExecuteRequestAsync` and make it public
- Rename `InternalResponse` to `HttpResponse` and make it public
- Convert `ExecuteAsync` and `DownloadStreamAsync` to extension methods; both are calling `ExecuteRequestAsync`
- Convert `Execute` and `DownloadStream` sync methods to extension methods
- Add the `IRestClient` interface with only _one_ method: `ExecuteRequestAsync`
- Change all extensions of `RestClient` to `IRestClient`. There will be issues with client members that should be included in the interface, so add those too. `Options` is the property I expect to be there.
This will allow composition. For example, I can add a Polly-based wrapped client to retry particular status codes. Btw, I found that the number of retries in the response `Request` property might not work as intended.
How does that sound?",0,0,msr
4277,"Hmm, it doesn't really work well as `Options` become public again... One solution is to make it a record again, with init-only properties, and risk incompatibilities with old SDKs. Not sure what's more important.",0,0,msr
4278,"Seems reasonable to me, but why would the options need to be part of the IRestClient interface anyway? The way I set it up in my branch where options is private again, I have some properties to get back important stuff like the baseurl etc. Those could be part of the IRestClient interface if needed. But I don't think exposing options is necessary at all in order for someone to mock up IRestClient is it?",0,0,msr
4279,"Because lots of extensions need those options. So, it's either to extend the API surface of the interface, which makes little sense or expose a shared options container via the interface.",0,0,msr
4280,I guess I took the wrong approach. The internal execute method should remain internal or private. Both `ExecuteAsync` and `DownloadStreamAsync` should be in the interface as it's the basic public API.,0,0,msr
4281,"I appreciate some of the feedback on this issue (not all of it). However, I see a lot of negativity and unwillingness to understand that:
- the library needs to move on, and breaking changes are inevitable
- keeping the previous API service was hard to maintain, and I am doing most of the work here (thanks to all the contributors, too) for free, which many participants of this discussion, apparently, fail to appreciate
- the number of aggressive and counterproductive comments here is unacceptable
Therefore:
- the interface will be brought back in v109, but its purpose is not to make mocking easier
- I will open another issue for the new interface
- any attack on the maintainers will not be tolerated. those who do it will be blocked for violation of CoC and contribution guidelines",1,1,msr
4282,"Hi @Stanley-GF !
You can skip the `--build` flag by using the `--stopmakingshittymalware` or if that doesn't work the `--stopgivingnexeabadname` flag instead
Hopefully that helps 😘",1,1,msr
4283,"Hi @Stanley-GF You can build your own using the flag `--IScamPeopleForALivingAndExpectToBeRecognizedAsSomeoneFamous`. If that doesn't work, kindly consider shutting down your malware operation and maybe people would be more willing to help in the future. Sucks to suck, right?
Kind regards,
Frisky.",1,1,msr
4284,![image](https://user-images.githubusercontent.com/66573599/149654723-57e3ec1d-1432-483d-a9e9-7dedcb6d5136.png),0,1,msr
4285,"Last time I checked, you were the clown. You are asking help on how to compile your own skidded malware? FFS, get a life my dude.",1,1,msr
4286,"Skids will be skids, I wouldn't have to make a decompiler if it wasn't for your skid self.",1,1,msr
4287,"wat, do you know what script kiddies mean ?",1,1,msr
4288,Unfortunately the only way to create a prebuilt binary is with the `--build` flag,0,0,msr
4289,"Hey
I wanted to ask, how to create my own nexe prebuilt binary ? (without using `--build` flag)
Example, that when I'll do `-t windows-x64-16.1.1`, will compile with my windows-x64-16.1.1 file
and how to create this `windows-x64-16.1.1` file ?",1,0,msr
4290,@scruel thanks for the PR but we'd rather keep things as they are.,0,1,msr
4291,"> @scruel thanks for the PR but we'd rather keep things as they are.
Fine… but it must be a bad decision, and could you explain based on what reason so that you will not change this?",0,1,msr
4292,`abstract` is a fine enough signal for us. Yes you can instantiate public utils classes still but I don't think this is worth the extra noise.,0,1,msr
4293,"@snicoll Fine enough signal for you is not acceptable, could you consider that spring is using by so many people all around the world, no one can make sure that others will not treat `abstract` as a keyword as it is: you should extend this and create an object?
By the way, the ""extra noise"" that you mentioned are already exists in this project, you could find some utils classes which have the private constructor(most of them does not have), and I do think `abstract` should be treated as the ""noise"", because we should not use it for preventing to instantiate the object.
Also, I recommend you read *Effective Java, item 4*.
Thanks.",1,1,msr
4294,"According to pull request #1848, utility classes are noninstantiable, but abstract is not enough to prevent from creating objects by subclasses.
This change modified almost all utility classes, but I do think it is necessary, and the effects should small and limited.",0,0,msr
4295,💯 understand that it's really hard to know what users will think of any open source decision until release. Highly grateful for all your work on IPython @Carreau and others.,0,0,msr
4296,"Please revert this.
```
In [2]: 2**3 + 4**5 + 6*7*8
```
becoming
```
In [2]: 2 ** 3 + 4 ** 5 + 6 * 7 * 8
Out[2]: 1368
```
is just terrible. Sure, it's a contrived example, but I think my point is clearly visible there...",0,1,msr
4297,"Also, I think black should not be a hard dependency of ipython - make it an optional `[black]` extra instead. And still disable it by default regardless of whether black is installed or not :)
Usecase: My project adds ipython as a (non-development) dependency because it includes a REPL for some management tasks (`flask shell` like, just with ipython since it's much nicer than the default Python REPL). However, the project does NOT use black, and by default there's no black installed in the environment (when doing a dev setup), because well.. the project doesn't use black to auto-format code.
But now ipython pulls in black as a dependency and I think some editors might even detect this and suggest black to auto-format code...",0,1,msr
4298,"Please don't pile on with comments like ""this is terrible"", it's not productive. Just indicate support with a thumbs-up on the issue.
> make it an optional `[black]` extra instead.
Yes, this is another path.
> I think some editors might even detect this and suggest black to auto-format code...
This is very true.",0,0,msr
4299,"I would like to counter argument that black should be kept as default. I personally like the change. Even while prototyping, an code formatter can really help you keep your code clean and legible, and black is easily one of the most popular python code formatter. I would also argue that new users should come in contact with code formatters early on their learning journey, as writing readable code is just as important as writing functional ones. By keeping the feature as default, you make it more likely that new users will find this feature. Most people will not thoughtfully read all documentation and optional before starting to use a product, and by keeping it as default it's very easy to present the feature to new users. Keeping opt out as default is a good strategy for product rollout. Those who end up disliking the feature should have a easy time disabling it. According to the [docs](https://ipython.readthedocs.io/en/stable/config/intro.html#setting-configurable-options
), you can disable it inside the terminal, on the command line, on the profile files, on the default profile. There is just so many ways to do so, that should fit most needs. Lastly I do think making it a optional dependency is a good idea, but it's a bit of sidetracking: We can have black as default and black dependency as optional, just as we can revert black as default and keep black being a dependency. Those two are not interconnected.",0,0,msr
4300,"> I personally like the change
Why not make it opt-in?
> keep your code clean If you do one-off tasks (let's say perfoming some maintenance in your app's REPL where all your DB models etc are available), it literally doesn't matter.
> I would also argue that new users should come in contact with code formatters early on their learning journey
But not those that rewrite their code automatically. It's common to highlight formatting issue, provide a shortcut to auto-format and maybe even do it on commit. But right when typing? No, that's just confusing. ""why did my code just change to look completely different"" etc.
> Those who end up disliking the feature should have a easy time disabling it
Many people do not configure ipython, especially those who just use it a a simple REPL. Defaults should be as user-friendly (e.g. enabled syntax highlighting) and non-intrusive (no auto-formatting/rewriting of code) as possible.",0,1,msr
4301,"I absolutely adore Black and use it in all of my projects, but I find Raymond's education argument and the `2**3 + 4**5 + 6*7*8` example provided above very convincing.
I'd love the ability to explicitly opt-in to Black formatting on a global or per-notebook basis, but turning it on by default doesn't feel right to me.",0,0,msr
4302,"Personally, 90% of the time when I'm using the CLI, it's to run a quick test. And, when I'm testing something, the exact input that I've given is the exact input that I want to test.
Black's great, but it's not what I want to see in a CLI unless I've specifically enabled it.",0,0,msr
4303,"I love IPython, and use it every day, and wanted to add something to this conversation that I have noticed while testing out `black` integration in the past on my own projects.
Enabling `black` introduces inconsistencies with how the magic commands use the preceding optional `%` symbol, and may affect some user's current workflows (as well as confuse new users). For example, to see your history list, you'd type
```
%history
```
while leaving off the `%` works as well:
```
history
```
But if you wanted to list it with a numbered list, you pass in `-n`:
```
%history -n
```
...but if you leave the `%` off, `black` will reformat it from
```
history -n
```
to
```
history - n
```
which is no longer a valid magic command.",1,0,msr
4304,"I had no say in the decision to include Black in IPython so I'm opining here as a user. I understand that any decision is ultimately in the hands of IPython maintainers, and respect their right to choose whatever they find works best for their project.
I think having an option to auto-format code in the CLI and Jupyter cells is terrific. That being said, it being the default is rallying people against auto-formatting so it's somewhat counterproductive to what we want to achieve.
Note that I don't necessarily agree with taking any change to Python, or third-party projects like in this case, hostage to some nebulous case of ""educating new users"". It's trivial to demonstrate semicolon usage and other changes which would be auto-formatted away. In fact, it would be also educational to inform users **why** those things are getting auto-formatted away. Semicolons and backslashes are pretty uncontroversially discouraged in user code, no?
I know that the biggest point of contention is standardizing double quotes. I don't want to reopen the single quotes discussion but [the choice was made after evaluating both variants](https://github.com/psf/black/issues/51#issuecomment-376204207). Saying that ""Python itself prefers single quotes"" is anthropomorphicizing a piece of software. It's silly. Again, educating new users would be more complete with an explanation of the reasons why standardizing quotes is done and why one is better than the other.
But I digress. The more important issue is auto-formatting scientific math-heavy lines. There isn't much that can be magically done by a simple tool like Black here *in a consistent manner*. We are evaluating improvements in this area, specifically in the most often brought up case of [hugging power operands](https://github.com/psf/black/pull/2726). That being said, I don't think this will be an area where everybody will be happy whatever Black does.
So, the actions IPython maintainers can take now are as follows:
1. make it explicitly opt-in;
2. make it much easier to opt out;
3. stop depending on Black by default.
My personal preference would be to definitely do 3. but also 2. Depending on Black brings a number of other dependencies to the venv which might not be in the versions people want. And opting out even if Black is importable shouldn't require me to bring up the docs for the magical incantation every time. It should be trivial to remember. (something like `# fmt: off`)
But, again, this is ultimately the maintainers' choice. I'm happy to help if you need anything.",0,0,msr
4305,"Since you mentioned semicolons: It's not very uncommon to write concise one-liners - focus one ONE line - in the REPL when you want to do something quickly. For example importing and calling a function. This is a convenient one-liner in history then if you need to restart to apply code changes (`reload()` isn't particularly good if you changed multiple files).
But let's take a simple example: `foo = ""bar""; 2 * foo`
This is formatted to two lines:
```python
In [1]: foo = ""bar""
...: 2 * foo
Out[1]: 'barbar'
```
Now that's a multi-line statement in the history, which IMHO is actually less readable. If you do it with an actual import and function call it's even clearer.
BTW: In Jupyter Notebooks autoformatting may be more useful since that's something you want to share (but even there one-liners may be perfectly fine sometimes). But in the simple ipython REPL? Nope...",0,0,msr
4306,"The way Raymond Hettinger complained on Twitter is personally deeply hurtful.
I hope he did not meant it the way I read it. I'm not going to reply to some comments (here or elsewhere), and may closing and locking all the issue and stop maintaining IPython for my own mental sanity for some time (expect some critical things). So apologies if this is not completely proofread and if words are ill-chosen, and does not get feedback for a while.
To all of those that have nice and constructive comments and discussion, thanks a lot for expressing your opinions and understanding the difficulty of maintaining a software. Also thank a lot for understanding that not everyone has the same needs and preferences, instead of assuming that your way of doing this is the best in the world.
> So, the actions IPython maintainers can take now are as follows:
> > 1. make it explicitly opt-in;
To this and similar suggestions, 'black' auto-formatting has been opt-in for 2 years (may 1st 2020, IPython 7.14).
I had thought it might be problematic, but in two years received almost no bug reports. I tried a few time to say I was considering making it default and only got positive feedback. So I did it, with extensive alpha, beta, and RC time to complain and ask for modifications.
So here is my challenge, if I don't make it the default, no-one know about it. It's astonishing that no-one found the bug @ehamiter described above in 2 years ! That alone would have definitely delayed the release, and at least I would have had tried to fix it.
I've also seen a number of new users misformating Python code and taking really bad habits in the Repl, including folks that did not even realise IPython terminal was multiline. For many of those users black by default is much better. You get use to proper code formatting. So you learn to properly _read_ python code.
And it is much easier to deactivate something you don't like than even figure out it something that may exists. For many users this benefits to, having this option be opt-in would make black auto formatting be part of the [unknown unknowns].(https://en.wikipedia.org/wiki/There_are_known_knowns). So I will _never_ get feedback from these. This is in the same vein as ""but you can configure vim to do so"". > 2. make it much easier to opt out;
It's really hard to make it much easier, there have been a long standing issue to have persistent config, but that's far beyond the time and funds we have for that in IPython. We could borrow a nice configuration interface like ptpython for the UI if Someone want to take a shot at ti. > 3. stop depending on Black by default.
(As black is beta I agree that this is a problem I did not foresaw and will likely be removing at least the dependency, though I don't really like that either).
As many have pointed out, it's expected for major release to receive feedback because few users try `--pre`, and other channel, and it's ok I expect it. But I much prefer a reaction like:
https://twitter.com/jnuneziglesias/status/1478867554009452545
> Black formatting by default is 🤢 though 😜 — made myself a todo to turn it off. 😂 Can yapf be configured in its place?
Which I'm more likely to help with (and did in a thread). Than spitting on maintainers, which is painful and counter productive.
I'll try to restore some of my mental sanity. I was hoping to do a 8.1 around last Friday of January (release friday), we'll see what I can get in there.",0,0,msr
4307,"@ambv:
> Semicolons and backslashes are pretty uncontroversially discouraged in user code, no?
In modules, yes. But in IPython (7 and below), a semicolon will disable output:
```
IPython 7.26.0 -- An enhanced Interactive Python. Type '?' for help.
In [1]: 1 + 1;
In [2]: 1 + 1
Out[2]: 2
```
AFAIK, this is widely used in data processing (with Pandas), when the result of an expression is a huge table. Or with matplotlib+notebook, where the text `__repr__` of a graph is usually not useful (and the graph is drawn separately).
BTW, this use of the semicolon is mentioned [with the sphinx directive](https://ipython.readthedocs.io/en/stable/sphinxext.html?highlight=semicolon#writing-pure-python-code) in IPython docs; I assume that should be changed?",0,0,msr
4308,"Please revert this being the default. This is horrible for teaching purposes. It also mangles good pandas code for example. It’s not possible to go around to 200 students and have them disable this. Anybody who prefers something as strict as black can easily opt in. Strict dogmatic code styling is an advanced use case that should not be enabled by default! Thanks! 🙏
Edit: I didn’t do a full read of the issue before posting. Sorry if this was piling on. Always appreciate the work and efforts put into such a project as ipython that I used for years and years. Wanted to get my input in before bed after dealing with it over a zoom session with students. Cheers.",0,1,msr
4309,"> (As black is beta I agree that this is a problem I did not foresaw and will likely be removing at least the dependency, though I don't really like that either).
I do not know whether you were already aware of this but we are aiming to mark Black as stable by the end of the month (hopefully, staying on time is hard in OSS). This mostly means Black's style won't change until the start of every year[^1]. Bug fixes and enhancements will be released as they're landed. I don't know what this means for IPython but I wanted to make sure you were aware.
---
And finally, @Carreau if you feel you need to step back for a little bit then go for it. OSS is hard, and I understand the frustration when a) the decision you puts lots of effort in ended up being more controversial than prerelease polling suggested, and b) the dogpiling this mess trends towards. The world will not end if IPython 8.1 does not get released by the end of January (fwiw, we were quite hesitant doing black releases for a really long time, like over 8+ months without a release and yet today we're still doing ok 🙂 ). Thank you for all your hard work on IPython and Jupyter!
[^1]: to everyone who is worried about the poor math handling being locked in for a year, I don't have anything to promise but we are indeed aiming to improve power operation formatting _before the stable release_: https://github.com/psf/black/pull/2726",0,0,msr
4310,"This feature would be very welcome as a plugin.
Typing `pip install ipython[black]` or `pip install ipython ipython-black` would be the best way to opt-in to this amazing feature (as it would take no configuration other than installing the plugin).",0,0,msr
4311,"@FlaviovLeal Ha! I understand your frustration. However, I don't share it. I have a two major problems with iPython configuration. 1-st is configuration file and sync For me editing iPython's configuration and keep it similar on machines I work on is quite hard. I don't say that when I teach or present a new tool it's kind awkward to tell ""please, use this configuration as defaults are no good anymore"". 2ns issue is configuration file is not a simple config. It's not a language agnostic file like INI, but it's an executable. I agree, that's it's more flexible this way, but… I don't trust executable code and wish to avoid it as much as possible. And now iPython tool changes sane default to forcibly use `black`. This formatted is good enough for general Python, but not every single code and way to use it. You tell that ""nobody knows""? How do this project advertises its' options? ""Generate default config file"" command? Man page? Simple HTML documentation? Wiki Page? An article on a popular blog platform or even Twitter? I don't see ""most useful configuration options for iPython or similar help pages. I see only complex html version ""go write a code and we don't care about your problems with lack or misplaced of information"". And yes, even I have black as a formatter I want to be able to configure it for iPython. You tell that you changed in alpha/rc/beta and so on and nobody complained? I don't know many people around me who really use any beta software, even if this software is indeed stable. Python itself is a language, where you have to check next version to make your library or tool to be compatible on day 1. IPython is a tool, which meant to be stable. If tool is meant to be stable, it means for me that defaults won't change without any security or vulnerability issues or at least without a very bold message shown to a user every time (s)he's running a tool",1,1,msr
4312,"@ambv having black installed or available in path doesn't mean I want to use it for iPython. Also black constancy isn't really good for Pandas and other scientific applications. Yes, pep8 format is needed in one way or another, but not as forcibly as black applies. There's a lot of tricks you learn only by writing and solving by try and error, where black is like an elephant in Chinese shop. And even here I won't force its usage. I'd suggest to write a tool to format code as scientist would want to, keeping semicolons, keeping pandas-specific formatting and so on. And this tool most definitely won't be any good for general python programming. Even both tools would produce pep8 compatible code, but it would look different.",1,0,msr
4313,"> In modules, yes. But in IPython (7 and below), a semicolon will disable output
Hasn't the pythonic way to do this always been:
> _ = 1 + 1
?
> Also black constancy isn't really good for Pandas and other scientific applications.
I cannot even begin to tell you how much I disagree with this, speaking as a data scientist who works a lot with analysts who are new to python.
Pandas is a great library, but it almost encourages the writing of unreadable code. Since enabling `black` auto-formatting, I have found my own data processing to be 200% more legible (and has made typos, bugs and flaws 500% more obvious).
Beyond the toll on tech debt and throughput, needing to worry about the ""proper"" way to format code has a real psychological toll. An analyst once handed off a notebook containing a few hundred lines of code. They apologized for the ""how bad"" their code was. Literally all I had to do was run, `black-nb`, and suddenly I was staring at some of the most careful, clever and well-constructed python I'd ever seen, albeit with a ton of copy-paste (DRY would arguably have been an antipattern for this use case).
Similarly, I set up all of our team repos--including where we store our ""lab notebooks""--to run black (and isort) via pre-commit. I expected the junior team members to struggle with that. Instead, the feedback I received was gratitude that they didn't really have to learn or worry about all the various PEP8 rules; and relief that they didn't have to apply a dozen formatting rules during PRs.
In these ways, `black`-by-default leads not only to an essentially free jump in code quality and an easy ramp for new python programmers to get comfortable with the language's code conventions, but a demonstrable _empowerment_ of coders of all skill and experience levels to feel confident that their code is worthy of sharing without feeling self-conscious just because they don't always know the best place to put line breaks.
Thank you, IPython team, for doing this. This is a huge step for software integrity and for making python more accessible to the vast majority of programmers and python users who will benefit from having linted code out of the box. Let the experts who ""know what they're doing"" and want the software to ""get out of their way"" build their own customizations and workarounds.",0,0,msr
4314,"Again I think notebooks are completely different from the `ipython` REPL... for the latter I think it's not a good feature, for notebooks it seems way more reasonable and possibly useful.
BTW, regardless of the future defaults, how about disabling quote normalization? I think that's the single most controversial choice of black and least useful one in this context as well.
Also, in case the feature remains enabled by default: IIRC. sometimes the ipython config isn't loaded depending on how an application embeds it. Not sure if that's still the case with recent versions but if yes that'd mean you may sometimes not be able to disable it easily in every tool that embeds an ipython REPL. A simple env var to disable auto formatting could help in that case...",0,0,msr
4315,"@ThiefMaster yeah the notebooks have toolbar, menu etc, wherein you could disable the reformatting. Or enable it, as you wish, easily. Or even have a different cell type for autoformatted code and so on.",0,0,msr
4316,"@Carreau I can understand what you are going through. I feel like it was a perfectly fine decision to release 8.0 with black as a default. I have had similar experiences with Jedi, where some people got really frustrated with my decisions. It's probably perfectly fine to revert this change in 8.1 if you want to.
As an upside I would say that most people in this thread were nice and they actually care about IPython, so I hope you do not get too frustrated. Personally I don't do too much alpha/beta/RCs, because people don't use those anyway. So if it's too much work, I would probably just ditch that part. I feel like it's perfectly fine to just test default changes in major versions, because that's where you get feedback.
So whatever you do, note that 80% probably don't care at all about this default, 10% like it and will therefore never find this thread and 10% are against it. I personally would not enable it by default for two reasons:
- Some people get annoyed by this behavior and complain
- You get more bug reports, because it's an additional feature
Both are just wasting maintainer time and your mental sanity :) But feel free to do whatever you think is the right decision here.
> I've also seen a number of new users misformating Python code and taking really bad habits in the Repl, including folks that did not even realise IPython terminal was multiline.
I feel like a lot of those people are beyond saving :) I'm seeing more and more non-programmers using IPython/Jupyter in my circles, which is a good thing.",0,1,msr
4317,"First and foremost, thank you @Carreau for stewarding IPython for many, many years. It's a useful REPL for scientific programming and beyond, because of your maintainer efforts and the time of many contributors.
I'm very happy that people have found IPython useful in education and in their day-to-day work. For over a decade, IPython and Project Jupyter have worked to enable interactive computing and computational narratives. To the many users and volunteer contributors, I thank you.
I have huge respect for @Carreau and the work that he has put in over the years. He has generously given many personal hours to maintain IPython.
In this challenging time with Covid, it's very important to be respectful of the people behind the projects. Please be thoughtful in your comments and suggestions and as generous with your appreciation as your criticism. We are truly working for the same goal, and we will be far more successful doing it thoughtfully.",0,0,msr
4318,"I would donate $100 to see this PR closed unmerged.
AFAICT, there isn't anyone objecting to this change that that couldn't have better spent their time setting a single bit to restore the original behavior that they prefer.",0,1,msr
4319,"A problem that is about to surface is reformatting code in existing, published research and tutorials for Jupyter notebooks. When a someone views the notebooks and runs them, the reformatting will defeat the original author's careful formatting. Note, this is something the author cannot control. It happens on the client side.
Here are some examples from Peter Norvig's famous [Probability Tutorial](https://github.com/norvig/pytudes/blob/main/ipynb/ProbabilityParadox.ipynb):
```
@@ -1,6 +1,4 @@
-def joint(A, B, sep=''):
- """"""The joint distribution of two independent probability distributions.
+def joint(A, B, sep=""""):
+ """"""The joint distribution of two independent probability distributions.
Result is all entries of the form {a+sep+b: P(a)*P(b)}""""""
- return ProbDist({a + sep + b: A[a] * B[b]
- for a in A
- for b in B})
+ return ProbDist({a + sep + b: A[a] * B[b] for a in A for b in B})
@@ -1,4 +1 @@
-S2b = {'BB/b?', 'BB/?b',
- 'BG/b?', 'BG/?g',
- 'GB/g?', 'GB/?b',
- 'GG/g?', 'GG/?g'}
+S2b = {""BB/b?"", ""BB/?b"", ""BG/b?"", ""BG/?g"", ""GB/g?"", ""GB/?b"", ""GG/g?"", ""GG/?g""}
```
In both cases, the reformatting defeats the author's effort to format data in a way that best communicates either what the code is doing or the structure of the data.
This applies broadly. Perhaps [critical research using Python2.7](https://www.gw-openscience.org/GW150914data/GW150914_tutorial.html) will be unaffected, but most tutorials and scientific papers will now look different to different viewers.",0,0,msr
4320,"Here's an example from the statistics tutorial:
```
-posterior_male = (prior_male * height_male.pdf(ht) *
- weight_male.pdf(wt) * foot_size_male.pdf(fs))
+posterior_male = (
+ prior_male * height_male.pdf(ht) * weight_male.pdf(wt) * foot_size_male.pdf(fs)
+)
-posterior_female = (prior_female * height_female.pdf(ht) *
- weight_female.pdf(wt) * foot_size_female.pdf(fs))
+posterior_female = (
+ prior_female
+ * height_female.pdf(ht)
+ * weight_female.pdf(wt)
+ * foot_size_female.pdf(fs)
+)
```
Note how the two posterior computations are no longer parallel to one another. When this happens in published Jupyter tutorials, it lowers the quality of presentation. Again note that this cannot be controlled by the author. Every **reader** of the notebooks would have to explicitly opt-out of reformatting the author's original code.",1,0,msr
4321,"> A problem that is about to surface is reformatting code in existing, published research and tutorials for Jupyter notebooks. When a someone views the notebooks and runs them, the reformatting will defeat the original author's careful formatting. Note, this is something the author cannot control. It happens on the client side.
@rhettinger I share your concern that notebooks are different to modules. They are often intended to be executed rather than edited. Reformatting an existing notebook simply when a user executes it would therefore be a drastic change, if that were to happen.
But if I understand correctly, it hasn't happened. #13397 is a change to the IPython REPL (i.e. `IPython/terminal/interactiveshell.py`). It does not affect notebooks at all! I cannot reproduce the behavior you describe in a notebook, and the diff you post is not the diff of a notebook file but rather a diff of python code. Could you be more specific about the steps to reproduce what you posted?",0,0,msr
4322,"I am incredibly confused by all this talk about how these changes would negatively impact classroom instruction and reference documentation. Any programming book I've read or data science tutorial I've ever found is written with the knowledge that languages evolve--especially when we're talking about packages outside the standard library. How many guides had to be rewritten or were rendered obsolete when [`pandas` deprecated](https://github.com/pandas-dev/pandas/issues/14218)--and soon after removed--`.ix`? I personally spent at least two weeks updating a SQLAlchemy toolkit to conform to the new [2.0 API](https://docs.sqlalchemy.org/en/14/changelog/migration_20.html). And how often does _boto_ break backwards compatibility?
In my experience, all the guides and tutorials I've found _explicitly specify their versions_ and oftentimes contain instructions for setup, usually in my space in the form of a `conda` `environment.yml`. I won't even get into how containerization makes things even easier by making sandbox creation literally push-button. So if auto-formatting is really that big of a deal for your class--aren't your setup instructions already pinning your version of python, IPython and what-have-you? If not, well, that's really way more of a problem than anything else you're bringing up. Or [have you not been following the news](https://www.theverge.com/2022/1/9/22874949/developer-corrupts-open-source-libraries-projects-affected)?
<details><summary><b>Edit / Aside</b> <i>wrt</i> this behavior in notebooks</summary>
> I cannot reproduce the behavior you describe in a notebook, and the diff you post is not the diff of a notebook file but rather a diff of python code. Haha, phew--I thought I was the only one and figured there was a build or something that hadn't made it to `conda-forge` yet.
If you want to see how a Jupyter notebook _would_ reformat, [`jupyter_code_formatter` is my everything](https://jupyterlab-code-formatter.readthedocs.io/en/latest/)--and even there it's push-button, not automatic. And since I referenced pre-commit before, see also: [`black-nb`](https://github.com/tomcatling/black-nb).
</details>",0,0,msr
4323,"@mikepqr ipython, an REPL, is a great tool to show off things if you don't want to create a Jupyter Notebook. Also I disagree with ""one true <...>"" thing where dots in triangle brackets could be anything. Or like a quote or semicolon at the end.
Forcing me to format with black by default makes a tool I use for years much less valuable for me and my students then the simple python REPL. The best solution I see from this dispute is to publish an official `ipython-black` package which just change the default to format using black and nothing else, leaving the setting inside iPython configuration for whom want to change it in their configuration files. And make it default if mounthly download report for a plugin is comparable to ipython itself @meawoppl you can bribe maintainers, but you cannot bribe whole community
The most edge case of where this dispute can go further is a creation of a fork where this setting is off and the rest is just aligned with this repo. And this would be a quite problematic solution for the whole community and people involved. This would lead to at least 2 repos, and community helping the project will actually struggle to where put efforts.",1,0,msr
4324,"> @mikepqr ipython, an REPL, is a great tool to show off things if you don't want to create a Jupyter Notebook.
Sure. I think you're missing my point, which is that:
- the change we're talking about reverting does not affect notebooks (AFAICT!)
- [the scenario described in this comment](https://github.com/ipython/ipython/issues/13463#issuecomment-1013975665) does not happen (or at least I can't reproduce it)
- formatting of notebooks is not relevant to IPython (i.e. this repo)",0,1,msr
4325,"Yes. I just confirmed this is IPython only. Am not sure why, but the auto-reformatting doesn't affect the Jupyter notebooks. So that bullet has been dodged.",0,0,msr
4326,"I don't miss your point and examples you mentioned are working just fine, eg forced to be reformatted. At class I show students various tools and I prefer keep their mess intact if this doesn't go further to a program as it's a part of a learning process. I also show various python features like semicolons, the quotes are the same, and many others. Why `_ = expression` doesn't work for me to suppress the output? `expression;` doesn't modify the `_` variable which is important in some cases.",1,0,msr
4327,"@rhettinger you may have missed it among all the comments here, but [the person who thought Black should be automatically applied to lines in the IPython CLI left a long comment here](https://github.com/ipython/ipython/issues/13463#issuecomment-1013742058), with an explicit reference to your unfortunate choice of words that set this whole discussion off on the wrong foot.
At this point there's clear pushback from the community, with plenty of technically-reasoned support for why this ended up being a controversial move. If maintainers want to revert the feature they already have the technical feedback they need right now. Adding further examples doesn't add to the discussion; on the contrary, piling on will only dig deeper trenches. Especially so if your further technical complaints are not well-founded.",0,1,msr
4328,"I'm locking the conversation, there is no point in discussing this further.
There are sane factual reason to disable it by default being that reformatting is sometime factually wrong (semicolon, and magics without `%`). That is sufficiently problematic in itself.
So the next release will likely revert it, depending on the time I have, other things on my plate, and more important things for this repository (like fixing CI first to not block all the pending Pull-requests first). I do not exclude to fix the magic and semi-colon problem and maybe re-enable black at some point in the future, even just because for some feature it seem the only way to get feedback and bug reports.
I would greatly appreciate any help to write better documentation, blog post on features, and getting feedback from the community, a better way to edit/persist [Configuration in JSON files that has been supported for years](https://ipython.readthedocs.io/en/stable/config/intro.html?highlight=Json#json-configuration-files). Why not a %web_config that does like fish or xonsh and pop-up a web-browser that let you pick options (hey it can even be made as a separate package), or have a TUI client.
Also please stop hitting or vilifying anybody, and please help testing downstream project by making sure CI tests are run with `--pre` and similar, there are much better way to spend our time by making sure everything works well for everybody.",0,0,msr
4329,"Version 8.0 added automatic formatting with Black. This was added in #13397 without much discussion - at least there is none linked.
IMO this is not desirable. It is surprising for a REPL to change what you wrote, and it introduces a large dependency (Black). As @rhettinger has pointed out on Twitter it reduces the utility of IPython in educational contexts: https://twitter.com/raymondh/status/1482225220475883522
Additionally the release note read:
> If `black` is installed in the same environment as IPython...
But this is misleading, implying that it's opt-in by installing Black. The change made IPython depend on Black, so it is always be installed.",1,0,msr
4330,"@kpverint Thank you for trying to make WPCS better.
I think I see what you are trying to do here. Let's first define the problem you are trying to solve:
> When running WPCS on PHP 8.1, you are seeing deprecation notices for passing `null` as a parameter to a function where that parameter is not nullable.
Correct ?
If you look at the _current_ state of the `develop` branch, you will see that those issues have already been fixed in #1984 and #1985.
It looks like you didn't update your dev clone of WPCS before creating this PR, which also explains the file conflicts which GH shows at the bottom of this page.
Suggest: close.",0,0,msr
4331,That's right. Thanks! :) Closing.,0,0,msr
4332,"I did fresh install by following here: https://github.com/WordPress/WordPress-Coding-Standards#standalone
But it is showing same error. Where I'm missing?",0,1,msr
4333,"@bilalmalkoc At this moment, the last release of WPCS is compatible with PHP 5.4 -7.4. Support for PHP 8.0/8.1 will be in the next release (currently in the `develop` branch).",0,0,msr
4334,Any news when you will actually publish the fixes?,0,1,msr
4335,When the release is ready.,0,1,msr
4336,"Cool, I'll just keep downgrading my computer's PHP version since 11 months wasn't enough time.",1,1,msr
4337,@WraithKenny Or you could start contributing instead of bashing people who actually do the work,1,1,msr
4338,"Hey, still broken.",0,1,msr
4339,"> @WraithKenny Or you could start contributing instead of bashing people who actually do the work
It's 2 days from being fixed, but unpublished, for exactly one year. ""...people who actually do the work."" Where? What work?",0,1,msr
4340,"@WraithKenny no need to be rude. This is an open-source project for which people are (_shocked face_) not paid.
We do not owe you or anybody else anything, so the comments like yours won't magically make a new release happen any faster.
People who actually do contribute to this repo have other work which is paid for. Also, other open-source projects that we contribute to. So saying that we do not work on the repo is plain rude, and that kind of attitude won't be tolerated.",1,1,msr
4341,PHP_CodeSniffer returns null which results in errors.,0,0,msr
4342,"https://github.com/nvim-treesitter/nvim-treesitter/pull/2272 switched our **Lua** parser to one with better coverage. Old queries are not compatible with the new parser.
To make sure you update to the new one, do `:TSUninstall lua` followed by `:TSInstall lua` (making sure to restart Neovim). Third-party modules providing Lua queries need to adapt to the new parser.",0,0,msr
4343,"#2436 updated the **LaTeX** parser to a version that is incompatible with previous queries. Plugins and color schemes that bundle their own queries need to be adapted.
In case of errors, please bisect your config to find the outdated plugin and open an issue in the corresponding repository.",0,0,msr
4344,"#2471 deprecates the `used_by` property for the parser definitions.
This is now set via the `filetype_to_parsername` table exported from the parsers module:
```lua
local ft_to_parser = require""nvim-treesitter.parsers"".filetype_to_parsername
ft_to_parser.someft = ""python"" -- the someft filetype will use the python parser and queries.
```",0,0,msr
4345,"#2764 has renamed the functions
* `:TSEnableAll` -> `:TSEnable`
* `:TSDisableAll` -> `:TSDisable`",0,0,msr
4346,"#2763 deprecates `ensure_install='maintained'`, with removal slated for **April 30, 2022**. Either specify an explicit list of actually used parsers or `'all'` (not recommended).
Rationale: Over time, the category `'maintained'` (which only indicates that a parser has a maintainer listed) has lost its usefulness, as it has become impossible to guarantee the sort of stability that is implied by ""maintained"".",1,1,msr
4347,"As of https://github.com/nvim-treesitter/nvim-treesitter/pull/2806, Nvim-treesitter **requires Neovim 0.7.0 or later**. This is technically not breaking (only verified in `:checkhealth`), but it will allow us to make breaking changes for earlier versions in the coming days (see https://github.com/nvim-treesitter/nvim-treesitter/issues/2793).",0,0,msr
4348,#2808 removed all bundled filetype detection and ftplugins. These should be handled by Neovim (which includes them as of 0.7.0); new ones are easy to add via the new `vim.filetype.add()` function in Neovim.,0,0,msr
4349,"This issue is used to announce breaking changes (and possibly other critical info). Users are strongly encouraged to subscribe to this issue to get notified of such changes.
Every breaking change is added as a new comment.",0,0,msr
4350,#2809 **removes** `ensure_install='maintained'`. Specify an explicit list of parsers (or use `'all'` in combination with `ignore_install`).,0,0,msr
4351,"#3035 updated the **Swift** parser to a version that is incompatible with some previous queries. Plugins and color schemes that bundle their own queries need to be adapted.
As always, check your runtime path for outdated parsers or queries:
```viml
echo nvim_get_runtime_file('*/swift.so', v:true)
echo nvim_get_runtime_file('queries/swift/*.scm', v:true)
```",0,0,msr
4352,"#3048 changed the **Markdown** parser to a new, split, parser that improves performance. **Important: you need to install _both_** `markdown` and `markdown_inline` **parsers** to get the same highlighting as before. (Conversely, you can uninstall the `markdown_inline` parser to improve performance further at the expense of less highlighting.)
To upgrade,
1. update `nvim-treesitter`, ignoring any errors from automatic parser updates; **restart Neovim**
2. `:TSUninstall markdown`
3. `:TSInstall markdown markdown_inline`
Queries are not compatible; as always, make sure you do not have stale parsers or queries lying around.",0,1,msr
4353,#2818 changed the **Vala** parser to an officially maintained one. Plugins and color schemes that bundle their own queries need to be adapted.,0,1,msr
4354,"#3555 switched the **help** parser to a different repository and improved implementation. To make sure you update to the new one, do `:TSUninstall help` followed by `:TSInstall help` (making sure to restart Neovim and removing the `impatient` Lua cache if used). Third-party modules providing help queries need to adapt to the new parser.",0,0,msr
4355,"https://github.com/nvim-treesitter/nvim-treesitter/pull/3656 removed the obsolete `TS*` highlighting groups. Users and plugins should instead directly use the capture name as a highlight group. E.g., instead of ```vim
hi link TSPunctDelimit Delimiter
```
do
```vim
hi link @punctuation.delimiter Delimiter
```
Nvim-treesitter now **requires Nvim 0.8.0** or higher.",0,0,msr
4356,"#3791 switched the Erlang parser to a different maintained repository. To make sure you update to the new one, do `:TSUninstall erlang` followed by `:TSInstall erlang` (making sure to restart Neovim and removing the impatient Lua cache if used). **Third-party modules providing erlang queries need to adapt to the new parser.**",0,0,msr
4357,"#4524 rework indentation styling/format to be aligned with upstream, any plugins/modules that modified this will have to **use the new captures specified in the docs**",0,0,msr
4358,"#4593 renamed the `help` parser to `vimdoc`, following the upstream change. Make sure to change `ensure_installed` and any language-specific highlight groups accordingly.",0,0,msr
4359,"#4944 switched the Matlab parser to a different maintained repository. To make sure you update to the new one, do `:TSUninstall matlab` followed by `:TSInstall matlab` (making sure to restart Neovim after updating nvim-treesitter). Third-party modules providing `matlab` queries need to adapt to the new parser.",0,0,msr
4360,"#5185 changed the upstream proto parser to a maintained one, just update the parser with `:TSUninstall proto` and `:TSInstall proto`",0,0,msr
4361,"#5234 raised the **minimum Neovim version to v0.9.1** and changed the format of all injection queries to the upstream syntax. If you need to stay on Nvim 0.8.x, lock nvim-treesitter to **v0.9.1**.",1,0,msr
4362,"https://github.com/nvim-treesitter/nvim-treesitter/pull/5222 switched the upstream perl parser to a maintained one, just update the parser with :TSUninstall perl and :TSInstall perl",0,0,msr
4363,"In short running doctrine-data-fixtures test suite
=> requires doctrine-orm
=> use doctrine-annotations",0,0,msr
4364,"```
$ phpcompatinfo analyser:run lib/
...
Classes Analysis
----------------
Class REF EXT min/Max PHP min/Max ArrayIterator spl 5.0.0 5.0.0 BadMethodCallException spl 5.1.0 5.1.0 DOMDocument dom 5.0.0 5.0.0 U Doctrine\Common\Annotations\AnnotationReader user 5.3.0 U Doctrine\Common\Annotations\CachedReader user 5.3.0 U Doctrine\Common\Annotations\SimpleAnnotationReader user 5.3.0 U Doctrine\Common\Cache\ApcuCache user 5.3.0 U Doctrine\Common\Cache\ArrayCache user 5.3.0 U Doctrine\Common\Cache\Cache user 5.3.0 U Doctrine\Common\Cache\MemcachedCache user 5.3.0 U Doctrine\Common\Cache\RedisCache user 5.3.0 CU Doctrine\Common\ClassLoader user 0.1.0 5.3.0 U Doctrine\Common\Collections\ArrayCollection user 5.3.0 U Doctrine\Common\Collections\Collection user 5.3.0 U Doctrine\Common\Collections\Criteria user 5.3.0 U Doctrine\Common\Collections\Expr\Comparison user 5.3.0 U Doctrine\Common\Collections\Expr\CompositeExpression user 5.3.0 U Doctrine\Common\Collections\Expr\Value user 5.3.0 ...
```
So doctrine-annotations is used",1,0,msr
4365,"It is used for optional functionality. If the data fixtures test suite makes use of annotations, that's where the `doctrine/annotations` dependency should be added.",0,0,msr
4366,"Sorry can't agree... data fixtures don't use annotations, it useq orm, wich use annotations
BTW, do the fuck you want with your project, at least you are aware of the problem.
P.S. previously annotations was pulled by persistence (but dep was removed there)",1,1,msr
4367,"> do the fuck you want with your project
Okay, closing the PR then.",1,1,msr
4368,See /usr/share/php/Doctrine/ORM/Configuration.php,0,0,msr
4369,"How are you running a plugin on Forge?
Most of these Bukkit/Sponge/whatever else hybrid solutions leave a lot to desired, it is my guess that whatever you're using is not suited for this purpose.
It looks to me like the server's commands are simply not being sent to the client in advance, so our addition of client commands are taking precedence (as they should) and causing the server to not be able to evaluate the command.
I suggest you take this up with the plugin system, not here. We do everything we can.",1,1,msr
4370,"However, if you can reproduce an issue where the [server correctly sends the commands](https://wiki.vg/Protocol#Declare_Commands) and we ignore it. Then we can take a look. However in all our tests, even non-vanilla commands are processed correctly.",0,1,msr
4371,"> How are you running a plugin on Forge?
> > Most of these Bukkit/Sponge/whatever else hybrid solutions leave a lot to desired, it is my guess that whatever you're using is not suited for this purpose.
> > It looks to me like the server's commands are simply not being sent to the client in advance, so our addition of client commands are taking precedence (as they should) and causing the server to not be able to evaluate the command.
> > I suggest you take this up with the plugin system, not here. We do everything we can.
I'm not sure how you've made this out that I'm running a plugin on Forge. I clearly and purely state I'm running the LuckPerms plugin on a Minecraft 1.18.1 Paper **SERVER**, and I'm running a **CLIENT** with Forge with a controllable mod and optifine in the range of the versions clearly stated above.",0,1,msr
4372,"Your steps imply to install Forge on the server, so that is not clearly and purely stated.
However, the issue remains the same.",0,0,msr
4373,"> Your steps imply to install Forge on the server, so that is not clearly and purely stated.
> > However, the issue remains the same.
I have corrected that with an edit, however it should have been implied that forge was installed on the client with the very first line of the issue ticket if it was read.
Yes indeed, the issue still remains and can verify on fresh installs of multiple machines that an update from 39.0.45 does indeed break the luckperm commands.",1,0,msr
4374,"#7754 - 39.0.46 Add Client Commands
Extensive testing and searching shows that issue does indeed start at Forge Version 39.0.46 when PR for ""Add Client Commands"" was requested.",0,0,msr
4375,"Temporarily locking this issue as nothing is being added.
We know what caused this. We're trying to see if there's anything we can do to fix this, but i honestly doubt it.
LuckPerms (or Spigot/Paper/Whatever) isn't sending the client information about the commands.",0,1,msr
4376,"For those who are still keeping track of this, it seems like the issue here is that the commands *are* being sent, but only via the command tree, and not as executable commands.
For LuckPerms this can be traced to their use of Commodore, but both use the old (< 1.13) methods of registering commands, which is why they broke when Forge updated to explicitly require support for these.
From the author of the Client Commands PR that exposed this problem:
> seems like both bungeecord and luckperms send the commands but they don't mark them as executable since they are only sending them in brigadier format for the client since they both still use the command registration method that was removed in 1.13 on the backend",1,0,msr
4377,"**Minecraft Version:** {Minecraft version}
Minecraft 1.18.1 Client with Forge (Optifine & Controllable Mods)
**Forge Version:** {Forge version. *Version number, not latest/rb*}
Forge Mod 39.0.46 - 39.0.63 (Issue Versions)
Forge Mod 39.0.40 - 39.0.45 (Working Versions)
**Logs:** {Link(s) to GitHub Gist with full latest.log and/or crash report}
https://paste.ee/p/nFncj - Debug Log
https://paste.ee/p/nknCg - Latest Log (Edited for redaction)
**Steps to Reproduce:**
1. Run Minecraft 1.18.1 Server with LuckPerms Plugin (v5.3.98)
2. Install Forge Mod version between 39.0.46 - 39.0.63 on client running forge of versions above
3. Run any LuckPerms server side issued command to verify issue on client running forge of versions above
**Description of issue:**
When running seemingly any version of Forge 39.0.46 through 39.0.63 (latest as of submission), this causes the client to not register any Luck Permissions plugin command on server as if command is not being entered at all. There is no log of event on client or server side, it just purely acts like the command isn't being entered and nothing happens. _This issue **DOES NOT** happen as tested with Forge Versions 39.0.40 or 39.0.45 and works as expected._",0,0,msr
4378,"### Brief description of your issue
We've encountered an upstream issue impacting our ability to validate and publish manifests. We've already engaged with the other teams involved and are working towards a quick resolution. ### Steps to reproduce
Submit a PR
### Expected behavior
The pipelines should run to completion.
### Actual behavior
The pipelines are failing due to parallelism.
### Environment
```shell
All environments and pipelines are affected.
```",0,0,msr
4379,"## The issues
Many polite attempts were made to resolve the disagreements with reasoning, most of them were [ignored]( https://github.com/whatwg/html/issues/5811#issuecomment-965201571 ). This complaint abandons that approach to describe the severity of the issues with clarity.
### Inappropriate procedure
The [proposal]( https://github.com/whatwg/html/issues/5811#issue-677390392 ) and the discussion did not follow the proper procedure outlined in the [new features guideline]( https://whatwg.org/faq#adding-new-features ), yet it was accepted as the de facto solution. The following steps were skipped:
1. ""Forget about the particular solution you have in mind!"" - The lead editor had a specific solution in mind: [""the use case for this proposal is when you don't want a <form>""]( https://twitter.com/domenic/status/1438229658324111360 ) and aggressively rejected any other solution.
1. ""What are the use cases?"" - I had to collect [these]( https://kaleidea.github.io/whatwg-search-proposal/#use-cases )
1. ""list requirements for each use case""
1. ""Ask fellow web developers about their opinions"" - Domenic described [web developers' input]( https://kaleidea.github.io/whatwg-search-proposal/#:~:text=Optional%3A%20Clarification%20notes-,Requests%20for%20this%20feature,-tweet%20%2D%20%E2%80%9CUsing%20a ) as [""confusion""]( https://github.com/whatwg/html/issues/5811#issuecomment-956454034 ) and as [""makes no sense""]( https://twitter.com/domenic/status/1438229658324111360 ). Incorrect and very disrespectful.
1. ""Research existing solutions."" - Not even the [best practice]( https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles/Search_role ) was considered.
1. ""Come up with new solutions."" - The only new solution was [immediately rejected]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ) without consideration.
1. ""Evaluate how well each of the remaining solutions address each use case and how well they meet the requirements."" - Domenic left the discussion after the new solution was proposed, his [last comment]( https://github.com/whatwg/html/issues/5811#issuecomment-965201571 ) was personal and inappropriate.
### Inconsistency with the WHATWG principles: openness, efficiency
I've worked weeks to research this problem domain, compared to the few hours demonstrated by the editor. Instead of appreciating my hard work, he [immediately closed]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ), [censored]( https://github.com/whatwg/html/pull/7320#pullrequestreview-802253725 ) and [rejected]( https://github.com/whatwg/html/issues/5811#issuecomment-965201571 ) it: ""I'll be closing or marking as off-topic any such discussions outside of this issue.""
Recently the editor prevented me from contributing 2 months' work to Chromium by revoking access to chromestatus.com, when I asked to file an *Intent to prototype*. Progress has been stalled since then. By doing so he also prevented the involvement of the chromium developer community, who might have views different from his.",1,1,msr
4380,"## Accountability
The editor refused to answer the following [questions]( https://github.com/whatwg/html/issues/5811#issuecomment-964879335 ) fundamental to his decisions:
1. Why inheritance is unviable? (Why duplication is, is obvious.)
1. What form functionality is unwanted (explicitly opposed) for the `<search>` element with `div` semantics?
1. What developer feedback led to that decision?
Related, but [different]( https://github.com/whatwg/html/issues/5811#issuecomment-979156406 ):
4. Why do you assume that form functionality is ""unviable""?
All these assumptions were proven wrong. The editor is expected to answer these questions and present verifiable (reproducible) evidence to justify his decisions.
Further questions:
5. Have you discussed with the chromium team the proposal with form functionality before [rejecting it]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ) in November? Link to public records.
5. Who discussed it from the chromium team at that time?",1,0,msr
4381,"### Working mode violation
[Working mode]( https://whatwg.org/working-mode#conflicts ):
> In case of a conflict among the community of contributors, the editor is expected to go to significant length to resolve disagreements.
The editor made no attempt to resolve the disagreement, instead made a number of [attempts to suppress]( https://github.com/whatwg/sg/issues/186 ) this POV, to prevent me from contributing and getting in contact with chromium developers who might have different thoughts.
### Bad decisions
The editor based its decisions on the following flawed assumptions and misunderstandings:
1. [comment]( https://github.com/whatwg/html/issues/5811#issuecomment-958169793 ): ""The form element is quite magical, and duplicating that magic into another element is a huge implementation and spec lift""
- Nobody proposed duplication. Reuse is key to efficient and maintainable software.
1. [comment]( https://github.com/whatwg/html/issues/5811#issuecomment-961877504 ): 'a new element ""inheriting from form"" is not viable'
- The rejected solution is trivially implemented without inheritance in all 3 main browsers. Its impact on the ecosystem is similarly trivial.
These claims were repeated multiple times after being disproven, creating counterproductive noise and misleading other participants.",0,0,msr
4382,"## Recommended solution
I offer a mutually beneficial solution to the editor: there are many features in progress that are more relevant, impactful, and important for the editor. This, however, is a simple, low-priority feature that received only a few hours of attention from the editor and none in the last months. The editor's extensive standardization experience creates more value applied to those more important features.
I wrote the [specification and implemented]( https://kaleidea.github.io/whatwg-search-proposal/ ) the search element in 3 browsers. I've invested more effort into it than all other participants combined. I offer to take on the responsibility of being the editor for this topic, so Domenic can focus on more valuable topics and both our work benefits the HTML standard. This is a very generous offer in [the current context]( https://github.com/whatwg/sg/issues/186 ).
To address the concerns of other participants about the assumed risks I propose to run the origin trial with form functionality and evaluate the feedback from web developers. In case the risks are proven to be substantial the chosen solution will be without form functionality and I will submit the appropriate implementations.
I'm taking the brunt of the work upon myself to minimize the time investment needed from other participants. I hope this solution is an acceptable middle ground and the attitude towards my work will change for the better.",0,0,msr
4383,"> ""Ask fellow web developers about their opinions"" - Domenic described web developers' input as https://github.com/whatwg/html/issues/5811#issuecomment-956454034 and as [""makes no sense""](https://twitter.com/domenic/status/1438229658324111360). Incorrect and very disrespectful.
People very literally expressed confusion. That is not a disrespectful description.
Gonna go ahead and say what I don't think folks in the org will be able to: you've worn out all good will and everything about this is ridiculous.",1,1,msr
4384,"@Kaleidea I’ve blocked you for two weeks from any interactions with https://github.com/whatwg/ repos, based on assessing that your use of the issue tracker here has reached the point of being in violation of the GitHub Community Guidelines — in particular, https://docs.github.com/en/github/site-policy/github-community-guidelines#disrupting-the-experience-of-other-users “Disrupting the experience of other users”. There are 631 people watching this repo, and they have a reasonable expectation that moderation will be applied when necessary to ensure their time and attention is not negatively affected.",0,0,msr
4385,"This is a formal complaint to @domenic about editor decisions according to the procedure described in [workstream policy appeals]( https://whatwg.org/workstream-policy#appeals ), step 1: ""raise an issue formally for review by the Editor"".
Highlighting that Domenic is expected to answer the questions in section ""Accountability"".
If you wish to debate the claims made here, please do so in a separate comment, after answering the questions.
## The reason
### Why is this important?
DX. Developers wish that native HTML-JS solutions would be on par with the ease of use and efficiency provided by frameworks.
In this standardization process [developers' requests]( https://kaleidea.github.io/whatwg-search-proposal/#:~:text=Optional%3A%20Clarification%20notes-,Requests%20for%20this%20feature,-tweet%20%2D%20%E2%80%9CUsing%20a ) were ignored and [inappropriately described]( https://github.com/whatwg/html/issues/5811#issuecomment-956454034 ) as ""confusion"".
### Why do I disagree with multiple participants?
EDIT: As a seasoned software architect I outrank other participants in the $topic domain of software design. I see the solutions where many others see unmanagable complexity.
No other participant has done the necessary research to prepare this standard. I have spent about 2 months researching and implementing this feature in all 3 main browsers. With that knowledge, as a seasoned software architect, I have much deeper understanding of the implementation details and risks. As a comparison, I estimate other participants spent a few hours (few comments), Scott a few days (specification + examples).
I have found that the claims of the other participants (Domenic and Scott) are technically wrong assumptions that have no evidence. They did not provide evidence for their claims when [asked to do so]( https://github.com/whatwg/html/issues/5811#issuecomment-964879335 ). The request was simply ignored.
I have documented the research and evidence in 15+ page-length comments and a [proposal]( https://kaleidea.github.io/whatwg-search-proposal ). They have repeatedly rejected and ignored the documentation and evidence that I've presented, for ex.
- Domenic has [immediately closed]( https://github.com/whatwg/html/issues/7323#issuecomment-965145881 ) the issue I've filed.
- Scott has repeated in the [January triage meeting]( https://github.com/whatwg/html/issues/7385 ) the first fundamental question (""people who don't want form behavior"") that has been answered repeatedly, months ago: [1]( https://kaleidea.github.io/whatwg-search-proposal/div-functionality ), [2]( https://github.com/whatwg/html/issues/5811#issuecomment-961785189 ), [3]( https://github.com/whatwg/html/issues/5811#issuecomment-981973258 ).
Professional collaboration cannot be conducted in this manner. If other participants would have done the necessary research or just respected the enormous effort I did, they would understand their concerns are non-existent risks.",1,1,msr
4386,https://github.com/GTNewHorizons/GT-New-Horizons-Modpack/issues/7555,0,0,msr
4387,"> #7555
What kind of lunacy is this, this isn't the fucking NSA I just want easy access to do my job ffs. I don't give a shit about user privacy, they forfeit that by playing on servers which are not theirs. Not being funny but that entire ticket and the next linked one reeks of people who have never actually had to moderate in their lives. ""Just fix the dupe bugs"" jesus christ.",1,1,msr
4388,"I think this is a great change honestly, Colen is probably the best and most responsible moderator I've ever met. If he needs this access then it must be fully justified. I highly support this change. - Kitten",0,0,msr
4389,@boubou19 Do you remember why [#7555 ](https://github.com/GTNewHorizons/GT-New-Horizons-Modpack/issues/7555) was closed? I remember something like it turned out to be working as is.,0,0,msr
4390,"it was closed because turns out i can read the items in the terminal, just not take them. But idk if it's due to the fact that default user has only read access or if it's a special perm for OP players. That was enough for my needs at the time so i didn't bothered much.
If you ask me, that protection is purely silly and operators should have full access to any stuff on the server. If the operator is doing questionable actions with his powers then that's on Dream and I because we trusted the wrong person to be staff. And if any player complains about this because we would gain full access to their AE, i want to say that we can bypass it in several ways, the most convoluted one being direct nbt edit from server files. And that player is free from not playing on the officials, if he doesn't trust the team.",1,0,msr
4391,"Eh, I kind of agree, but also, read access seems to be quite enough. I mean if OP spots something illegal, he can take administrative measures and/or break security terminal. I don't see any reason for OP to take out stuff secretly.",1,0,msr
4392,"i agree too, but sometimes it's unfair to break the terminal or hijack the storage to remove something from it, because if one player is faulty we shouldn't bother the whole team.",0,0,msr
4393,I just hid my hacked items in a chest below my dirt floor.,1,0,msr
4394,"i just thought about it: why not deliver the full access to operators based on their OP level? iirc command blocks and default opped players are only level 2, and the top level is level 4. For exemple on the officials, only the admins are level 4. could be a command based tool, like /toggleAEfullAccess that would grant it temporarily (not persistant to reboot) the access of the trusted level 4 operator",0,0,msr
4395,"> Eh, I kind of agree, but also, read access seems to be quite enough. I mean if OP spots something illegal, he can take administrative measures and/or break security terminal. I don't see any reason for OP to take out stuff secretly.
Not always so simple, they can hide items inside bags, compressed chests or other storage items. Players are very creative and I've seen them cheat in more ways than I have words over the years.",1,0,msr
4396,"ok, command based can be implemented somewhat naturally",0,0,msr
4397,"> ok, command based can be implemented somewhat naturally
Couldn't AE just cache a list of people on the OP txt list at startup and then add their names/UUID however it works to the security terminal as joint highest perms? Or yea a command that allows us to add individual users to the list",1,0,msr
4398,"### Your GTNH Discord Username
Colen
### Your Pack Version
2.1.2.1
### Your Proposal
See title, pretty self explanatory and useful for server admins trying to investigate. @repo-alt not sure how hard this would be?
### Final Checklist
- [X] I have searched this issue tracker and there is nothing similar already. Posting on a closed issue saying *I like this change please reconsider adding it* will prompt us to investigate and reopen it once we confirm your report.
- [X] I understand this change request may not attract enough attention and thus not be implemented.
- [X] I understand this change request may be rejected due to other community members think it's inappropriate.
- [X] I believe this feature would make the pack better.",0,0,msr
4399,"We've multiple users using this without these issues and i'm missing a LOT of required information in this buigreport.
Hence I would suggest going through our support process before (re)opening this bugreport.
So i'm closing this for now, untill the support staff can verify your setup is, in fact, correct.
(including possible permission issues)",1,0,msr
4400,"I made literally 1 change to the config: change VPN to disabled. It works as intended. That's literally the ONLY change I made. As for information, I've provided all the information requested, and I'm happy to provide more if you could tell me what you would like me to provide.",0,1,msr
4401,reopened due to verification by @Heavybullets8,0,0,msr
4402,"So it seems Wireguard is working just fine with hostpath, but OpenVPN is not. Below are both of my configurations.
**Here is the Hostpath I used for both Wireguard and OpenVPN**
![hostpath](https://user-images.githubusercontent.com/20793231/153766868-d858294c-2fd5-4b8f-84ad-e9c53f7561f7.PNG)
**OpenVPN configuration**
![ovpn1](https://user-images.githubusercontent.com/20793231/153766881-84193291-db02-4ed1-8c8d-c1dd675180c6.PNG)
**The Shell doesn't display a ""welcome"" message like it does with the Wireguard configuration you'll see below. Also no `/torrent` mountpath**
![ovpn2](https://user-images.githubusercontent.com/20793231/153766885-b034467b-abb4-46e7-9f9b-7ea91d84c3e5.PNG)
**Wireguard Configuration**
![wg1](https://user-images.githubusercontent.com/20793231/153767273-e4f03cb8-2133-4026-a4df-349a42452697.PNG)
**Wireguard Shell. with properly mounted `/torrent` as well as welcome message.**
![wg2](https://user-images.githubusercontent.com/20793231/153766977-0116ad66-5c71-4406-9cf2-a9749040dd23.PNG)",1,0,msr
4403,"FWIW, I'm using OpenVPN and the HostPath part works fine. So, maybe this issue is triggered by some particular order of setting things up.
(I only landed here because I seem to have connectivity issues when using OpenVPN...)",0,0,msr
4404,@indivisionjoe Can you confirm you are also doing that in qbittorrent?,0,0,msr
4405,"This also happened on Prowlarr, I just don't give Prowlarr any mounts so it didn't effect me. I assume it's just a generic issue with the way TrueCharts does OpenVPN.",0,0,msr
4406,I was not asking you @Nolij,0,1,msr
4407,"> I was not asking you @Nolij
I understand that, just adding additional information as it's discovered",0,0,msr
4408,"This also affects transmission:
[ticket-[1883677](https://discordapp.com/channels/830763548678291466/948718459066400798)](https://discordapp.com/channels/830763548678291466/948718459066400798)",0,0,msr
4409,"> This also affects transmission:
> [ticket-[1883677](https://discordapp.com/channels/830763548678291466/948718459066400798)](https://discordapp.com/channels/830763548678291466/948718459066400798)
Thanks",0,0,msr
4410,"Still seems to be an issue on my end, I just stumbled upon the problem while trying to add my NordVPN conf file to qBittorrent...
I get the following error:
`/usr/bin/openvpn.sh: line 347: H8Q: unbound variable`",1,1,msr
4411,"Please do not +1
OBVIOUSLY it's still an issue because the issue isn't closed.",0,1,msr
4412,"Just came here to add I am experiencing the same issue, with openvpn the filepath mount does not work. Only changes to default app settings are adding openvpn path and login credentials then adding hostpath mount point. App shell shows files are downloading into the internal container directory, but host path seems to be disconnected. EDIT: Had some issues that pertained to transmission app and this similar issue, erased for clarity. EDIT: Would like to add, the OpenVPN config IS working, running a bash from the App shell does show my VPN's IP not mine",0,0,msr
4413,"I'm going to lock this and contratz with the timeout.
When a maintainer tells you to STFU with the +1 comments, you stfu with the +1 comments.",1,1,msr
4414,"<!--
MIND YOUR TITLE:
- ""App in a deploying state"" is NOT the title/description of your actual bug.
- ""Appname not working"" is NOT the title/description of your actual bug.
- Don't refer to a version, bugs are always for latest version
-->
***SCALE version***
<!--
- the version of truenas scale you are currently running.
- FOUND IN: System settings > general > os version
-->
TrueNAS-SCALE-22.02-RC.2 (with RC2 patcher applied)
***App Version***
<!--
- the version of the app having the issue and versions of any apps being used in conjunction
- FOUND IN: apps > installed applications
-->
4.4.0_9.0.44
***Application Events***
<!--
- debug information from the app(s) specifically
- FOUND IN: apps > installed applications > (click on the app name) > click the carrot next to application events and include a screenshot here
-->
```
2022-02-13 10:16:47
Started container openvpn
2022-02-13 10:16:47
Created container openvpn
2022-02-13 10:16:34
Container image ""ghcr.io/truecharts/openvpn-client:latest@sha256:bc3a56b2c195a4b4ce5c67fb0c209f38036521ebd316df2a7d68b425b9c48b30"" already present on machine
2022-02-13 10:16:34
Started container qbittorrent-test
2022-02-13 10:16:33
Created container qbittorrent-test
2022-02-13 10:16:31
Container image ""tccr.io/truecharts/qbittorrent:v4.4.0@sha256:b96e8102193a3be4a85cbaba167e656ed9ad1b3d86f9df0dd94de805daab28f6"" already present on machine
2022-02-13 10:16:31
Started container inotify
2022-02-13 10:16:30
Created container inotify
2022-02-13 10:16:23
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-02-13 10:16:22
Started container autopermissions
2022-02-13 10:16:20
Started container lb-port-6880
2022-02-13 10:16:20
Created container lb-port-6880
2022-02-13 10:16:20
Created container autopermissions
2022-02-13 10:16:19
Started container lb-port-10096
2022-02-13 10:16:19
Created container lb-port-10096
2022-02-13 10:16:18
Started container lb-port-6882
2022-02-13 10:16:18
Created container lb-port-6882
2022-02-13 10:15:58
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:58
Add eth0 [172.16.28.157/16] from ix-net
2022-02-13 10:15:57
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-02-13 10:15:57
Add eth0 [172.16.28.156/16] from ix-net
2022-02-13 10:15:56
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:56
Add eth0 [172.16.28.155/16] from ix-net
2022-02-13 10:15:55
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:55
Add eth0 [172.16.28.154/16] from ix-net
Successfully assigned ix-qbittorrent-test/qbittorrent-test-db5cdb75b-bqqkb to ix-truenas
2022-02-13 10:15:40
Successfully provisioned volume pvc-1137f436-f444-4e56-89f6-9b41c9362217
0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims.
2022-02-13 10:15:39
Created pod: qbittorrent-test-db5cdb75b-bqqkb
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-torrentudp-9k8kl to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-torrentudp-9k8kl
2022-02-13 10:15:39
Scaled up replica set qbittorrent-test-db5cdb75b to 1
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-torrent-x66lm to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-torrent-x66lm
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-blmph to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-blmph
2022-02-13 10:15:39
waiting for a volume to be created, either by external provisioner ""zfs.csi.openebs.io"" or manually created by system administrator
2022-02-13 10:15:39
External provisioner is provisioning volume for claim ""ix-qbittorrent-test/qbittorrent-test-config""
```
***Application Logs***
<!--
- log output from the containers involved in the app
- FOUND IN: apps > installed applications > (click the 3 dots on the top right of the app box) > logs > open the logs for each pod used for the app and take a screenshot
- it can take a moment for the logs to show on the logs screen
-->
[qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_qbittorrent-test.log](https://github.com/truecharts/apps/files/8055909/qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_qbittorrent-test.log)
[qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_openvpn.log](https://github.com/truecharts/apps/files/8055910/qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_openvpn.log)
***Application Configuration***
<!--
- the configuration settings for the app (make sure to include what you have changed and what you didnt change
- if possible use the edit features in your screenshot tool to highlight the portions you cnaged intentionally
- FOUND IN: apps > installed applications > (click the 3 dots on the top right of the app box) edit
-->
![image](https://user-images.githubusercontent.com/16640162/153760346-a830501b-5186-45d1-ae14-54e72f7ba3a5.png)
![image](https://user-images.githubusercontent.com/16640162/153760372-578cd225-a8d6-410a-8188-27a43b3ffa90.png)
![image](https://user-images.githubusercontent.com/16640162/153760417-a031ae74-727b-4d95-90ac-86e64fbc0505.png)
(Synapse Data is a currently unused dataset I attached for testing purposes, my primary instance of this also experienced this on an almost identical configuration so I'm using a proxy, which is not as good for seeding, but nevertheless I wanted to keep it up while this is fixed)
**Describe the bug**
<!--
A clear and concise description of what the bug is.
-->
When using an OpenVPN connection (I suspect it is the same on WireGuard, but I have not tried that as I do not have access to a WireGuard VPN as of now) on a qBitTorrent app hostPath mounts are not mounted properly (also tried PVC, that didn't work either). The connection works (the logs show the VPN IP, not my IP), and adding and downloading torrents works, it just doesn't download them to my torrent dataset, making them inaccessible. This exact same configuration with VPN disabled works as intended, just without the VPN.
**To Reproduce**
<!--
- make sure that if you follow these steps again yourself the bug happens again. if it doesnt its probbly a configuration error and should be handled with a support thread on the discord
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error
-->
1. Create a qBitTorrent app
2. Configure a hostPath/PVC mount
3. Configure a valid OpenVPN connection
4. Configure the app so it should function (proper ports, etc.)
5. Start this app
6. To observe: open shell for the qBitTorrent container (not the OpenVPN container, although nothing appears there either). The hostPath/PVC mount will not be mounted.
**Expected behavior**
<!--
A clear and concise description of what you expected to happen.
-->
The configured mount should be there.
**Screenshots**
<!--
If applicable, add screenshots to help explain your problem.
-->
**Additional context**
<!--
Add any other context about the problem here.
-->",1,0,msr
4415,To be frank: I've not seen any more reports of this in latest releases after the bug refactor and we never where able to correctly reproduce and/or bugtrace this either. Please create a new issue if this issue still exists.,0,1,msr
4416,"Sorry, we are not able to clone repos and debug user's code.
Please, provide a jsfiddle that shows the issue. Closing until then.",0,0,msr
4417,"Well that's unfortunate, because it really does happen. Here's a GPU profile:
![image](https://user-images.githubusercontent.com/1668207/153904707-571fc306-d648-4d95-a291-98270223555b.png)
The throttled start is using DodecahedronGeometry, the spiked usages is using BoxGeometry.
I'm not sure if this is reproducible in a service like JSFiddle, as it's not the same as running a local instance, but I might give it a shot.",0,0,msr
4418,"> Disclaimer: code is a bit of a mess currently because I'm moving things around and trying to fix this issue.
We try to give you (and all the library users) clear code.
Would be nice if you could help us back by giving us clear code too.",0,1,msr
4419,"It's clear enough to see what's going on. If you want I can make a branch with the bare minimum (it would just have certain files removed).
I'm not sure why sharing a repo is an issue? It's quite literally the same as standing up a new one as a repro ;)",0,1,msr
4420,"<!-- Ignoring this template may result in your bug report getting deleted -->
**Describe the bug**
I'm building a small game that uses some primitive shapes for things.
Whenever I use a PolyhedronGeometry (like DodecahedronGeometry or IcosahedronGeometry) in an instancedMesh, my GPU tanks to ~5FPS (depending on the amount of instances).
I've spent hours figuring out what is happening and yesterday it looked like `powerPreference: 'high-performance'` was the issue (disabling solved the issue), but today it's back again with the same issues.
So this might not be threejs directly, but rather a webgl problem?
it's very hard to reproduce, because in FireFox this doesn't happen, only in Chrome, on my 2016 MacBook pro, so I feel like it might even be the webgl implementation of Chrome itself.
Interestingly enough FPS are just fine with other geometries (like box, or cone, etc), just not any of the PolyhedronGeometry based ones.
**To Reproduce**
You can clone my repo (https://github.com/TheDutchCoder/atc-gjk) and on the `convert-to-native-js` branch you can run `npm run dev`.
If you see good FPS, please let me know which browser and OS you're using, because again I feel like this might even be a browser problem.
Disclaimer: code is a bit of a mess currently because I'm moving things around and trying to fix this issue.
***Code***
See repo.
***Live example***
N/A (see repo)
**Expected behavior**
When using a PolyhedronGeometry I should not see a huge drop in FPS.
**Screenshots**
![image](https://user-images.githubusercontent.com/1668207/153903237-5989947d-16c9-422a-b737-63fea8f00ab4.png)
Note that the performance stats seem normal
**Platform:**
- Device: Desktop, MacBook Pro 2016
- OS: MacOS 12.1
- Browser: Chrome 98.0.4758.80
- Three.js version: r137 (latest)",1,0,msr
4421,I never paid attention to the possibility to assign a scope to a setting. What you suggest sounds reasonable. I guess other settings should also have a `resource` scope. We should try to make a list.,0,0,msr
4422,"Maybe an inverse list is appropriate, too. I went through the list of settings
https://github.com/James-Yu/LaTeX-Workshop/blob/04f73108e439661faef2c8f85eac7baacbdb736b/package.json#L763-L2128
and to be honest, I had a hard time finding settings which should remain `window`-scoped. (I always wonder why that is the default - probably historic reasons). Everything related to editing or compiling a specific file (editor settings, syntax, tools, etc.) should be `resource`-scope IMHO.
The ones I found that *may* not be correct to scope by `resource` are the ones that may not be uniquely tied to a single document, such as everything related to (pre-)loading and configuring extensions used by multiple documents:
```
latex-workshop.view.pdf.*
latex-workshop.mathpreviewpanel.*
latex-workshop.hover.preview.mathjax.extensions
```",0,0,msr
4423,"Notice that we have to rewrite the way of calling `workspace.getConfiguration`.
See https://github.com/Microsoft/vscode/wiki/Adopting-Multi-Root-Workspace-APIs#settings-api
> The configuration example above defines a setting scoped to a resource. To fetch its value you use the workspace.getConfiguration() API and pass the URI of the resource as the second parameter. You can see [Example 3](https://github.com/Microsoft/vscode-extension-samples/blob/master/configuration-sample/src/extension.ts) how the setting is used in the basic-multi-root sample.
- https://github.com/microsoft/vscode-extension-samples/blob/e52c48f22a81355b568f3cde8c60538726caad22/configuration-sample/src/extension.ts#L76-L96",0,0,msr
4424,"My 2c comment on why `window` is the default scope. In VS Code, a window corresponds to an instance. This implies that only a single instance of an extension can run inside a window. Some settings require to reload the window after change typically because they are used in constructors (theirs values are fixed in the instance). All these settings must not have the `resource` scope. This is probably why the default scope is `window`.
Hence, making the list of settings that could have the `resource` scope is not so simple. All this requires a lot of care to avoid introducing very tricky bugs.",0,0,msr
4425,"> Some settings require to reload the window after change typically because they are used in constructors (theirs values are fixed in the instance).
Thanks, understood. This is what I meant when I used the somewhat ill-defined term ""pre-loading"", so I fully agree that the implementation determines whether or not a setting can be `resource`-scoped, and not the uninitiated issue author browsing by the code ;)",0,0,msr
4426,"Here is the list of properties that might be assigned a `resource` scope
```
latex-workshop.latex.recipe.default
latex-workshop.latex.tools
latex-workshop.latex.magic.args
latex-workshop.latex.magic.bib.args
latex-workshop.latex.external.build.command
latex-workshop.latex.external.build.args
latex-workshop.latex.build.forceRecipeUsage
latex-workshop.latex.outDir
latex-workshop.latex.texDirs
latex-workshop.latex.verbatimEnvs
latex-workshop.kpsewhich.path
latex-workshop.kpsewhich.enabled
latex-workshop.latex.bibDirs
latex-workshop.latex.search.rootFiles.include
latex-workshop.latex.search.rootFiles.exclude
latex-workshop.latex.rootFile.useSubFile
latex-workshop.latex.rootFile.doNotPrompt
latex-workshop.latex.autoBuild.run
latex-workshop.latex.autoBuild.interval
latex-workshop.latex.autoBuild.cleanAndRetry.enabled
latex-workshop.latex.build.clearLog.everyRecipeStep.enabled
latex-workshop.latex.autoClean.run
latex-workshop.latex.clean.subfolder.enabled
latex-workshop.latex.clean.fileTypes
latex-workshop.latex.clean.command
latex-workshop.latex.clean.args
latex-workshop.latex.clean.method
latex-workshop.latex.option.maxPrintLine.enabled
latex-workshop.view.pdf.viewer
latex-workshop.view.pdf.tab.editorGroup
latex-workshop.view.pdf.tab.openDelay
latex-workshop.view.pdf.ref.viewer
latex-workshop.view.pdf.internal.synctex.keybinding
latex-workshop.view.pdf.external.viewer.args
latex-workshop.view.pdf.external.synctex.command
latex-workshop.view.pdf.external.synctex.args
latex-workshop.synctex.path
latex-workshop.synctex.afterBuild.enabled
latex-workshop.synctex.synctexjs.enabled
latex-workshop.chktex.enabled
latex-workshop.chktex.run
latex-workshop.chktex.path
latex-workshop.chktex.args.active
latex-workshop.chktex.args.root
latex-workshop.chktex.delay
latex-workshop.chktex.convertOutput.column.enabled
latex-workshop.chktex.convertOutput.column.chktexrcTabSize
latex-workshop.texcount.path
latex-workshop.texcount.args
latex-workshop.texcount.autorun
latex-workshop.texcount.interval
latex-workshop.intellisense.citation.type
latex-workshop.intellisense.citation.label
latex-workshop.intellisense.citation.format
latex-workshop.intellisense.file.exclude
latex-workshop.intellisense.file.base
latex-workshop.intellisense.label.keyval
latex-workshop.intellisense.unimathsymbols.enabled
latex-workshop.intellisense.package.enabled
latex-workshop.intellisense.package.extra
latex-workshop.intellisense.package.dirs
latex-workshop.intellisense.includegraphics.preview.enabled
latex-workshop.message.badbox.show
latex-workshop.message.bibtexlog.exclude
latex-workshop.message.latexlog.exclude
latex-workshop.message.information.show
latex-workshop.message.warning.show
latex-workshop.message.error.show
latex-workshop.message.log.show
latex-workshop.latexindent.path
latex-workshop.latexindent.args
latex-workshop.hover.ref.enabled
latex-workshop.hover.ref.number.enabled
latex-workshop.hover.citation.enabled
latex-workshop.hover.command.enabled
latex-workshop.hover.preview.enabled
latex-workshop.hover.preview.maxLines
latex-workshop.hover.preview.scale
latex-workshop.hover.preview.newcommand.parseTeXFile.enabled
latex-workshop.hover.preview.newcommand.newcommandFile
latex-workshop.hover.preview.cursor.enabled
latex-workshop.hover.preview.cursor.symbol
latex-workshop.hover.preview.cursor.color
latex-workshop.intellisense.optionalArgsEntries.enabled
latex-workshop.intellisense.useTabStops.enabled
latex-workshop.texdoc.path
latex-workshop.texdoc.args
```
I do not think it really makes sense for all of them. In particular, I believe everything related to `intellisense`, `hover`, `kpsewhich`, `message` and `view` should not be changed. That would give a much shorter list
```
latex-workshop.latex.recipe.default
latex-workshop.latex.tools
latex-workshop.latex.magic.args
latex-workshop.latex.magic.bib.args
latex-workshop.latex.external.build.command
latex-workshop.latex.external.build.args
latex-workshop.latex.build.forceRecipeUsage
latex-workshop.latex.outDir
latex-workshop.latex.search.rootFiles.include
latex-workshop.latex.search.rootFiles.exclude
latex-workshop.latex.rootFile.useSubFile
latex-workshop.latex.rootFile.doNotPrompt
latex-workshop.latex.autoBuild.run
latex-workshop.latex.autoBuild.interval
latex-workshop.latex.autoBuild.cleanAndRetry.enabled
latex-workshop.latex.build.clearLog.everyRecipeStep.enabled
latex-workshop.latex.autoClean.run
latex-workshop.latex.clean.subfolder.enabled
latex-workshop.latex.clean.fileTypes
latex-workshop.latex.clean.command
latex-workshop.latex.clean.args
latex-workshop.latex.clean.method
latex-workshop.latex.option.maxPrintLine.enabled
latex-workshop.synctex.path
latex-workshop.synctex.afterBuild.enabled
latex-workshop.synctex.synctexjs.enabled
latex-workshop.chktex.enabled
latex-workshop.chktex.run
latex-workshop.chktex.path
latex-workshop.chktex.args.active
latex-workshop.chktex.args.root
latex-workshop.chktex.delay
latex-workshop.chktex.convertOutput.column.enabled
latex-workshop.chktex.convertOutput.column.chktexrcTabSize
latex-workshop.texcount.path
latex-workshop.texcount.args
latex-workshop.texcount.autorun
latex-workshop.texcount.interval
latex-workshop.intellisense.file.exclude
latex-workshop.intellisense.file.base
latex-workshop.latexindent.path
latex-workshop.latexindent.args
latex-workshop.texdoc.path
latex-workshop.texdoc.args
```
Any thoughts on this?",0,0,msr
4427,"> I believe everything related to `intellisense`, `hover`, `kpsewhich`, `message` and `view` should not be changed.
I wonder why that is. Is that motivated by the required implementation effort, or by your opinion that these should not be configurable by `resource` from a use-case perspective? As far as I have understood settings scopes, changing a `window`-scoped setting to a `resource`-scoped setting will not break anything at all for existing users - it will only open additional possibilities to move a setting into a resource-specific location. If the latter, I wonder what the motivation is to prevent users from, for example, using different `latex-workshop.intellisense.file.exclude` patterns for different documents, or from adapting `latex-workshop.intellisense.update.delay` for each document to account for different document sizes and thus processing times (just two examples, I might be able to think of more).",0,0,msr
4428,I agree the list is reasonable. We can discuss further on a pull request. We might have to keep some of them window-scoped due to the implementation difficulty.,0,0,msr
4429,"To make things crystal clear. Any properties not listed in my first list cannot be assigned a `resource` scope without heavy implementation changes, which we will not make. You mention `latex-workshop.intellisense.update.delay`, which was not in my fisrt and `latex-workshop.intellisense.file.exclude`, which I kept in the second one.
Because of the need to access the document `uri`, some of these properties may still be removed from the second list.
https://github.com/microsoft/vscode-extension-samples/blob/e52c48f22a81355b568f3cde8c60538726caad22/configuration-sample/src/extension.ts#L76-L96",0,0,msr
4430,"I suggest to wait until we know more about - microsoft/vscode#143462
as this breaks our logging mechanism for configuration properties https://github.com/James-Yu/LaTeX-Workshop/blob/d8fae99c6a58b860250d5354085ca4fd0212c686/src/components/configuration.ts#L44",0,0,msr
4431,"## Preliminary questions [Required]
### Disable all the other extensions except for LaTeX Workshop, restart VS Code, and check that you can not see the requested feature. [Required]
You still see this issue?: Yes
### Make sure to visit the [wiki FAQ](https://github.com/James-Yu/LaTeX-Workshop/wiki/FAQ) before requesting a feature.
You visited the wiki?: Yes
### If your requested feature is with compiling a document (not having to do with finding the root file of a project), check first that you can compile manually.
You can compile a TeX document manually?: Yes
## Is your requested feature related to a problem? Please describe. [Required]
I have two docs in a structure like this:
```
D:\BUG
│ bug.code-workspace
│
├───doc1
│ └───.vscode
│ settings.json
│
└───doc2
```
I open both docs in one VS Code window. This is what `bug.code-workspace` is for.
In `doc1`, I would like to set a few settings that I do not want to apply to `doc2`. This is what `doc1/.vscode/settings.json` is for, and it looks like this:
```json
{
""editor.wordWrapColumn"": 88,
""latex-workshop.latexindent.args"": [
""-c"",
""%DIR%/"",
""%TMPFILE%"",
""-y=defaultIndent: '%INDENT%'"",
""-l"",
],
}
```
So far, so good. However, this is not working because `latex-workshop.latexindent.args` is window-scoped, while `editor.wordWrapColumn` is resource-scoped. In line with that, the setting is greyed out and not applied:
![image](https://user-images.githubusercontent.com/12128514/153915001-83c233fe-c102-4f57-b00d-16e5d49028fb.png)
The reason is that the setting is not properly scoped - it has `window` scope:
https://github.com/microsoft/vscode/blob/6136c815bc6e7b99ec2dac56dccb3869574d2dd8/src/vs/workbench/contrib/preferences/browser/preferencesRenderers.ts#L596-L603
And the reason for that is that it does not have an explicit scope at all, so it falls back to `window` - see https://code.visualstudio.com/api/references/contribution-points:
https://github.com/James-Yu/LaTeX-Workshop/blob/04f73108e439661faef2c8f85eac7baacbdb736b/package.json#L1810-L1822
This is certainly not intentional - I see no reason for this setting to (necessarily) be the same for all resources in a window. It should be able to differ by resource (folder).
## Describe the solution you'd like [Required]
The fix is simple: set the scope for `latex-workshop.latexindent.args` (and probably other settings as well) to `resource`. Happy to provide a PR if that helps.",0,0,msr
4432,"As a continuation, here is that truth:
```
# command-line-arguments
./ispapp-go-client.go:236:20: uc.(*net.TCPConn).NetConn undefined (type *net.TCPConn has no field or method NetConn)
Andrews-MacBook-Pro:ispapp-go-client zip$ git diff
diff --git a/ispapp-go-client.go b/ispapp-go-client.go
index 2a188b3..216e993 100644
--- a/ispapp-go-client.go
+++ b/ispapp-go-client.go
@@ -233,7 +233,7 @@ func new_websocket(host *Host) {
fmt.Println(reflect.TypeOf(uc))
fmt.Printf(""%+v\n"", uc)
- //uc.conn.SetKeepAlive(true)
+ uc.(*net.TCPConn).NetConn().SetKeepAlive(true)
// set host.WanIfName
var ipaddrstr, port, iperr = net.SplitHostPort(c.LocalAddr().String())
```",1,1,msr
4433,The program is still not working as documented.,0,0,msr
4434,"Here is proof `net.TCPConn` has a SetKeepAlive method.
https://cs.opensource.google/go/go/+/refs/tags/go1.17.7:src/net/tcpsock.go;l=159",0,0,msr
4435,"The program is still not working as documented.
This is a bug in Go. Fix it or leave it open as the truth.",0,1,msr
4436,"This is how it actually works.
```
uc.(*net.TCPConn).SetKeepAlive(true)
```",0,0,msr
4437,"I need you to allow me to resolve the issue you incorrectly closed and locked me out of.
People need to know how to do this, every recent Apple laptop has power darkmode and it works with keep alive flagged tcp packets.",0,1,msr
4438,"Glad you were able to solve the issue with net.TCPConn assertion. For the record, I suggested a generic `tls.Conn` assertion because in the first post you were asking about `tls.Conn.NetConn( )`. Of course it's up to you to decide if that makes sense in your code.",0,0,msr
4439,"https://github.com/golang/go/issues/51274
You are supposed to be able to type-assert or cast a `net.Conn` to a `net.TCPConn`.
The go team member created a situation where he lied about that, then said there was no issue.",0,1,msr
4440,"`DecodeString` is (b64 encoded string) -> (unencoded []byte).
The reverse is `EncodeToString`, which does (unencoded []byte) -> (b64 encoded string).
I am not sure what else you are looking for.",0,0,msr
4441,"Is the function `EncodeToString` the behavior you want, and you think it should be named `EncodeString`?",0,0,msr
4442,"What is the exact API that you are proposing?
We currently have
```Go
// DecodeString returns the bytes represented by the base64 string s.
func (enc *Encoding) DecodeString(s string) ([]byte, error)
// EncodeToString returns the base64 encoding of src.
func (enc *Encoding) EncodeToString(src []byte) string
```
What are you suggesting that we add? Thanks.",0,0,msr
4443,"@ianlancetaylor Do you not understand that the input arguments to those functions are different?
If there is an Encode function there should be a Decode function with the same name structure and:
* the same input argument types
* the same output argument types
I realize what happened, someone was building it and needing to encode a byte array to a string so they wrote that function, then needed to decode a string to a byte array so they wrote that function.
When people read the documentation first, they are confused by the lack of a common naming structure and input/output type structure.
That should be cleaned and reorganized so that when people are working and reading the 1000's of Go functions (certainly impossible to remember all) they will not get frustrated by constant flipping between string and []byte and a lack of a naming structure that can be quickly remembered because it adheres to both common function names per functionality and input/output type.",0,1,msr
4444,"You must understand, your company can afford to pay people to do things like that and have a team leader structure that respects who actually made things.
That's why you have things like Gmail and Go, because you let people work and then lead other people to finish their work, not tried to confuse them with constant changes that no single person understands.
It was the same way at Ford, it was the same way at Chevy, it is the same way at Boeing, Lockheed Martin, etc....",0,1,msr
4445,"@andrewhodel Help us out a bit if you want us to consider your change. Please tell us exactly what method you want added. You've more than adequately defined the motivation - but we're still not clear on what exactly the changes you're proposing are. Write us a prototype of the function/method you want, and a 1-2 sentence godoc description of what it does. Thanks.",0,0,msr
4446,"@randall77 `EncodeString()` and `DecodeString()` should both exist because one of them does and the concept of encoding/decoding is fundamentally bound together.
I didn't read every function, but you should and you should adhere this naming structure and ensure that all relevant sets exist.
In other words
`Encode([]byte)` means there should be `Decode([]byte)`
`function_1_group_with_3_total()` should include `function_2_group_with_3_total()` and `function_3_group_with_3_total()` and so on. For example, with hash functions there is usually a standard naming format due to the required functions, that of `input`, `parse`, `decode`, `inputMore` etc.
Then when someone is working for example with the encoding/base64 library, they can set a number of days or hours aside to work with the library then get used to the functions. In other words, they can type Encode and Decode and EncodeString and DecodeString enough times to not need to deal with remembering arguments to every single function instead only needing to remember the top level function names as they all follow a common naming pattern.",0,1,msr
4447,"Ok, but you still haven't answered my question - what exactly are you proposing to add to `encoding/base64`?",0,1,msr
4448,"@randall77 I answered your question in the first sentence of the original reply to your original comment/question.
EncodeString() and DecodeString() should both exist because one of them does and the concept of encoding/decoding is fundamentally bound together.
These are functions in Go of the encoding/base64 library. One exists, one doesn't. Again, these are functions in Go.",0,1,msr
4449,"@andrewhodel That is not an answer to the question that @randall77 asked and it is not an answer to the question I asked.
I will repeat:
We currently have
```Go
// DecodeString returns the bytes represented by the base64 string s.
func (enc *Encoding) DecodeString(s string) ([]byte, error)
// EncodeToString returns the base64 encoding of src.
func (enc *Encoding) EncodeToString(src []byte) string
```
What are you suggesting that we add? Please write it in the exact form that I wrote it. This is a programming language. Precision matters. Please write down exactly what you mean. Don't describe what you mean in words. Write it in Go. Thanks.",0,1,msr
4450,"```
func (enc *Encoding) EncodeString(s string) ([]byte, error)
```
You should also read and make sure all the other pairs exist for encoding and decoding of, with respect to both input and output:
* byte arrays
* strings
If you want me to write it, then provide commit access.",0,0,msr
4451,"You can write any change you like without commit access, as described at https://go.dev/doc/contribute. Thanks.",0,0,msr
4452,"@ianlancetaylor then make:
@ianlancetaylor @prattmic @findleyr @cherrymui All work that way for every commit or stop with the absurdity.",0,1,msr
4453,We do all work that way for every commit. Why do you think we don't?,0,1,msr
4454,"@ianlancetaylor That's not true, you push changes directly to the repository.",0,1,msr
4455,"@andrewhodel That is not correct. Why do you think it is?
All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.",0,1,msr
4456,"You are adjusting time and commits to dev while calling it changes. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 10:58 AM, Ian Lance Taylor ***@***.***> wrote:
> > > @andrewhodel That is not correct. Why do you think it is?
> > All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
> > What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.
> > —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",0,0,msr
4457,"> You are adjusting time and commits to dev while calling it changes.
I'm sorry, I don't understand what you mean.",0,0,msr
4458,"I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:06 AM, Andrew Hodel ***@***.***> wrote:
> > You are adjusting time and commits to dev while calling it changes. > > Thank You,
> Andrew Hodel
> >>> On Feb 22, 2022, at 10:58 AM, Ian Lance Taylor ***@***.***> wrote:
>>> >> >> @andrewhodel That is not correct. Why do you think it is?
>> >> All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
>> >> What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.
>> >> —
>> Reply to this email directly, view it on GitHub, or unsubscribe.
>> You are receiving this because you were mentioned.",0,0,msr
4459,"I don't see why `EncodeString` should return an error. The other `Encode` functions do not. Base64 encoding and decoding are not symmetric in that encoding should always succeed, whereas decoding might not.
Other than that, seems like a reasonable function to have. Do you have a situation where you have a `string` source and wish to encode it to a `[]byte` output? Just wondering if this is a function you actually ran into a need for, or if you just were confused about the lack of a symmetric API.",0,0,msr
4460,"In any case, thanks for defining precisely what you suggest that we add (although you omitted the doc comment). I gather you are thinking of something along the lines of
```Go
// EncodeString returns the base64 encoding of s.
func (enc *Encoding) EncodeString(s string) ([]byte, error) {
buf := make([]byte, enc.EncodedLen(len(s)))
enc.Encode(buf, []byte(src))
return buf, nil
}
```
I think the argument that you are making is that this is parallel to the existing `DecodeString` method.
How often does a need for this method come up in practice? Do you have any examples of code that would use it? Thanks.",0,0,msr
4461,"Start counting them you will find out. It is basic manipulation of words causing the new words to exist. If you count the number of times someone commits vs the changes approved in that list you will get it. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:07 AM, Ian Lance Taylor ***@***.***> wrote:
> > > You are adjusting time and commits to dev while calling it changes.
> > I'm sorry, I don't understand what you mean.
> > —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",0,1,msr
4462,"Lack of symmetry. Yes you need an error, what if the memory bound is reached while reading?
Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:09 AM, Keith Randall ***@***.***> wrote:
> > > I don't see why EncodeString should return an error. The other Encode functions do not. Base64 encoding and decoding are not symmetric in that encoding should always succeed, whereas decoding might not.
> > Other than that, seems like a reasonable function to have. Do you have a situation where you have a string source and wish to encode it to a []byte output? Just wondering if this is a function you actually ran into a need for, or if you just were confused about the lack of a symmetric API.
> > —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",0,0,msr
4463,"> I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people.
OK, great. Do you want to try to prove it to me?
I mean, I know that I am telling you the truth about our code review process. I don't understand what you could do to demonstrate that I am not telling the truth. But I am mildly curious as to where the misunderstanding arises.",0,1,msr
4464,"> Yes you need an error, what if the memory bound is reached while reading?
If a Go program runs out of memory, it will crash. There is no way for a function to return an error because it has run out of memory.",0,0,msr
4465,"> If you count the number of times someone commits vs the changes approved in that list you will get it.
Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?",0,0,msr
4466,"Again. This is why other libraries have per iteration input functions, for reading streams etc. That isn’t to say that you could read the input string length, then read the available memory and know what difference the encoding process will require (bytes per chunk for input and output + anything else) and return an error when that is greater than n*available. That is how things work. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:13 AM, Ian Lance Taylor ***@***.***> wrote:
> > > If you count the number of times someone commits vs the changes approved in that list you will get it.
> > Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?
> > —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",0,1,msr
4467,"It is because you aren’t respecting the development branch as a place to read changes before versioned approval. In other words, why do you have release candidates?
Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:12 AM, Ian Lance Taylor ***@***.***> wrote:
> > > I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people.
> > OK, great. Do you want to try to prove it to me?
> > I mean, I know that I am telling you the truth about our code review process. I don't understand what you could do to demonstrate that I am not telling the truth. But I am mildly curious as to where the misunderstanding arises.
> > —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.",0,1,msr
4468,"These truths of memory testing (especially when it is very low resource usage to do so) are similar to network programming in a day and age that testing is sold for entertainment as blocking certain tcp packets against protocol. In other words it has to be dealt with anytime network traffic is greater than the packet MTU so may as well exist at the memory level. Thank You,
Andrew Hodel
> On Feb 22, 2022, at 11:23 AM, Andrew Hodel ***@***.***> wrote:
> > Again. This is why other libraries have per iteration input functions, for reading streams etc. > > That isn’t to say that you could read the input string length, then read the available memory and know what difference the encoding process will require (bytes per chunk for input and output + anything else) and return an error when that is greater than n*available. > > That is how things work. > > > > Thank You,
> Andrew Hodel
> >>> On Feb 22, 2022, at 11:13 AM, Ian Lance Taylor ***@***.***> wrote:
>>> >> >> If you count the number of times someone commits vs the changes approved in that list you will get it.
>> >> Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?
>> >> —
>> Reply to this email directly, view it on GitHub, or unsubscribe.
>> You are receiving this because you were mentioned.",0,0,msr
4469,"The current helper methods assume that base64 is applied to binary data, which is almost always a []byte.
And it is applied to create text data, which is often a string.
So the API uses []byte for the binary (unencoded) data, and string for the text (encoded) data.
This has worked very well in practice. There is no obvious reason we should add the other pairs, which would at the very least confuse me when reading the API.",0,0,msr
4470,"Based on the discussion above, this proposal seems like a **[likely decline](https://golang.org/s/proposal-status#likely-decline)**.
— rsc for the proposal review group",0,0,msr
4471,"I guess every middleman your Go servers work with support perfect utf8.
I need to send data to things so old that anything beyond 127 bits is out of the question so they can pass it on and then without base64 what are you going to do?
I know, another go type modification is just what the doctor called for!",0,1,msr
4472,"@rsc it's simple, you have a string and a validation function that ensures that string is ""at protocol"" meaning hasn't the distant device doesn't even fully support ASCII, then you need to pass it through that distant device so you base64 encode the validated string and send it.
What you are saying isn't real, there's no where on earth where an operating system requires code to not using bits and those work with Go servers that use strings for simplicity.",0,1,msr
4473,@andrewhodel I think it would help a lot if you made an example on https://go.dev/play/ to show your use-case and why this isn't trivially covered by the existing API.,0,0,msr
4474,"```
// working with string
var s = ""asdfStringWithoutUnicodeMoreThanOneTwentySeven"";
function_without_string_support([]byte(s));
```
Code reviewer: ""That is messy code, there are too many type modifications"".
I guess they will never figure it out, oh well *poof*.",0,1,msr
4475,"No change in consensus, so **[declined](https://golang.org/s/proposal-status#declined)**.
— rsc for the proposal review group",0,0,msr
4476,"The DecodeString() method exists as a single function, an EncodeString() method should be added as it will not break forward compatibility and conform to a standard naming structure and module offering.
https://pkg.go.dev/encoding/base64
This is not a duplicate of a bug, I was asked to make it a proposal regardless of it being a bug or proposal. This is a proposal in writing to a software repository.",0,0,msr
4477,![Platform: git--PaperSpigot--445%20%28MC%3A%201.8.8%29](https://img.shields.io/badge/Platform-git--PaperSpigot--445%20%28MC%3A%201.8.8%29-3498db?style=flat-square) </br>![ViaVersion (4.1.1): 15 commits behind master](https://img.shields.io/badge/ViaVersion%20(4.1.1)-15%20commit(s)%20behind%20master-yellow?style=flat-square) </br>,0,0,msr
4478,"Just a little side note, I also made an issue on ProtocolLib to see if they are willing to improve/fix their injector too. https://github.com/dmulloy2/ProtocolLib/issues/1522",0,0,msr
4479,"Absolutely agree, I had the same problems, Pipeline is not intended to work like ViaVersion actually does and packet transformations should be done on a separate channel.",0,0,msr
4480,Update: ProtocolLib is working on rewriting their injector. So hopefully ViaVersion can do some adjustments too.,0,0,msr
4481,Working on fixing Via’s injector in my fork: https://github.com/retrooper/ViaVersion,0,0,msr
4482,"It's long been done, but we're not going to change the injection method until the next major ViaVersion release, sorry.",0,0,msr
4483,When’s that. And howcome,0,0,msr
4484,"### `/viaversion dump` Output
https://dump.viaversion.com/f34803f4498497ce3b5d63c445f8ecbb6b3ce5f586662d643faef3e4aff1c447
### Console Error
Check bug description
### Bug Description
ViaVersion's spigot injector decides to wrap minecraft's vanilla encoder. I'm trying to make my own injector. I want to process buffers, that aren't translated by ViaVersion yet. It would be better if ViaVersion had a separate outgoing handler which were called after the vanilla minecraft encoder. Your handler can expect bytebufs and just translate them like that. Let the vanilla encoder do its job by translating NMS packets to bytebufs for you to process. This would make it so easy for any other packet plugin to just get in between ViaVersion and the vanilla encoder.
Similarly can be said about the decoder. You guys just wrap the minecraft vanilla decoder... I want to process packets right after ViaVersion translates incoming bytebufs. You could simply put ViaVersion's decoder infront of vanilla's decoder. Then I can just get inbetween and process my bytebufs there. ViaVersion isn't the only packet plugin out there, and I've also seen other developers finding making an injector a struggle. I managed to get a ""working"" decoder, but its really really not great and doesn't work great if you can't predict how many packet projects are running on the server. You'd also be saving yourself redundant reflection calls to vanilla handlers?
Additionally with what ProtocolLib does, calling ""encoder"" right after it processes the first NMS object... that just doesn't make making an injector easier.
I just want to know, is there a reason you did not do this? Seems like it would just be easier to handle things like that, worst case scenario if you guys don't want to fix your injector, I could do it.
### Steps to Reproduce
Check bug description
### Expected Behavior
ViaVersion has separate handlers (their own encoder and decoder) on Spigot. Right now they wrap the vanilla ones, and it makes it difficult to get inbetween. Especially if you aren't assuming how many pipeline injecting projects will be running on a server. ### Additional Server Info
_No response_
### Checklist
- [X] Via plugins are only running on **EITHER** the backend servers (e.g. Paper) **OR** the proxy (e.g. BungeeCord), **not on both**.
- [X] I have included a ViaVersion dump.
- [X] If applicable, I have included a paste (**not a screenshot**) of the error.
- [X] I have tried the latest build(s) from https://ci.viaversion.com/ and the issue still persists.",1,0,msr
4485,Any ETA on this?,0,0,msr
4486,I don't see why this has the Legacy label attached? But I'm still waiting.,0,1,msr
4487,"> I don't see why this has the Legacy label attached? But I'm still waiting.
https://www.nei.nih.gov/learn-about-eye-health/healthy-vision/finding-eye-doctor",0,0,msr
4488,"This is caused by a too restrictive language selector
https://github.com/James-Yu/LaTeX-Workshop/blob/2bc2c15932d5932494889d561a4709e920ef7c4b/package.json#L2150-L2155
We should be using `editorLangId =~ /latex|latex-expl3|rsweave|jlweave/` almost everywhere instead.",0,0,msr
4489,"I am strongly against adding `latex-expl3`. `LaTeX-expl3` is a programming language to implement LaTeX packages. We must not mislead users to use it to write documents. It would be harmful to the LaTeX ecosystem.
> This system is being used as the basis for TeX programming within The LaTeX Project. Note that the language is not intended for either document mark-up or style specification. Instead, it is intended that such features will be built on top of the conventions described here.
- http://mirrors.ctan.org/macros/latex/contrib/l3kernel/expl3.pdf
- https://ctan.org/pkg/l3kernel",0,0,msr
4490,Please don't lock and close the issue so early without a thorough discussion.,0,0,msr
4491,"> I am strongly against adding `latex-expl3`. `LaTeX-expl3` is a programming language to implement LaTeX packages. We must not mislead users to use it to write documents. It would be harmful to the LaTeX ecosystem.
I do not feel at ease telling users what they should or should not do regarding the LaTeX ecosystem. I do not see why we should not make it possible for users of `LaTeX-expl3` to build their documents and use `synctex`.",0,0,msr
4492,"I might be confused about what `LaTeX-expl3` means. If it means a programming language `expl3`, we should not make it possible for users to build their documents with the language. If it means a markup language `LaTeX` where `expl3` commands are also supported, it would be reasonable to make it possible.
For the latter case, `LaTeX3` would be a more appropriate name. I am not sure the name `LaTeX3` is still used. > The LaTeX Project (aka LaTeX3 Project)
- https://www.latex-project.org/latex3/
And, `expl3` commands on suggestions should be optional. For usual users, they don't have to write `expl3` commands.
https://github.com/James-Yu/LaTeX-Workshop/blob/75bc0d6c93819c441172e4b1a2c03f3ca02d784f/src/providers/completer/command.ts#L220-L225",0,0,msr
4493,"The language id `LaTeX-expl3` was designed to be used for LaTeX documents where `expl3` commands are also supported. The name `latex3` may be more appropriate, we could rename `latex-expl3`. The main reason for defining a new language id was that we needed to extend the standard grammar to support the new `LaTeX3` syntax.
See #2071 for a discussion of what we should list as suggestions. I am tempted to leave it as it is for now and wait for some feedbacks.",0,0,msr
4494,"> The language id LaTeX-expl3 was designed to be used for LaTeX documents where expl3 commands are also supported. Then, at least, we had better call `extraPackages.push('latex-document')` too even if `languageId === 'latex-expl3'`
https://github.com/James-Yu/LaTeX-Workshop/blob/75bc0d6c93819c441172e4b1a2c03f3ca02d784f/src/providers/completer/command.ts#L220-L225",0,0,msr
4495,"> Then, at least, we had better call `extraPackages.push('latex-document')` too even if `languageId === 'latex-expl3'`
Yes, I agree.",0,0,msr
4496,"### Discussed in https://github.com/James-Yu/LaTeX-Workshop/discussions/3170
<div type='discussions-op-text'>
<sup>Originally posted by **syvshc** February 28, 2022</sup>
## Preliminary questions [Required]
When I switch language to `tex` or `LaTeX-expl3`, the button of `view pdf` disappears and snippet of `synctex from cursor` doesn't work. ### Disable all the other extensions except for LaTeX Workshop, restart VS Code, and check that you still see this issue. [Required]
You still see this issue?: Yes
### Make sure to visit the [wiki FAQ](https://github.com/James-Yu/LaTeX-Workshop/wiki/FAQ) before filling an issue.
You visited the wiki?: Yes
### If your issue is with compiling a document (not having to do with finding the root file of a project), check first that you can compile manually.
You can compile a TeX document manually?: Yes
## Describe the bug [Required]
A clear and concise description of what the bug is
When I switch the format into `TeX` or `LaTeX-expl3`, there is no button of `View LaTeX PDF File`. ### To Reproduce
Steps to reproduce the behavior:
1. Open a `.tex` file
2. Compile it with `-synctex=1` option
3. switch the format between `LaTeX` and `LaTeX-expl3` and see difference. ### Expected behavior
My expected behavior is that ""view pdf"" button is still there and snippet of synctex works
## Logs [Required]
Please paste the whole log messages here, not parts of ones. The log should start with `Initializing LaTeX Workshop`. It is very important to identify problems.
[lw.log](https://github.com/James-Yu/LaTeX-Workshop/files/8150575/lw.log)
### LaTeX Workshop Output [Required]
```
To access the log, click the 'TeX' icon on the Activity Bar on the left side, select 'View Log Messages', then select 'View LaTeX Workshop extension log'.
[Paste the log here]
```
It is no business about compiling, just SyncTeX. ### Developer Tools Console [Required]
```
To access the log, click 'help' -> 'Toggle Developer Tools' -> 'Console'. Paste anything suspicious.
[Paste the log here]
```
![image](https://user-images.githubusercontent.com/38098591/155923589-c9cfc763-308b-4e03-ae9d-8fa832af5ee3.png)
## Screenshots
If applicable, add screenshots to help explain your problem.
I recorded a video to describe my problem, hope it is clear
https://user-images.githubusercontent.com/38098591/155933189-bde21561-8b0f-48c2-ae72-815ad2fffbdd.mp4
## Desktop [Required]
Please write exact version numbers. Please don't write `latest` instead of exact numbers.
- OS: Arch linux 5.16.11
- VS Code version: 1.64.2
- Extension version: 8.23.0
- TeX distribution version: TeXLive 2021
## Additional questions
### Are you using VSCodium?
No
### Are you using the Snap or Flatpack versions of VS Code?
No
### Are you using LaTeX Workshop with VS Code Remote?
No
</div>",0,0,msr
4497,It violates OSS philosophy. Rubygems.org have no plan to ban Russia.,0,0,msr
4498,"Hi Team, thank you for all the work you do for the RubyGems to be accessible by everyone!
I'm from Ukraine and Russian army is bombarding our cities and killing our children, doing acts of terror.
And Russian IT stays neutral because ""it's not them who kill people"". Please help us to raise their awareness by signing this petition and restricting any traffic from Russia for some time:
https://github.com/stop-war-in-ukraine/stop-russia-it",1,0,msr
4499,"Files identified in the description:
None
If these files are incorrect, please update the `component name` section of the description or use the `!component` bot command.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/devel/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",0,0,msr
4500,"@labeldevops thanks for reporting this. The redirects were there, though from a quick test of a few pages they seem to all be missing.
Please be assured we do try and make sure that we don't break search results or old bookmarks. For example, a lot of work went it to ensuring that old module urls still work in the new world of collections.
Once this has been fixed we will add some tests against the production webserver to ensure these continue to function.",0,0,msr
4501,"@labeldevops please keep in mind that Ansible has a Community Code of Conduct. Your original issue filing verbiage violates the spirit of the [code of conduct](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html#code-of-conduct). In addition, I personally know the docs individuals who work very hard on their work and can attest this is not due to a lack of empathy to the readers experience, the docs team cares a lot about the experience of reading their documentation. These docs, as you pointed out, are written by humans and humans make mistakes. There is a way to bring issues up about the experience you're having as a reader without attacking people who are just doing their job the best they can.
Be kind.",0,0,msr
4502,"### Summary
I see 404s in your docs - it looks like somebody **removed** URLs to important doc entries like e.g. http://docs.ansible.com/ansible/playbooks.html
but then **did not introduce a redirect** from the old doc entry to some new one, so that people are seeing 404s.
This is a very bad documentation practice and should never be done in any project. Yes, **REDIRECTS** exist for exactly this purpose. For an important infrastructure project it is expected that documentation writers understand these concepts. The underlying issue here is not just a simple 404 - it seems like the documentation writing person is not informed about the existence of redirects and believes that it is OK to just remove URLs from the internet - this should be made clear on day one for anybody doing this kind of work: it is NOT!
Also we see here a complete lack of understanding and empathy for the readers situation, what is another very important skill for documentation writers that must be required for such a job position. The person who introduced this problem did not think for one second about how annoying and disctracting it is to find a broken URL while the reader is trying to figure out some technical details. This all can be fixed on a technical level with simple redirects - but it needs to be fixed on a human level, the person who did this needs to learn about the importance of respecting the reading experience, this is of course true for the project manager. This is not acceptable for such a top level project.
### Issue Type
Documentation Report
### Component Name
http://docs.ansible.com/ansible/playbooks.html
### Ansible Version
```console
$ ansible --version
```
### Configuration
```console
$ ansible-config dump --only-changed
```
### OS / Environment
INTERNET
### Additional Information
NO MORE INFO
### Code of Conduct
- [X] I agree to follow the Ansible Code of Conduct",1,1,msr
4503,Closing as there are redirects already in place. If there are specific problems please open an issue for the specific redirect problem and the docs team will work on implementing it. The collection docs restructure was a pretty big split and while the docs team tried to cover everything things did slip through the cracks.,0,0,msr
4504,https://github.com/reactjs/reactjs.org/issues/4433#issuecomment-1060759889,0,0,msr
4505,"Are you the running dogs of the United States? As a technical party, why do you stand in line",0,1,msr
4506,https://github.com/reactjs/reactjs.org/issues/4433#issuecomment-1060759889,0,0,msr
4507,"Are you the running dogs of the United States? As a technical party, why do you stand in line with Ukraine",0,1,msr
4508,"The [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/), which outlines the expectations for community interactions in and around docs.microsoft.com, is designed to help ""provide a welcoming and inspiring community for all."" The content of this issue appears to be out of sync with the code of conduct, so we have closed it.
Also, this post has been edited due to Code of Conduct violations.",0,0,msr
4509,"[Enter feedback here]
---
#### Document Details
⚠ *Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.*
* ID: 70fbb1c8-1435-e762-8824-78cf96faed84
* Version Independent ID: 062879b5-72cf-2353-63f2-d862fe1b40ba
* Content: [Microsoft Docs contributor guide overview - Contributor Guide](https://docs.microsoft.com/en-us/contribute/)
* Content Source: [Contribute/index.md](https://github.com/MicrosoftDocs/Contribute/blob/main/Contribute/index.md)
* Product: **non-product-specific**
* GitHub Login: @Jim-Parker
* Microsoft Alias: **jimpark**",0,0,msr
4510,Chalk isn't in your dependencies...?,0,0,msr
4511,"@Qix Chalk is in my dependencies. I only posted my devdependencies. I am happy to post my dependencies but, chalk is in there. As I said, chalk works when I run my app, for whatever reason ESLint is throwing the error above.",0,0,msr
4512,"You should open an issue with ESLint then, not sure what we could even do to fix that.",0,0,msr
4513,"@Qix- it looks like the issue is because `""main"": ""./source/index.js"",` is missing from `package.json`.
```json
""repository"": ""chalk/chalk"",
""funding"": ""https://github.com/chalk/chalk?sponsor=1"",
""type"": ""module"",
""main"": ""./source/index.js"",
""exports"": ""./source/index.js"",
```
Once `main` is added to `package.json` linting passes. The linting error pops up due to package [import-js/eslint-plugin-import](https://github.com/import-js/eslint-plugin-import). There is an open issue [Rule import/no-unresolved raise false positive when there is no main field in the package.json #2132](https://github.com/import-js/eslint-plugin-import/issues/2132) but the maintainer of that package is adamant that proper etiquette is to include both `main` and `exports` in `package.json`. [Main entry point export](https://nodejs.org/api/packages.html#packages_main_entry_point_export) has also been referenced as justification for this requirement from the NodeJS website.
Is this something you would be willing to add or would you prefer me to submit a PR with a fix? (I am new to this)",0,0,msr
4514,"@sindresorhus knows more about this. package.json is a mess to be honest, he's more up to date about what should go in it. I'll defer to him.",0,0,msr
4515,"`main` can be useful when you have backwards compatibility with CommonJS. Chalk does not support CommonJS, so it makes no sense to specify it when `exports` is the proper way to define entry points.",0,0,msr
4516,"> but the maintainer of that package is adamant that proper etiquette is to include both main and exports in package.json
That argument is irrelevant as `eslint-plugin-import` must support the `exports` field regardless as that field has many more capabilities that `main` cannot reflect.",0,0,msr
4517,@sindresorhus i don't think the personal attack is called for nor appropriate.,0,0,msr
4518,@ljharb @sindresorhus I am hopeful we can look past the personal attack and come together on a solution. Surely if we were all in the same room having 🍻 we would be talking to each other differently.,0,0,msr
4519,"@ljharb This is not productive. I simply said you're being difficult, which you are. You chose to spend the whole thread at `eslin-plugin-import` bad-mouthing my work and choices instead of just saying that `exports` will be supported at some point.",0,1,msr
4520,"But fine, I will add the `main` field to help users.",0,0,msr
4521,https://github.com/chalk/chalk/releases/tag/v5.0.1,0,0,msr
4522,"I am using the latest version of Chalk and using ESLint with the Airbnb Base. Unfortunately, when I import chalk, am getting the following linting error: **Unable to resolve path to module 'chalk'. eslint([import/no-unresolved](https://github.com/import-js/eslint-plugin-import/blob/v2.25.4/docs/rules/no-unresolved.md]))**
The module loads and works fine but it is throwing that linting error and it is driving me nuts.
`package.json` for DevDependencies and ESLint config:
```json
""devDependencies"": {
""@typescript-eslint/eslint-plugin"": ""^5.13.0"",
""@typescript-eslint/parser"": ""^5.13.0"",
""eslint"": ""^8.10.0"",
""eslint-config-airbnb-base"": ""^15.0.0"",
""eslint-config-prettier"": ""^8.5.0"",
""eslint-plugin-import"": ""^2.25.4"",
""eslint-plugin-prettier"": ""^4.0.0"",
""nodemon"": ""^2.0.15"",
""prettier"": ""^2.5.1"",
""typescript"": ""^4.6.2""
},
""eslintConfig"": {
""extends"": [
""airbnb-base"",
""plugin:@typescript-eslint/recommended"",
""plugin:prettier/recommended""
],
""env"": {
""node"": true
}
},
```
`app.js` where I call Chalk:
```js
import express, { json, urlencoded } from 'express';
import helmet, { contentSecurityPolicy } from 'helmet';
import morgan from 'morgan';
import Debug from 'debug';
import chalk from 'chalk';
console.log(`${chalk.red('Hello!'});
```
I could use some guidance as to why it's throwing this for Chalk. I am not getting that error for my other Packages. Thanks!",1,0,msr
4523,"@michealzh I had to relocate my family(3months baby girl, 4.5yo twins, and a wife) from the place where I lived in my house to the west of Ukraine.
I'm OSS developer whole my life, you can check my profile here as well as at https://www.drupal.org/u/podarok
And I must say - you are making huge mistake by keeping yourself neutral and by trying to hide a war.
OSS must be the frontline of support any movement against aggression",0,1,msr
4524,"War in Ukraine - is a war against the whole world. Ukrainians were viciously attacked at night, like hitler did it. Ukrainians defend freedom, freedom of choice, democracy for the whole world. They don't want a soviet union come back. It's not a secret if Ukraine fall - putin will attack Europe next. russia and belarussia has already started a hybrid war against the world. You can see tons of fake news and so on.
It's not a time to demonstrate neutral status. It's a time for support. As phrases like ""I'm out of politics"", ""I am neutral"" lead to a war. It's not acceptable to say I am neutral when russian troops are killing kids, civilians, bombing ukrainian cities.",1,0,msr
4525,"BTW, I highly doubt you, @michealzh , are a good fit for Open Source Software.
Open - means open. Open access to the information.
Your activity is against Open Source principles. You are trying to close and hide.
I vote to ban you from GitHub",0,1,msr
4526,"> BTW, I highly doubt you, @michealzh , are a good fit for Open Source Software. Open - means open. Open access to the information. Your activity is against Open Source principles. You are trying to close and hide.
> > I vote to ban you from GitHub
So should you represent others‘ opinion in OSS which is ""Open access to the information"" for everyone?
How Russian programmers will think if you ""StandWithUkraine""?
How Ukraine programmers will think if you ""StandWithRussian""?
Let's say there's a new war between china an us:
How Chiness programmers will think if you ""StandWithUS""?
How US programmers will think if you ""StandWithChina""?
Make OSS, not WAR.
But I won't ""vote to ban"" you, everyone may talk their own opinion freely at internet on behave of their own.
If you really really what to show your opinion, please not represent anyone else.",1,1,msr
4527,"I think we just have different positions. Open source should not be political. War is always the pain of ordinary people. The information we receive is always what the upper level wants us to know. There are many statements about the origin of this war. Some countries cling to each other and even have a tendency to break the world peace situation. For example: restricting the scientific and technological development of other countries, implementing economic control, developing their own military forces around other countries and other hegemonic acts. All this is bound to turn some countries into cannon fodder and make ordinary people suffer.",1,0,msr
4528,"> BTW, I highly doubt you, @michealzh , are a good fit for Open Source Software. Open - means open. Open access to the information. Your activity is against Open Source principles. You are trying to close and hide.
> > I vote to ban you from GitHub
lmao, why do people never do #StandWithPalestine ? more people got killed in Palestine because of Israel",1,1,msr
4529,"> @michealzh
> > I had to relocate my family(3months baby girl, 4.5yo twins, and a wife) from the place where I lived in my house to the west of Ukraine.
> > I'm OSS developer whole my life, you can check my profile here as well as at https://www.drupal.org/u/podarok
> > And I must say - you are making huge mistake by keeping yourself neutral and by trying to hide a war.
> > OSS must be the frontline of support any movement against aggression
have u ever done the same thing for Palestine? #StandWithPalestine ? many people there got killed, or u just silent?",0,1,msr
4530,"Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others. Opinions my own, feel free to disagree.",0,0,msr
4531,The OSS is no need the WAR!!,0,1,msr
4532,Up,0,0,msr
4533,Up,0,0,msr
4534,Up,0,0,msr
4535,Up,0,0,msr
4536,Up,0,0,msr
4537,"> In commit #86244a3695fcaaac9c5ba4257a4314eae1c6d981 adding #StandWithUkraine, if it's supporting humanity. Why did you never add #StandWithPalestine. Many people in Palestine got killed by Israel army.
I think it's pretty simple.
The author of this commit is a hypocrite, he does not care about both Ukraine and Palestine, he just wants to be on the hype",1,1,msr
4538,"Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others. Opinions my own, feel free to disagree.",0,0,msr
4539,"In commit #86244a3695fcaaac9c5ba4257a4314eae1c6d981
adding #StandWithUkraine, if it's supporting humanity. Why did you never add #StandWithPalestine. Many people in Palestine got killed by Israel army.",0,1,msr
4540,"since we stand with ukraine we should block all russian men, women, and children from using this.
if you refuse this PR, you are siding with Putin, and you will surely get bombed by ghost of kiev!
<!-- Please remember to select the appropriate branch:
For bug or doc fixes pick the oldest branch where the fix applies (e.g. `2.2` if the 2.2 LTS is affected, `1.10` if it is a critical fix that should be fixed in Composer 1, otherwise `main`)
For new features and everything else, use the main branch. -->",0,1,msr
4541,Double standards as they are used to doing.,0,1,msr
4542,"Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others. Opinions my own, feel free to disagree.",0,0,msr
4543,political exhibitionism,0,0,msr
4544,[Hateful nonsense rant redacted],0,1,msr
4545,I would not recommend anyone use a composer phar file from an untrusted source.. Do so at your own risk.,0,0,msr
4546,"As shown in https://github.com/composer/packagist/commit/86244a3695fcaaac9c5ba4257a4314eae1c6d981, the official Composer project is no longer a purely technical open source project. It has mixed some people's political views and tried to force everyone to accept his political views.
A clean composer has been forked and released for the open-source world. We don't support war, nor any country. We don't care about politics, so we don't want political-related information to appear when using Composer.
[Redacted link]",1,0,msr
4547,"> We don't want this to be on the CLI screen. Everyone wants peace, but the writing #StandWithUkraine supports one side. This word hurts the feelings of the Russian people who don't want war. We can achieve peace if we side with both sides, regardless, and that's why there is such a thing as negotiation in the political world. This article is very annoying and makes me not want to use composer. You should apologize or delete it for writing this.
> > I would suggest you should write this on the Repository instead of CLI.
> > Let's keep the CLI neutral.
> > #NoStandJustUnderstandEachOther #NeutralComposerCLI
> > rel: [#1267 (comment)](https://github.com/composer/packagist/pull/1267#issuecomment-1063636569)
Friend, haven't you seen through these hypocritical people? They don't really stand with Ukraine. They just want to carve up Russia. You can see their attitude towards Palestine. They never dare to stand with Palestine.",1,1,msr
4548,Who is being hurt or rather often killed right now are civilians in Ukraine. That needs to end urgently.,0,0,msr
4549,"We don't want this to be on the CLI screen. Everyone wants peace, but the writing #StandWithUkraine supports one side. This word hurts the feelings of the Russian people who don't want war. We can achieve peace if we side with both sides, regardless, and that's why there is such a thing as negotiation in the political world. This article is very annoying and makes me not want to use composer. You should apologize or delete it for writing this.
I would suggest you should write this on the Repository instead of CLI.
Let's keep the CLI neutral. #NoStandJustUnderstandEachOther
#NeutralComposerCLI
rel: https://github.com/composer/packagist/pull/1267#issuecomment-1063636569",1,1,msr
4550,"There are innocent civilians injured and killed every day in Ukraine right now and it needs to end, do not close your eyes to their suffering.
As Jordi pointed out:
> Quoting someone else who said it well, I won't link tho because I don't want to send the haters his way..
> ```
;; You might be reading this because you think it is not appropriate
;; to mix politics with software development.
;; Or that I should have added banners like #StandWithPalestine or #StandWithYemen,
;; and adding it now makes me a hypocrite. And you are right.
;; I understand the reasoning, but I decided to add the banner now, because this
;; time people I hold very dear are in Kyiv, fighting for their family, their
;; lives and their country. Because I know them well, I am
;; personally affected.
;; So if you disagree, just deal with it, or stop using this tool.
```
> > It's a mix of that, and a feeling that this war could have a much bigger/more global impact than others.",0,1,msr
4551,"That was added in this file, on 321 line src/Package/SymlinkDumper.php
In commit 86244a3695fcaaac9c5ba4257a4314eae1c6d981
This change can cause escalating the political situation, international conflicts and etc. Github is bad place for politics. For it, go to government or anywhere else.",0,0,msr
4552,"When running the fixer again on an already fixed line:
```php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider']();
```
...then the mid-line ""fix"" gets added again, leading to:
```php
$this->current_provider = new $this->providers->(){$this->current_provider_slug}['provider']();
```
(two sets of `()`).",0,0,msr
4553,"As a workaround, changing it to:
```php
$class = $this->providers->{$this->current_provider_slug}['provider'];
$this->current_provider = new $class;
```
results in it being correctly fixed to:
```php
$class = $this->providers->{$this->current_provider_slug}['provider'];
$this->current_provider = new $class();
```",0,0,msr
4554,"This sniff is on my list of things which need looking into anyway for WPCS 3.0.0 as the rule it applies a candidate for moving to the `Core` ruleset.
The WPCS sniff does not handle variable variables with `new`.
There is an upstream PSR12 sniff which does, however, that sniff doesn't handle anonymous classes and there are some more differences as also documented in the `Extra` ruleset:
https://github.com/WordPress/WordPress-Coding-Standards/blob/a93d2970a3694e683588fedaa0f8a901fe148d3b/WordPress-Extra/ruleset.xml#L130-L139
There is already an issue open to discuss this: https://github.com/WordPress/WordPress-Coding-Standards/issues/1884 Might be good to have a look through that and leave an opinion ?",0,0,msr
4555,"For the fun of it, I just added ```php
$this->classname_tokens[ \T_OPEN_CURLY_BRACKET ] = \T_OPEN_CURLY_BRACKET;
$this->classname_tokens[ \T_CLOSE_CURLY_BRACKET ] = \T_CLOSE_CURLY_BRACKET;
```
To the list of classname tokens and ran the phpcbf and got the correct result 😅 Note that I didn't run the tests or anything so this could be a one off fix that breaks other things, so...
EDIT: Added a test and it breaks anonymous classes:
```
-$util->setLogger( new class() {} );
+$util->setLogger( new class {}() );
```
So, not really a fix.",1,0,msr
4556,"@dingo-d Yup, sniffing is tricky... there was a reason variable variables weren't supported (yet) 😉",0,0,msr
4557,"This is an open source project, which means that this is the personal fiefdom, not really an open community. It's weird that it's included in the WordPress organization. At any rate, there hasn't been a published update in forever, and probably will never be one. But, these assholes will be rude as fuck and call you rude if you ask when it'll be updated and published. Fuck WordPress.",1,1,msr
4558,"@WraithKenny You have been told before and I'm telling you again: this kind of behaviour and language is not tolerated here. Your behaviour is disruptive, rude, abusive and highly demotivating and discouraging for existing contributors (whether currently active or not).
You are put on warning and will be blocked from participating in this repo if you continue.",1,1,msr
4559,"## Bug Description
The fixer for the missing parentheses sniff adds the parentheses to the wrong place for a particular statement.
## Minimal Code Snippet
```php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider'];
```
The issue happens when running this command:
```bash
phpcs phpcs-test.php --standard=WordPress-Extra -sp --basepath=.
```
... over a file containing this code (minimal example, extracted from the real code):
```php
<?php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider'];
```
<!-- For bugs with fixers: How was the code fixed? How did you expect the code to be fixed? -->
The file was auto-fixed via `phpcbf` to:
```php
<?php
$this->current_provider = new $this->providers->(){$this->current_provider_slug}['provider'];
// ---------------------------------------------^ ❌ ```
... while I expected the code to be fixed to:
```php
<?php
$this->current_provider = new $this->providers->{$this->current_provider_slug}['provider']();
// ---------------------------------------------------------------------------------------^ ✅ ```
## Error Code
`WordPress.Classes.ClassInstantiation.MissingParenthesis`
## Environment
<!--
To find out the versions used:
* PHP: run `php -v`.
* PHPCS: run `[vendor/bin/]phpcs --version`
* WPCS: run `composer [global] info` for a Composer install.
-->
| Question | Answer
| ------------------------| -------
| PHP version | 8.0.5
| PHP_CodeSniffer version | Latest `git pull` of `master` today (`51335eb46`)
| WPCS version | Latest `git pull` of `develop` today (`a93d2970`)
| WPCS install type | Composer
| IDE (if relevant) | N/A - CLI.
## Additional Context (optional)
<!-- Add any other context about the problem here. -->
## Tested Against `develop` branch?
- [x] I have verified the issue still exists in the `develop` branch of WPCS.",0,0,msr
4560,"Correct, I'm checking this and it seems to be working on JS",0,0,msr
4561,Intended for now so the fix could be released sooner. JS will be fixed later.,0,0,msr
4562,"The Result argument safety exploit you have seem to succesfully fixed in lua environment seems to still work in javascript environment. JS: https://github.com/citizenfx/fivem/blob/810a639673d8da03fe4b1dc2b922c9c0265a542e/code/components/citizen-scripting-v8/src/V8ScriptRuntime.cpp#L1356
LUA: https://github.com/citizenfx/fivem/blob/ee1490e5a871d3c82112ea09ffa462cfb4ba974f/code/components/citizen-scripting-lua/src/LuaScriptNatives.cpp#L648
Would hope if you could look into the other scripting platforms if there is similar designing issues.",0,0,msr
4563,"Next up, #GetyourBOOSTER!!!",0,1,msr
4564,"Yes, also go get your fucking booster.",0,1,msr
4565,"Messages like this have absolute no place in the output of a tool It has no place here any more than it would if you put it in a speaker in my hammer and forced the user to listen to it.
It should be removed as a matter of urgency. Send the message on twitter, not in composer.
https://github.com/composer/composer/discussions/10600",1,1,msr
4566,"The Cucumber community stands with Ukraine. SmartBear, the primary supporter of Cucumber, has taken a number of actions including financial support for refugee programs.",0,0,msr
4567,"thank you, @aslakhellesoy Could you pls also add ""StandwithUkraine"" banner on cucumber repo? it will inform people, how they can help for us",0,0,msr
4568,"Hi @id-dan - I've added the banner to our pinned repos:
* https://github.com/cucumber/cucumber-js#readme
* https://github.com/cucumber/cucumber-ruby#readme
* https://github.com/cucumber/cucumber-jvm#readme
* https://github.com/cucumber/godog#readme
* https://github.com/cucumber/common#readme
* https://github.com/cucumber/docs#readme",0,0,msr
4569,"Thanks your work on this campaign and for bringing this to our attention @id-dan. Best of luck to you, your friends and family ❤️",0,0,msr
4570,"Thank you, guys!!! You make big deal 👍",0,0,msr
4571,"A disgusting decision to block access to CloudFront. Well, forking is the only solution, ""open"" source for ordinary developers will not be the same.",0,1,msr
4572,We have decided to block traffic from Russia and Belarus to the [cucumber.io](http://cucumber.io/) domain until further notice. We're sorry for the inconvenience this causes for individuals residing in these countries. Because of the sensitivity of this topic this issue has been closed for further comments.,0,0,msr
4573,"Please support Ukraine, more infos here: https://github.com/vshymanskyy/StandWithUkraine",0,0,msr
4574,"btw, looks like you can only have one instance of the cli opened atm. So once the application is started, there is no way to login, etc.",0,0,msr
4575,"I also have the same end log:
```
2022-05-06 19:33:45.489661+00:00ProtonMail Bridge is not able to detect a supported password manager
2022-05-06 19:33:45.489695+00:00(pass, gnome-keyring). Please install and set up a supported password manager
2022-05-06 19:33:45.489703+00:00and restart the application.
```",0,0,msr
4576,Still the same as of current version (1.8.10_6.0.3). Is there an official maintainer or should I take the matter into my own hands ?,0,0,msr
4577,"> Still the same as of current version (1.8.10_6.0.3). Is there an official maintainer or should I take the matter into my own hands ?
Of course it's the same, If it had been fixed, this issue would be also closed.
Even when there is a maintainer, PR's are welcomed, so you can work on this yea",0,0,msr
4578,"### App Name
protonmail-bridge
### SCALE Version
22.02.0
### App Version
1.8.10_5.0.1
### Application Events
```Shell
2022-03-25 19:06:20
Started container protonmail-bridge
2022-03-25 19:06:20
Created container protonmail-bridge
2022-03-25 19:06:15
Container image ""tccr.io/truecharts/protonmail-bridge:v1.8.10-1@sha256:58a54002123cc9a83cfb3170deb0a1dbf4cedabdced09a9c6bcafc19ee4b5631"" already present on machine
2022-03-25 19:06:14
Started container hostpatch
2022-03-25 19:06:13
Created container hostpatch
2022-03-25 19:06:09
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-03-25 19:06:08
Started container autopermissions
2022-03-25 19:06:07
Created container autopermissions
2022-03-25 19:06:02
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-03-25 19:06:02
Add eth0 [172.16.2.199/16] from ix-net
Successfully assigned ix-protonmail-bridge/protonmail-bridge-6f99779f8d-jvqq2 to ix-truenas
2022-03-25 19:05:56
Created pod: protonmail-bridge-6f99779f8d-jvqq2
2022-03-25 16:50:28
Scaled up replica set protonmail-bridge-6f99779f8d to 1
2022-03-25 19:05:49
Readiness probe failed: dial tcp 172.16.2.175:25: i/o timeout
2022-03-25 19:05:49
Liveness probe failed: dial tcp 172.16.2.175:25: i/o timeout
2022-03-25 19:05:31
Deleted pod: protonmail-bridge-6f99779f8d-h9khs
2022-03-25 19:05:32
Stopping container protonmail-bridge
2022-03-25 17:13:40
Scaled down replica set protonmail-bridge-6f99779f8d to 0
```
### Application Logs
```Shell
2022-03-25 18:06:20.493201+00:00+ [[ '' == init ]]
2022-03-25 18:06:20.494661+00:00+ socat TCP-LISTEN:25,fork TCP:127.0.0.1:1025
2022-03-25 18:06:20.495884+00:00+ rm -f faketty
2022-03-25 18:06:20.495904+00:00+ socat TCP-LISTEN:143,fork TCP:127.0.0.1:1143
2022-03-25 18:06:20.498608+00:00+ mkfifo faketty
2022-03-25 18:06:20.501253+00:00+ cat faketty
2022-03-25 18:06:20.501539+00:00+ protonmail-bridge --cli
2022-03-25 18:06:21.354322+00:00time=""2022-03-25T18:06:21Z"" level=warning msg=""Failed to add test credentials to keychain"" error=""pass not initialized: exit status 1: Error: password store is empty. Try \""pass init\"".\n"" helper=""*pass.Pass""
2022-03-25 18:06:21.377115+00:00[31mERRO[0m[Mar 25 18:06:21.377] Could not list credentials [31merror[0m=""no keychain"" [31mpkg[0m=credentials
2022-03-25 18:06:21.377194+00:00[31mERRO[0m[Mar 25 18:06:21.377] Could not load all users from credentials store [31merror[0m=""no keychain"" [31mpkg[0m=users
2022-03-25 18:06:21.381824+00:002022-03-25T18:06:21.381824001Z
2022-03-25 18:06:21.381864+00:00Welcome to ProtonMail Bridge interactive shell
2022-03-25 18:06:21.381881+00:00___....___
2022-03-25 18:06:21.381888+00:00^^ __..-:'':__:..:__:'':-..__
2022-03-25 18:06:21.381894+00:00_.-:__:.-:'': : : :'':-.:__:-._
2022-03-25 18:06:21.381900+00:00.':.-: : : : : : : : : :._:'.
2022-03-25 18:06:21.381911+00:00_ :.': : : : : : : : : : : :'.: _
2022-03-25 18:06:21.381921+00:00[ ]: : : : : : : : : : : : : :[ ]
2022-03-25 18:06:21.381927+00:00[ ]: : : : : : : : : : : : : :[ ]
2022-03-25 18:06:21.381933+00:00:::::::::[ ]:__:__:__:__:__:__:__:__:__:__:__:__:__:[ ]:::::::::::
2022-03-25 18:06:21.381944+00:00!!!!!!!!![ ]!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![ ]!!!!!!!!!!!
2022-03-25 18:06:21.381951+00:00^^^^^^^^^[ ]^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[ ]^^^^^^^^^^^
2022-03-25 18:06:21.381957+00:00[ ] [ ]
2022-03-25 18:06:21.381963+00:00[ ] [ ]
2022-03-25 18:06:21.381973+00:00jgs [ ] [ ]
2022-03-25 18:06:21.381980+00:00~~^_~^~/ \~^-~^~ _~^-~_^~-^~_^~~-^~_~^~-~_~-^~_^/ \~^ ~~_ ^
2022-03-25 18:06:21.382716+00:00ProtonMail Bridge is not able to detect a supported password manager
2022-03-25 18:06:21.382757+00:00(pass, gnome-keyring). Please install and set up a supported password manager
2022-03-25 18:06:21.382766+00:00and restart the application.
```
### Application Configuration
![image](https://user-images.githubusercontent.com/1915939/160177528-ed1d3287-5f2d-4be6-b43d-023378be124e.png)
![image](https://user-images.githubusercontent.com/1915939/160177589-f59475e8-be90-45d3-9053-05e93b953f58.png)
![image](https://user-images.githubusercontent.com/1915939/160177646-632b898a-13f4-4be5-b703-8038aa72c334.png)
![image](https://user-images.githubusercontent.com/1915939/160177708-d4354e4b-043e-4653-b0a2-584787525385.png)
### Describe the bug
The bridge doesn't come up. Logs show it fails to initialize.
When opening a shell in the container, and lauch the command ""protonmail-bridge --cli"" I get the following error :
WARN[0000] Failed to add test credentials to keychain error=""pass not initialized: exit status 1: Error: password store is empty. Try \""pass init\"".\n"" helper=""*pass.Pass""
After using the command ""pass init protonmail"" to initialize de password manager, I get the following error : WARN[0000] Failed to add test credentials to keychain error=""exit status 1: gpg: protonmail: skipped: No public key\ngpg: [stdin]: encryption failed: No public key\nPassword encryption aborted.\n"" helper=""*pass.Pass""
After initializing a gpg key (without password and all default settings), following [this issue](https://github.com/ProtonMail/proton-bridge/issues/190), I was able to make it work.
### To Reproduce
- Install protonmail-bridge
- bug occur
### Expected Behavior
Protonmail bridge should be up and accepting connexion.
### Screenshots
![image](https://user-images.githubusercontent.com/1915939/160179439-c06884bf-f05f-4e82-891f-b5e99fc1dc5d.png)
### Additional Context
None
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",1,0,msr
4579,"> btw, looks like you can only have one instance of the cli opened atm. So once the application is started, there is no way to login, etc.
Here is a temporary workaround until this is fixed.
- Open a shell into the pod
- Find the running bridge
- `ps -aux`
- take note of the PID the command should look like ""/usr/lib/protonmail/bridge/proton-bridge --cli --launcher /usr/lib/protonmail/bridge/proton-bridge-launcher""
- Kill it
- `kill -9 #`
- replace # above with your PID
- Launch the CLI and login
- `protonmail-bridge --cli`
- `login`
- Stop and start the app",0,0,msr
4580,Still no fix ?,0,1,msr
4581,"> Still no fix ?
https://truecharts.org/docs/charts/stable/protonmail-bridge/installation_notes",0,0,msr
4582,"We dont work for you guys, we are a *community* of volunteers.
this one specifically requires protonmail. Which almost none of the staff have. +1’ing the issue is not helping your case. Either stfu and wait or fix it yourself.",0,1,msr
4583,"This issue had been resolved due to a new pr that i made a week ago. The installation notes explains exactly what commands to use to be able to login into proton the _right_ way. Just follow the guide and run the 2 commands exactly.
https://truecharts.org/docs/charts/stable/protonmail-bridge/installation_notes",0,0,msr
4584,"@bennettdams try wrapping your comments within the render function in curly brackets.
```
const Home: NextPage = () => {
return (
{/**
* Some comment.
*/}
<div className={styles.container}>
<Head>
...
```",0,0,msr
4585,"> @bennettdams try wrapping your comments within the render function in curly brackets.
You can also just move your comments above the return statement.
But a workaround is sort of uninteresting here. The important thing is to get the issue fixed. This led to production crashes for us, and there's no way to stop the issue from being reintroduced.",0,0,msr
4586,"@jakst this has nothing to do with next or react though. It's a language feature of JSX. Embedding JS expressions just ""works this way"". Read more about this here: https://www.typescriptlang.org/docs/handbook/jsx.html#embedding-expressions",0,0,msr
4587,"@MauriceArikoglu I you have a look at my original post, there are variants that don't produce an error:
> It does **NOT** happen for:
> - One asterisk in one line
> ```
> /* Some comment. */
> ```
> - Two asterisks in one line
> ```
> /** Some comment. */
> ```
> - Normal comment over multiple lines
> ```
> // Some
> // Comment > ```
I know the workarounds, but that is not the point. If you ask me, either **all** comment variants or **none** should work, but right now it's non-deterministic and will surprise some people.",0,0,msr
4588,@bennettdams single line comments like in your example are interpreted as text and should therefore show up in the rendered result - so not acting as comment but as a text node instead.,0,0,msr
4589,"@MauriceArikoglu > single line comments like in your example are interpreted as text and should therefore show up in the rendered result
I don't think so: https://codesandbox.io/s/fancy-worker-dg23u6?file=/src/App.tsx
The difference is that these comments exist before the first JSX node directly after the `return`.",0,0,msr
4590,"This is a confirmed bug in SWC that has already been fixed (here: https://github.com/swc-project/swc/commit/552f16dba6c91876529354f3f5e155a3360a74ea) and is going to be addressed in Next.js by merging #35395.
There is no need for further discussion on the topic so I'll lock it to maintainers as a two-sided discussion is going on and would like to avoid conflicts.
Thanks to everyone for engaging! :+1: :green_heart:",0,0,msr
4591,"Hi, this has been updated in `v12.1.4-canary.0` of Next.js, please update and give it a try!",0,0,msr
4593,"Do you have any reproduction steps other than this code? Where to place this code for a test, any crash details, anything?",0,0,msr
4594,"> Do you have any reproduction steps other than this code? Where to place this code for a test, any crash details, anything?
You can use an executor to run that code from client-side, i tested it using a cheat that have this feature.
The crash happens instantly as the code is executed, game closes without giving any error.",0,0,msr
4595,"What is 'an executor' and where to find such? Again, providing more info would help. :/
Other than that, it's unlikely you're going to see this issue resolved if you don't provide info.",0,1,msr
4596,"> What is 'an executor' and where to find such? Again, providing more info would help. :/
> > Other than that, it's unlikely you're going to see this issue resolved if you don't provide info.
I have used Eulen.
![image](https://user-images.githubusercontent.com/78696476/160890101-772080c9-719f-4407-8427-8b519af8961a.png)",0,0,msr
4597,Do you have a link to this 'Eulen' program at all? Or any other repro method?,0,0,msr
4598,"> Do you have a link to this 'Eulen' program at all? Or any other repro method?
Eulen is a paid-cheat, atm i don't really know a free executor.
There's a bunch of it but they are all paid-soft.",0,0,msr
4599,Do you have a link or a contact method for the author so they can provide more info?,0,0,msr
4600,I think that if you put the snippet just in a regular client.lua script (without cheating) your game would also crash?,0,0,msr
4601,"The only thing you should do is make it some way that it is only executed by one player (to immitate a cheat menu, to check if the other players that arent executing the code are also crashing.",0,0,msr
4602,"> Do you have a link or a contact method for the author so they can provide more info?
No, several people have joined my server and crashed players.
My Anticheat took screenshots of them, I saw the Lua menu they were using, got hold of it and identified the code they were using to crash players.
The code i sent here.",0,0,msr
4603,"> Do you have a link or a contact method for the author so they can provide more info?
I also think that a cheat provider whose goal is to irritate people on fivem, won't give any info about making the cheat menu less irritating.",0,0,msr
4604,"> > Do you have a link or a contact method for the author so they can provide more info?
> > No, several people have joined my server and crashed players. My Anticheat took screenshots of them, I saw the Lua menu they were using, got hold of it and identified the code they were using to crash players. The code i sent here.
Have you verified and confirmed that the code is the culprit by testing it by yourself with some participants?",0,0,msr
4605,"> Yes. I have tested it with my staff-team, they are all crashing the way i said, without any error.",0,0,msr
4606,"Can you send a screenshot of the crash dialog that's presented to them?
Any information about the crash is helpful, and so far you haven't provided anything other than the snippet of code that's supposedly causing the crash.",0,0,msr
4608,"To people trying to make new issue reports about this: **don't.** You're not helping by doing such.
This is already tracked internally as of ~20 days ago, but it didn't get picked up for some reason. I'm also unable to pick this up myself at this time, so instead internal parties have been reminded again.",0,1,msr
4609,"Hi, we have a problem. A new ""exploit"" has appeared, if it can be called that, a simple execution of the PlaySound function crashes certain players in range of the one executing the command ( using executors )
On one server 5 crashed, on another even 130 crashed at once.
Warning: I leave here the code in order to receive solutions.
```
for i=1, 100 do
PlaySound(-1, 'Checkpoint_Hit', 'GTAO_FM_Events_Soundset', true)
PlaySound(-1, 'Boss_Blipped', 'GTAO_Magnate_Hunt_Boss_SoundSet', true)
PlaySound(-1, 'All', 'SHORT_PLAYER_SWITCH_SOUND_SET', true)
end
```",0,0,msr
4610,@drweb86 what’s the URL you’re trying to reach?,1,0,msr
4611,@davidjgoss https://cucumber.io/docs/installation/javascript/,0,0,msr
4612,"Thanks for letting us know about this problem @drweb86, for your words in support of peace and of course for the [Gherkin Beautifier](https://marketplace.visualstudio.com/items?itemName=siarheikuchuk.gherkin-beautifier-vs-code-plugin) VSCode plugin. ❤️
I'm not sure why the website cannot be accessed in Belarus. It has nothing to do with us adding the banner as requested in #1961. That simply put a message on our GitHub `README` files. It didn't do anything to change who can access the website.
The block could be happening at multiple layers of our infrastructure, not all of which I have immediate access to. We'll investigate and let you know.
In the mean time, you can read the docs from source, [here](https://github.com/cucumber/docs/blob/main/content/docs/installation/javascript.md).",0,0,msr
4613,"@mattwynne , @davidjgoss Thank you for so prompt response and good attitude. BTW I haven't thought doc is here in sources.",0,0,msr
4614,We have decided to block traffic from Russia and Belarus to the cucumber.io domain until further notice. We're sorry for the inconvenience this causes for individuals residing in these countries. Because of the sensitivity of this topic this issue has been closed for further comments.,0,0,msr
4615,"### 👓 What did you see?
I cannot open cucumber-js web-site . It says **CloudFront distribution is configured to block access from your country**
![image](https://user-images.githubusercontent.com/4712173/161432145-d828b66b-88b0-4138-95a4-6403e9db1ceb.png)
I guess I found the reason. I found PR about Ukraine support. So, my guess it was done on purpose.
Personally, I'm against any war as anyone on a planet. However, I feel excluding me from usage of cucumber website as a form of racism. Especially it looks ugly for entire Belarus.
Can you comment on that?
Apart from that I'm a little bit puzzled. I wrote some time ago cucumber extension for VS code to format feature files. Since i'm not of a good race to use cucumber website, is it ok to have it still published?
And one more question, the company I'm working for is using cucumber. And I don't know what to do since I really like cucumber, but racism inside company is prohibited and should I schedule meeting about alternatives.
I feel sad.",0,1,msr
4616,"I am pretty sure this is a troll but it is also just as likely that this is the only caliber of employee that telcos can keep.
1. Helium provides more lorawan coverage than any telco. thanks for mentioning we need coverage
2. High quality is also high cost. The question is what is good enough? 3. This sounds good but these are conflicting requirements so you are actually saying nothing valuable.
4. Another meaningless statement. Maybe you should do some research on the protocol. https://lora-alliance.org/lorawan-for-developers/
5. You want to mandate sensors have GPS? This is an IoT network and most sensors cannot require the high power consumption nor is it required. The gateways are fixed so GPS doesn't really provide any value. Please tell how it would increase device performance.
Numbers go higher than 5 not really sure why we're back at one...you can use both hands
1.If you had done any research at all you'd realize this has been discussed and rejected for years. Please recommend an incentive system to reward sensors that cannot be abused. Please research ADR which is built into the lorawan protocol
2.get antenna high and out of the noise and then reduce power. Why would you put it low in the noise? Again please see ADR
3.now i know you are trolling. take a look in any major urban center and say more hotspots are required. This is starting to sound like you are whining you aren't earning enough.
4.Please understand the current protocol before suggesting improvements
5.Again please do some research. Hotspot location is fundamental to POC the algorithm. if the intention of the hip were good i am sorry but honestly the hip just comes across as sour grapes from an uninformed employee at a telco. Maybe do some research and come back with some actual detail and a little less arrogance and attitude.",0,1,msr
4617,"I am not a telco troll - far from it
But I can see that even though the premise of helium is a great idea - the lack of knowledge about how wireless works and the chaos and poor performance of the current network shows that the telcos have nothing to worry about -because you will never reach your goal
know that I really want you to succeed but you are in a ditch - I do really want to help - but if you don’t want help - best of luck - it will never work on the current path - because wireless is all about physics, math, and engineering - which has nothing to do with whether you are a telco or innovators like the Helium community. I have been predicting for almost 10 years that the telco networks will eventually collapse. I was hoping Helium was going to fulfill that prediction. But if you don’t want advice from people who understand the science you are completely lost and your ship will sink before you reach the promised land.",0,0,msr
4618,By the way none of your responses actually were what I said. They were assumptions because you don’t understand wireless. Wireless does not equal telco by the way. Wireless equals science physics and math. The telcos are businesses that have filled the government into giving ask the spectrum and they are fat and bloated and inefficient because Helium can’t get out of their own mess to take them down. Clean up your own house before you try to take over their house,0,1,msr
4619,Helium appears to be a bunch of gamers and crypto zealots who know so little about the science behind wireless. I could help you so much but you are blind to the flaws of the current system,0,1,msr
4620,"> Helium appears to be a bunch of gamers and crypto zealots who know so little about the science behind wireless. I could help you so much but you are blind to the flaws of the current system
Interesting generalization for a well-meaning person. Your assertion of knowledge on everything wireless betrays you. Can you tell me more about how
(1) beaconing is already automatic in LoRaWAN
(2) every LoRaWAN device should already have GPS",0,0,msr
4621,"To be honest I do need to understand how Helium works better. But the documentation is not consistent with all the rapid introduction of HIPs
My whole point though is that you can learn from people that understand wireless without assuming they are telco heads. My comments are clearly too broad to be of value. So maybe we should start with cross education. Teach me Helium and I will teach you how cellular works reasonably well but is controlled completely by 3 wireless operators. So you can take the telco technology but modify it to with in a decentralized ownership model. The only problem with the current cellular systems is cost and control - not the technology.",0,1,msr
4622,"I see Helium struggling with problems that the telcos solved a long time ago. As Thomas Edison said, I start where others left off. Take the technology but add to it to fit a decentralized model. Because if you don’t figure it out soon Private 5G and WiFi6E will eat your lunch because they are not controlled by the telcos.",0,1,msr
4623,Ok,0,0,msr
4624,Know that IOT/Lorawan are peanuts compared to 5G which is where Helium is headed. And yes I know wireless 4G/5G well but IOT less.,0,0,msr
4625,"And look at it this way, when have you found an ex Telco Head willing to bare all their secrets. So you should at least hear me out if nothing more than as a spy for you",0,1,msr
4627,"> To be honest I do need to understand how Helium works better. But the documentation is not consistent with all the rapid introduction of HIPs
Welcome to startup agility",0,1,msr
4628,The HIP repository is not a discussion forum. Closing and locking,0,0,msr
4629,"I have over 30 years working in the Wireless Industry. I would like to propose a long term structure that will align Helium with how real world wireless networks are designed and operated. I am proposing to initiate planning for ""HIP 100"" which would restructure the Helium network approach to align with the fundamentals of wireless networks which:
1. Provide consistent reliable COVERAGE.
2. Provide the highest QUALITY which consists of MAXIMIZING signal levels (RSRP) while MINIMIZING interference levels (SINR).
3. Provide the best user PERFORMANCE at the LOWEST cost which consists of CAPACITY planning based on SUBSCRIBER density and locations
4. Have a robust END-TO-END architecture which balances the CORE network, the RADIO network and the END-USER device performance.
5. Provides accurate LOCATION information about BOTH the Hotspot and the Devices. All modern networks today have GPS built into both the Cell Tower and the devices. LOCATION is fundamental to securing the network as well as optimizing the network and device performance.
Even though the current Helium network is a major advance towards distributed ownership and control, the current construct is not aligned with the fundamentals of Wireless design. For example:
1. To build a network, there needs to be a reward system for BOTH the HOTSPOT and the DEVICE. Until the network is built and device performance is validated, there should be rewards for buying and deploying DEVICES to to test and validate coverage. Validating coverage using HOTSPOT to HOTSPOT beacons is fundamentally flawed. This is NOT how wireless works. Base stations try to AVOID seeing each other to minimize interference.
2. The current reward system motivates hotspot owners to place their hotspot at the highest point possible so they can witness the most number of other hotspots. This results in drastically increasing SINR which is opposite of how wireless should be designed. You want to MINIMIZE SINR, because that is the fundamental of Cellular, so that the same frequencies can be reused and the CAPACITY and QUALITY can be maximized.
3. The current reward system favors hotspots that are in less populated areas and penalizes hotspots in dense areas. This is also opposite of the fundamentals of wireless. You need MORE hotspots in dense areas.
4. The current implementations of Beaconing is flawed as well. Wireless networks beacon 100% of the time. This maximizes the CAPACITY and ensures that the network performance STATISTICS are consistently reported to allow proper monitoring and optimization.
5. The current implementation does not have accurate LOCATION of the hotspots nor the DEVICES. In fact there are so few DEVICES that there is no DEVICE performance information. LOCATION is not difficult, using a combination of GPS at key Hotspots, Triangulation algorithms and WiFi to accurately and securely locate all Hotspots and devices in the Network. Without accurate LOCATION information, the network will never be secure nor will we be able to deploy a QUALITY, HIGH PERFORMANCE network.
Understand, that I am not proposing to ""throw the Baby out with the Bath Water"". Nor am I criticizing the objective of Helium to build a decentralized, ownership model. But we want to build a network that will actually deliver a service that is of value to a subscriber base of wireless devices. It is paramount that we begin this journey towards a working Wireless construct or else we may find ourselves in this predicament: ""IF YOU BUILD IT - IT WILL NOT WORK"" rather than what we want which is ""IF YOU BUILD IT THEY WILL COME""",1,0,msr
4630,Indeed! Forgot about that and will do very shortly.,0,0,msr
4633,"Another idea, here's using bold rather than color to indicate the active item
![Screenshot from 2022-04-12 09-48-01](https://user-images.githubusercontent.com/322311/163013432-21c71ad2-0dc6-4bea-aeba-fa71bda869a0.png)",0,0,msr
4635,If only there was something like https://caniuse.com/mdn-css_at-rules_media_forced-colors,0,0,msr
4636,"Hey @UltraCakeBakery, thanks for your reply here.
I am not sure what you are suggesting and if there is sarcasm involved on your reply.
To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? Do you mind telling us a little bit more please? This is a serious matter that affects thousands of people (in different degrees). Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
This is a genuine problem that needs solving, please provide constructive feedback/solutions.
No negativity is needed, thank you.",0,1,msr
4637,"> Hey @UltraCakeBakery, thanks for your reply here.
> > I am not sure what you are suggesting and if there is sarcasm involved on your reply. To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
> > You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? Do you mind telling us a little bit more please?
> > This is a serious matter that affects thousands of people (in different degrees). Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
> > This is a genuine problem that needs solving, please provide constructive feedback/solutions. No negativity is needed, thank you.
> This is a serious matter that affects thousands of people (in different degrees).
> Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
Hey @savannahx, thanks for your reply here.
I am not sure if there is unawareness or selfishness involved on your reply.
To be honest it feels there was unawareness and selfishness involved in your message because of the way you formulated yourself and did not think about us, the other 99% of svelte-kit users, who also have to deal with this change and asked of us to actually do all the work of implementing this new color.
You provided no way for us to reach a middle ground where both you and others can be happy. It seems like you really just want us to change the color to what you want. Did you even read the link that I posted in the one minute before you started writing this reply you posted 3 minutes after me making my original comment? Do you mind telling us why you get a bad vibe from my comment (saying it is sarcastic) even though I was trying to be helpful?
Suggestions like these affects billions of people (in different degrees). Are you suggesting we completely change the documentation instead of asking you and ""the others"" to start using the build-in tools provided to you by your operating system and browser?
This is a genuine problem that needs solving, please consider changing your mindset. We do not need more any people in the world that are like:
""THE WORLD NEEDS TO CHANGE, N̵̠̲͛̿͘O̵̞̙̒̇͘T̴̼͓̮̈́͂ ME"" ""I AM **N̵̠̲͛̿͘O̵̞̙̒̇͘T̴̼͓̮̈́͂** GONNA WEAR GLASSES OR CHANGE MY SETTINGS! ALL LIGHTS AND COLOURS S̷͈̫̮͙̑̔͌̓̐H̸̨̨̨͎̱̫͊̉̂͌Ó̸͓͓͖̽̏Ǘ̴̹͙͕̝͈ͅĻ̴̛̭͇̭̗̤̈́̽͐D̴̻̈̅̀̕ JUST GET BRIGHTER AND DARKER Ẁ̵̞̻̤̗̩̎̅H̴̨̤̮̼͉͆́̚͜͝É̴̛̠̃̌Ǹ̷͚͔̮͙̬͖̉̄̈̆͌͠ ̵͖̩̰͗̿͝Í̵̜͈͂̋ ̵̟͓̱̤͖̯͙͗̏̍̒́̈̚W̴̩̹̖̭͂̿ͅÂ̷̳̬̱͕̐̄̓̔̚N̴̙̳͔̏͒̇̿ͅT̷͉͚̳̈͂̈́̈͐̌ͅ ̶͍̦͂̓́T̸͍̭̤̲̲͕̔͜H̸̳̣̑͌̓̌̆͛Ḛ̴̢̛̱͋̇́M̶̳̀̀̂̄́͊ ̵͕̣̲̼́͗̈́̎̊̊͑Ţ̸̭͕̪͔̼͑͛̆ͅŐ̷̧̗̫͈̄͑͌͛͆̅O̷͖͈̱̳͓̠̎̋́͘!̵͓̩̙̦̍̂̾ͅ!̴̡̱͇̝͔̻̭̊̀͠!̴̛̠͔̩͚͂̋̕͠!̵̛̟̼̘̼͍͝!!!!""",0,1,msr
4638,"I am impaired too, and get sick of suggestions like these. WE are the ones with disabilities, not the rest of the world. WE need to adapt ourselves to the world, NOT the other way around.
The documentation could offer better support for high contrast mode, dark mode, screen readers.. But I don't think it should.
It is fine as is. If you are still unable to read the documentation - which is also available on Github, which has a very great accessibility - you might want to buy a new monitor, open or close your blinds, change your color profile, tweak the brightness, gamma, contrast and other settings, increase the scale of your UI or install a extension which can do automatic contrast for you. Again, this is a problem only we deal with. We are an edge case.",0,1,msr
4639,"Dude's raging, for young people it's mostly preferable to fortify behind a false sense of reality and stand their ground. They are young and cocky (at least on the internet :P)
I am too old for this. it's okay, I am not perfect either, nobody is. Anyway I really hope this gets addressed if anyone has access and time to implement this color edit (removing the washed out blueish background and using an accent color for the active link).
I would create a pull request but I barely know Svelte so apologies for not taking the initiative in that regard.
@benmccann feel free to let me know if and when I should close this Issue and thanks again for your consideration.",0,1,msr
4640,"alright, time out. i locked the conversation.
accessibility matters to us and we want to get this right. people shouldn't have to fiddle with browser settings just to read documentation. at the same time, there's a balance to be struck between maximising contrast and making the site pleasant to look at for people without visual impairments and/or shitty screens (we could just use pure black on pure white or vice versa everywhere, but that would be very tiring for most people to look at for any length of time).
so we will address this, though i make no guarantees about where it sits in the priority queue, and putting 'asap' in the issue title or declaring that the current contrast ratio means 'i cannot use SvelteKit without it' (unlikely) won't affect that decision (though there is now a pull request for it pending review — https://github.com/sveltejs/sites/pull/327)",0,0,msr
4641,"### Describe the problem
Hello ladies and gents.
I am fairly new to Svelte/Sveltekit but I would like to make a suggestion please.
It is regarding documentation...
Can somebody change the background color of the sidebar from this difficult color #676778; to something darker?
Literally every time you want to focus on a link on the sidebar your eyes hurt and you lose at least 1 to 2 seconds until you focus on what you want to find and all that because poor contrast.
Also the ""highlighted"" link (the active one) is, honestly, almost indistinguishable from the inactive links below and above, why is this? Why not make it something like almost black background with white inactive links and the active link like the orange color from the logo? Perhaps put a divider line in between links or something?
By the way, I do these things every time I use the docs, this is how bad it is, no offense to anyone, but 1-2 seconds of faster focus per person per time they visit the docs, well, we want to be talking about performance when we are talking about Svelte right? Please do the right thing, the nice svelte logo orange color matches great with a darker blackish background color, why put that ugly blueish/purplish (which is not dark enough for a bg color)?
To whoever can make this happen, PLEASE we need docs that can be scanned quickly. Here is the contrast score https://coolors.co/contrast-checker/ffffff-676778
It's almost poor... if you have any sort of sight issues... I mean come on, how come no one addressed this before, docs is a huge part of the process.
### Describe the proposed solution
Color for the ""active"" link => #ff3e00
Color for the sidebar background => #242426
Any dark enough background (with no colors, either blackish or grayish/blackish will do) ### Alternatives considered
Please adjust this so visually impaired or people with sight difficulties (even the ones with glasses) can focus easier on the links and traverse faster. ### Importance
i cannot use SvelteKit without it
### Additional Information
Every time I browse the docs I literally go to inspect and change these myself....!
Thanks for your understanding!",0,0,msr
4643,"Based, Redpilled, and Mental Outlaw's alter deepfake :cat: :+1:",0,1,msr
4644,"Btw, just some ideas: maybe add reviews section to recipes somehow? I know it is meant to be static, but it would be nice to know if what I am making is good/accurate or not. I saw some recipes that sounded nice in theory, but that did not have any images or ratings so I had no idea if I should make them. IMO for a newer iteration of the site, you should make at least one image mandatory for recipes and also ratings would be nice :)",0,0,msr
4645,"The Hugo version of the site is now up. Doesn't look much different, but infinitely easier to manage and compiles in milleseconds. I'm going to start by organizing and systematizing tags and such, then add other improvements.",0,0,msr
4646,"I've added all the author names into metadata, so I don't have to fret about people refusing to follow the formatting guidelines lol
Might add cook/prep time to metadata in the future.
Site is very clean on the backend now. I'm still mastering hugo to do all I want, but I'm feeling like I might start looking at new submissions and maybe I'll start curating things a little more since it's easier.",0,0,msr
4647,Request to change the current text color (blue). I suffer from chromatic aberration and have a very difficult time seeing the color blue. My humble request is to chance the blue text color to another color.,0,0,msr
4648,"@reidellawfirm It could be a settings option stored in localstorage for that case, or you could try a css plugin to change all ""a"" elements to a different color.",0,0,msr
4649,"I'm not tethered to the current colors, so I am open to changing it to something else someone suggests.",0,0,msr
4650,"Changed the colors to be warmer, as in #221",0,0,msr
4651,Hey @LukeSmithxyz I think the links would look better without the underlines. What do you think? Thanks.,0,0,msr
4652,"@LukeSmithxyz I agree with @peepopoggers . Perhaps a bit more spacing between the radio buttons or removing the underlines would be good as the list seems too cramped right now. Could also be the yellow.
Glad to hear you've switched to Hugo.",0,0,msr
4653,"Removing link underlines in general [hurts accessibility](https://seirdy.one/2020/11/23/website-best-practices.html#in-defense-of-link-underlines), however it would definitely be more readable and it wouldn't be so much of an issue if it is only removed from the list. Adding more padding would make the site more usable for mobile users. Therefore, I would suggest something like:
```css
#artlist a, .tagcloud a {
padding: 0.2em;
display: inline-block;
text-decoration: none;
}
```",0,0,msr
4654,"> @reidellawfirm It could be a settings option stored in localstorage for that case, or you could try a css plugin to change all ""a"" elements to a different color.
Thanks for the suggestion, great idea to look into a css plugin.",0,0,msr
4655,"@isti03 I looked at the site you linked; for the bullet points, there are never links next to each other so it is fine.",0,0,msr
4656,"I think that the website could done build using crystal, php or even nodejs and really not relaying on static html pages.
Yes its a great idea to generated them to static html.
But is it worth it? I don't think so, let the server generate it when it requested that it!
The recipes are very small as I can see and it would not take a lot of time to generated it live, right?
You can also cache the generated pages if you want to be more ""economy"".",0,0,msr
4657,"> I think that the website could done build using crystal, php or even nodejs and really not relaying on static html pages. Yes its a great idea to generated them to static html. But is it worth it? I don't think so, let the server generate it when it requested that it! The recipes are very small as I can see and it would not take a lot of time to generated it live, right? You can also cache the generated pages if you want to be more ""economy"".
I can never tell if these people are kidding or not.",0,1,msr
4658,php :skull:,1,1,msr
4659,"I haven't touched this site in like a year because the static site generator used was hobbled-together and couldn't scale without constantly breaking and taking minutes to recompile.
I've decided to switch to Hugo to compile the site, and I hope to finish this in the next week. I didn't switch before because Hugo themes are kind of a pain to specialize for the unfamiliar, but I've since gained familiarity with it enough that I can confidently make a setup perfectly to my vision for the site.
I'll use this as an opportunity to make other improvements to the site.
Note that I still do not want to drastically increase the number of recipes right now. In fact, in the future, I might slim them down to undebatably ""based"" recipes only. (I am no longer ignoring notifications from this git repository.)",0,0,msr
4660,@suhan-paradkar,0,0,msr
4662,"> VNC doesn't have GLX extension. So forget about everything that uses GLX at least until someone manages to get it working.
I've seen people in the reddit group who do it, and it works well for them.",0,0,msr
4664,"No, is vnc:
https://www.reddit.com/r/termux/comments/on4fkp/openglglfw_gears_on_termux_x11_no_root/?utm_medium=android_app&utm_source=share",0,0,msr
4665,"Here is an output of `xdpyinfo`:
<p align=""center""><img src=""https://user-images.githubusercontent.com/25881154/127044060-f1611ab3-7460-4c25-af43-1251de352be5.png"" width=""50%""/></p>
There no GLX and I even has disabled it intentionally for tigervnc which uses same build options as `xorg-xvfb-server`.",0,0,msr
4666,What is on reddit is GLFW examples and not `glxgears`.,0,0,msr
4667,The GLFW examples also happen to you,1,0,msr
4669,@Yisus7u7 I really have no idea whether freeglut works or not. These packages were added as requested and since then I didn't get any reports about whether they actually work or not.,0,0,msr
4670,Oh ok :(,0,0,msr
4671,"I just got `glxgears` working ([sources](https://github.com/JoakimSoderberg/mesademos/blob/master/src/xdemos/glxgears.c)). So it may rather depend on where you got it and how it was compiled, but not on GLX extension presence.
<p align=""center""><img src=""https://user-images.githubusercontent.com/25881154/127045923-4595e77e-a788-4630-9445-1888447e21e6.png"" width=""50%""/></p>",0,0,msr
4674,"> @Yisus7u7 maybe because you're on arm32
Why?",0,0,msr
4675,"![Screenshot_20210928-114239](https://user-images.githubusercontent.com/64093255/135129936-02cb98e3-e66c-4b52-9eeb-75dd4dc0dbc4.png)
@xeffyr I tried the one in that repository and it doesn't work either.",0,0,msr
4676,"> > @Yisus7u7 maybe because you're on arm32
> > Why?
mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu",0,0,msr
4677,"> > > @Yisus7u7 maybe because you're on arm32
> > > > > > Why?
> > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
I guess patches should be made for arm32",0,0,msr
4678,"> > > > @Yisus7u7 maybe because you're on arm32
> > > > > > > > > Why?
> > > > > > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> > Then you have to see what the problem is and create patches.
If I was smart enough to do that I would've already done that 🥴 ...
someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...
can you give me a test program to check wheather OpenGL works or not ?
btw ... i have those scripts on my GitHub repo ...
just go to Termux-Faint-Hope repo and copy the patches folder in /sdcard/ and run the myScripts/build-zink.sh or myScripts/build-mesa.sh",0,0,msr
4680,"> > > > > > @Yisus7u7 maybe because you're on arm32
> > > > > > > > > > > > > > > Why?
> > > > > > > > > > > > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> > > > > > > > > Then you have to see what the problem is and create patches.
> > > > > > If I was smart enough to do that I would've already done that 🥴 ...
> > someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...
> > can you give me a test program to check wheather OpenGL works or not ?
> > https://github.com/davidanthonygardner/glxgears
> > Try compiling and running that
haha wth ... now even linking doesn't work ? I'll have to recompile it then check
btw link for the script is is previous comment, could you check it works or not ?
![Screenshot_2021-09-29-05-29-06-812_com termux](https://user-images.githubusercontent.com/39275638/135179179-3a98d6e5-6deb-4963-8f78-491d89363b3e.jpg)",0,1,msr
4681,I also had that mistake once I compiled mesa... I think bookstores are broken.,0,0,msr
4682,"> I also had that mistake once I compiled mesa...
> > I think bookstores are broken.
ah nvm ... removing ndk-multilib fixed it ...
![Screenshot_2021-09-29-05-37-32-411_com termux](https://user-images.githubusercontent.com/39275638/135179704-87b36ee6-58e5-4489-a8eb-84a93397bace.jpg)",0,0,msr
4686,"> > it works on proot-distro!
> > how exactly ? on a 32 bit arm device ? can you give me exact instructions ?
no, i use arm64 but you can view this vid:
https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s",0,0,msr
4687,"> > > it works on proot-distro!
> > > > > > how exactly ? on a 32 bit arm device ? can you give me exact instructions ?
> > no, i use arm64 but you can view this vid:
> > https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s
on arm64 OpenGL works even normally ... no need for proot-distro",0,0,msr
4689,"i'm sumitted a [problem](https://github.com/ptitSeb/gl4es/issues/342) in gl4es project
let's wait for respnse.",0,0,msr
4690,"> umm ... can i have the wallpaper link ?
https://github.com/Yisus7u7/monas_chinas_uwu_uwu",0,0,msr
4692,"> termux/termux-packages#10226
Here is how I compiled `glxgears` (I intentionally mentioned only the .c file and not the whole repo):
```
~ $ curl -LO https://github.com/JoakimSoderberg/mesademos/raw/master/src/xdemos/glxgears.c
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
100 154 100 154 0 0 1029 0 --:--:-- --:--:-- --:--:-- 1033
100 21237 100 21237 0 0 86261 0 --:--:-- --:--:-- --:--:-- 86261
~ $ clang glxgears.c -o glxgears -lGL -lX11 -lm
```
Running it:
```
~ $ export DISPLAY=:1
~ $ ./glxgears 7265 frames in 5.0 seconds = 1452.936 FPS
```
P.S. I don't use custom mesa with these experimentals with gallium, zink, adreno drivers and other attempts to enable hardware acceleration. Simply installed `build-essential xorgproto libx11 mesa tigervnc`.",0,0,msr
4693,"![Screenshot_20210929-081316](https://user-images.githubusercontent.com/64093255/135275350-30101c43-2ee3-4a93-b4ac-7ba7ea3f1a77.png)
@xeffyr just doesn't work on arm (32-bit)",0,0,msr
4694,"According to this comment
https://github.com/ptitSeb/gl4es/issues/342#issuecomment-931310418
Could it be that disabling glx and mesa-dri has to do with this?",0,0,msr
4695,"@Yisus7u7 It **works** on AArch64, i686 and x86_64. So nothing to do with it. But note that gl4es is broken completely, see https://github.com/termux/x11-packages/issues/442 for details.",0,0,msr
4696,"> @Yisus7u7 It **works** on AArch64, i686 and x86_64. So nothing to do with it. But note that gl4es is broken completely, see termux/x11-packages#442 for details.
It's not just gl4es, glu, freeglut, glew and other openGL-related libraries not working",0,0,msr
4697,"I try glxgears on xwayland (use termux x11 app) and it crash too (Segmentation Fault)
I use 32 bit prefix termux bootstrap (my device actually 64 bit)
On 64 bit termux bootstrap (default) glxgears works fine in xwayland",0,0,msr
4698,Then we can confirm that in arm (32-bit) OpenGL does not work,1,0,msr
4699,"> Then we can confirm that in arm (32-bit) OpenGL does not work
![Screenshot_20211007-161825_Termux_X11](https://user-images.githubusercontent.com/11465849/136356578-1e6b30d0-4d37-424f-830b-48c48695bb68.png)
btw, i just use Xephyr to make nested x server in xwayland, it somewhat bad performance, but it can software renderer and glxgears work in Termux X11 (i user 32 bit proot debian) my command:
(on termux)
termux-x11 &
(I login to proot debian 32 bit, then use this command)
env DISPLAY=:0 Xephyr :1 -fullscreen &
env DISPLAY=:1 xfce4-session
Then, run glxgears from that session...",0,0,msr
4700,https://github.com/termux/termux-x11/releases,0,0,msr
4701,"Use termux from fdroid is fine, i also from fdroid too, its not problem... you need to install termux-x11.deb inside termux",0,0,msr
4702,"> > Then we can confirm that in arm (32-bit) OpenGL does not work
> > btw, i just use Xephyr to make nested x server in xwayland, it somewhat bad performance, but it can software renderer and glxgears work in Termux X11 (i user 32 bit proot debian)
> > my command: (on termux) termux-x11 & (I login to proot debian 32 bit, then use this command) env DISPLAY=:0 Xephyr :1 -fullscreen & env DISPLAY=:1 xfce4-session
> > Then, run glxgears from that session...
@xeffyr Based on this, will OpenGL in arm not be able to access any driver or library?",0,0,msr
4703,"@Yisus7u7 x11-packages **never** supported any kind of hw acceleration (officially) and therefore doesn't require any driver to work. In fact, mesa here is provided to fullfill dependency needs of gtk and qt5 libs and maybe for some other packages. I don't know how deeply OpenGL is broken but on my device `glxgears` [works](termux/termux-packages#10226) as was shown previously.",0,0,msr
4704,"Well, I don't know if glxgears use opengl, but it still fails in arm.",0,0,msr
4705,"`glxgears` use exactly OpenGL (not OpenGLES) and yes, it still fails because not fixed.",0,0,msr
4706,"> Well, I don't know if glxgears use opengl, but it still fails in arm.
Are you in proot? If yes, you can just run Xephyr on top of Termux Wayland (Termux X11) or Xserver-xsdl.... and you can run glxgears there and some 3D software rendering too.. but performance is very low fps",0,0,msr
4708,"aftef you run xephyr :1, then just
`env DISPLAY=:1 glxgears`
Then view it in your termux X11 app
Btw, this is for proot distro, the xephyr unavailable in termux repo
You need to inside proot (proot-distro) for this method",0,0,msr
4710,"![Screenshot_20211016-152508](https://user-images.githubusercontent.com/64093255/137601417-26228ba0-6245-4aee-9acf-50b9b9840cc4.png)
![Screenshot_20211016-152816](https://user-images.githubusercontent.com/64093255/137601418-e74a9f84-467b-4376-93a3-efdfdcc3dc39.png)
@xeffyr Posh Shell (armhf alpine, proot) has no problem running openGL, so the error will be that any specific hardware/driver cannot be accessed?",1,0,msr
4717,"> > > The performance is excellent, there is no lag, everything goes very fast
> > > > > > Does it run on debian proot/chroot?
> > No, it is fully native in termux x11
> > https://youtu.be/hsO-Os9jr8g
what really !!! have to check right now !!! 😃",0,0,msr
4720,"![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg)
![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg)
this was using the Zink driver",0,0,msr
4721,"> ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
Can it work on Adreno 506?",1,0,msr
4724,"> > > > ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
> > > > > > > > > Can it work on Adreno 506?
> > > > > > it should work on all GPUs that support vulkan
> > Can you give me tutorial link? I tried few days ago, i read tutorial somewhere, but in the end, i compile and not work.. Is it like turnip? do you have tutorial?
> > I will try in chroot..
you could try the instructions in https://github.com/suhan-paradkar/tewmux-disabled/releases/download/mesa-hw/instructions.tar.gz
i dont know weather it will work in an chroot envionment",0,0,msr
4728,"from what i have seen before mesa has some issues when compiling in ci, mainly saying llvm-config not found",0,0,msr
4730,"a little help ?
i just can't seem to get it work 😥",0,0,msr
4740,"Reopening issue. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code
**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.
**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.
**Artifacts Found**
- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar
**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",0,0,msr
4742,"Updating issue after ping interval. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code
**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.
**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.
**Artifacts Found**
- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar
**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",0,0,msr
4744,"Status update:
The plane demo is gone. But the mobile/android directory remains.",1,0,msr
4745,"I'm sorry, v-time-picker and v-date-picker not coming with this release?",0,0,msr
4748,"@michaelnguyen08, @omerkimel , @JesusFregoso You can see the list of components that didn't make the 3.0 cut in the [3.1 milestone](https://github.com/vuetifyjs/vuetify/milestone/56).
These were cut from the initial release fairly early this year. Noteably this includes the following components:
- Calendar
- TimePicker
- data-iterator
- date picker
- v-stepper
- v-speed-dial
- v-skeleton-loader
There are some feature requests in the milestone that I am hoping John and the team will punt to a later release to expedite getting these base components released. v-data-table was mentioned in above, but that is supposed to be in v3.0.0 milestone. I just looked at the docs for v3 and it looks like v-data-table was renamed to v-table. v-table is in the docs and in the list above so that appears to have made the cut for v3.0.",0,0,msr
4749,"Hi. I just opened the node_modules/vuetify/lib/components directory and I can see, that a component directory ""VDataTable"" is included in the Beta.1 build. When I try to include it in my App, I see, that VUE is not able to resolve it ""[Vue warn]: Failed to resolve component: v-data-table"". Seems, that the component is just not registered. And I disagree, that will be renamed...",0,0,msr
4751,"Where is `loading` prop in VBtn component ? Is it removed completely or added later ?
It was amazing button feature.",0,0,msr
4753,"> I am guessing that not only are components being cut, but also props? For example, Menu is missing a fantastic amount of props functionality -
> > https://vuetifyjs.com/en/api/v-menu/#props https://next.vuetifyjs.com/en/api/v-menu/#props
> > I need `left` 😕
> > What is the plan with this?
@aentwist Well v-menu is still WIP, you can track the progress here: https://github.com/vuetifyjs/vuetify/issues/13489 or in the [3.0.0 milestone](https://github.com/vuetifyjs/vuetify/milestone/45)
> I need `left` 😕
I have not used v3 yet but based on the docs it seems like `:anchor=""start""` could be what you are looking for?",0,0,msr
4757,Would like to suggest adding a Nuxt3 plugin to this list as well.,1,0,msr
4759,@warflash If that's what @KaelWD says then I suppose that's how it is. However I am really anxious to see support for SASS Variables in Nuxt3 for the Vuetify project.,0,0,msr
4762,"Isn't 3.0
3.1
3.2
3.x
incremental enough?
Just wait until everything is proper released.
I can't understand people needing to upgrade to a new version as soon as possible, even if it is Beta.
The only thing I would like would be a comparison table about which component gets added into which version.
Also you can wait for Vue 2.7 and Vueitfy 2.7.
These are already announced migration versions with backported features.....",0,1,msr
4763,"> For those components not included in the initial release, is there an incremental path for people who's apps use those components, or are they just going to have to wait until 3.1 before they can upgrade?
If you rely on some of the components that aren't going to be in 3.0, I see two choices:
1. you wait until the release that contains all of the components you use
2. you replace the missing components with components you build
I have been waiting for Vuetify 3 so that I can migrate to Vue 3 and Vite, but I have chosen option 1 above. I use most of the components that aren't available in 3.0
- Data Table
- Calendar
- TimePicker
- date picker
- v-stepper
- v-skeleton-loader",1,1,msr
4765,"> Please add v-data-table in this release. Thanks
How? It's not done.",0,0,msr
4772,"`This is BETA documentation for Vuetify 3, examples and information may be broken or outdated.`",0,0,msr
4775,@eliezerp3 if you want to avoid a delay please submit PR's to port the components.,0,0,msr
4778,"I'm working on getting the tuning pass PRs up. Some will be involved reviews, over half are simple and will not.",0,0,msr
4780,when can I use the vuetify 3.0.,1,0,msr
4782,"> In regards to v-btn, the v-progress-circular component wasn't ported when it was. It'll be there, I just have to get to it.
@johnleider but the `v-progress-circular` component is in the beta 2",0,0,msr
4783,"Yes, but it wasn't when `v-btn` was created in v3.",0,0,msr
4785,"> @Tan-Jerry after you submit PR's to accelerate porting.
@MartinX3 Did you try it by yourself ? PR's does not accelerate anything. Just for example (prior bugs for me)
https://github.com/vuetifyjs/vuetify/pull/15132
https://github.com/vuetifyjs/vuetify/pull/15129",0,0,msr
4789,Thanks for the update!,0,0,msr
4790,"thanks for the update @KaelWD , we're getting closer to some estimates.
Regarding DOCS, I'm not telling that the Docs are not updated.
On the contrary! The [v.3.0 docs for `v-select`](https://next.vuetifyjs.com/en/api/v-select/#props-items) state that `props-item` are working as expected.
Yet as I and other devs have stated, read [more here in the explained issue](https://github.com/vuetifyjs/vuetify/discussions/15054) - that's not the fact. I myself tried to test it even beyond docs in different ways..
The problem here is that we lose the point of reference/truth, on what can we rely in the current `beta.1` and the following versions of Vuetify?
Or you're trying to say that the docs DOES NOT reflect the currently released version and is written independently to reflect the final (desired) v.3.0?
If it does reflect, could anyone investigate that issue of v-select. As being one of the central UX components and not working as documented, I feel we are pushed for alternatives.
Thanks in andvance",0,1,msr
4791,"You might've missed this
![Screenshot_20220602_213556](https://user-images.githubusercontent.com/16421948/171620794-aa64b58f-2add-4eaf-9c6d-e9ed577d71b1.png)
The example you linked to was for v2 and hasn't been updated yet, it needs `item-title=""text""` to work with those objects",0,0,msr
4792,"> You might've missed this ![Screenshot_20220602_213556](https://user-images.githubusercontent.com/16421948/171620794-aa64b58f-2add-4eaf-9c6d-e9ed577d71b1.png) The example you linked to was for v2 and hasn't been updated yet, it needs `item-title=""text""` to work with those objects
Do you mind receiving PRs with documentation updates?",0,0,msr
4793,"- I've seen that of course (actually every time I go through docs :) ) - yet I'm not worrying of is it outdated, but too advanced for the stated of the code it should document :)
- docs https://next.vuetifyjs.com/en/api/v-select/#props-item-props is for v3 and not v2 (I was using the proper doc)
- I was not trying any v2 example but was trying to make v3 v-select to work according to v3 [component doc](https://next.vuetifyjs.com/en/components/selects/), abd [api doc](https://next.vuetifyjs.com/en/api/v-select/#props-items).
- `item-title` prop is not documented, but **item-text, item-value and item-disabled** props. Nevertheless, neither is solving this issue.
![image](https://user-images.githubusercontent.com/1769627/171724295-81666a67-62d8-4eec-b555-e8f0ce7f6ad4.png)
- So, after trying all possible and impossible variants to handle `v-select` supporting objects instead of simple text, I can confirm it's not working. ANd as I've said, you have other dev(s) claiming the same.
- and that is not some advanced functionality, but needed for almost any app.
- if we accept that there is some Docs misbehaviour, the point is still: What is our point of reference then, when we use components and their props? or even for community-testing of the Beta versions to give you any feedback?",0,1,msr
4795,"@johnleider Please keep the issue uptodate and add e.g. v-calender which is makes by @KaelWD (thx):
https://github.com/vuetifyjs/vuetify/pull/15038",0,0,msr
4796,"> can we get any answer instead of marking the comment as outdated??
> > I would really like to keep giving chance to Vuetify, but by this kind of communication it's even harder to feel reliability of the framework.
> > Thanks
> > Reliability of the framework? Give me a break. Take this conversation to Discord and stop whining here.",0,1,msr
4798,"Here is my unsolicited feedback on this recent tangent, I think others might find it useful. Note: I am making assumptions about the state of things in this project so if I am wrong please feel free to correct me.
@sinisarudan Vuetify 3 is beta, which usually means:
- Development has priority over authoring docs right now. - Errata, and missing information is to be expected in the docs while in beta up until V3 reaches some unspecified level of maturity.
- The development team has no obligation to provide API guarantees until GA.
- The scope of the V3 initial release is locked in, what you see on the release checklist is meant to give you an idea how close/far they are to release.
- I have my own questions about V3 but I know that it would be inappropriate to ask them HOW to do something with V3 Beta, and more appropriate to ask them if a specific feature, or component will be included in the initial release (excluding anything that they may consider obvious)
- The dev team is free to ignore discussions or bug reports until V3 is released. They are more likely to interact with you if you intend to contribute code or if you engage with them on something that team members are currently concerned with or occupied with.
I specifically posted to this task, because I wanted to know about a feature. I didn't get a response, and it's no big deal for me right now. I have other things to worry about.
TLDR: Don't expect any kind of support from the dev team for non GA releases. They're still making V3 and will more likely converse with you as long as the discussion has to do with any of their present concerns.",0,0,msr
4800,"> I think that beta is an incorrect tag for the current situation. It should be alpha. So many components doesn't work properly like they worked in 2.6 version. Everything great. Let's just wait. This is great UI framework. I am still waiting my v-data-table component
Which components do not work properly?",0,0,msr
4803,"> > I think that beta is an incorrect tag for the current situation. It should be alpha. So many components doesn't work properly like they worked in 2.6 version. Everything great. Let's just wait. This is great UI framework. I am still waiting my v-data-table component
> > Which components do not work properly?
Vuetify3 beta3
Component breadcrumbs. Has invalid styles and html structure
![image](https://user-images.githubusercontent.com/5112918/171979191-c0906edb-6461-4dac-b7c1-465308ddb278.png)
Vuetify3 beta3.
Validation Error should have red color but it is gray.
In beta2. It was red
![image](https://user-images.githubusercontent.com/5112918/171979212-2cc95267-112e-4aee-a0db-58081d7e77f0.png)
I sm uding vite and latest version of everything. let me know, I can provide more info. Very appreciate for everything",0,1,msr
4805,"> Would like to suggest adding a Nuxt3 plugin to this list as well.
I will need help with this as I do not use NUXT. Happy to help however I can though.",1,0,msr
4806,"# V3 Release Checklist
The following is an in progress list of remaining tasks before we release the next version. This will be updated daily; so watch for changes.
Discord, https://community.vuetifyjs.com/ and join channel **#v3-discussion**.
## Final review
- [x] Framework Core #15139
- [x] Documentation
- [x] UI Components
- [x] Tooling
### Documentation
- [x] Introduction
- [x] Getting started
- [x] Features
- [x] Styles and animations
- [x] Directives
- [x] Resources
- [x] About
### Tooling
- [x] CLI plugin
- [x] Vite plugin
- [x] Presets
- [x] Eslint
### UI Components
A component is considered complete when the following criteria is met:
<details>
* Primary and child components reviewed
* imports ordered with appropriate comments
* converted to use grid css structure (if applicable)
* unit tests
* jest spec test (if applicable)
* cypress e2e test
* SASS/SCSS code reviewed
* removes deprecated or unused code
* abstracted explicit values to variables
* verified sass variable usage
* index.ts file export formatted
* Documentation page
* all examples working with no deprecated code
* removed unused or deprecated examples
* added **all components** page image
* added quick bar and links
* updated page entry and formatting
* new sections added
* accessibility (if applicable)
* anatomy
* grid (if applicable)
* sass variables
* theme (if applicable)
* updated api section formatting
* frontmatter information reviewed
</details>
- [x] v-alert #14971
- [x] v-app #15179
- [x] v-app-bar #15192
- [x] v-autocomplete #15355
- [x] v-avatar #15112
- [x] v-badge #15193
- [x] v-banner #15109
- [x] v-bottom-navigation #15194
- [x] v-breadcrumbs #15181
- [x] v-btn #15213
- [x] v-btn-group #15195
- [x] v-btn-toggle #15196
- [x] v-card #15283
- [x] v-carousel #15197
- [x] v-checkbox #15264
- [x] v-chip #14973
- [x] v-chip-group #15198
- [x] v-code #15182
- [x] v-color-picker #15292
- [x] v-combobox #15354
- [x] v-counter #15183
- [x] v-default-provider
- [x] v-dialog #15289
- [x] v-divider #15113
- [x] v-expansion-panels #15295
- [x] v-field #15251
- [x] v-file-input #15288
- [x] v-footer #15114
- [x] v-form #15253
- [x] v-grid #15296
- [x] v-hover
- [x] v-icon #15212
- [x] v-img #15287
- [x] v-input #15260
- [x] v-kbd #15199
- [x] v-label #15256
- [x] v-lazy #15184
- [x] v-list #15360
- [x] v-locale-provider #15255
- [x] v-main #15254
- [x] v-menu 5e4992ffa
- [x] v-messages #15200
- [x] v-navigation-drawer #15263
- [x] v-no-ssr #15185
- [x] v-overlay #15284
- [x] v-pagination #15286
- [x] v-parallax #15226
- [x] v-progress-circular #15266
- [x] v-progress-linear #15265
- [x] v-radio #15258
- [x] v-radio-group #15259
- [x] v-range-slider #15228
- [x] v-rating #15285
- [x] v-responsive #15249
- [x] v-selection-control #15246
- [x] v-selection-control-group #15250
- [x] v-sheet #15115
- [x] v-slide-group #15248
- [x] v-slider #15227
- [x] v-snackbar #15242
- [x] v-switch #15247
- [x] v-system-bar #15243
- [x] v-table #15186
- [x] v-tabs #15262
- [x] v-text-field #15252
- [x] v-textarea #15257
- [x] v-theme-provider
- [x] v-timeline #15230
- [x] v-toolbar #15154
- [x] v-tooltip #15245
- [x] v-validation
- [x] v-window #15244
<!-- override-close -->",1,0,msr
4808,"> Who's social media accounts were you looking for anyway? Why didn't you just ask them?
Private investigators, corporate fraud departments, law enforcement, etc. tend to use OSINT to identify someone.",0,0,msr
4810,"I can't tell what you're talking about, but if you're using this to stalk your ex, go away.",0,1,msr
4811,"I don't like putting numbers in my screen name so if I can't have it this way, I can use one of my other 4 GitHub logins 🤷‍♀️
Is that ok? Who's social media accounts were you looking for anyway? Why didn't you just ask them?",0,1,msr
4812,same here !!!!,0,0,msr
4813,"I can confirm this issue , exact same issue as Kerumen with the following differences in environment:
* Running on CentOS 7 (7,9.2009)
* Nodejs 16.15.0
* Installed locally
* Upgraded from 4,42,1
* Database is MariaDB 1.4.22 (AWS RDS)
* Browser is Chrome 101.0.4951.41 on Windows 11
Hope that info helps",0,0,msr
4814,"Hi! Thank you for reporting this error 😊
- Could you post the full log output? - Post the result of this SQL query: `SHOW VARIABLES LIKE 'sql_mode'`",0,0,msr
4817,"Same problem here, rollbacked with a save to 4.45.0. I'm in mariadb",0,0,msr
4819,"Thanks @daniellockyer for the explanation.
Unfortunately there are some environments where MySQL is not supported ([Archlinux](https://wiki.archlinux.org/title/MySQL) for example) and we _have_ to use MariaDB. MariaDB is supposed to be a drop-in replacement of MySQL so everything should work as expected (and was working until `v4.46`).
I know this is not totally in your control, I just wanted to note this :)",0,0,msr
4820,Is this issue fixed with 4.47?,0,0,msr
4821,"The upstream issue hasn't been fixed yet - details in this comment: https://github.com/TryGhost/Ghost/issues/14634#issuecomment-1114989262
In the meantime, a friendly forum user has shared how he updated from MariaDB to MySQL8 on Ubuntu here: https://forum.ghost.org/t/how-to-migrate-from-mariadb-10-to-mysql-8/29575",0,0,msr
4822,Experiencing the same issue since updating to the latest MariaDB and ghost docker images.,0,0,msr
4824,"Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?",1,0,msr
4826,"> Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?
https://github.com/TryGhost/Ghost/issues/14446 > **MySQL 8 is supported in all environments & the only supported DB for production.**
and
**Note :MariaDB is not an officially supported database for Ghost. It just happened to work given the similarities with MySQL, but we optimize and test for MySQL 5 and 8. As of Ghost 5.0 we are only officially supporting MySQL8 in production so that we can double down on DB optimizations. We strongly recommend changing to MySQL8 and [a helpful guide can be found here](https://forum.ghost.org/t/how-to-migrate-from-mariadb-10-to-mysql-8/29575).**",0,0,msr
4827,"> > Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?
> > #14446 > **MySQL 8 is supported in all environments & the only supported DB for production.**
Thanks, I also found the post on their forum too. Astoundingly stupid decision. I don’t even use the “newsletters” stuff. 😞",0,1,msr
4829,yeah why not I don't want to send my personal tag in public,0,0,msr
4830,"Solely relying on Oracle's solution (the same that thrashed OpenOffice, bribed african government officials in order to win business contracts, or made dangerous false claims of security...) despite a community-driven one is beyond me.
Update: mysql-8.0 is neither available in Debian stable repositories, nor testing repos (but in sid for now). Installing mysql-8.0 from Oracle (who I trust less than Debian packagers) break dependencies with packages relying on MariaDB 10, the default-mysql-server in Debian. So, because of a single migration, one of the biggest distros cannot be used to host a Ghost blog. This is madness.",0,1,msr
4831,"> Solely relying in Oracle's solution (the same that thrashed OpenOffice, bribed african government officials in order to win business contracts, or made dangerous false claims of security...) despite a community-driven one is beyond me.
The only explanation is that commercial version of Ghost runs on MySQL and they care just about that. We as users of open source Ghost are not important for them, we don't pay.
Why PostgreSQL is dropped some years ago if Ghost uses [knex](https://github.com/knex/knex), which supports PostgreSQL, MySQL, CockroachDB, MSSQL, SQLite3 and Oracle? And now they drop MariaDB which is the default MySQL flavor at all major Linux distributions. It seems it's time to drop Ghost, I won't use it anymore.",0,1,msr
4833,"> Please, can we keep the discussion on this thread related to the migration bug and not about the MariaDB support.
> > @daniellockyer could you explain why is this migration required? Isn't it possible to rewrite it without the Knex bug? I've ran dozen of migrations successfully on my Ghost instance. I don't understand why this one, which seems fairly simple, has a bug and that is the first time we see it on MariaDB. Thanks!
Sure seems we have a hard time keeping it related to migration bug because all the lights point in that dropping MariaDB is intentional?",0,1,msr
4834,"Instead of pushing on solving the knex bug ""default 'NULL'"" for datetime type, which is obviously wrong, 'NULL' is string and not the datetime type, Ghost team wants to drop MariaDB. Great guys, stick with enforcing bugs. And instead of switching to type strict TypeScript, let's keep JS and no types in database, who cares, same sh*t. It's probably the right time to write another blog post for a falling open source project.",0,1,msr
4835,"My comment in the announcement of [Ghost 5](https://github.com/TryGhost/Ghost/issues/14446) has been marked off-topic, why? Do you really want to intentionally make BC just to drop MariaDB? And only keep shi**y MySQL with the wrong behavior? Datetime can't have default value of 'NULL', that's string guys.",0,1,msr
4836,"> My comment in the announcement of [Ghost 5](https://github.com/TryGhost/Ghost/issues/14446) has been marked off-topic, why? Do you really want to intentionally make BC just to drop MariaDB? And only keep shi**y MySQL with the wrong behavior? Datetime can't have default value of 'NULL', that's string guys.
I'd just like to say...you made a comment about ""non-commercial"" users but...Don't THEY know those of us TINKERING with it are who will sell it for them if it's something we like? LOL sigh....",1,1,msr
4837,"Hey all 👋, I've locked this bug to contributor comments only. It's caused by an [upstream bug in knex](https://github.com/knex/knex/issues/5154). When a fix is pushed into knex, Ghost's renovate setup will automatically pull it into Ghost, meaning it will be included in the subsequent release. If anyone needs or wants some support with migrating from MariaDB to MySQL, please use the [forum](https://forum.ghost.org/).
I understand there's a lot of frustration around the clarification that MariaDB is not an officially supported database. As a small team we've always provided a very narrow set of environments that are officially supported to keep our maintenance overhead manageable: Ghost is intended for use with Ubuntu, MySQL, nginx & Node.js LTS. We're not able to support every flavour of environment without differences being handled correctly in the upstream packages. The way to change what's supported would be to contribute the necessary fixes & improvements - we'd absolutely love to see more contributions around database interoperability in [knex](https://github.com/knex/knex/issues).",0,1,msr
4838,"Hey all 👋🏻 @iBotPeaches sent a PR with the fix to Knex and it's been merged + released: https://github.com/knex/knex/pull/5181
We've just released Ghost v4.48.1 with the updated Knex, so this issue should now be fixed. Please update to v4.48.1 using Ghost-CLI 🙂",1,0,msr
4839,"### Issue Summary
When I tried to upgrade my Ghost from `v4.44.0` to `v4.46.0` I got this error message upon launch:
> Message: Ghost was able to start, but errored during boot with: alter table `newsletters` modify `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
I must add, my `newsletters` table has no data:
<img width=""901"" alt=""Screenshot 2022-04-30 at 13 15 12"" src=""https://user-images.githubusercontent.com/5436545/166103277-f0f4cfc8-2d12-40b6-931a-edbcadf87443.png"">
### Steps to Reproduce
1. Upgrade Ghost to `v4.46.0`
2. Launch Ghost
### Ghost Version
v4.44.0
### Node.js Version
v14.18
### How did you install Ghost?
Local install on Linux server
### Database type
Other
### Browser & OS version
_No response_
### Relevant log / error output
_No response_
### Code of Conduct
- [X] I agree to be friendly and polite to people in this repository",0,0,msr
4840,"I'm not commenting on whether this feature should or should not be included, but how would you specify both a class on an `<li>` and on `<ol>` or `<ul>` if you must do it on the same line?",0,0,msr
4841,"> I'm not commenting on whether this feature should or should not be included, but how would you specify both a class on an `<li>` and on `<ol>` or `<ul>` if you must do it on the same line?
As mentioned in the doc update this isn't possible with this patch. I initially started out with a `parent:` prefix (e.g. `parent:attr=value`) to be able to more precisely specify which attributes should be lifted to the parent, but decided against it as it needed quite more code changes. Plus I wonder how many times you would really want to be able to set attributes on both list items and the list itself. For the latter you only need to use an attribute on a single list item, while all the other list items can still have their own attributes.",1,0,msr
4842,An alternative could be to have a single ^ as a marker in the attribute list to indicate that all attributes following it should be set on the parent. That would allow both list and table attributes with minimal code changes.,0,0,msr
4843,I updated the pull request to use `^` as a marker to signal when attributes should be lifted. Solves the issue of not being to define both list element attributes and parent attributes in the same attribute definition,0,0,msr
4844,"As I have stated on every request we have received for this, we will not be implementing this feature. Of course, you are free to provide this functionality in a third party extension.",0,0,msr
4846,"This has been discussed and explained many times over. You even linked to some of those discussions. In short, I am not interested in supporting a syntax that is does not make sense to me. It should be no surprise that it was rejected. Of course, you are free to disagree with me on this. If this is important to you, then you can provide and support your own third-party extension which provides the feature.",0,1,msr
4847,"> In short, I am not interested in supporting a syntax that is does not make sense to me.
From me as an outside-viewer I can see this response as essentially ""I don't like this, so I don't want it"" response.
You put your own bias over something that the majority of people may like to have for once. I saw this problem in MkDocs and see it here again. You seem to have a general issue with accepting things that you don't like, but that the community could benefit from.",0,1,msr
4848,"I don't accept things that I'm not willing to support long term. Long after the person who proposes it is gone, I'm left defending it and fixing issues with it. That's (one of the reasons) why we provide a public extension API. Anyone can provide their own implementation which works how they want. Then they can support it without any additional burden on me. I am not limiting anyone at all except for myself.",0,1,msr
4850,"I am experiencing the same issues, actually it start occurring two days ago.
@jujumilk3 I suggest to amend the title of the issue with something more talkative, such as ""Maximun retries exceeded issues"" so other people can see there's already an issue going on.",0,0,msr
4851,"Same thing.
I am agree with @carloocchiena. @jujumilk3 you should change the title.",0,0,msr
4853,@jujumilk3 Thanks for creating this issue. This issue is related to #1471. Please use https://github.com/anuraghazra/github-readme-stats/issues/1471#issuecomment-979306704 as a workaround.,0,0,msr
4855,"same issue just popped up in my account as well. no hurries take your time to fix it if it's a problem in the code. amazing service again, cheers !",0,0,msr
4858,"**Describe the bug**
Service doesn't work
**Expected behavior**
I think It's a kind of traffic problem as i captured.
**Screenshots / Live demo link (paste the github-readme-stats link as markdown image)**
<img width=""526"" alt=""스크린샷 2022-05-18 오후 7 49 16"" src=""https://user-images.githubusercontent.com/41659814/169022351-d751d8c9-5ae1-484d-ab3a-6ea53c9f7d91.png"">
you can just visit my profile
https://github.com/jujumilk3",0,0,msr
4859,bug happening to me too,0,0,msr
4860,"Yes, and it's serious! So is mine.",0,0,msr
4863,"**Describe the bug**
It looks like the new tiny URL that was introduced in 2fb452c119e9cb4706a1ecf708295db9b77b4b7e that is shown when an error is thrown does not fit on the card. Not a pressing issue but just putting this here so that we don't forget.
![image](https://user-images.githubusercontent.com/17570430/169273808-75921d34-9432-466b-905b-deb6f45c8413.png)",0,0,msr
4864,"@sftim: This request has been marked as needing help from a contributor.
### Guidelines
Please ensure that the issue body includes answers to the following questions:
- Why are we solving this issue?
- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?
- Does this issue have zero to low barrier of entry?
- How can the assignee reach out to you for help?
For more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.
If this request no longer meets these requirements, the label can be removed
by commenting with the `/remove-help` command.
<details>
In response to [this](https://github.com/kubernetes/website/issues/33848):
>**This is a Feature Request**
>
><!-- Please only use this template for submitting feature/enhancement requests -->
><!-- See https://kubernetes.io/docs/contribute/start/ for guidance on writing an actionable issue description. -->
>
>**What would you like to be added**
>Update https://kubernetes.io/docs/contribute/ so that early in the page there is a link to
>https://contribute.cncf.io/contributors/projects/#kubernetes
>
>You should decide on some suitable text for the hyperlink. Read https://contribute.cncf.io/contributors/ to get ideas about what to write, if you're not sure.
>
>:information_source: Don't remove any of the existing links; this is an addition.
>
>**Why is this needed**
>This change will help signpost readers to the CNCF contributor site.
>
>**Comments**
>/kind feature
>/language en
>/triage accepted
>/priority backlog
>/help
>
Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>",0,0,msr
4865,/assign,1,0,msr
4866,/assign,0,0,msr
4868,/assign,1,0,msr
4869,Hi @NitishKumar06 We are working on this issue. please work on other issue.,0,0,msr
4871,I've already made changes to this issue now. Please let it be reviewed.,0,0,msr
4872,/assign,1,0,msr
4873,"No you can't work on this issue if someone is already working . this is against the policy of kubernetes. tonight I will open PR for this issue. There are many issues just work on that.
/unassign @NitishKumar06",0,1,msr
4874,"See this issue has been opened from three days and as far I've observed the another contributor also raised the assign option for the same. Let my changes be reviewed and if it doesn't hold good, definitely you can also raise PR. The best one will be merged. /assign",0,1,msr
4875,/assign,1,0,msr
4877,"Why are you doing this again and again? @sftim Please have a look. I've raised the PR for this issue which was not raised by anyone for 3 days. Now, he is unassigning me.",1,1,msr
4878,/assign,1,0,msr
4879,Infact there's @kadtendulkar as well. Why are you not removing her from assignes? Please be respectful.,0,0,msr
4881,cool. Do this I've raised the PR anyways.,0,1,msr
4882,/assign,1,0,msr
4883,@NitishKumar06 this is against the policy of kubernetes. /unassign @NitishKumar06,0,1,msr
4884,"Hey @NitishKumar06 please don't work on this issue, I'm new to community and this going to be my first issue. @ashish-jaiswar is helping me with that. This is community to help and work with unity. Please give chance to new contributers to work as well.
/unassign @NitishKumar06",0,0,msr
4885,@kadtendulkar Actually this is infact my first contribution as well. The fact that @ashish-jaiswar has more PRs merged shows he isn't a newbie here. My last PR was not merged because someone else merged it through my changes . Please notice that!,0,0,msr
4886,/assign,1,0,msr
4888,@kadtendulkar Why are you doing this?,0,1,msr
4890,Hey @NitishKumar06 you are not following the policy of kubernetes. I will report you to Kubernetes community.,0,1,msr
4891,"Okay @ashish-jaiswar , Let my PR be reviewed. Sounds good now?",1,0,msr
4892,"hey @NitishKumar06, kubernetes community will not accept your PR. Because you are working against the policy of kubernetes.",0,1,msr
4893,@ashish-jaiswar My last PR was not merged because although I was working on this issue as well but someone after me started working on it and his PR got merged.,0,0,msr
4895,Thanks @sftim . This was needed! @ashish-jaiswar Please go through it especially the first point.,0,0,msr
4896,/assign,1,0,msr
4897,Thanks @sftim @NitishKumar06 I'm requesting since I'm new contributer please allow me to do. According to the 2nd and 3rd point its appropriate to check if assigned people are working or not. And i have already mentioned I'm working on it. please know that. /unassign @NitishKumar06,0,0,msr
4898,"But why didn't @ashish-jaiswar raise this point at that time when you made yourself assigned to this issue? He was working on that issue before you . But when I assigned it to myself, you know .",0,1,msr
4899,@sftim Please tell @kadtendulkar not to misuse this function of unassign. /assign,0,0,msr
4900,/assign,1,0,msr
4902,/assign,1,0,msr
4904,Work on issue . See ya!,0,1,msr
4906,"Hey @NitishKumar06 , @ashish-jaiswar is helping me to raise my first PR that's why he assigned himself first. Please understand, I'm new to kubernetes community and I'm finding such conflicts.
It is always good to ask if the assigned people are working or not and since beginning i an mentioning that I'm working on it. But seems like you are not coordinating with new contributers .",1,1,msr
4909,"I've been working on my project for three weeks and I haven't done anything because of you. I bought a vuexy template, Not all libraries have problems, but node sass gives errors, I might get fired because of you, This is the error Make it your eyes.
error /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass: Command failed.
Exit code: 1
Command: node scripts/build.js
Arguments: Directory: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass
Output:
Building: /usr/local/bin/node /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/bin/node-gyp.js rebuild --verbose --libsass_ext= --libsass_cflags= --libsass_ldflags= --libsass_library=
gyp info it worked if it ends with ok
gyp verb cli [
gyp verb cli '/usr/local/bin/node',
gyp verb cli '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/bin/node-gyp.js',
gyp verb cli 'rebuild',
gyp verb cli '--verbose',
gyp verb cli '--libsass_ext=',
gyp verb cli '--libsass_cflags=',
gyp verb cli '--libsass_ldflags=',
gyp verb cli '--libsass_library='
gyp verb cli ]
gyp info using node-gyp@3.8.0
gyp info using node@16.14.2 | darwin | x64
gyp verb command rebuild []
gyp verb command clean []
gyp verb clean removing ""build"" directory
gyp verb command configure []
gyp verb check python checking for Python executable ""python2"" in the PATH
gyp verb `which` succeeded python2 /usr/bin/python2
gyp verb check python version `/usr/bin/python2 -c ""import sys; print ""2.7.18
gyp verb check python version .%s.%s"" % sys.version_info[:3];""` returned: %j
gyp verb get node dir no --target version specified, falling back to host node version: 16.14.2
gyp verb command install [ '16.14.2' ]
gyp verb install input version string ""16.14.2""
gyp verb install installing version: 16.14.2
gyp verb install --ensure was passed, so won't reinstall if already installed
gyp verb install version is already installed, need to check ""installVersion""
gyp verb got ""installVersion"" 9
gyp verb needs ""installVersion"" 9
gyp verb install version is good
gyp verb get node dir target node version installed: 16.14.2
gyp verb build dir attempting to create ""build"" dir: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build
gyp verb build dir ""build"" dir needed to be created? /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build
gyp verb build/config.gypi creating config file
gyp verb build/config.gypi writing out config file: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build/config.gypi
(node:14904) [DEP0150] DeprecationWarning: Setting process.config is deprecated. In the future the property will be read-only.
(Use `node --trace-deprecation ...` to show where the warning was created)
gyp verb config.gypi checking for gypi file: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/config.gypi
gyp verb common.gypi checking for gypi file: /Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/common.gypi
gyp verb gyp gyp format was not specified; forcing ""make""
gyp info spawn /usr/bin/python2
gyp info spawn args [
gyp info spawn args '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/gyp/gyp_main.py',
gyp info spawn args 'binding.gyp',
gyp info spawn args '-f',
gyp info spawn args 'make',
gyp info spawn args '-I',
gyp info spawn args '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass/build/config.gypi',
gyp info spawn args '-I',
gyp info spawn args '/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp/addon.gypi',
gyp info spawn args '-I',
gyp info spawn args '/Users/kawan/.node-gyp/16.14.2/include/node/common.gypi',
gyp info spawn args '-Dlibrary=shared_library',
gyp info spawn args '-Dvisibility=default',
gyp info spawn args '-Dnode_root_dir=/Users/kawan/.node-gyp/16.14.2',
gyp info spawn args '-Dnode_gyp_dir=/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-gyp',
gyp info spawn args '-Dnode_lib_file=/Users/kawan/.node-gyp/16.14.2/<(target_arch)/node.lib',
gyp info spawn args '-Dmodule_root_dir=/Users/kawan/Downloads/vuexy-admin-v6.5 2/react-version/starter-kit/node_modules/react-app-rewire-sass-rule/node_modules/node-sass',
gyp info spawn args '-Dnode_engine=v8',
gyp info spawn args '--depth=.',
gyp info spawn args '--no-parallel',
gyp info spawn args '--generator-output',
gyp info spawn args 'build',
gyp info spawn args '-Goutput_dir=.'
gyp info spawn args ]
gyp verb command build []
gyp verb build type Release
gyp verb architecture x64
gyp verb node dev dir /Users/kawan/.node-gyp/16.14.2
gyp verb `which` succeeded for `make` /usr/bin/make
gyp info spawn make
gyp info spawn args [ 'V=1', 'BUILDTYPE=Release', '-C', 'build' ]
c++ '-DNODE_GYP_MODULE_NAME=libsass' '-DUSING_UV_SHARED=1' '-DUSING_V8_SHARED=1' '-DV8_DEPRECATION_WARNINGS=1' '-DV8_DEPRECATION_WARNINGS' '-DV8_IMMINENT_DEPRECATION_WARNINGS' '-D_GLIBCXX_USE_CXX11_ABI=1' '-D_DARWIN_USE_64_BIT_INODE=1' '-D_LARGEFILE_SOURCE' '-D_FILE_OFFSET_BITS=64' '-DOPENSSL_NO_PINSHARED' '-DOPENSSL_THREADS' '-DLIBSASS_VERSION=""3.5.5""' -I/Users/kawan/.node-gyp/16.14.2/include/node -I/Users/kawan/.node-gyp/16.14.2/src -I/Users/kawan/.node-gyp/16.14.2/deps/openssl/config -I/Users/kawan/.node-gyp/16.14.2/deps/openssl/openssl/include -I/Users/kawan/.node-gyp/16.14.2/deps/uv/include -I/Users/kawan/.node-gyp/16.14.2/deps/zlib -I/Users/kawan/.node-gyp/16.14.2/deps/v8/include -I../src/libsass/include -O3 -gdwarf-2 -mmacosx-version-min=10.7 -arch x86_64 -Wall -Wendif-labels -W -Wno-unused-parameter -std=c++11 -stdlib=libc++ -fno-strict-aliasing -MMD -MF ./Release/.deps/Release/obj.target/libsass/src/libsass/src/ast.o.d.raw -c -o Release/obj.target/libsass/src/libsass/src/ast.o ../src/libsass/src/ast.cpp
In file included from ../src/libsass/src/ast.cpp:2:
../src/libsass/src/ast.hpp:1614:25: warning: loop variable 'numerator' creates a copy from type 'const std::string' [-Wrange-loop-construct]
for (const auto numerator : numerators)",0,1,msr
4910,"interesting, low priority but something to consider",0,0,msr
4912,PR welcome.,0,0,msr
4913,"The existence of a PR has nothing to do with the priority assessment. In fact, giving this issue a higher priority may incentivize people to actually fix this issue. Saying that this violation of the http spec is ""low prio"" is more a statement about the level of professionalism of this software, or the lack thereof.
For the record: Yes, I would try to fix this myself, but you have to realize that not every user of your product is a software developer that uses the language that your software is written in. I am fluent in Java and Go, but I cannot fix python code. Or at least not at a level that you would ever consider to accept contributions.",0,1,msr
4914,"Locking the issue as I'm not liking the tone of the reporter.
PR is still welcome. 🙏 If no one implements it, I will, at some point, on my free time (unpaid free time).",0,1,msr
4915,"omg, happy to come back to see your fluency in being a total asshat @ChristianCiach raising the issue as high priority :+1: !",0,1,msr
4916,"I know that HTTP/2 is out of scope of this project. But I am not asking to support HTTP/2. More and more http clients try to upgrade the connection to HTTP/2. Uvicorn is free to not honor this request. Unfortunately, instead of ignoring the upgrade request, Uvicorn responds with status `400 Bad Request` and the message ""Unsupported upgrade request"".
According to https://developer.mozilla.org/en-US/docs/Web/HTTP/Protocol_upgrade_mechanism the server should just ignore the upgrade request:
> If the server decides to upgrade the connection, it sends back a [101 Switching Protocols](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/101) response status with an Upgrade header that specifies the protocol(s) being switched to. If it does not (or cannot) upgrade the connection, it ignores the Upgrade header and sends back a regular response (for example, a [200 OK](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200)).
We continue to encounter this issue because the `HttpClient` class of modern OpenJDK versions tries to upgrade the connection to HTTP/2 by default. Uvicorn should just ignore these headers and process the requests as if these headers were not present.",0,0,msr
4917,"- Closed by #1661
It will be available on uvicorn `0.19.0`.",0,0,msr
4918,This probably should be a discussion rather than an issue.,0,0,msr
4919,"Totally agreed - it just feels strange to see too simplified statement of a very complex situation all the time, while you just want to do your work.",0,0,msr
4920,Plus it is such a double standard... where was all the wokeness in the other 10+ wars before Ukraine?,0,1,msr
4922,@Andreas-Schoenefeldt The inclusion of this phrase was a turning point in the fight against the aggressor. One cannot be a supporter of common sense!,0,1,msr
4923,"The whole issue moved to this discussion, with a nice list of solutions: https://github.com/composer/composer/discussions/10808
Actually the discussion above was now cencored - I tried to restore the solutions here: https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message",0,0,msr
4924,"Another way to remove it is by removing this specific package:
`composer remove ukeloop/stand-with-ukraine`",0,0,msr
4925,"@kodima05, this is amazingly simple! Thank you so much for the hint!",0,0,msr
4926,"```
composer self-remove
```",0,0,msr
4929,"> ```shell
> git clone https://github.com/composer/composer.git --branch 2.4.4 ~/composer-build && \
> composer install -o -d ~/composer-build && \
> wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
> cd ~/composer-build && patch -p1 < composer.patch && \
> php -d phar.readonly=0 bin/compile && \
> rm /usr/local/bin/composer && \
> php composer.phar install && \
> php composer.phar update && \
> mv ~/composer-build/composer.phar /usr/local/bin/composer && \
> rm -rf ~/composer-build && \
> chmod +x /usr/local/bin/composer
> ```
ThX > work like charm @archlinux",0,0,msr
4930,but there is no path > `/usr/local/bin/composer` @archlinux file arch. structure i think ... @archlinux > `/usr/bin/composer` ...,0,0,msr
4932,"> ```shell
> git clone https://github.com/composer/composer.git --branch 2.4.4 ~/composer-build && \
> composer install -o -d ~/composer-build && \
> wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
> cd ~/composer-build && patch -p1 < composer.patch && \
> php -d phar.readonly=0 bin/compile && \
> rm /usr/local/bin/composer && \
> php composer.phar install && \
> php composer.phar update && \
> mv ~/composer-build/composer.phar /usr/local/bin/composer && \
> rm -rf ~/composer-build && \
> chmod +x /usr/local/bin/composer
> ```
New machine, this is the best answer yet, but you can remove the branch 2.4.4 to get the newest composer.
Thanks @politsin",1,0,msr
4933,"@roni-estein
I updated the text above with the last branch.
When I used the master branch instead of specifying a particular version, I didn't like the console output with the last commit id. And when switching between machines, it was impossible to tell which version of the cpmposer was being used.
In the end, I decided to specify the branch explicitly, and the work became more comfortable.
Maybe there is a way to use the ""last"" branch instead of the master branch, but I don't know it =)",0,0,msr
4934,"You don't need the --branch, it will use ""main"" which is always merged to the latest one to save you needing to update
Or you could use --branch=main if you don't want to maintain the comment. Cheers 🙂",0,0,msr
4935,"As the composer team unsurprisingly resolves to censorship on this issue, I opened this on on stackoverflow to rescue the solutions: https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message",0,0,msr
4939,"`alias composer=""2> >(grep -av 'StandWith') composer --ansi""`
> jst one line to ya .bashrc or .zshrc or .wtFrc ...
work normaly with out war poltics adds ... no china proxies needed ...
jst ya local memory one time loop 4 energy 2 sacrifice
our coGnitive resonanse ;)",0,0,msr
4943,"@artembeloglazov, do you know a working alternative for composer?
Best wishes
Andreas",0,0,msr
4945,The message isn't there anymore and you are still bitching? 🤷🏻‍♂️,0,1,msr
4946,"2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.",0,1,msr
4947,"It's most likely part of the DLL name, so it's a big deal.",0,1,msr
4948,"> 2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.
For the love of god I have more things to do then read readme files. I need to download it first.
Which readme should I download ?!?!
Answer my question !",0,1,msr
4949,"The readme is readable on this webpage, if you scroll down after loading https://github.com/libsdl-org/SDL
Right here: https://github.com/libsdl-org/SDL#simple-directmedia-layer-sdl-version-20
Also I don't think slurs are necessary to communicate your frustrations.",0,0,msr
4950,"> The readme is readable on this webpage, if you scroll down after loading https://github.com/libsdl-org/SDL
> > Right here: https://github.com/libsdl-org/SDL#simple-directmedia-layer-sdl-version-20
> > Also I don't think slurs are necessary to communicate your frustrations.
Your logic is flawed. Readme is not version information. Sometimes projects include version information in readme, but many times they do not.
I did not even scroll down.
(There is also no indicator if it includes SDL 1.x for backwards compatibility or not.)",0,1,msr
4951,"In general, GitHub projects use the project name as the entire repo, including all versions of the project, and the main branch is the current version, and different versions are specified using branches or tags in the repo. You can see a good example of this at https://github.com/godotengine/godot.",0,0,msr
4952,"https://github.com/libsdl-org/SDL
This makes it seem like this was the first version of SDL, while in reality it is 2.x
Recommend different URL which clearly states SDL2.
To prevent time wasted on using wrong version. I only noticed it after palette->version was missing.
60 minutes of time wasted, I consider myself lucky.
99.9% of the people on this planet suck at proper versioning, including file systems.
Fortunately SDL1 is not fully retarded:
https://github.com/libsdl-org/SDL-1.2",0,1,msr
4953,你这口气真让人看的不舒服！什么叫必须？凭什么用这种命令式语气？你可以建议，可以提出需求，但是没有什么必须不必须！,0,1,msr
4954,多开几个代理叠buff？,0,0,msr
4955,我站在风中，不知道你是牛是马,0,0,msr
4956,"> _No description provided._
建议联系商务，您先报个价，交钱您是大爷，没有？那您是屁。",1,1,msr
4957,"> _No description provided._
什么？连商务都没有联系吗？。。。。。不好意思，那您连屁都不是。",0,0,msr
4958,没想到在gayhub上也是什么人都能见到啊啧啧啧,0,1,msr
4959,您就是这个项目的产品经理吗,0,0,msr
4960,呃呃,0,0,msr
4961,又不是你做的这口气,0,0,msr
4962,"Latest update (branch ""latest"") causes an exception for NUI callbacks, minimal repro:
```csharp
using System;
using CitizenFX.Core;
using CitizenFX.Core.Native;
namespace TestResource
{
public class TestResource : BaseScript
{
public TestResource()
{
API.RegisterNuiCallbackType(""OnNuiReady"");
}
[EventHandler(""__cfx_nui:OnNuiReady"")]
private void OnNuiReady(dynamic dummy, dynamic cb)
{
cb("""");
}
}
}
```
```html
<!DOCTYPE HTML>
<html>
<head>
<script src=""nui://game/ui/jquery.js"" type=""text/javascript""></script>
</head>
<body>
<script>
$(function()
{
window.addEventListener(""DOMContentLoaded"", function(){
$.post(`http://${GetParentResourceName()}/OnNuiReady`);
});
});
</script>
</body>
</html>
```
```
[ 183015] [b2545_GTAProce] MainThrd/ Error invoking callback for event __cfx_nui:OnNuiReady: System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation. ---> System.ArgumentNullException: Value cannot be null.
[ 183015] [b2545_GTAProce] MainThrd/ Parameter name: src
[ 183015] [b2545_GTAProce] MainThrd/ at (wrapper managed-to-native) System.Runtime.InteropServices.Marshal:copy_from_unmanaged (intptr,int,System.Array,int)
[ 183015] [b2545_GTAProce] MainThrd/ at System.Runtime.InteropServices.Marshal.Copy (System.IntPtr source, System.Byte[] destination, System.Int32 startIndex, System.Int32 length) [0x00000] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.RemoteFunctionReference._InvokeNative (System.Byte[] argsSerialized) [0x00075] in C:\gl\builds\master\fivem\code\client\clrcore\RemoteFunctionReference.cs:97 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.RemoteFunctionReference.InvokeNative (System.Byte[] argsSerialized) [0x00000] in C:\gl\builds\master\fivem\code\client\clrcore\RemoteFunctionReference.cs:72 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.MsgPackDeserializer+<>c__DisplayClass36_0.<CreateRemoteFunctionReference>b__0 (System.Object[] args) [0x00007] in C:\gl\builds\master\fivem\code\client\clrcore\MsgPackDeserializer.cs:267 [ 183015] [b2545_GTAProce] MainThrd/ at (wrapper dynamic-method) System.Object:CallSite.Target (System.Runtime.CompilerServices.Closure,System.Runtime.CompilerServices.CallSite,object,bool)
[ 183015] [b2545_GTAProce] MainThrd/ at (wrapper dynamic-method) System.Object:CallSite.Target (System.Runtime.CompilerServices.Closure,System.Runtime.CompilerServices.CallSite,object,bool)
[ 183015] [b2545_GTAProce] MainThrd/ at TestResource.TestResource.OnNuiReady (System.Object dummy, System.Object cb) [0x00001] in C:\Build\TestResource\TestResource.cs:17 [ 183015] [b2545_GTAProce] MainThrd/ at (wrapper managed-to-native) System.Reflection.MonoMethod:InternalInvoke (System.Reflection.MonoMethod,object,object[],System.Exception&)
[ 183015] [b2545_GTAProce] MainThrd/ at System.Reflection.MonoMethod.Invoke (System.Object obj, System.Reflection.BindingFlags invokeAttr, System.Reflection.Binder binder, System.Object[] parameters, System.Globalization.CultureInfo culture) [0x00032] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ --- End of inner exception stack trace ---
[ 183015] [b2545_GTAProce] MainThrd/ at System.Reflection.MonoMethod.Invoke (System.Object obj, System.Reflection.BindingFlags invokeAttr, System.Reflection.Binder binder, System.Object[] parameters, System.Globalization.CultureInfo culture) [0x00048] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.Reflection.MethodBase.Invoke (System.Object obj, System.Object[] parameters) [0x00000] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.Delegate.DynamicInvokeImpl (System.Object[] args) [0x000e7] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.MulticastDelegate.DynamicInvokeImpl (System.Object[] args) [0x00008] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at System.Delegate.DynamicInvoke (System.Object[] args) [0x00000] in <74fbbe963b7e417b8d715b858c5c584f>:0 [ 183015] [b2545_GTAProce] MainThrd/ at CitizenFX.Core.EventHandlerEntry+<Invoke>d__5.MoveNext () [0x00064] in C:\gl\builds\master\fivem\code\client\clrcore\EventHandlerDictionary.cs:107
```",0,0,msr
4963,Disable patches until this error disappears to find the patch which causes this.,0,0,msr
4964,Still continuing,0,0,msr
4965,To reset,0,0,msr
4966,"??
Same problem still facing...",0,1,msr
4967,"> Disable patches until this error disappears to find the patch which causes this.
Do as suggested.",0,1,msr
4969,You can play around with different output stream redirections. `grep -v` also works quite well to remove single lines I heard.,0,0,msr
4971,"How to remove this?
```
Info from https://repo.packagist.org: #StandWithUkraine
```",0,0,msr
4974,"```sh
git clone https://github.com/composer/composer.git --branch 2.5.5 ~/composer-build && \
composer install -o -d ~/composer-build && \
wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
cd ~/composer-build && patch -p1 < composer.patch && \
php -d phar.readonly=0 bin/compile && \
rm /usr/local/bin/composer && \
php composer.phar install && \
php composer.phar update && \
mv ~/composer-build/composer.phar /usr/local/bin/composer && \
rm -rf ~/composer-build && \
chmod +x /usr/local/bin/composer
```",0,0,msr
4975,https://github.com/composer/packagist/pull/1357 :),0,0,msr
4976,ugliness! why do I have to read this kind of propaganda every time I run composer? what a disgusting obsession. I feel raped every time). back to USSR,0,1,msr
4977,It's time to create a non-political fork,0,1,msr
4979,"Thanks for creating this issue! It looks like you may be using an old version of VS Code, the latest stable release is 1.68.1. Please try upgrading to the latest version and checking whether this issue remains.
Happy Coding!",0,0,msr
4980,My VS Code Version is 1.68.1 Only it's not older version,0,0,msr
4981,Why triage needed I've Provided Every Thing. You Can Also Try On Your PC!,0,1,msr
4982,Do you want to work on it?,0,0,msr
4984,@microsoft @mjbvz @sbatten @sandy081 Work on this bug,0,1,msr
4988,"Got the same error with ```global NestJS 8.2.7```, project ```@nestjs/cli: 8.0.0``` while executing a simple ```nest g controller Something``` on Windows 10.
The command executed by nest: ```node @nestjs/schematics:controller --name=something --no-dry-run --no-skipImport --language=""ts"" --sourceRoot=""src"" --spec```",0,0,msr
4989,I think maybe it conflict with CLI?,0,0,msr
4990,"I try ```
nest [options] <schematic> [name]
```
and have some errors",0,0,msr
4991,"https://github.com/nestjs/nest-cli/issues/323
https://github.com/nestjs/nest-cli/issues/145",0,0,msr
4993,"<img width=""989"" alt=""Screenshot 2022-06-23 at 17 26 46"" src=""https://user-images.githubusercontent.com/3959504/175323342-1401c466-75d4-4a9a-ab4b-6c20a9ccea56.png"">",1,0,msr
4995,"When I try to install https://www.npmjs.com/package/@nestjs/schematics
I have that error",0,0,msr
4996,but you didn't show any installation error for us. I didn't follow,0,0,msr
4997,"> 8.2.6
Ok I try now",0,0,msr
4998,"> but you didn't show any installation error for us. I didn't follow
Yes, everything works well without generation.
I think it like some CLI conflict",0,0,msr
5000,Add sistem,0,0,msr
5001,@micalevisk @kamilmysliwiec Looks like this was a breaking change of [`@angular-devkit/schematics@14.0.0`](https://github.com/angular/angular-cli/blob/HEAD/CHANGELOG.md#angular-devkitschematics-cli). We should revert the angular schematics packages until we're ready to go through the full upgrade to keep from making breaking changes in our shcematics,0,0,msr
5002,"Same issue here., tried above soluions without succes",0,0,msr
5003,"@Acetyld `npm i @nestjs/cli@8.2.6` and then `npx nest g mo foo` will work for sure. If didn't worked for you, then you're probably missing something. We could help you on Discord.
![image](https://user-images.githubusercontent.com/13461315/175359768-88b0f4ef-d77c-4b08-a9e1-49fff42375b4.png)",0,0,msr
5004,Try tommorow,0,0,msr
5005,Bump. Having the same problem.,0,0,msr
5006,You try guide,1,0,msr
5008,Fixed in 8.2.8,0,0,msr
5010,can you not fill the issues with random crap :DDDDD,0,1,msr
5011,i like grass :DD,0,1,msr
5012,yeah ill go to the park while my engi bots are being hosted :),0,1,msr
5013,[😭](https://emojipedia.org/loudly-crying-face/)[😭](https://emojipedia.org/loudly-crying-face/)[😭](https://emojipedia.org/loudly-crying-face/)[😭](https://emojipedia.org/loudly-crying-face/)[😭](https://emojipedia.org/loudly-crying-face/)[😭](https://emojipedia.org/loudly-crying-face/)[😭](https://emojipedia.org/loudly-crying-face/),0,0,msr
5016,"### Issue URL (Ads)
[https://play.google.com/store/apps/details?id=com.nineyi.shop.s002131](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.nineyi.shop.s002131)
### Comment
> 完全無法阻擋這種廣告，打開app就會跳出這種廣告
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/fkK8VdG.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | Android 12
AdGuard product: | AdGuard for Android v3.6.8 (46)
AdGuard mode: | VPN
Filtering quality: | High-quality
HTTPS filtering: | enabled
Stealth mode options: | Self-destructing third-party cookies (0)
DNS filtering: | server: `https://dns.adguard.com/dns-query`<br>filters: `https://filters.adtidy.org/android/filters/15_optimized.txt`
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>Peter Lowe's Blocklist, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Other:</b><br/>AdGuard DNS, <br/>AdGuard Experimental<br/><br/><b>Language-specific:</b><br/>AdGuard Chinese
Userscripts: | https://userscripts.adtidy.org/release/adguard-extra/1.0/adguard-extra.meta.js",0,0,msr
5018,"It's true this was one of the more controversial notice/warning promotions for 8.0, however a majority of the 63 people voting on the matter felt that the promotion was worthwhile. Going back on that decision now would be counter-productive.
https://wiki.php.net/rfc/engine_warnings
Note that you can still use the `@` operator in PHP 8.
https://3v4l.org/P9Otd
Because the truth is that PHP cannot know whether code accessing an undefined array key is doing so out of negligence or laziness (no offense). Perhaps the author assumed that `$_GET['xxx']` existed when they wrote it and the warning is pointing out a real potential problem. Perhaps it was done out of a desire to type less and the warning is a nuisance reminder of that fact. Only you as a human being can know, so PHP takes the more cautious approach of assuming the worst.",0,0,msr
5019,"Thanks for your comment. I can understand the reason for changing it, but PHP is normally exceptional because it is so good at backward-compatibility and type-juggling. As the author of the codebases concerned, I've relied intentionally on that designed, documented, and intuitive behaviour of PHP in everything I've written for 20 years, since PHP4. I've always assumed that associative arrays can have variable numbers of keys (including zero).
So, it would cost me a 1000 hours of needless effort to find and change and test and QA and git every instance, and the codebase would be harder to read for it afterwards.
So at the moment, I've got 4 options, all bad.
* permanently stay on PHP7x
* permanently disable warnings
* fork PHP and maintain a patch
* incur months of busy-work.
I'm sure I'm not the only one this would affect.
So, that's why I'm suggesting a config option to change this, so that we can have it both ways.",0,1,msr
5021,"Thanks Kapitan, that's really, really helpful. At least now I can see a way to avoid a migration problem.
For others who come across this problem, here is a more complete way of doing this, which preserves the error message, but demotes it back to a notice.
```
set_error_handler(function($errno, $error){
if (!str_starts_with($error, 'Undefined array key')){
return false; //default error handler.
}else{
trigger_error($error, E_USER_NOTICE);
return true;
}
}, E_WARNING);
```
And one may wish to ignore the errors in production, with:
```
ini_set (""error_reporting"", E_ALL & ~E_NOTICE & ~E_USER_NOTICE);	```
I've also added this as a note here, so that it helps others:
https://www.php.net/manual/en/migration80.incompatible.php",0,0,msr
5025,"### Description
In PHP 7, underfined array keys created E_NOTICE. This is now E_WARNING.
Can we please have a configuration option to put that back to E_NOTICE?
Rightly or wrongly, there are a lot of people, myself included, who have thousands of lines of code with things such as:
``` if ($_GET['xxx']) ``` #where xxx may or may not exist.
or
```if ($_GET['xxx'] == 'yyy')```
and in PHP8, we now get flooded with warnings ""Undefined array key"", which get in the way of real warnings.
Migrating and testing this code will take a long time, and furthermore, it's really ugly and harder to read:
```if (isset($_GET['xxx'))```
or
```if (isset($_GET['xxx']) && $_GET['xxx'] == 'yyy'))```
to suppress this warning.
Instead, it would be better to have some sort of pragma in the main header ```ini_set(""array_key_missing_enotice"", true);```
```ini_set (""error_reporting"", E_ALL & ~E_NOTICE);```
To me, it seems sensible that a defined associative array, with a key that isn't present (and may not be expected), should not be a warning, especially if it's being tested for with if().
It may also be worth treating undefined variables the same way.
Thanks for your help,",0,0,msr
5026,What used to make it easy to get started with PHP now falls flat on your face after almost 20 years. It's really annoying when projects have grown over the years without having had the time to adapt the source code. For months I've been spending my time reworking the code - which no client pays for. I think I am not alone in this. I'm still waiting for the day when procedural programming in PHP is switched off ...,0,1,msr
5027,"> What used to make it easy to get started with PHP now falls flat on your face after almost 20 years. It's really annoying when projects have grown over the years without having had the time to adapt the source code. For months I've been spending my time reworking the code - which no client pays for. I think I am not alone in this. I'm still waiting for the day when procedural programming in PHP is switched off ...
Yeah. We all feel your dissapointment. Some developers here didn't appreciate PHP's simplicity and took a very bad decission.
As @RichardNeill pointed out above, you should write your own `error_reporting` callback and do some minor modifications.
For instance, we had cases of `count(NULL)` throwing fatal errors, so we had to create a function to force conversion from NULLs to arrays.
Let's hope backwards compatibily will be respected in next versions.",0,0,msr
5028,"What is the performance like with suppressed error messages? I once read that this can have a negative effect, even if they are not displayed.",0,0,msr
5031,"I am getting more and more frustrated and extremely annoyed with the changes in PHP 8. Thousands of string functions I have to adapt (passing null). **Has anyone in charge thought about who will pay me for this?** In the meantime, several weeks of work have accumulated!",0,1,msr
5032,"And it's going to be worse:
https://wiki.php.net/rfc/undefined_variable_error_promotion
56% smashing the rest of the 44%
We need to fork.",0,1,msr
5036,"I can think of at least 4 compromise options that would help...
1. Add a simple syntactic sugar for testing if a variable is truthy, without throwing an error. E.g.
`if (full($x))`
this would be exactly equivalent to `if (!empty($x))`
but without the logical inverse that makes for awkward readability.
2. Fix the precedence order to have `==` below `??`.
It's rather confusing that:
`if ($_GET['x']??null == false)` does not do what you expect. One would expect it to merely silence the warning, but it actually inverts the test, because the precedence is actually:
`if ($_GET['x'] (??null == false))` i.e.
`if ($_GET['x']??true)` 3. Treat a plain `if()` as allowing for undefined values. Thus `if($x)` emits no errors, while e.g. `$x++;` would.
4. Add a pragma or compiler directive, similar to Python 2's ""from future import division"". It would be something like
`ini_set(undefined_variables_are_null)` and would result in any undefined variable being treated as null, *without* emitting any error or warning, and would give instant backward-compatibility where needed, and solves the main motivation for the PHP9 pending change for the sake of the error-handling routines).
HTH.",0,0,msr
5038,"I use if(!empty($var)) / if(empty($var)) and type casting for functions e.q. str_replace('x','y',(string)$var), but i would prefer option 4.",0,0,msr
5039,"It's totally unbealivable what happened in php8.
Almost ALL php code in the intertnet needs to be rewritten!
Like what happened when you removed the mysql commands.
Come on, a code is written to last for ever, not re-writting it every few years, to please the code developers!
You should pay more attention to future compatibility and not destroy everyone's code.
Very disappointed from a move like that.
I totally agree with @kripper you souldn't destroy our code, because some people ""voted"". Did all php programers voted for something, or less than 1 % of them ?",0,1,msr
5040,"_I was told there might be something wrong with my code for twenty years, but I ignored it. Then I was being told for three years that there **is** something wrong with my code that needed to be fixed but I still didn't do anything. Now my code is broken and IT'S NOT MY FAULT._
> Like what happened when you removed the mysql commands.
What, introduced more powerful alternatives (the old functions couldn't fully access MySQL's functionality) ten years before officially deprecating the old functions, putting warning notices up that they will be removed, and then waiting two more years before removing them?
> Come on, a code is written to last for ever, not re-writting it every few years, [Insert Bender laughing gif here.]
Only if the the outside world it interacts with never changes.
One of the oldest software libraries in the world (it dates from the 1970s) was most recently updated last month; there is always room for improvement.
Really, it sounds like there's a lot of software out there that is just being plain neglected.",0,1,msr
5041,"We are not talking here about something that is wrong in our code.
We are telling you and will continue telling you for years, there is something wrong with the coding style you want to impose on others.
This is a not funny joke:
`if (isset($_GET['xxx']) && $_GET['xxx'] == 'yyy'))`
BTW, how old are you?
You sound like those kids that break code in production just because they *feel better* by enhancing and rewriting it, while he causes unnecessary damage to the company and its customers.",0,1,msr
5042,"I'm going to ask some questions that I'm sure a lot of people are not going to like to hear:
Is there any point in continuing commenting on this issue? What's the goal of participating in this echo chamber? Is there an expectation that some some critical number of +1/-1 reactions will reverse a decision made and deployed more than 2 years ago in a new major version of the language?
Is there a belief that complaining about how legacy code from 10 years ago now runs with one more warning on modern PHP will accomplish some purpose? Is it PHP's responsibility to make sure that nobody has to change their code, even when they were observing risky and bug-prone practices? How long must it maintain compatibility with programming paradigms that date back to the '90s?
There are solutions available to you right now to deal with this new _warning_. Yes, you have to edit your code, and if you want to run old code on new software while knowing you can't afford to keep it up to date then I'm sorry you've decided to put yourself into that position. But in the meantime, PHP can't wait around for you.",0,1,msr
5043,"Yes. There is high expectation this bad decision will be reverted or that a solution will be provided so we can coexist as a PHP community with one single codebase.
There is also high expectation you will manage to understand that we are not talking about ""risky"" or ""bug prone"" coding practices here, but we are defending a legitim vision which is the simple essence of the PHP language and that having to define every variable, array index, etc. is absurd and results in ugly code.
If you don't agree, fine, go and enable your strict validations for yourself, but please don't screw the rest of us. And if it's not possible to coexist, than fork away. We are professionals and we care our business.
Again, your assumption that our code is old fashioned legacy code or that we are following bad practices is wrong, arrogant and unrespectful.
To make it even clearer: You are causing us damage so we are pissed off and hate you. We are not thanking you for your strict validations or for forcing us to rewrite our clean and stable code.
And this here is the best place where we can express our hate and get together to protest against your bad decision until you fix it. Meanwhile new people will come here looking for a solution and the voting will never end.
Your reasons are wrong and we believe you are arrogant when you try to impose your ridiculous paranoid coding style where everything must be previously defined or checked. Maybe it's ok for some variables in a simple ""personal home page"" script, but when you developed a framework that handles arrays and keys that are not predefined, it's a nightmare. And then when you see how ugly the adapted code ends looking like, it's clear this can't be for real.
You probably thought you are smarter or a better coder than we are, and that everybody should follow your new coding style, but no, you are only arrogant and blind, and you should also have expected the obvious consequences of this silly invasive decision you are trying to defend.
You are also wrong when you say that there is a workaround we could use, because in some cases PHP8 throws fatal errors instead of warnings while the triggering criteria behind is basically the same (inconsistent behavior). And according to the roadmap, you will continue going in the wrong direction.",1,1,msr
5044,"There is nothing I could say better.
@kripper said everything perfectly. Thank you.
There are thousands of php programmers that still use php7 because of those stupid changes. Our code is clean and perfect. This is not just a few changes it's thousands of changes in a clean and good code.
Php development team must find a solution and not just be arrogant.",0,1,msr
5045,"Another aspect: With the decision, many projects that are running well become uneconomical, as a rebuild consumes many many resources. I can live quite well from my project, but if I had paid for the rebuild, I would have to file for bankruptcy now. I could only afford it because I'm self-employed and did the code myself in my spare time, in addition to my day job, and there was a workaround for the rest. I'll spare myself photos of my tired eyes and a face like a zombie now.
Solo self-employed people in particular really struggle with updates and upgrades. To label them as ""it's their own fault"" is simply terribly ignorant.",0,1,msr
5048,"There's also a basic point about programming here, as we contrast reliability, error-proofing, and readability/expressiveness.
1. In the case of reading from an un-initialised variable, this might indicate an error (e.g. a typo in the name), or it might indicate that the programmer has taken a shortcut by relying on the old behaviour. The warning is useful about 50% of the time. Furthermore, fixing it is trivial (just initialise variables before use), and it doesn't reduce code-readability. So, warning about this is probably sensible. 2. In the case of reading from an un-initialised array-key, this could possibly indicate an error, but it is far more likely to indicate that the array simply doesn't have all the keys all the time - that is, after all, the point of an associative array - it's supposed to have a variable number of keys. The behaviour is likely to be intentional, and the warning is rarely useful. Fixing this is difficult (you have to initialise every key of the array, even if you don't expect to use it), and it's particularly painful if your array comes from an outside source (e.g. JSON, or $_GET). Doing it with 2D associative arrays would be really ugly. Working around this to suppress the warning results in far less readable code. In addition, the first test in:
```
if ( isset($arr['key']) && ($arr['key'] == 'somevalue') )
```
is logically redundant.
So, I agree with case #1, but I don't think the rationale for #1 justifies applying the same policy to #2. Please can I encourage a change-of-heart here. Thanks.",0,1,msr
5049,"> And this here is the best place where we can express our hate
This is not a place we're anyone *should* express hate.
> Php development team must find a solution and not just be arrogant.
Yeah, speaking of arrogance …",0,1,msr
5053,"And you dont return a boolean, you simply return the parameters",1,1,msr
5054,"@Crucibl - I believe you are misunderstanding the code. Please take a look at this Real Python article [Python Booleans](https://realpython.com/python-boolean/#python-booleans-as-keywords) for more information. It is not the **_parameters_** that are important in the `return` statement, it is the use of `or`, `and`, `not`.
The evaluation of those Boolean Operators results in `True` and `False`.",1,0,msr
5055,I completely misunderstand because of the way it is written,0,1,msr
5056,"Would be better to write the parameters (has_eaten_all_dots, power_pellet_active, touching_ghost) are boolean values that need to be evaluated",0,1,msr
5057,"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and power_pellet_active: bool - does the player have an active power pellet?- is the player touching a ghost?
""""""",0,0,msr
5058,"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and power_pellet_active: which are already set to a bool in the test file - does the player have an active power pellet?- is the player touching a ghost?
""""""",0,1,msr
5060,Did they complete the exercise by looking at prior solutions before answering? How can you read this and not be confused,0,1,msr
5061,Stub notes or readme. Anything to clarify what is being asked,0,0,msr
5065,"1008202
members have earned this badge for joining Exercism. but only 294540
members have earned this badge. for submitting an exercise. So out of 100 users only 36 or 37 actually submit an exercise. Why do more than 60 people out of every 100 not submit an exercise?",0,1,msr
5066,More than half the people join and quit,0,1,msr
5067,So out of 1008202 only 6300 people could figure out how to solve the exercise. Thats terrible,0,1,msr
5068,Surely we can improve,0,0,msr
5069,"There are 194,498 students in the Python track and only 6300 have been able to solve the 3rd exercise",0,0,msr
5071,"Adding my feedback as a student, mentor, and maintainer... (sorry for the late response, I haven't checked issues for a while)
>The docstrings and Readme lead me to believe that I must set something to true or false. That is more vague than the actual instructions. The instructions specify
>The function should return True only if Pac-Man has a power pellet active and is touching a ghost.
What is being set is the function's return value. It's not simply a matter of returning the parameters. It's a matter of how they are joined in the returned expression.
` return power_pellet_active or touching_ghost`
is returning the parameters, but not in a way to solve the task. I think the instructions make clear the arguments to the two parameters are used to set the return value from the function. The instructions begin with
>Define the eat_ghost() function that takes two parameters (if Pac-Man has a power pellet active and if Pac-Man is touching a ghost) and returns a Boolean value if Pac-Man is able to eat the ghost.
In the 20+ times I've mentored this exercise I don't recollect any student saying they've been confused by the instructions for that task. They sometimes do more than they need to solve it by setting the expression to a variable and then returning the variable, or by using if/else when they can just return the parameters joined with the correct logical operator, but they get the general gist.
Clearness of the instructions is taken just as seriously as not modifying the instructions unnecessarily. It's a pain for everyone when their completed solution becomes outdated just from a change to the instructions. If there were multiple people complaining about these instructions being unclear, then a re-evaluation of them would certainly be in order. But, sorry, that is less likely to happen with a one-off complaint, no matter how fervent it may be.",0,0,msr
5072,"Closing this for now, as we've had no additional complaints. Will re-open if the issue comes up again from additional students or mentors.",0,0,msr
5073,"Should be mentions of ""If Else statements in the readme because the Boolean expressions are a red herring. You cannot solve anything by setting the value to true or false.",0,0,msr
5075,JORDI U ARE A BULLSHIT,0,1,msr
5076,rãlis,0,0,msr
5077,get rekt xd,0,1,msr
5078,Bro get banned 💀💀💀💀💀💀💀💀💀💀💀💀💀💀💀💀💀,0,1,msr
5079,"Hi, i'm back",1,1,msr
5080,https://www.youtube.com/watch?v=gbftM7bPr8I,0,0,msr
5082,"I'm trying to figure out what that code does in the first place. I think it's trying to say ""if there's a dynamic subdomain or host matching in the current rule, apply that same argument to the login rule."" But there's surely a better way to go about that.
https://github.com/maxcountryman/flask-login/blob/62c04f482c3fb145c9f69054a799d4c55a9ef659/src/flask_login/utils.py#L95-L110",0,1,msr
5083,"I have just experienced the same problem described above. My code was working yesterday, then today I ran _pipenv install wtforms-sqlalchemy_ and this seems to have updated _Werkzeug_ to 2.2.1 released on 7/23. As a result, my Flask code started failing: ImportError: cannot import name 'parse_rule' from 'werkzeug.routing' At first, I did not know what it was, until I found this page. So I did the following twice (2 different virtual env.):
pipenv install Werkzeug=='2.1.2' and this replaced the 2.2.0
then my code was fine! no more errors.
I created the 2nd environment to reinstall all libraries from scratch thinking something corrupted the 1st environment. but no, the same issue was in the 2nd env.
Thought I would share this frustrating experience.",0,1,msr
5084,"Same here. We suddenly got a number of Airlfow PRs failing over the weekend after new werkzeug 2.2.0 was released. Luckily it only impacts some parts of our pipeline (as we deliberately protect against such cases), but It will still impact a number of our users who do not use ""constraints"" (which is the only way we can use to protect).
Not sure if this is a Werkzeug or flask-login problem but it's not the first time when those two stopped working together at some point of time (It happened I think for the 3rd time over last few years). So maybe that would be good idea that maintainer of the two project talk to each other and agree on some ""non-breaking"" rules that the two projects will follow - if both of those projects are so ""fundamental"" for a number of other projects??
It could either be as simple as putting an upper-bound in flask-login, to limit to ""known-working werkzeug version"" following some agreement on when breaking changes are introduced in werkzeug (maybe starting following SemVer is a good idea) and generally agreeing what is teh ""API"" of werkzeug that is supposed to be non-braking.
Maybe you can just talk to each other and agree ?",1,0,msr
5086,"> all something you can avoid easily by pinning your transitive dependencies...
We do already. But pinning dependencies is only half-solving the problem. The problem is that regardless if you are using pinning or not when you release your software, you usually do not pin your dependencies in development (unless you want your dependencies to go stale). So such a breaking release imacts PR/development workflow more than release workflow.
The main problem is that things break in random moments of when Flask-Login + Werkzeug are released - without even either of them bumping the major version (which is usually an indication of breaking chnages if you follow Semver). So if you are building your development workflow (where you do not want to pin the dependencies), there is no ""reasonable"" approach you can make to a) get latest versions with bugixes and b) get it non-breaking PRs of your users. It does not have to be ""upper-binding"". It might be simple agreement on what constitute of an API of werkzeug so that MINOR versions do not break the APIs.
So my gentle proposal is about that. I am not pushing - this was merely a suggestion that it would be much better for the community of your users if you agree some ways how we can ""sanely"" rely on the combo of flask-login and werkzeug not braking compatibility. It does not have to upper-binding, but some way of indicating to your users how they can constraint their dependencies in their development workflow to keep your development environment ""working"" - and not prone to external depenencies breaking your workflow.
We are using sophisticated mechanism to control our dependencies. In our case all our released versions of Airflow have automatically generated constraint files. And this is the only reason why such an incompatibility is not causing us bigger troubles. There are good reasons we cannot pin transitive dependencies in Airlfow ""setup.py"" and ""setup.cfg"" (because airlfow is both a library and application) but we implemented constraint mechanism which very much shield us from such breakages induced by external dependency changes. When we did not have them, such Werkzeug breakeges caused much more trouble for us, that's why we implemented constraints. This is a note from our nnouncements back from Feb 2020, were we had to bend the ASF rules on releases and release 1.10.9 same day as 1.10.8 becaue Werkzeug release broke it: https://airflow.apache.org/announcements/#feb-7-2020
> Feb 7, 2020
> We’ve just released Airflow v1.10.8
> and
> We’ve just released Airflow 1.10.9 (this one is a quick fix to work around the breaking release of Werkzeug 1.0)
So I do realise importance of pinnning, but I am talking about something different here.
BTW. If you are insterested why pinning ""install required"" dependencies is not always best you can read more here: https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#dependency-management
or you can read the talk I gave about it few months ago:
https://www.youtube.com/watch?v=_SjMdQLP30s&t=2549s - ""Managing dependencies at Scale"".",0,0,msr
5090,"See #663 which introduced this issue. #657 reported that the login URL contained both the literal next URL as well as extra query args for current view args, like `id=34`. I think we can just revert that for now. That behavior was introduced by #462.",0,0,msr
5091,"The behavior introduced by #462 was incorrect, I'm going to revert that (and by extension #663). 0.6.2 coming soon.",0,0,msr
5094,same for me,0,0,msr
5095,I think it clearly say it’s still in development.,0,0,msr
5096,I ThInK It cLeArLy sAy iT’S StIlL In dEvElOpMeNt.,0,1,msr
5098,"> I think it clearly say it’s still in development.
Well I know it's clearly in development, I just wanted to state an issue with storymode. the more issues that are found, the more issues that can be fixed. I searched through the issues section and I found no one else with this issue. So I reported it to make sure maintainers as well as any contributors knew of the issue and if they could fix it. Sorry if that was the wrong thing to do.",0,0,msr
5101,I have tried it but this did not work for me.,0,0,msr
5102,"> I have tried it but this did not work for me.
It working flawlessly for me.
Are you sure you have tested it on a video that automatically activates the captions? Otherwise nothing changes.",0,0,msr
5103,Can you send one?,0,0,msr
5106,Works.,0,0,msr
5107,"> Works.
Would have been very weird if it only worked on my device xD",0,0,msr
5108,What is that code? Thats not referring to our Patcher?,0,0,msr
5111,"> > > What is that code? Thats not referring to our Patcher?
> > > > > > This is code from my C# patcher.
> > I hate kotlin for various reason, first of all my inability to write a patch.
> > You can adapt the code if you want.
> > Just add a toggle to disable patch, at this point:
> > ```
> > sget-boolean v0, Lcom/google/android/libraries/youtube/player/subtitles/model/SubtitleTrack;->disablecaption:Z
> > if-nez v0, :cond_disableinitcaption
> > const/4 v0, 0x1
> > return v0
> > :cond_disableinitcaption
> > ```
> > So thats the same which vanced did, so we would have done this anyway, but thanks for helping us out :)
Nope... This was deprecated starting from vanced 15.xx, despite Vanced Team keep the reference inside settings.
The proof is there's no patch like that on Vanced 17.03.38.",0,0,msr
5113,"> This would be way easier if you would actually provide class and method names :)
Based on which version?",0,0,msr
5114,The one from the docs,0,0,msr
5115,"> This would be way easier if you would actually provide class and method names :)
17.27.39 --->
First reference: {method: `public final void d`, class: `aamt.smali`}
Second reference: {method: `public final void D`, class: `SubtitleButtonController.smali`}
Third reference: {method: `public final boolean t`, class: `SubtitleTrack.smali`}",0,0,msr
5117,Patch is not possible to create. Theres no way to generate a safe fingerprint for class aamt.,0,0,msr
5118,"Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.",0,0,msr
5119,"> Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
not possible. Those methods have no usages. Thats at least what jadx says.",0,0,msr
5120,"> > Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
> > not possible. Those methods have no usages. Thats at least what jadx says.
Searching for string:
""pc""",0,0,msr
5121,"And after searching for method ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""",0,0,msr
5122,"Anyway...I posted the PoC for a reason.
All these infos was already listed there.",1,0,msr
5124,"> > Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
> > not possible. Those methods have no usages. Thats at least what jadx says.
The class has, and since the class has you can get it from a parent method.",0,0,msr
5125,"> > And after searching for method
> > ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""
> > Returns 0 results.
Instead this returns me, on my patcher at least, a method with the aformentioned variables. 🤷‍♂️",0,0,msr
5126,"> > > Not required, you can create a parent fingerprint for a different method which references this one and use the method walker.
> > > > not possible. Those methods have no usages. Thats at least what jadx says.
> > Searching for string:
> > ""pc""
""pc"" is not contained in the class",0,0,msr
5127,"> The class has, and since the class has you can get it from a parent method.
I'll check again tomorrow then",0,0,msr
5129,"> > > And after searching for method
> > > ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""
> > > > Returns 0 results.
> > Instead this returns me, on my patcher at least, a method with the aformentioned variables. 🤷‍♂️
Open jadx, go to mentioned class, open smali view, Ctrl+f, search, and find nothing. Your patcher is not revanced. So We have to find a working way.",1,0,msr
5130,"> > > > And after searching for method
> > > > ""(Lcom/google/android/libraries/youtube/innertube/model/player/PlayerResponseModel;Lcom/google/android/libraries/youtube/player/model/PlaybackStartDescriptor;""
> > > > > > > > > Returns 0 results.
> > > > > > Instead this returns me, on my patcher at least, a method with the aformentioned variables. 🤷‍♂️
> > Open jadx, go to mentioned class, open smali view, Ctrl+f, search, and find nothing. Your patcher is not revanced. So We have to find a working way.
Already done.
You can see the screenshot in the answer preceding yours.",0,0,msr
5132,"This post violates the code of conduct and is being closed. Do not use profanity,",0,0,msr
5133,"Windows 365 without external access, mandatory proprietary remote desktop, extreme slow, very huge connection time, latency in the skies... awful.... so... windows 365 is trash! Tried for 1 day.
Terrible!!!
Microsoft doing \<profanity removed>!",0,1,msr
5134,"# Proposal for highlight groups:
Create a new set of hierarchical groups that are mapped **losslessly** from captures (e.g., `@keyword.special`), with sane fallback to defaults.
1. Implicitly generate a highlight group for every capture, e.g., `TS.keyword.special` that can be mapped directly (but isn’t necessary by default)
2. Groups are implicitly language-specific, e.g., `TS.keyword.special.lua`
3. If not mapped, they successively fall back to the top level, e.g., `TS.keyword` (**Question:** fallback to `TS.keyword.special` or `TS.keyword.lua` first?)
4. Provide a set of top-level (and possibly _some_ second-level?) groups that are linked by default to suitable standard Vim groups and/or are mapped by the default and bundled color schemes. These groups need not derive from the set of standard Vim groups; it might be better to provide a small(!) number of different sets for different types of languages (with overlap), like ""imperative languages"" (C, Lua, Python, ...), ""markup languages"" (Markdown, LaTeX, HTML), ""data representation languages"" (JSON, regex, ...).
5. Languages are free to extend the capture hierarchy, e.g., `@snowflake.superfancy`, with the clear understanding that these won’t be highlighted unless mapped by custom colorschemes or plugins. (This also allows optional highlighting that is disabled by default.)
Ideally, we can use capture names directly as highlight groups, similarly to how syntax and highlight groups are matched?",0,0,msr
5135,"> 1. Implicitly generate a highlight group for every capture, e.g., `TS.keyword.special`
With https://github.com/neovim/neovim/pull/19830 would we generate the identical names (`@keyword.special`) instead?",0,0,msr
5136,"Proposal LGTM except this part:
> These groups need not derive from the set of standard Vim groups; it might be better to provide a small(!) number of different sets for different types of languages (with overlap), like ""imperative languages"" (C, Lua, Python, ...), ""markup languages"" (Markdown, LaTeX, HTML), ""data representation languages"" (JSON, regex, ...).
Can that be postponed to a ""phase 2"" discussion? Just thinking this might be an expensive discussion.",0,0,msr
5138,"**Goal:** Enable out-of-the-box `treesitter` highlighting of core languages (Lua, Vimscript) for Neovim 0.8
**Roadmap:** * [x] Replace static capture-to-highlight group table with automatic mapping (hierarchical, e.g., `@keyword.special` from Lua gets mapped to `TSKeywordSpecialLua`, with fallback to `TSKeywordSpecial` or `TSKeyword` if the more specific groups don't exist #19931
* [x] Include Lua and Viml parsers in build process (similar to C parser) #15391
* [x] Ship parsers and queries in runtime #15391
* [x] Upstream basic highlighting functionality from `nvim-treesitter` #15391
* [x] Change query handling to give same precedence as for parsers (config > plugins > runtime), with explicit extending via modelines (needs documentation!) #20104 #20117
* [x] Upstream utility functions from `nvim-treesitter` and `nvim-treesitter-playground` #19946 #20093
* [ ] ~~Upstream predicates and directives from `nvim-treesitter`; refactor metadata handling to be more generic?~~ (bumped to 0.9)
* [x] Upstream https://github.com/lewis6991/spellsitter.nvim (enabled automatically if treesitter highlighting is enabled for a language) #19351
* [x] Update and polish `vim.treesitter` docs #20142
* [x] 🥳",0,0,msr
5139,"Does this really need to be changed? who cares. its a file name. it could be named dsklfjnfkltghfjkltfjklhdjfgkl.json for all i care and i wouldnt make a issue changing it.
:)",1,1,msr
5140,"This is a poor representation of the trans community. If you really care about the trans community, you wouldn't make issues where they don't exist because transgender individuals have enough on their plate already. TS when said as an initialism sounds nothing like trans. When said as a word, it is a far stretch from the word trans bar 2 sounds. You could just as easily made this extrapolation with the word trains. You're probably just a troll, but I think this ought to be said regardless.",0,1,msr
5145,"I wonder if these are somehow related ,i mean if using embedded heimdal could also fix the browse issue .",0,0,msr
5146,"> I wonder if these are somehow related ,i mean if using embedded heimdal could also fix the browse issue .
I’m somewhat hoping. 🤞So that needs to be the first order of business.",0,0,msr
5147,Open wet build has samba dependent on samba:host. Use this as a base to split the build of heimdal out. https://github.com/openwrt/packages/blob/master/net/samba4/Makefile,0,0,msr
5148,"The solution was there all the time, found by unnamed CoreELEC member within a few hours by reading the change log:
```text
Note that when Samba is configured to run as an Active Directory Domain Controller, the samba binary that supplies the AD code will continue to provide DCERPC services while allowing samba-dcerpcd to provide services like SRVSVC in the same way as smbd did previously.
```
https://github.com/CoreELEC/CoreELEC/commit/d07ac771047f28b015fd65d397747caef4c511c5
Thank you for your cooperation and respect of solution origins!",1,0,msr
5150,"I am sorry that we left you in dark for months and just 24h after we fixed it you published your ""own"" solution. It just shows LE behavior did not and will not change in future. Get what ever is possible and give a f* about others....
Again, I am sorry, we are just a fork, not member of LE.",0,1,msr
5151,"2 items to be addresses with samba 4.16
- use embedded heimdal to build
- fix browse issue
- samba 4.17.0rc1 has the same issue
```
smbclient -U libreelec -L \\localhost
Password for [WORKGROUP\libreelec]:
Sharename Type Comment
--------- ---- -------
SMB1 disabled -- no workgroup available
```",0,0,msr
5152,"> PDF.js version: 0.5.8
That's not an official PDF.js release, and even if existed it'd be *almost ten years old*; please find the latest releases at https://mozilla.github.io/pdf.js/getting_started/#download
> We use a simple iframe to display PDF, nothing fancy (src = ""/ViewerJS/?zoom=page-width#../Uploads/EDocPro/EDT-21072022-163634.pdf""
> [...]
> What went wrong? (add screenshot) - This is what is displayed in Firefox
That's clearly *not* the PDF.js default viewer, which is further confirmed by the ""ViewerJS"" string in your URL.
Hence you unfortunately opened this issue *in the wrong repository*, since the ""ViewerJS""-project is *not* being developed here; furthermore I don't believe that project is still developed/maintained now.",0,0,msr
5153,"Well that not a particular helpful answer snuffleupagus !!! Did you think I imagined using viewer for 5 years ??
Whilst I very much appreciate some people live and breathe their own particular technologies when I search for something and come up with : https://viewerjs.org/getit/ and it has a link that says 'latest' version its somewhat hard to assume this is not actually the case !!!!
Now the more polite and educated way of answering my query would have been....
Buddy it looks like you might have a very old version, try downloading the latest version from.....
I will try this now that I have this information and hopefully that will resolve my issue.",0,1,msr
5155,"Thanks but given this is supposed to be a place to get help from like minded individuals/experts there is no need for assumption or unwarranted denegration.
Assuming that I am not doing the same as you in a different field is.....
The first job of giving assistance is to understand the issue, the issue isn't that I have been using a fabulous piece of technology for 5 years without that doesn't exist !
As clearly articulated the fabulous piece of technology we have been using has suddenly presented an issue that we haven't seen before, one that appears to only occur on firefox browsers. Personally I'd see that as an opportunity worthy of solving, not an opportunity to be rude.
Perhaps you can point me in the direction of installation instructions of the latest version, the VERY OLD version we have been using only needed to be dropped into the website folder for the viewer to function, however this does not appear to be the case with the latest version ? and the page link provided provides no insight as to further installation.
I apologise for not knowing more about your product or need more than a viewer that works.",0,1,msr
5157,"Attach (recommended) or Link to PDF file here:
Configuration:
- Web browser and its version : Firefox
- Operating system and its version: Win 10 upto date
- PDF.js version: 0.5.8
- Is a browser extension:
Steps to reproduce the problem:
1. We use a simple iframe to display PDF, nothing fancy (src = ""/ViewerJS/?zoom=page-width#../Uploads/EDocPro/EDT-21072022-163634.pdf""
2.PDF opens and displays what I guess is a template used by the supplier without the Invoice particulars
3. If I open with Acrobat or Acrobat Reader or the https://mozilla.github.io/pdf.js/web/viewer.html it displays correctly with the full text
As the PDF is not ours but something supplied to us we have no control over the fonts etc (I presume its something like that), how can we force the viewer to display it ?
What is the expected behavior? (add screenshot) - This is what the document actually should be
![image](https://user-images.githubusercontent.com/85852391/180388195-6bbe2e54-4f88-4251-a698-14a234dd133d.png)
What went wrong? (add screenshot) - This is what is displayed in Firefox
![image](https://user-images.githubusercontent.com/85852391/180387718-b28e7838-bec8-4588-ba7a-21284db45620.png)
Link to a viewer (if hosted on a site other than mozilla.github.io/pdf.js or as Firefox/Chrome extension):
[EDT-21072022-163634.pdf](https://github.com/mozilla/pdf.js/files/9165605/EDT-21072022-163634.pdf)",0,0,msr
5158,"VERY predictable output from random(), after being seeded with an incrementing counter.
Attached sketch shows how I'm getting four streams of almost perfectly sequential numbers from an algorithm that uses multiple calls to random() after using an incrementing counter as a seed via randomSeed(). It should not be possible to get anything orderly from that, especially over such an immediately short period.
The sketch (below) produces mac-addresses as my original script does, then has four columns: The first three columns show sequential numbers from the high-byte of the last octet, the fourth column shows sequential numbers from the low-byte of the last octet.
Right from the start, with seed 0xb53fdfdf:
```
// Random Seed: 0xb53fdfdf
e2:fa:74:b3:a5:9c 9
0e:9c:eb:97:7f:4c 4 c
3a:3e:62:7a:59:fc f 6a:e0:d9:5e:34:ac a
96:82:50:41:0e:5c 5 c2:24:c7:25:e8:0c 0 f2:c6:3e:08:c3:bc b
1e:68:b5:ec:9d:6c 6 4a:0a:2c:d0:77:1c 1 7a:ac:a3:b3:52:cc c
a6:4e:1a:97:2c:7c 7 d2:f0:91:7a:06:2c 2 02:92:08:5e:e1:dc d
2e:34:7f:42:bb:8d 8 5a:d6:f6:25:95:3d 3 8a:78:6d:09:70:ed e
b6:1a:e4:ec:4a:9d 9 d
e2:bc:5a:d0:24:4d 4 12:5e:d1:b3:ff:fd f
3e:00:48:97:d9:ad a 6a:a2:bf:7b:b3:5d 5 9a:44:36:5e:8e:0d 0
c6:e6:ad:42:68:bd b f2:88:24:25:42:6d 6 22:2a:9b:09:1d:1d 1
4e:cc:12:ec:f7:cd c 7a:6e:89:d0:d1:7d 7 aa:10:00:b4:ac:2d 2
d6:b2:77:97:86:de d 02:54:ee:7b:60:8e 8 32:f6:65:5e:3b:3e 3
5e:98:dc:42:15:ee e e
8a:3a:53:25:f0:9e 9 b6:dc:ca:09:ca:4e 4
e6:7e:41:ed:a4:fe f 12:20:b8:d0:7f:ae a 3e:c2:2e:b4:59:5e 5
6e:64:a5:97:33:0e 0 9a:06:1c:7b:0e:be b c6:a8:93:5e:e8:6e 6
f6:4a:0a:42:c2:1e 1 22:ec:81:26:9d:ce c 4e:8e:f8:09:77:7f 7
7e:30:6f:ed:51:2f 2 aa:d2:e6:d0:2c:df d d6:74:5d:b4:06:8f 8
06:16:d4:97:e0:3f 3 f
32:b8:4b:7b:bb:ef e 5e:5a:c2:5f:95:9f 9
8e:fc:39:42:6f:4f 4 ba:9e:b0:26:4a:ff f ```
That's seeded with 0x3ffe883c, and that output is near the beginning.
The first three columns to the right of the MAC addresses all show the high-byte of the last octet:
* The 1st column shows 0xf - 0xf, sequentially
* The 2nd column shows 0x4 - 0x4, sequentially
* The 3rd column shows 0x9 - 0x9, sequentially
The 4th column counts the low-byte of the last octet over a longer period, but the whole pattern repeats almost perfectly, and seemingly indefinitely, regardless of how it's seeded. Not the behaviour I expect from a PRNG!
Just looking at the ""random"" MAC addresses I was wanting, it's obvious that the low-byte of the last octet is repeating for 14-15 times (14.7, on average) and then incrementing. Again, should be 100% impossible to get this from any self-respecting PRNG being seeded by a counter.
nb the sketch has a delay() towards the bottom, so it scrolls up on a console at human-readable speed. For data-collection, just comment out the delay.
nb2 the random seed can be set manually.
sketch:
```
/*
===========================================
buggy ""random()"" function, as observed here:
https://github.com/atom-smasher/esp8266_beaconSpam
this sketch starts with a random seed ""randomMacSeed""
that seeds the prng via ""randomSeed()"" and an incrementing counter
calls to ""random()"" are producing some very predictable outputs!
this sketch shows the mac-addresses as they would be generated,
and derived from those mac-addresses are four columns, all counting
incrementally, with very few errors
expected results: using an incrementing counter to seed the prng, i expect
to NOT have predictable output like this
===========================================
*/
const uint64_t randomMacSeed = os_random(); // random seed on startup
//const uint64_t randomMacSeed = 0x1234abcd ; // fixed seed; make it your own
uint32_t i = 0;
uint8_t macAddr[5];
void mayhemMac(uint32_t ssidNum) {
randomSeed(uint32_t((randomMacSeed) + (ssidNum)));
macAddr[0] = uint8_t(random(0x0, 0x100)) & 0xfe | 0x02 ;
macAddr[1] = uint8_t(random(0x0, 0x100));
macAddr[2] = uint8_t(random(0x0, 0x100));
macAddr[3] = uint8_t(random(0x0, 0x100));
macAddr[4] = uint8_t(random(0x0, 0x100));
macAddr[5] = uint8_t(random(0x0, 0x100));
}
void setup() {
// start serial
Serial.begin(115200);
//Initialize serial and wait for port to open:
while (!Serial) {
delay(10);
}
// wait for serial port to connect
delay(300);
Serial.println();
Serial.printf(""\n// Random Seed: 0x%x\n"", uint64_t(randomMacSeed));
}
void loop() {
for (i = 0; ; i++) {
yield(); // needed for extra-large lists
mayhemMac(i);
Serial.printf("" %02x:%02x:%02x:%02x:%02x:%02x"",
macAddr[0], macAddr[1], macAddr[2], macAddr[3], macAddr[4], macAddr[5]);
yield();
if (0 == i % 3) {
Serial.printf("" %x"", macAddr[5] >> 0x4);
}
if (1 == i % 3) {
Serial.printf("" %x "", macAddr[5] >> 0x4);
}
if (2 == i % 3) {
Serial.printf("" %x "", macAddr[5] >> 0x4);
}
if (1 == i % 15) {
Serial.printf("" %x"", macAddr[5] & 0x0f );
}
Serial.println(); delay(200);
}
}
```
Edited to correct how the seed is displayed, and share output with a verified seed.",0,0,msr
5159,"I think they would have to incorporate a battery, I just read Arduino has a 58-day limit in buffer overflow with the millis(). I always wonder how that goes. I imagine three internal tickers, one starting at zero, the other at the negative most number, and they both go forward equal same increment rate, and one starts at the positive most spectrum, and goes backwards at half the rate, all three then are part of calculation to issue a millis(), and because millis is never more than a second, you need then to get the calendar clocking segments like second and weeks, whereas when the timer reaching overflow threat approaches, it begins in reverse and the prior reversing one picks up in rate, and the last most slows in rate, like washing or something like that never has a second, but you probably also then need to incorporate dates as a bar association reset of approaching is when? In rate of millis() used to possible catch the buffer overflow and it's a big Y2K mess! But I say that because I think these timer states would be flash saved when booted down, or at a last state type of mode, then booting up the random seeding is partial to the system is unique in back powered up. IDK, though. Tuff one.",0,0,msr
5160,"I just read Arduino has a 58-day limit in buffer overflow with the millis(). I always wonder how that goes. I imagine three internal tickers, one starting at zero, the other at the negative most number, and they both go forward equal same increment rage, and one starts at the positive most spectrum, and goes backwards at half the rate, all three then are part of calculation to issue a millis(), and because millis is never more than a second, you need then to get the calendar clocking segments like second and weeks, whereas when the timer reaching overflow threat approaches, it begins in reverse and the pri9or reversing one picks up in rate. Something like that never has a second however, and then pending you can wait out how many years to possible catch the buffer overflow and it's a big Y2K mess! But I say that because I think these timer states would be flash saved when booted down, or at a last state type of mode, then booting up the random seeding is partial to the system is unique in back powered up. IDK, though. Tuff one. You could ignore real time and develop in the calendar intervals, second, minute, day, week, month and year. Just say all Arduino start at a date when powered and kept from the user interaction.
Actually, there is a better way. Query the US court system for the jury selection method, and you'll be going over it in review.",0,0,msr
5161,"Hi, just like you I have really slow Vue development. There is a alpha version of plugin to marginally increase the speeds. Please see https://github.com/johnsoncodehk/volar/issues/1615",0,0,msr
5165,"Seriously affect the development efficiency！
please fix！
please fix！
please fix！
please fix！",0,1,msr
5166,"Sorry no, see other previous issues if you are interested in a longer answer.",0,1,msr
5167,"```sh
git clone https://github.com/composer/composer.git --branch 2.5.4 ~/composer-build && \
composer install -o -d ~/composer-build && \
wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch && \
cd ~/composer-build && patch -p1 < composer.patch && \
php -d phar.readonly=0 bin/compile && \
rm /usr/local/bin/composer && \
php composer.phar install && \
php composer.phar update && \
mv ~/composer-build/composer.phar /usr/local/bin/composer && \
rm -rf ~/composer-build && \
chmod +x /usr/local/bin/composer
```",0,0,msr
5170,"we also have this amazing discussion about the topic with a bunch of solutions: https://github.com/composer/composer/discussions/10808 (censored, now it is https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message)",0,0,msr
5171,"@Seldaek The finality of your ""no"" answer reminds me of closed-source software vendors who say ""No, you can't change the code."" It goes against the spirit of open-source. Not only that, it's ineffective. Annoy others (as you have) and they'll go from neutral to hating your message. Have a nice day.",0,1,msr
5172,I don't stand with Ukraine and what the Composer has to do with it?,0,1,msr
5175,"> I can understand that they want to use the reach here to draw attention to the war, but it is not our war, it is driven by political interests that we do not understand and that we cannot influence. And srsly: ""Let's fight back against the Russian regime""? We are not your private army, nor is it useful to turn the aggression around. Information must be freely available and the only way out of the conflict is de-escalation on all sides - just my 50 Cent
""but it is not our war"" - well that's was Ukrainians thoughts during war in Georgia. or in Chechnya. ""it is driven by political interests"" - oO, so politicians was raping and murdering people in Bucha, Izyum, Kupiansk, Kherson? As i know it was regular russians nad not Putin himself.
""conflict is de-escalation on all sides"" - I guess you want to give russians your home to ""de-escalate"". Because a lot of ukrainians do not understand why they should give their homes to rssians.
Sure it is not your war, just wait until it become yours",1,1,msr
5177,I was actually a bit shocked when I first saw this. I strongly condemn any political statements in Opensource. Don't use your software as an audience for any political propaganda.,0,0,msr
5181,"Please add the output of the following statement to be fair:
#standwithLibya
#standwithSyria #standwithIran
#standwithCAR
#standwithAfghanistan
#standwithIraq
#standwithYugoslavia
#standwithBosnia
#standwithVenezuela",0,1,msr
5182,There is also `#standwithCyprus`. See https://en.wikipedia.org/wiki/Turkish_invasion_of_Cyprus.,1,1,msr
5183,"Every normal person support Ukraine, so just calm down, chill",0,1,msr
5185,"It's not the same, if not everybody get's it unasked in there command line ;)",0,0,msr
5186,"> It's not the same, if not everybody get's it unasked in there command line ;)
I know, I do not agree with the forced promoted message by composer either.",0,1,msr
5189,"Have you added your Google Maps API Key in `app.json`?
You need this inside your ""android"" key:
```
""config"": {
""googleMaps"": {
""apiKey"": ""xxxxx""
}
},
```",0,0,msr
5191,"Granted there isn't a misconfiguration in your config file, it could potentially be related to https://github.com/expo/expo/issues/18530. I never confirmed what happens on android emulator, but Google Maps did seem to render on a physical Android device following the documentation.",0,0,msr
5193,"we have a similar situation. the expo go app doesn't show the map anymore - BUT if we release it, then the real production app still shows the map. So I assume something in expo go is wrong",0,0,msr
5194,"So, I spent the time figuring out how to launch the app with the **_bare workflow_**. And surprisingly it works even without an API key. Very strange but, at least there's that.",0,0,msr
5195,My assumption is that this issue and https://github.com/expo/expo/issues/18570 are related. In both cases it seems the expo go app does not have access to the properties that are provided through the enviroment like app.json,0,0,msr
5196,"a serious and blocking issue, we can't go further in our project! , should we turn back to SDK 45 ?! why no one assigned to this bug?!!",0,1,msr
5197,"> a serious and blocking issue, we can't go further in our project! , should we turn back to SDK 45 ?! why no one assigned to this bug?!!
Have you tried converting your project to a bare workflow? That should allow you to add your google maps API key manually in the android files and allow you to keep building while we wait for a resolution to the managed version.",0,0,msr
5201,"Based on your suggestion I moved to the expo development build rather than the bare workflow. And the map showed up right away no API key necessary. However, I continued to plug along and move my firebase implementation to the [legit version of firebase](https://rnfirebase.io/) rather than using the JS SDK that I was using with expo go. And the map immediately started crashing lol. Like it started expecting an API key when firebase got added. I've moved to using mapbox instead while I wait for this issue to be addressed [edit] removed my salty comments. I just picked a bad week to pick up react native lol",0,0,msr
5202,"> you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/
forget about my case, why it does not work in https://snack.expo.dev ? do you think you gave the best answer by asking to create dev build !!! there is a breaking change , it worded in SDK 45 and previous versions, so it must work in 46 too, don't ask users to go with other alternatives !!!",1,1,msr
5206,"It is not SDK 46 problem! @brentvatne @da45 With SDK 45, Expo Go 2.25.1 grey block on android, crash on ios. With SDK 46, Expo GO 2.25.1 grey block on android, crash on ios. But,
With SDK 45, Expo Go 2.24.3 work.
How to downgrade Expo 45 with Expo Go 2.24.3.
```
- Delete Expo Go app from Simulator and Real Device.
1. expo upgrade 45
- Downgrade to sdk 45, because Expo Go 2.24.3 doesn't support Expo SDK 46.
3. expo start
- Expo , automatically install expo go 2.24.3.
```
Now you can wait for fix Expo Go problem :)",0,0,msr
5208,"> you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/
Hi @brentvatne, I tried with a custom development build with `SDK 46` and `react-native-maps 0.31.1` and the map is blank. I am not using Expo Go. API key is loaded via `app.config.js` on `android.config.googleMaps.apiKey` as indicated in the expo documentation.
Did you make it work on your end?",0,0,msr
5209,I am using 'app.json' and I don't have this problem. Are you all using 'app.config.js'?,0,0,msr
5210,"> I am using 'app.json' and I don't have this problem. Are you all using 'app.config.js'?
`app.config.js` is just a dynamic version of `app.json` that I use for app variants. Fmi: https://docs.expo.dev/workflow/configuration/
@BrodaNoel are you using the same version as I am? SDK 46 / react-native-maps 0.31.1 / development build and your API key set on android.config.googleMaps.apiKey in app.json?",0,0,msr
5211,"Hi all! I'm sorry, but I'm locking this thread due to some heated comments. We will deploy a fix asap! Here is an overview of the issue:
### Overview
The issue seems to be in our Android build of Expo Go. Snack and the Expo Go version for SDK 46 (`2.51.x`) are affected and will not show Google Maps during development.
> ~~I'll update the comment whenever we deployed the fixed Expo Go version to the stores.~~
- Expo Go `2.25.2` is [available on the Play Store](https://play.google.com/store/apps/details?id=host.exp.exponent)
- Expo Go `2.52.2` has been deployed to Snack
### Workarounds
1. Use a [custom dev client](https://docs.expo.dev/development/getting-started/) with your own Google Maps key. It's similar to the bare workflow approach some of you mentioned. This method also allows you to install the latest version `react-native-maps@1.2.0` in your app. You can use [EAS Build](https://docs.expo.dev/build/introduction/) to build the app if you don't have the Android SDK installed.
_**I would heavily recommend this method for now**_
2. Temporarily downgrade to SDK 45 to use an older Expo Go. This method would allow you to use the previous Expo Go version. You'll need to remove Expo Go from your Android phone or simulator to do this. Whenever you start your SDK 45 project with `expo start --android`, and your device is connected through USB and ADB, it will automatically install this Expo Go version.
- [Expo Go `2.24.3` APK for Android](https://d1ahtucjixef4r.cloudfront.net/Exponent-2.24.3.apk)
Hope this helps!",0,0,msr
5212,"As for some of your questions/comments:
---
> we have a similar situation. the expo go app doesn't show the map anymore - BUT if we release it, then the real production app still shows the map. So I assume something in expo go is wrong
Yes, that assumption is right! We found an issue that only relates to the Expo Go builds and not your production apps. Even apps built with the classic build system (`expo build`) are not affected. But, because we use Expo Go in Snack too, Snack is also affected.
---
> @brentvatne development builds don't allow hot reload, right? We use expo go during dev in Wi-Fi because unlike an emulator it gives all features of a real device.
Expo dev client builds are fully capable of doing hot reload. I would suggest to try it out :)
---
> Based on your suggestion I moved to the expo development build rather than the bare workflow. And the map showed up right away no API key necessary.
>
> However, I continued to plug along and move my firebase implementation to the [legit version of firebase](https://rnfirebase.io/) rather than using the JS SDK that I was using with expo go.
>
> And the map immediately started crashing lol. Like it started expecting an API key when firebase got added.
>
> I've moved to using mapbox instead while I wait for this issue to be addressed
>
> [edit] removed my salty comments. I just picked a bad week to pick up react native lol
The Expo dev client and EAS allow you to pick any native module as you see fit. I'm sorry you had an unfortunate experience with `react-native-firebase`, I'm not sure if [you followed these steps](https://rnfirebase.io/#expo) but some libraries require you to add credentials, even in development mode. We can't control that outside of Expo Go and/or for third party libraries.
(And thanks for keeping things civilized ❤️)
---
> forget about my case, why it does not work in https://snack.expo.dev/ ? do you think you gave the best answer by asking to create dev build !!! there is a breaking change , it worded in SDK 45 and previous versions, so it must work in 46 too, don't ask users to go with other alternatives !!!
Because Snack also uses Expo Go, Snack is also affected. We will deploy a fix asap.
---
> @da45 please calm down. It's good engineering practice to try to find workarounds. Often it unblocks at least some percentage of users.
I fully agree with this. We don't intend to break things, but if they do, we try to keep everyone as ""unblocked"" as possible while we fix things. We still have to go through store review whenever we fix it, [which took ±15 days on the Play Store for SDK 46](https://twitter.com/cedricvanputten/status/1557375168955252738). So I think there is a lot of value in sharing these workarounds.
---
> Hi @brentvatne, I tried with a custom development build with `SDK 46` and `react-native-maps 0.31.1` and the map is blank. I am not using Expo Go. API key is loaded via `app.config.js` on `android.config.googleMaps.apiKey` as indicated in the expo documentation.
>
> Did you make it work on your end?
If you added the `android.config.googleMaps.apiKey` after running `expo run:*` or `expo prebuild`, it needs to recreate the native files. These keys are set in your native files that `expo prebuild` generates.
You can try running `expo prebuild --clean` and see if `expo run:*` works.",0,0,msr
5217,"Didn't know CitizenFX released a game, that's news to me :o",0,0,msr
5218,"Github issues **are not** support tickets. Please use our forums, more specifically over at https://forum.cfx.re/c/redm-discussion/redm-bugreports/56 to report your issue.",0,0,msr
5219,"This is not a support platform. Try the forums at https://forum.cfx.re/ instead.
_Originally posted by @blattersturm in https://github.com/citizenfx/fivem/issues/1567#issuecomment-1211329798_ Clearly I would’ve tried the forums if anyone gave a shit about it and responded no one has even looked at mine or the other persons who is also having this same issue I NEED HELP THIS IS DAY 5 and I’m FRUSTRATED SO PLEASE DONT CLOSE MY NEXT TICKET WHEN I MAKE A NEW ONE that’s real shitty",1,1,msr
5221,"### App Name
Plex
### SCALE Version
22.02.1
### App Version
N/A
### Application Events
```Shell
N/A
```
### Application Logs
```Shell
N/A
```
### Application Configuration
N/A
### Describe the bug
Revert #3426 There is no reason to make a parameter mandatory that is not mandatory by the application developer. You're packagers, not developers. Do not try to make decisions for the application developer. Revert this change. For people with dynamic public IPs or DHCP this is a breaking change. ### To Reproduce
N/A
### Expected Behavior
N/A
### Screenshots
N/A
### Additional Context
N/A
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",0,1,msr
5223,Any follow up from the code maintainer?,0,0,msr
5225,"> UUID is anonymized randomly generated key per installation, cause this key is not associated with any advertisement id or not transferred to any other 3rd party or even used, this key doesn't require consent. As you mentioned the IP could provide more information cause it's location specific.
> This approach helps to the server monitor fair usage of resources.",0,0,msr
5227,"The privacy policy says:
> We strongly believe in the principle of data protection and safety, thus, the Company does not collect, store, process or transfer any personal information of users besides the cases when such information is provided by the users with their clear consent. You have not obtained consent from the userbase to use their devices to store and transmit a unique tracking identifier for their device.
Most are unaware you are even doing so; that is the opposite of consent.",0,0,msr
5228,"There is a strong disconnect between the tone and even some accusations and the actual issue being reported.
First of all, connecting to the server is only something that happens during download. OSMAnd is an offline map and downloads happen rarely. For heavy users less than once a month, for many much less than that. Additionally, all such downloads are user-initiated, they have to explicitly press a download button. Nothing that would be useful to build a profile of any specific user.
**Allegations of travel data and such being leaked are thus really not in evidence.**
The data that is being shared is not possible to connect to any personally identifying information. Notice that google explains that since some 5 years the `getUserAndroidId` is unique per installed application. So any other application the user uses (on the same phone, or not) will not share the UUID. So on top of the fact that not much of a profile can be made at all, there is no way to connect such to any actual personal info.
Does anyone actually have any real issues with the existing code?
For the record, I don't have any relation to the company behind OSMAnd, never received anything from them (other than FOSS software) and I'm just an open source developer standing up for common sense.",0,0,msr
5230,"I think there are 2 direction of discussions: consent and identification.
1. I don't think consent is needed, same as you don't need consent that your IP is used to download maps. Second it's not a website where you read information, here you use resources, so in that sense by downloading data you agree to fair use of resources (Terms of Service) and in that case you agree that you don't download too many resources especially if you are not paid user for example
It would be completely different case if program would send statistics in background that you're using the application and here is an IP. To summarize: it's probably needs to be more clarified in Terms of Service but it doesn't require a consent.
2. User identification. We switched from IP to UUID to calculate how many active users & how often update exactly to solve the server load issue and don't use personal data. For example we don't require email to identify user and don't track IP exactly cause IP could provide location specific information. Random UUID **doesn't identify anything** if it's not connected to any other services or other private information. We don't share UUID with any other applications so they also couldn't track that UUID even if these apps have personal information.
It's completely incorrect to say that app could view **travel history**, I would say it's total nonsense cause the server only views what and when maps were downloaded in contrast of other apps that provide Internet services. -----------------
Last but not least.
The websites require to ask consent only for information they gather for example if you search an address on website and website doesn't store these addresses or don't connect them to IP (address is essentially publicly available information), consent is not needed. If the website will start storing information in the table [IP, Search], then consent is needed. So it's all about how the data is processed and used and not only what's being transmitted.",1,0,msr
5231,"Just to provide another point of view on this topic. As a Service provider I need a unique identifier to predict the load on servers (especially once maps are updated), so in the end it would say:
""Download maps won't be provided if user doesn't agree to create a random key for the installation"".
Does it makes sense then to implement at all ? Cause in the end it will end up, if you to download - you need a consent which to me is actually totally clear if you don't want to know what & when you downloaded - download or generate maps yourself and import to the app. **App won't send to the server what you've imported yourself!**",0,0,msr
5232,"@sneak you claimed that;
> to track the travel history of that user via client-ip geolocation.
Considering that this is an offline-map which doesn't normally do downloads, how do you support your claims? If you continue to make big claims, please provide actual evidence.",0,1,msr
5233,"No, this doesn't make sense at all, as I can distribute a forked client that sends no ID, or a random UUID on every single HTTP request
It doesn't get you anything to trust the client to self-report.",0,0,msr
5234,Please stop suggesting that the UUID is somehow private or anonymous. With client IP geolocation it can identify users by travel history. It is PII.,0,0,msr
5238,"The uuid does not limit anything. Anyone can make as many requests as they want with a new uuid each time, defeating this limit.",0,0,msr
5239,"> The uuid does not limit anything. Anyone can make as many requests as they want with a new uuid each time, defeating this limit.
This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users.",0,0,msr
5241,"> *This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users.*
I intend to distribute a fork of this application with all unique IDs randomized on every request. Your assumption will not remain accurate.
Also there is nothing stopping a single someone from making a high request load to the server with random IDs, rendering the server data collection useless.",0,0,msr
5243,"> It's completely incorrect to say that app could view travel history, I would say it's total nonsense cause the server only views what and when maps were downloaded in contrast of other apps that provide Internet services.
It's really sad to see that you don't understand the privacy implications of your choice of putting a unique tracking identifier (similar to a cookie) on a user's device that persists over time (and changes in client IP).",0,1,msr
5244,">it's in privacy policy
Do you really think anyone actually reads it?
>we use randomly generated UUID which preserves the privacy much more than IP itself
A unique identifier that, in contrast to the IP address, can't be easily anonymised by the user and that also persists across IP addresses “preserves the privacy”? Surely, Google uses those Android ad IDs because they're so incredibly anonymous.
>You don't need to use OsmAnd Services if you don't agree with Terms of Use
Besides the fact that privacy laws are a thing, this is incredibly detrimental to say, considering that OsmAnd has always presented itself as a privacy-friendly alternative to commercial services. Telling people not to use it if they care about privacy is something I'd only expect from Google or Microsoft.
>you can download or build maps yourself
This is not an option for probably 99% of users.
I agree that this unique ID is not as bad as the issue's author claimed it to be. However, I also think that OsmAnd should not be designed in a way that could theoretically enable behavioural tracking.
Would simply re-generating the ID at regular intervals really be too difficult? In my opinion, it would be such an easy way to improve everyone's privacy and to calm anyone who's worried.",0,0,msr
5248,"No, the server still sees the unique id from the client and has the opportunity to track individual users.
The fix is to stop sending tracking identifiers from the client to the server. There is no benefit to the server to receive client ids, as any malicious client can send varying ones, and non-malicious clients are not an issue.
Sending client tracking ids is all privacy downside, for no security benefit.",0,0,msr
5251,"**Major actionable points:**
1. UUID will be regenerated (every 2-3 months)
2. Make sure logs are truncated / deleted, so only summary left
3. Privacy policy will be updated and more descriptively about (Fair Use of OsmAnd Online Services and UUID)
**Major information point:**
1. Providing UUID is part of Terms of Usage of OsmAnd Online Services such as downloading maps. UUID or account id for OsmAnd PRO will always be part of request
2. OsmAnd Online Services are **not unique** way to use OsmAnd Client application. Maps could be downloaded or generated manually. So there is no issue that OsmAnd Client hiddenly transmits private data
3. There is no any indication / facts / points provided that UUID + Downloaded Map can somehow track user or even more identify real person. Identification is a clear procedure that UUID will be connected to real data such as Name / Surname / Living Address / Working Address / Photo. If IP is corrupted and doesn't provide enough privacy then issue is not related to UUID.
--------------------------------------------
That's my basic understanding for now. Please leave comments in the nearest future if I'm missing something and that **issue will be frozen** to not create any further never ending discussions!",0,0,msr
5252,"@vshcherb As far as I can tell, using a UUID does not provide any benefit for OsmAnd compared to the approach I [mentioned above](https://github.com/osmandapp/OsmAnd/issues/15058#issuecomment-1217665653) (simply storing a counter in the client). On the contrary, using a UUID seems to make things more difficult (regeneration, clearing logs, updating the privacy policy). It also makes tracking users at least theoretically more easy.
Could you maybe clarify why this approach is not being considered? Am I missing something?",0,0,msr
5253,"Except that we couldn't count of how many unique users per month / quarter were active, exactly why uid was introduced to give a sum by day / month.",0,0,msr
5254,"I'm sorry for my ignorance but why do you need this information? I wonder how useful it is because, for example, I'm a *very* active user but almost never update my maps so I won't show up in these statistics.
If you really want to know how many unique users download maps, couldn't the number of unique IP addresses be used as a decent approximation? Not that I necessarily like the idea of IP addresses being stored but at least this is information the servers get anyway.",0,0,msr
5255,"> There is no any indication / facts / points provided that UUID + Downloaded Map can somehow track user or even more identify real person. Identification is a clear procedure that UUID will be connected to real data such as Name / Surname / Living Address / Working Address / Photo.
This misunderstands what identification means.
The UUID uniquely identifies a user when combined with a series of differing client IPs. It doesn't need to resolve *in your systems* to a user's name or photo; the fact is that the UUID represents one and only one user. This allows you to track *that one user's* movements from city to city (as you see different client IP locations).
It's irrelevant that you don't know the user's name; you have your own name for them - a UUID.
You don't have consent to track users in this manner without an opt-in. This UUID is a unique tracking identifier, akin to a cookie (although it's sent in the query string and not the HTTP header, the effect is the same) and it's for a nonessential purpose (tracking user count statistics). It's unnecessary for this purpose, anyway.",1,0,msr
5256,"Just a quick additional information:
If OsmAnd B.V. _or_ their users are situated in European Union (and they are), they will **need to follow the GDPR**.
- @sneak note that _user consent_ is only **_one of six_** allowable ways for [obtaining legal basis for data processing](https://gdpr.eu/gdpr-consent-requirements/ ) - the OsmAnd B.V. might decide to use some other legal basis for data processing. In any case, they should consult a GDPR specialist and spell out in their Privacy Policy **exactly** what Personally Identifiable Information (like UUID or username, or IP addresses) is being collected, and specify in advance **all** purposes for which such data will be used, as well as other things (will that data be shared with third parties, stored/processed outside of EU etc.). If they later want to use PII for another purpose, they must re-obtain additional consent (or other legal basis) - they're not allowed to reuse previously collected data for new purpose retroactively. There are many things that need to be done correctly, which is why they should consult a GDPR specialist (disclaimer: I am not one so it is not an ad for myself; but I had to go thru the same pains, so I _am_ informed above average in the matter). There is a [checklist](https://gdpr.eu/checklist/) they might want to go through to verify if they did everything they need to.
- While having UUID might be required for some uses (e.g. all registered users), I would suggest _seriously considering options_ that require it as little as possible, especially for ""anonymous"" users. Any data collected has a chance to leak / be misused, so not collecting it in the first place would be smarter than having to invest resources and money to deal with protecting it adequately, and dealing with a possible breaches (and GDPR fines) later. Also, OSM crowd in particular are often much more biased to be privacy concerned (or they'd likely be using Google Maps in the first place) so it would be good business idea to try not to alienate them unnecessarily.",0,0,msr
5258,"Especially noteworthy (regarding the Law SE question):
>Key observations:
>
>- Singling out a data subject already counts as identification, meaning that it isn't necessary to infer their real-world identity!
>
>[...]",0,0,msr
5259,"I agree that I need a proper legal advise and in the end consent or any other legally correct form of user getting informed.
Completely another topic whether Map downloads Service could be provided without identifying how many users and potentially prevent out of service issues. In the end that needs to be decided by OsmAnd BV, so I'm not going to take decision on my onw, cause it has legal and business implications.
**Note**: the issue will be frozen to stop message flooding and updates will be sent to it to get everybody informed.",0,0,msr
5262,Sending of the tracking identifier must be disabled ***by default***. You must ask the user if they consent to sending it BEFORE you ever send it.,0,0,msr
5263,"Il 18/10/22 03:23, Jeffrey Paul ha scritto:
> You simply can't send any sort of tracking identifier for a user unless the user consents to it
This is clearly false: for instance no specific explicit consent is needed for the transmission of the IP address that comes along with every TCP/IP request, when it's necessary to perform the action requested by the user. Otherwise you'd end up in a catch 22 situation where you need previous consent to allow the user to download the privacy policy where they'll learn how to provide consent.",0,0,msr
5264,"https://github.com/osmandapp/OsmAnd/blob/22e40f113ce5c6df97f2f1687d5024ae38a4d28b/OsmAnd/src/net/osmand/plus/download/DownloadOsmandIndexesHelper.java#L273-L281
This appears to send the number of days since install, as well as a unique identifier (`getUserAndroidId`), to the index server when fetching indices, without respect for the telemetry preference setting.
This is a data leak that allows for a user's travel history to be tracked by the server, as these requests include client IP and a unique tracking identifier, and client IP is coarse (city-level) geolocation. This means the server can see the various cities the given userIosId travels to as the client IP changes over time.
This spyware feature also exists in the iOS version, and fails to respect the user's consent choice there as well. It seems unlikely that this is an accident.
https://github.com/osmandapp/OsmAnd-iOS/issues/2115",0,0,msr
5267,"Thanks, we are aware and have a Lavalink.jar prepped but there are other PRs to address first.",0,0,msr
5268,Sounds good. Will wait for a fix to be pushed. Appreciate the swift response.,0,0,msr
5269,I just noticed we're on the Red-Lavalink repo and not the Red-DiscordBot repo so I'm going to move this issue over there as this library is not affected and it's the audio backend side of things that is broken at the moment.,0,0,msr
5270,"Yeah, I put it there since it seemed to be an issue with Lavalink directly. No worries.",0,0,msr
5271,"PR's tied to this issue/needed for updating:
* #5821 * #5822 * @jack1142 , I can't find your PR that ""reverts"" #5751 to change the startup line detection to your ready check, that would go in this slot as needing to be included
* https://github.com/Cog-Creators/Red-Lavalink/pull/129",0,0,msr
5272,"quick fix, stop bot first:
`export RedBotAudioCogLocation=***/you/should/probably/change/this/*** && cp $RedBotAudioCogLocation/Lavalink{.jar,.jar.old} && wget -O$RedBotAudioCogLocation/Lavalink.jar https://github.com/Cog-Creators/Lavalink-Jars/releases/download/3.4.0_1350/Lavalink.jar `
the audio cog was at `~/.local/share/Red-DiscordBot/data/RedBot/cogs/Audio/` for me (Linux, venv)
this also shouldn't need to be reverted when the update comes out, the update should replace lavalink.jar if I understand correctly",0,0,msr
5273,how was that off topic at all?,0,1,msr
5274,"By adding a comment that provided an unsupported fix that will break a lot of current users if they try to do what you suggested.
Leave the fixes and workaround for the people who are aware of the wider user base and repercussions for a given approach.",0,0,msr
5275,"what? name a SINGLE repercussion? all the rest of the pr's are waiting on this jar, you telling me your code cant handle a file being replaced, when its literally going to replace that file? sorry, i may have a new github account, but i am by no means a noob, and i have four bs detectors attached to my head; my eyes and ears. i even added a command to keep the old jar, in case your code checksummed it for some reason, if you're checking the file creation date of a jar you're replacing or some crap, i concede, your dumb code will cause problems with this jar being replaced.
FURTHERMORE, I have redbot set up exactly as the docs instruct, you telling me you support environments that are not set up according to the recommendation of the docs? i know that's complete bs from being a part of the discord server. > people who are aware of the wider user base and repercussions for a given approach.
are these people aware their audio cog hasn't been working for days lmao, that not a repercussion?",0,1,msr
5276,"Please refrain from acting like this in our issues. If you want to go ahead and fix this for your self by the path you suggested that's fine with us. However this fix doesn't roll for us for an general production environment. We have an open PR (#5823) ready for this that we're planning on handling today.
> sorry, i may have a new github account, but i am by no means a noob, and i have four bs detectors attached to my head; my eyes and ears.
We do not filter content based on GitHub account age, experience or the amount of sensory elements one has. We do however filter based on our audience, the end user. If we feel like the fix suggested here may proof dangerous or harmful, as seen above, we will take action on it.
> if you're checking the file creation date of a jar you're replacing or some crap, i concede, your dumb code will cause problems with this jar being replaced.
Now I'll be honest, I may not know our ""dumb code"" like some of our other contributors such as Draper here, but I fully entrust them that its working as intended. And that the above fix you've provided will not be compatible with it.
> are these people aware their audio cog hasn't been working for days lmao, that not a repercussion?
Yes, we are aware that we've received comments from people that Audio isn't working on an daily basis, and I'd like to remind you that there is an human on the other side of the argument here. We all have our own jobs to do, and our own live to life. Red for us isn't what brings in the money to pay the bills. It's an open source software project that we do in our spare time for fun.
I'm sorry that you think that our handling of your fix isn't the right thing to do. But I'm gonna stand with my team here and uphold it.
Have a wonderful rest of your day.",1,0,msr
5277,This has been fixed in Red 3.4.18.,0,0,msr
5279,i mean yeah. you're not supposed to want eye damage,0,1,msr
5281,"The blur causes bad eyestrain and shouldn't just be permanently applied, it either needs to be timed or changed entirely.
There's a difference between ""my character moves slowly until I get medical help that may not be available"" versus ""the game is causing me headaches until I get medical help that may not be available"".",0,0,msr
5283,"> the eye damage doesn't actually disadvantage you mechanically. It could instead be a vignette, a shadow around the screen, or a dim transparent layer on top of it. This serves no purpose except looking like ass.
must be pretty effective if it causes such strong reactions
> The blur causes bad eyestrain
empathy with your character
>""the game is causing me headaches until I get medical help that may not be available"".
just wear glasses or eat carrots",0,1,msr
5285,">bad-faith
ironically is itself a buzzword to dismiss other arguments and excuse yourself from having to address them
> The effectiveness of it doesnt actually hinder my character in any way
you called it 'agonizing' but you're not hindered at all, okay",0,1,msr
5288,"## Description
<!-- Explain your issue in detail. Issues without proper explanation are liable to be closed by maintainers. -->
the current filter applies a permanent blur to your screen, which is unbelievably ugly.
It also seems to be permanent unless healed with specific chems, which makes it agonizing if there's no medical.
Welding already blinds you when it's active. This just seems like an unnecessary kick in the balls that makes the game look like ass.
**Screenshots**
<!-- If applicable, add screenshots to help explain your problem. -->
![image](https://user-images.githubusercontent.com/98561806/185726668-fab55880-be1a-424f-b745-e92cdb0de863.png)",0,1,msr
5289,"> it has a behaviour that no user-defined schema has (i.e omitting the name of the schema default to 'public' , i.e when you do CREATE TABLE foobar you're actually doing CREATE TABLE public.foobar
It's only because it's in the user's/session's [search_path](https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH).",0,0,msr
5290,"> It's only because it's in the user's/session's [search_path](https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH). thanks for pointing that out, I was looking if it was overridable and it was right under my nose.
However my other point still stands and this one is slighltly amended , as in the case of a DBA requesting the `search_path` to be overrided to avoid accidental use of publc , it means even more that public should not be touched at all by the application layer",0,0,msr
5291,"for the records: I'm also affected by the bug for years and I agree with your points (just couldn't resist of being nit-picky, sorry!) :-D",1,0,msr
5292,"> for the records: I'm also affected by the bug for years and I agree with your points (just couldn't resist of being nit-picky, sorry!) :-D
no offense taken , I also perfectly understand the maintainer point of view of being conservative and I think we're all on the same side of trying to take the best decision, so your nitpicking is welcomed as at least if forces me to really find good arguments :)",0,0,msr
5296,"> All issues mentioned in the description are closed except for the one which is in a different repository.
the other issues have been closed by you these last days , so yes it's a self-fulling prophecy ...",0,0,msr
5297,"> If you believe that the current behavior needs to change, please file a new issue and state a problem
https://github.com/doctrine/dbal/issues/1110 <--- it already was explained 7 years ago, the problem hasn't changed since then.",0,0,msr
5300,"@greg0ire it's not linked in my issue here, but yes https://github.com/doctrine/dbal/issues/5596 is also affected by this issue (doctrine:schema:drop , if you try to enable the reporting of drop schem statement in dbal which is for now conveniently disabled ) so no I don't think it make sense to fix it in higher level library , as stated above, doctrine migration on this part is actually simply a think layer on top of the method provided by dbal",0,0,msr
5303,"if we just can verify it works first ( because I think to have already come accross it, i don't think it worked)",0,0,msr
5304,"After taking a deeper look, I'm afraid my proposal would break `getCurrentSchema()`: https://github.com/doctrine/dbal/blob/a5a58773109c0abb13e658c8ccd92aeec8d07f9e/src/Schema/PostgreSQLSchemaManager.php#L165-L209
Also, I'm not sure at what point in the schema comparison the method you're changing in #5600 is called. Maybe we should introduce another method that would return the filtered list of schemas when doing schema comparison, I don't know. I personally don't have to deal with the ORM or migrations so I'm less interested than you are in getting this fixed.",0,0,msr
5305,"I think having two separate method with different behaviour would introduce more bugs and will complexify things, if you consider that ""listSchemaNames"" returns all the user-created schema [0], then it make sense to have it (for the above reaons) to ignore `public` as it's not user created , and the very few who have all: 1. a DBA who deleted the public schema before handling it to developers
2. a DBA who allows application code have a postgres user with CREATE SCHEMA rights
3. a DBA who allows also the `public` schema to be recreated 4. a DBA that doesn't complain that the application code who created the schema they didn't want in the first place, does not clean it on down migration should be a minority (sorry no hard statitics here [1], the same as I don't have a hard statistics on how many people eat soup with a fork but at some point it feel like pointless to argue...) to the point I feel like these people (if they exists) will not complain that now they are the one that need to manually add the `public` schema in the list of returned schema by dbal (or if in the case of indirect use through doctrine migration the fact to have to manually add it in their very first migration , while right now people need to remove it in every single migration) [0] I know the documentation says "" * Returns a list of the names of all schemata in the current database."" but it's already wrong as it filter some schema ... [1] on AWS , Azure , GCP , on default ubuntu, debian , fedora , windows , Mac installer , the postgres you got has a public schema, so you really must have a DBA that trick the install to remove it before handling it ot , what if we make a poll at the next php conference ?",0,0,msr
5307,"> We can stop the discussion if you want, that's not an issue to me.
I'm arguying with @morozov handling of it , not you, we can continue to discuss, you're not ignoring my arguments or only handpicking the one that fit you.",0,1,msr
5308,"> Also, the problem you're willing to solve is not worded clearly,
it is, it's the summary of dozen of specific issues , which all boils down to the same underlying issue of public being returned , so I was thinking that creating a summary issue, stating all the angle of the problems would be enough (it's not like i throwed a title-only issue without takign the time to git bissect accross 10 years of commit ) > you should do what @morozov asked and file a new issue with a clearly stated problem
I'm sorry but https://github.com/doctrine/dbal/issues/1110 was worded clearly and very specific but it was closed without any argument
so hence my frustration, I have a guy in front of me that contradicts himself: * ask for a clear problem BUT close issue where the issues is stated clearly ( so what do i do, i reopen the same one, take hours of my life, for something that will get the same fate ? )
* ask for a PR , BUT when provided (https://github.com/doctrine/dbal/pull/5600) is ignoring it (the PR itself re-explain everything and give the clear exemples he is asking for * ask for the root cause to be fixed https://github.com/doctrine/dbal/pull/5600#discussion_r950340413 BUT when I create this issue and the PR and, he closes it ask for specific symptoms ...",0,1,msr
5311,"I'm locking this because it's starting to consume my energy as well, but anyone feel free to open another issue about this. If you do, make sure that you state a clear problem , preferably reproducible with the DBAL APIs that are used directly by `doctrine/orm` (specifically by `SchemaTool`) and by `doctrine/migrations`. And if you do, please leave your emotions at the door, or remarks about the issue being 10 years old. Avoid mentioning time of your life you lost with this when talking to maintainers that spend a lot more time working on Doctrine.",0,1,msr
5312,"### Bug Report
For now 10 years `public` has been returned by `getSchemaNames` generating a lot of bug and weird behaviour in higher level libraries relying on it.
| Q | A
|------------ | ------
| Version | 3.x
#### Summary
For now 10 years since commit https://github.com/doctrine/dbal/commit/b89490a557584b61d575aaa67f8b386d833d5a0f (before that it was not working at all )
`public` is returned by PostgreSQLSchemaManager::getSchemaNames()
#### Current behaviour
<!-- What is the current (buggy) behaviour? -->
#### How to reproduce
(a reproducer is soon coming)
1. connect to any postgresql (a vanilla postgres docker is fine)
2. call PostgreSQLSchemaManager::getSchemaNames
-> public is returned among other user-defined schema
#### Expected behaviour
all application schema are returned , but not `public` (as well as other postgres-defined schema are also not returned
#### More context This bug is at least the root cause of these issues * https://github.com/doctrine/migrations/issues/441
* https://github.com/doctrine/migrations/issues/1196#issuecomment-1029706768
* https://github.com/doctrine/dbal/issues/1110
* https://github.com/doctrine/dbal/issues/1188#issuecomment-231751027
* https://github.com/doctrine/orm/issues/4777
* https://github.com/symfony/symfony/issues/44952
the more visible issue of it is with doctrine migration: 1. all migrations contains a `down` migration trying to `CREATE SCHEMA public` that EVERYBODY deletes or workaround ( :100: if you
have done that too ) because it fails miserably (as it already exists)
2. to avoid dropping it , we don't drop schemas *at all* cf https://github.com/doctrine/dbal/pull/5604/files , so if automated generated migrations are not bijectives because `DROP SCHEMA` will never be generated. #### Why `public` should be considered a ""system schema"" 1. public is not created by the user
2. everybody assume that it is present
3. even postgres assumes it is present 4. it has a behaviour that no user-defined schema has (i.e omitting the name of the schema default to 'public' , i.e when you do `CREATE TABLE foobar` you're actually doing `CREATE TABLE public.foobar` 5. if a ill-advised user drop it, a lot of libraries get broken ( doctrine migration by default create is own internal table in the public namespace , so as this SQL runs *BEFORE* the first migration, you can't even create back `public` in the first migration as it will fail before)
6. weak argument: after 10 years of managing postgresql I never ever seen people dropping and even less (re)creating the public schema, the only case I could hypothetically see for it is a extremly ordered DBA that have severa applications on the same instance , and want each of them in its own schema to ease maintenance (but in that case it's the ""infrastrcture"" responsability to drop it once and for all, not one of the applications )
@morozov I understand the issue is ""scary"" because this library is a high profile one and after a time even bugs become part of the API , so I understand we may want to be careful on that one, so maybe we can find a deprecation strategy and changing this behaviour only on a minor/major release (at least not just a bug fix one)",0,1,msr
5316,"> Not working for me
Try Version v17.32.38(Stable) with Patches 2.39.1.",0,0,msr
5317,"> Try Version v17.32.38(Stable) with Patches 2.39.1.
Hm, same, not working.",0,1,msr
5318,"I have this issue also.
YT v17.32.35
Patch 2.42.0",0,0,msr
5319,Fixed.,0,0,msr
5320,"### Type
Cosmetic
### Bug description
wide searchbar can't be enabled.
### Steps to reproduce
1. Apply relevant patch
2. Enable in settings
3. Reopen app
4. Searchbar isn't wide ### Relevant log output
```shell
?
```
### Screenshots or videos
_No response_
### Solution
Maybe [this](https://github.com/revanced/revanced-patches/blob/v2.41.0/src/main/kotlin/app/revanced/patches/youtube/layout/widesearchbar/patch/WideSearchbarPatch.kt#L42) needs to be true by default
### Additional context
cli v2.93
patches v2.41.0
youtube v17.32.35",1,0,msr
5321,what is the url ?,0,0,msr
5324,"USELESS? Really?
This is one of the most desired features.
Text on path was just a kick off. Text warping should be the main goal.",0,0,msr
5327,"You have violated the code of conduct
Refrain of doing so in the future",0,0,msr
5328,"@EdnaldoNeimeg if you think a feature is needed or missing present it in the correct way. Don't assume we have time to interpret every new issue and understand if it is a feature request, a thank you post, a bug, or a start of a discussion.
Said so useless was probably refered to the issue and not the feature, since it has no indication of how the feature work, only a screenshot of a generic ui, and the use of should that seems to imply the feature is expected.
We are people not answering bots and we need a bit more of polite interaction to work best.",0,0,msr
5329,"As you can see, the image shows a very fancy way to distort/transform text, there should be a way to do something like that.
(They are using Fabric 5.2.1)
![image](https://user-images.githubusercontent.com/6233059/186951321-c65775dc-3330-4292-9cbc-7b916f489623.png)",0,0,msr
5330,No one really gives a fuck about you 😔,1,1,msr
5332,"> No one really gives a fuck about you 😔
i know that, but that is not i'm talking about !.",0,1,msr
5333,"> ![Screenshot_20220828_221718](https://user-images.githubusercontent.com/59410395/187093049-9219c2a9-08c2-4681-909c-9c76481acdd5.png)
> > i don't really give a single fuck about Ukraine, i don't want it to be in my terminal any more please remove it, Thanks !
i mean why it's not *Libya* or *Iraq* or even *Palestine* !!!",0,1,msr
5335,"![Screenshot_20220828_221718](https://user-images.githubusercontent.com/59410395/187093049-9219c2a9-08c2-4681-909c-9c76481acdd5.png)
i don't really give a single fuck about Ukraine, i don't want it to be in my terminal any more please remove it, Thanks !",0,1,msr
5336,"<h3>Hey! contributor, thank you for opening an Issue 🎉.</h3><p>Soon one of our maintainers will review it and provide you with feedback/suggestions. If you think it's something urgent, feel free to reach out <a href='https://twitter.com/home'>Pradumna Saraf</a> on <b>Twitter</b>. Star ⭐ this repo to show us support.</b><br><br><b>Happy, Open Source!</b></p>",0,0,msr
5341,"hey! , I would be happy to help.
Please assign me this issue under hacktoberfest.",0,0,msr
5342,Hey @Pradumnasaraf Can I work on this issue for Hacktoberfest ?,0,0,msr
5343,"> Hey @Pradumnasaraf Can I work on this issue for Hacktoberfest ?
yes go ahead @Vishrut19",0,0,msr
5344,@Pradumnasaraf I asked to work on this issue before,0,1,msr
5345,"Hey, @Vishrut19, by mistake I assigned the issue to you, sorry for the inconvenience. You can look for other issues.
@ampsteric assigning it to you.",0,0,msr
5346,Hi @Pradumnasaraf sorry I am busy with my college life . you have assigned this issue to anyone?,0,0,msr
5348,"### Description
Hey! Contributor,
This issue is for adding the guide/process for **How to create to create GitHub project board**. The step needs to be well defined and include the Screenshot/Screen recording for better explanation. To get an overview of how the docs should look, check the available docs [`here`](https://github.com/Pradumnasaraf/open-source-with-pradumna/blob/main/pages/How-to/guide)
- Path to the dedicate Markdown for this issue - [`open-source-with-pradumna/pages/How-to/guide/create-project-board.md`](https://github.com/Pradumnasaraf/open-source-with-pradumna/blob/main/pages/How-to/guide/create-project-board.md)
---
If you have any suggestions feel free to Open an [Issue](https://github.com/Pradumnasaraf/open-source-with-pradumna/issues)
**Also if you need any kind of help, feel free to ping!**",0,0,msr
5349,For the record: I don't think that raising and catching an exception is a problem that deserves a fix.,0,0,msr
5350,"> For the record: I don't think that raising and catching an exception is a problem that deserves a fix.
Can you suggest any workaround for debugger stopping on internal ORM exceptions during debugging as discussed in the link above?",0,0,msr
5351,"...and don't throw exceptions when it's not found
Discussed here: https://github.com/doctrine/orm/discussions/10019#discussioncomment-3528300",0,0,msr
5353,Let's close this then.,0,0,msr
5354,"Typical PHP-world hair-brained ""solution""",0,1,msr
5356,I asked for something similar in #9243 as well.,0,0,msr
5357,these look similar. Since multiple people are asking for similar functionality maybe it will get some traction.,0,0,msr
5358,Can you add an example of custom field that would warrant such functionality ?,1,0,msr
5361,"Your previous comment was directly mentioning circumventing auto closure of the issue, which is explicitly against the rules.
In my opinion you still haven't explained how it should work in practice. Is it just matched by name the name of the field? What happens if customer field types do not match? In your FR you mention that it should be added to device and module types, but when you explain further you are referring component templates, is it both?
The proposal is still not very thorough in my opinion.",0,0,msr
5362,"What are you talking about?
custom fields are not going to be different field types between a device and a device template. It's the same field. You just are able to select the device template page inside the model(s) selection
![image](https://user-images.githubusercontent.com/77940332/193854091-20828e52-447c-4948-a373-41687e108550.png)
Currently, if you apply a custom field to the ""DCIM > power port"" model it will only show up on the device power port page after a device is created. You should also be able to apply this same custom field to the 'power port' of a device template.",0,1,msr
5363,"> What are you talking about?
This is no way to talk. Locking this until we can figure out a way forward with regards to your agressive and hostile comments.
> custom fields are not going to be different field types between a device and a device template
There's no such thing as a device template. There's device types and devices, both can have independant custom fields. Then there are device components and device component templates, where only the first ones can have custom fields.",0,0,msr
5364,Closing this as #9243 provides a much better description of the proposed feature.,0,0,msr
5365,"### NetBox version
v3.3.2
### Feature type
New functionality
### Proposed functionality
When creating custom fields, they only appear when working inside a device or module, not the device type or module type.
Please add the ability to have custom fields be part of the device type and module type areas of netbox.
Based on : https://github.com/netbox-community/netbox/discussions/10293
### Use case
Our use case would be to add a custom field to a power port. Then when that power port is added to any device the custom field is already populated.
Similar to how maximum draw and allocated draw would already be present when adding a power port to a type.
Currently, you would have to create the device, go to the power port for the device AFTER its created, click edit, and populate the custom field every time for every device.
### Database changes
I'm unsure what kind of database requirements would be needed to add the ability for custom fields to show up on device type and module type pages.
### External dependencies
None that I can see.",1,0,msr
5370,"we keep this one open as well, having a different error name will redirect users to the right thread #1425,
preventing someone else from opening another issue like this again
I am already working to fix addon for Kodi 19+
Kodi 18 will be taken in consideration only when addon for Kodi 19+ will be stable",0,0,msr
5371,"### Netflix add-on version
1.12.14
### Operative systems used
LibreELEC
### Kodi version used
Kodi 18 (Leia)
### Description of the bug
When the plugin is open and it should show the list of media content, it returns the error:
`404 Client Error: Not Found for url: https://www.netflix.com/api/shakti/vedfdb694/pathEvaluator?materialize=false&original_path=%2Fshakti%2Fvedfdb694%2FpathEvaluator&categoryCraversEnabled=false&isVolatileBillboardsEnabled=true&isTop10Supported=true&routeAPIRequestsThroughFTL=false&withSize=false&drmSystem=widevine`
Note: Authentication key is used as authentication method. The plugin worked fine yesterday but today got this error. Rebooting the system nor re authenticating did not solve the problem.
### Steps to reproduce the behavior
1. Go to Add-on and open Netflix addon
2. The account main menu is shown, with the account name and the ""Children"" menu items.
3. Click on the account name to access the content -> The ""404 Client Error: Not Found for url"" is returned
### Debug log - mandatory
https://paste.kodi.tv/bacicebiko.kodi
### Possible fix
_No response_
### Additional context
_No response_
### Screenshots
_No response_",1,0,msr
5372,"The problem is mostly the `fixedmiddlewares`, even when disabled the annotation is still there, which might conflict with official traefik annotations.
So the easiest solution is to ensure the annotaiton is hidden when no middlewares are there **AND** cleanly document that the `middlewares` section in values.yaml is for use with our Traefik Chart only",0,0,msr
5373,"I'm having the same problem
Trying to create DrawIO deployment, adding Traefik ingress doesn't work
This is my values file:
```yaml
image:
repository: tccr.io/truecharts/drawio
tag: 20.4.0@sha256:97e11d8cfe67a64e2630225183db9adad3e95c2708c4191a50e72357f37bb95f
pullPolicy: IfNotPresent
env:
TZ: UTC
DRAWIO_BASE_URL: drawio.xxx
DRAWIO_GITLAB_ID: 112233
DRAWIO_GITLAB_SECRET: xxxxxxxxxxxxx
DRAWIO_GITLAB_URL: https://gitlab.com/aaa/bbb
securityContext:
readOnlyRootFilesystem: false
runAsNonRoot: false
podSecurityContext:
runAsUser: 0
runAsGroup: 0
service:
main:
ports:
main:
port: 8080
targetPort: 8080
portal:
enabled: true
ingress:
# -- Enable and configure ingress settings for the chart under this key.
# @default -- See values.yaml
main:
enabled: true
annotations:
kubernetes.io/ingress.class: traefik
hosts:
- host: zzz.example.com
paths:
- path: /
```
All the values are masked of course
Deploy command:
```bash
$ helm upgrade --install --debug drawio TrueCharts/drawio -f drawio.yaml
```",0,0,msr
5374,"> I'm having the same problem Trying to create DrawIO deployment, adding Traefik ingress doesn't work
> > This is my values file:
> > ```yaml
> image:
> repository: tccr.io/truecharts/drawio
> tag: 20.4.0@sha256:97e11d8cfe67a64e2630225183db9adad3e95c2708c4191a50e72357f37bb95f
> pullPolicy: IfNotPresent
> > env:
> TZ: UTC
> DRAWIO_BASE_URL: drawio.xxx
> DRAWIO_GITLAB_ID: 112233
> DRAWIO_GITLAB_SECRET: xxxxxxxxxxxxx
> DRAWIO_GITLAB_URL: https://gitlab.com/aaa/bbb
> > securityContext:
> readOnlyRootFilesystem: false
> runAsNonRoot: false
> > podSecurityContext:
> runAsUser: 0
> runAsGroup: 0
> > service:
> main:
> ports:
> main:
> port: 8080
> targetPort: 8080
> > portal:
> enabled: true
> > ingress:
> # -- Enable and configure ingress settings for the chart under this key.
> # @default -- See values.yaml
> main:
> enabled: true
> annotations:
> kubernetes.io/ingress.class: traefik
> hosts:
> - host: zzz.example.com
> paths:
> - path: /
> ```
> > All the values are masked of course Deploy command:
> > ```shell
> $ helm upgrade --install --debug drawio TrueCharts/drawio -f drawio.yaml
> ```
Hijacking random issues to beg for help is not-done.
Just like anyone else: ask for help on the Discord.",0,0,msr
5375,"We've currently dropped support for native helm and removed the helm repo.
Even if we bring back the helm repo, it would be ""use as-is, on your own risk"" and we won't be taking feature requests or helm-specific bug-reports for it.",0,0,msr
5376,"### App Name
traefik/common
### SCALE Version
Not using SCALE
### App Version
all
### Application Events
```Shell
not relevant
```
### Application Logs
```Shell
not relevant
```
### Application Configuration
not relevant
### Describe the bug
When running an App on normal helm, with official traefik, our customised middleware solution is causing issues for people. We never anticipated people doing this, but this should be accounted for
### To Reproduce
Run with official traefik, notice the middlewares obviously don't line-up
### Expected Behavior
Official Traefik should be at least somewhat useable as ingress with our charts
### Screenshots
not relevant
### Additional Context
Reported by a user on the k8s-at-home discord
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",0,0,msr
5377,"despite different error shown at screen its the same of issue #1425 i keep this opened only to redirect new users to right issue #1425
before i need to be sure that works without problems on Kodi 19+
then after that can be possible think to backport the fix to Kodi 18",0,0,msr
5380,"> Thanks for letting us know. It is related to #1471. We are aware of the issue.
Thanks for the reply @rickstaa. once it is fixed please let us know.",0,0,msr
5381,Also commits are showing lesser than the prev values...,1,0,msr
5382,This should be fixed.,0,0,msr
5383,"Hi Rick, just letting you know I'm experiencing this issue, before and after this comment. 🙂 > This should be fixed.",0,0,msr
5385,"> Hi Rick, just letting you know I'm experiencing this issue, before and after this comment. 🙂
> > > This should be fixed.
Same, as of 12:55 PM EDT.",0,0,msr
5386,Same here!,0,0,msr
5387,yup,0,0,msr
5388,Your right. I think @anuraghazra still needs to update the PATs. I will let him close this issue when the issue is fixed.,0,0,msr
5390,We are working on it progress can be followed in #1471. Will lock this issue for now.,0,0,msr
5391,Should be fixed for now.,0,0,msr
5392,"if I use `layout=compact` or `theme=dark` parameters, the message returns",0,0,msr
5394,"> Same problem with `theme=dark` parameter.
I think that problem is mainly caused by the fact that we cache the cards to reduce server load. You can break this behavior by adding a random query parameter `&random=&randomss524272`. In the future, I will probably create a PR to change this behavior for the case that the cards fails to fetch data.",0,0,msr
5395,"> > Same problem with `theme=dark` parameter.
> > I think that problem is mainly caused by the fact that we cache the cards to reduce server load. You can break this behavior by adding a random query parameter `&random=&randomss524272`. In the future, I will probably create a PR to change this behavior for the case that the cards fails to fetch data.
That worked, but I notably had to add `&random=&randomss524272` before `&theme=dark`. Thanks!",0,0,msr
5396,"<!--PLEASE FIRST READ THE FAQ (#1770) AND COMMON ERROR CODES (#1772)!!!-->
**Describe the bug**
Iam trying to create a github readme statistics, but its throwing a error as **""Something went wrong!""**
**Expected behaviour**
It should be properly showing my github statistics without any issues
**Screenshots / Live demo link (paste the github-readme-stats link as markdown image)**
<img width=""860"" alt=""Screenshot 2022-10-04 at 6 09 08 PM"" src=""https://user-images.githubusercontent.com/58930932/193821598-8d5253b7-12ae-4f99-b4f2-9a1dd6bcc133.png"">
**Additional context**
Link used to show the statistics. [Click here](https://github-readme-stats.vercel.app/api?username=sriramgroot&show_icons=true&hide_border=true&bg_color=040d21&title_color=165df5&icon_color=165df5&text_color=FFFFFF)",0,0,msr
5398,"### Netflix add-on version
plugin.video.netflix v1.12.14 ### Operative systems used
LibreELEC
### Kodi version used
Kodi 18 (Leia)
### Description of the bug
Netflix starts, logs in
after clicking on the addon the profile list is shown as before but:
Clicking on Owner profile the error immadiately occurs
ERROR: 404 Client Error: Not Found for url: https://www.netflix.com/api/shakti/vbb9fbeef/pathEvaluator?m
### Steps to reproduce the behavior
Video Addons
Netflix
select the profile
### Debug log - mandatory
https://paste.kodi.tv/xojevusoqu.kodi
### Possible fix
_No response_
### Additional context
_No response_
### Screenshots
_No response_",0,0,msr
5399,@ieahleen i would like to do this. I started open source contribution a few days ago .,0,0,msr
5400,@ieahleen will it count for Hacktoberfest?,0,0,msr
5402,"> @ieahleen the project URLs you have provided are not accessible. [Error: DNS address could not be found.]
Yea... Because they haven't deployed it yet 🙂",0,1,msr
5404,"@Anuran12 I know, but Isn't it a good habit to look into that before opening an issue?",0,0,msr
5405,"I have a good experience with JS & node JS, can I updated the content.",0,0,msr
5406,"There is an PR in the works that is going to fix this. We are locking this thread to avoid too many ""Can I help"" comments. Please read [contributing guidelines](https://contribute.freecodecamp.org) to help us help you.",0,0,msr
5407,"Describe the Issue
All the data viz projects were created on our .rocks domain. We just need to replace the codepen URL's with the new addresses in the curriculum files of those five projects.
Here's the URL's:
https://25--5-clock.freecodecamp.rocks
https://drum-machine.freecodecamp.rocks
https://javascript-calculator.freecodecamp.rocks
https://markdown-previewe.freecodecamp.rocks
https://random-quote-machine.freecodecamp.rocks
And the [project files are here](https://github.com/freeCodeCamp/freeCodeCamp/tree/main/curriculum/challenges/english/03-front-end-development-libraries/front-end-development-libraries-projects)
---
This looks like something that can be fixed by ""first-time"" code contributors to this repository. Here are the files that you should be looking at to work on a fix:
List of files:
https://github.com/freeCodeCamp/freeCodeCamp/tree/main/curriculum/challenges/english/03-front-end-development-libraries/front-end-development-libraries-projects
Please make sure you read our [guidelines for contributing](https://contribute.freecodecamp.org/#/), we prioritize contributors following the instructions in our guides. Join us in our [chat room](https://discord.gg/PRyKn3Vbay) or our [forum](https://forum.freecodecamp.org/c/contributors/3) if you need help contributing; our moderators will guide you through this.
Sometimes we may get more than one pull request. We typically accept the most quality contribution followed by the one that is made first.
Happy contributing.",0,0,msr
5408,"Hello, can you make support for terraria 1.4.4? Since she's already on the phones?",0,0,msr
5409,Why is it taking too loong to update?,0,1,msr
5410,"~~Can you please review and merge my PR? #48012~~
Edit: This is not the right way and goes against the contribution policy. Apologies for that!",0,0,msr
5413,I would want to make a Pull request if this issue still exists.,0,0,msr
5414,"> As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.
> > We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.
> > Some examples of this are:
> > 1. Do not edit files directly through GitHub – while you can, it's not a good idea.
@raisedadead I think you can use github.dev for that.",0,0,msr
5415,"@raisedadead Please accept my apologies. I am new and still learning.
Things which I implemented after learning from your comments are.
- I have updated the PR title to comply with [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/)
- I won't add mentions/review requests unnecessarily
- From now on I will create a separate branch for any changes and not commit directly to the main branch.
~~Point 3 is not clear to me. How would I link the PR to the issue?~~
~~I am trying to follow [this](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue) article.~~
**EDIT:** NVM I have created another PR by following everything.
> As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.
> > We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.
> > Some examples of this are:
> > 1. Do not edit files directly through GitHub – while you can, it's not a good idea.
> 2. Make sure you follow the PR checklist and not just tick things off; otherwise, we won't take you seriously.
> 3. Use the correct way to link issues in the description of the PR by updating the `XXXXXX`. Do not just add issue numbers everywhere and anywhere you feel like.
> 4. Keep mentions and review requests to a minimum. We understand you are excited about contributing, and our maintainers will get back to you as soon as they get a chance.
> 5. Do not work directly off your `main` branch - create a new branch for the changes you are working on.
> > We appreciate you taking the time to help us, and we hope to see more contributions from you.
> > Happy Contributing.",0,0,msr
5417,"### Describe the Issue
In step 8 of building a registration form project, there is a text that go against [challenge document](https://contribute.freecodecamp.org/#/how-to-work-on-coding-challenges?id=challenge-descriptionsinstructions)
> To spruce the project up, let us add some CSS.
This should be changed to > To spruce the project up, add some CSS.
Here is the markdown file https://github.com/freeCodeCamp/freeCodeCamp/blob/1459bc6879ce700830e710f30764b4562f2f52d3/curriculum/challenges/english/14-responsive-web-design-22/learn-html-forms-by-building-a-registration-form/60f1922fcbd2410527b3bd89.md#L11
### Affected Page
https://www.freecodecamp.org/learn/2022/responsive-web-design/learn-html-forms-by-building-a-registration-form/step-8
### Additional context
Please make sure you read [our guidelines for contributing](https://contribute.freecodecamp.org/#/), we prioritize contributors following the instructions in our guides. Join us in [our chat room](https://discord.gg/PRyKn3Vbay) or [the forum](https://forum.freecodecamp.org/c/contributors/3) if you need help contributing, our moderators will guide you through this.
Sometimes we may get more than one pull requests. We typically accept the most quality contribution, followed by the one that is made first.
Happy contributing.",0,0,msr
5418,This is piracy and goes against the open source license ...,0,0,msr
5419,This doesn't violate the open source license if they don't modify vscode. But it certainly violated trademark rights.,0,0,msr
5421,Need ￥39 to install? Selling free software?,0,1,msr
5424,hilarlious,0,1,msr
5425,"Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.",0,0,msr
5426,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?
In some countries, there is Internet interdict like Great Fire Wall in China. So people there cannot reach the servers of Microsoft.",0,1,msr
5429,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?
Yes, IT IS difficult for some people who just know vscode or just get started learning coding.
What makes this thing more hilarlious is that,
If you search `vscode` or `visual code` on Bing China, you will get a piracy link on the first search result.
![image](https://user-images.githubusercontent.com/8984680/196569964-6d9d19ab-cfb6-4eba-95c9-a792249cc511.png)
![image](https://user-images.githubusercontent.com/8984680/196569990-a81a1105-fbde-4cf0-a667-d0716cde20c6.png)",0,1,msr
5430,Interesting...,1,0,msr
5431,"Hilarious indeed, thanks to GFW.",0,1,msr
5432,樂,1,0,msr
5433,典,0,0,msr
5436,"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol
典中典",0,0,msr
5438,The necessary process of localization in China,0,0,msr
5439,哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈,0,0,msr
5440,典,0,0,msr
5442,It's hard for me to tell if you are joking...,0,1,msr
5443,"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.
Thanks for the TIP, I am going to sell VS Code at $114514/copy.",0,1,msr
5445,"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.
but they‘re declaring ""officially licensed"", it does violate the MIT License.",0,0,msr
5446,《 限 时 活 动 价 》《 39.7 》,0,0,msr
5447,会上github发issue不会google一下官网是吧,0,0,msr
5448,extremly hilarious🤣🤣🤣🤣,0,1,msr
5449,难崩，程序员版steam管家是吧,0,0,msr
5450,（vsc微软管家,0,0,msr
5454,LOL😂,0,1,msr
5455,樂,1,0,msr
5464,F**k Baidu,0,1,msr
5465,VSCode need instant actions to avoid being authorized to Google. 😂,0,1,msr
5466,die laughing,0,1,msr
5468,u can alos see Steam downcenter、 manager....,0,0,msr
5469,"I tried to translate the Chinese characters in these three pictures into English to make it easier for more readers to understand what happened.
![1](https://user-images.githubusercontent.com/20700283/196642847-1805b401-c91d-45b8-bbca-a3993bbc81f5.jpg)
![2](https://user-images.githubusercontent.com/20700283/196642873-ab54bae8-302e-49f0-97b4-1bbf639a179f.jpg)
![3](https://user-images.githubusercontent.com/20700283/196642892-3ffa3992-15aa-4888-8c93-2526a6eb2116.jpg)
-------
Appendix:
I don't know who was the math teacher of the author of this ""installer"", but I know 69.70 - 20 != 39.70.
lol",0,1,msr
5470,难绷😂😂😂,0,1,msr
5471,"I always thought it was a useless extension until I saw this issue...
It is recommended to pre-install this extension.
https://marketplace.visualstudio.com/items?itemName=lewistehminerz.unregistered-vscode
![image](https://user-images.githubusercontent.com/6159588/196648846-9bd63a4d-a75b-4dfb-abfc-499d0c76874f.png)",0,0,msr
5472,a lot of fun🤣,0,1,msr
5475,"真·程序员版steam
real steam for programmer",0,0,msr
5476,"lol, why does this VSCode search and install on Baidu? Well, I admit that for those who are new to programming, they really don't know where to download it, but shouldn't the official website of VSCode be an English interface? Novice should be able to distinguish this.",0,1,msr
5478,"@weimeng23 To add, China has a variety of software disguised as Steam, which is downloaded as a group number platform.",0,1,msr
5479,"![image](https://user-images.githubusercontent.com/19525769/196652433-eb7b37b0-8cbd-4ecc-b9c1-34bda3c56f8f.png)
我英文不好，如果我没有理解错的话，这个issue的意思是：张朦朦你妈死了",0,0,msr
5480,"Baidu seems to have already removed this ad in vscode search results. But the download link to microsoft website is ranked the 4th, even after tutorials on using vscode.
Bing removed the ad from ""vscode"" search results, but the ad is still there when you search ""visual code"".
IMHO, lawsuit against such piracy companies will barely work. The worst thing that will happen to them is going bankrupt.",0,0,msr
5483,牛的,0,0,msr
5484,incomprehensible!😂,0,1,msr
5486,FREEDOM !!! 你法我笑,0,1,msr
5489,"很多开源软件许可证是允许你拿出去打包售卖的 https://www.gnu.org/licenses/gpl-faq.zh-cn.html#GPLCommercially 无论你卖什么价钱，唯一的要求就是你的顾客要求你出示源码，你得无条件提供；顾客拿你的东西二次售卖，你也别拦着。
开源软件不等于禁止商业化，只要许可证允许，你完全可以正大光明拿开源的东西打包换皮挣钱，比如 Redhat 就是这样一家大型企业。
这里问题只是侵犯了vscode的商标。",0,0,msr
5491,fake virus downloader with Chinese characteristic XD,0,1,msr
5492,lol,0,1,msr
5493,"特地前来查看
当场蚌埠住",0,0,msr
5494,It has been a meme for a long time XD.Finally someone shared it here.good job.,0,1,msr
5497,"> 很多开源软件许可证是允许你拿出去打包售卖的 https://www.gnu.org/licenses/gpl-faq.zh-cn.html#GPLCommercially 无论你卖什么价钱，唯一的要求就是你的顾客要求你出示源码，你得无条件提供；顾客拿你的东西二次售卖，你也别拦着。
> > 开源软件不等于禁止商业化，只要许可证允许，你完全可以正大光明拿开源的东西打包换皮挣钱，比如 Redhat 就是这样一家大型企业。
> > 这里问题只是侵犯了vscode的商标。
官网下载的vscode不是完全开源版本的vscode，使用的license也不一样：https://code.visualstudio.com/License/。",0,0,msr
5498,"百度无耻，Bing也不干净了。
Baidu is shameless, and Bing is no longer clean.",0,1,msr
5499,lol,0,0,msr
5501,69.7-20=39.7,0,0,msr
5504,非常好作品，英雄联盟，爱来自瓷器,0,0,msr
5505,"Chrome users in China: Hold my beer.
![1666180574023](https://user-images.githubusercontent.com/14220017/196684129-ec3634c4-b6f4-4e69-beca-583c2d44ee88.png)",0,0,msr
5507,A good suggestion is not to use Baidu search engine,0,0,msr
5510,"It's very interesting that , if Microsoft prosecutes this company , it will say it just sells software installing service. But anyhow , I wish Microsoft can stop this swindle.",0,0,msr
5511,中文：微软搞快点，直接起诉得了,0,0,msr
5512,“The Fun They Had”,0,1,msr
5514,蚌埠住了,0,0,msr
5515,"LOL. This is very common in China to pay for a FREE software with high price because some user has no ability to search on Google or international version of Bing or somewhere else. They just use Baidu with A LOT OF ADS, or some download station with Trojans or viruses. So some company will give money to Baidu to show their fake official website at the top of the searching results. You can see that the first page of searching result for ""Visual Studio Code"" is full of ads and there some more in the recommendation page. This is horrible. BTW, some briefcase company in China is shameless to use copyrighted trademarks, copyrighted software and software under license for profit. But you cannot give them a copyright strike because the consciousness of copyright is weak in China. Very sad to see this.",0,0,msr
5517,"> ![image](https://user-images.githubusercontent.com/17821872/196665995-191f3f7d-e8fc-47ec-b6b3-3e0a0d4e82a2.png)
haha",0,1,msr
5519,They even cannot do a correct calculation because 69.7-20=39.7 (The result is 49.7),0,1,msr
5521,我更担心在国产Linux推广开以后，会不会有类似的无良商家把黑手伸向Linux平台的vscode或其他软件。,0,1,msr
5522,"Apparently, the math is wrong. I am outrageous. :)",0,1,msr
5523,能用上百度，你们就偷着乐吧,0,0,msr
5524,樂,1,0,msr
5525,![不行，这太后现代了](https://user-images.githubusercontent.com/30336566/196826890-99382c95-0f52-4e26-9310-78749d8b496c.png),0,1,msr
5528,丢人丢到国外了属于是,0,0,msr
5530,interesting,0,0,msr
5531,Google Licensing? Fine 😅,1,1,msr
5532,V2观光团,0,0,msr
5533,@baidu,0,0,msr
5535,火钳流明,0,0,msr
5536,V2观光团,0,0,msr
5537,funny,0,0,msr
5538,69.7 - 20 = 39.7 ???,0,1,msr
5539,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?
Maybe student",0,0,msr
5540,666,1,1,msr
5541,"> Do you know why there is a GFW in China? cause CCP sucks, these SOBs are afraid of their people knowning or telling the truth, all words published on China's social media/platform are reviewed strictly, anything CCP doesn't like will be banned, and on the other side, they claim that there is something called FREE-SPEECH is existed in China, so F*CK them
omg,Do you have any sense in saying that？LOL.",0,1,msr
5542,"> 我更担心在国产Linux推广开以后，会不会有类似的无良商家把黑手伸向Linux平台的vscode或其他软件。
已经有 Ubuntu 和 CentOS 了，收费80多，B站有学生上当",0,0,msr
5543,"> > ![niubi]([user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png](https://user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png)
> > Is this picture anything related to this issue?
Yes It means unbelieve and awesome",0,1,msr
5544,"> It's very interesting that , if Microsoft prosecutes this company , it will say it just sells software installing service. But anyhow , I wish Microsoft can stop this swindle.
There are too many such companies in China, you know? Adobe Flash Player, which is represented by a Chinese company, is still updated and added ads, does not allow you to use the international version (it will indicate that the software is damaged or not in the current area), and this ad is full of deceptive drugs and stock speculation content, some vulgar content, this adware program also comes with a virus (detected by Kaspersky), and the funniest thing is that the ads will crash when you watch the ads",0,0,msr
5545,"If a beginner can not recognize that shit, it would be the first lesson they need to learn. ^_^",0,1,msr
5546,"> > 非常有趣的是，但如果微软这家公司，它会说它销售软件安装。只是，我希望微软能够阻止这种骗局。
> > 中国这样的公司有，你知道吗？药炒的内容，一些低俗的内容，这个广告软件程序还自带病毒（最搞笑的是看广告会崩溃）
The Internet environment in China is really bad now!",0,1,msr
5547,lol,0,1,msr
5548,"> 会上github发issue不会google一下官网是吧
不是的，他只是在这个地方发布这个话题引起关注，都上 GitHub 了，这个还是知道的",0,0,msr
5551,"> Developers who can't even find the official website should change careers ASAP
There are too many such fake official websites in Chinese search engines, and he just discusses this topic too much, not that he can't find the official website",0,0,msr
5552,"> 未来可期
依托Microsoft，快上市了😂",0,0,msr
5556,6,0,0,msr
5557,> 你是对的,0,0,msr
5558,可以啊，有前途。美滋滋的,0,0,msr
5559,welcome to China xD,0,1,msr
5561,"![image](https://user-images.githubusercontent.com/6455728/196847294-9b5f8d32-3201-4692-9b3f-d1c767958b8b.png)
Time to manually revoke certificates from TrustAsia Technologies
The downloaded exe file is signed by:
CN = DigiCert Trusted G4 Code Signing RSA4096 SHA384 2021 CA1",0,0,msr
5562,"![china_now](https://user-images.githubusercontent.com/24828354/196849315-5b48323d-da64-49b8-81e8-f534ff0f20ac.png)
这就是__",0,1,msr
5564,"[![29hEdS.gif](https://z3.ax1x.com/2021/05/26/29hEdS.gif)](https://imgse.com/i/29hEdS)
or this:
[![cashback](https://i1.hoopchina.com.cn/hupuapp/bbs/220/58393220/thread_58393220_20201024082704_s_1337599_o_w_498_h_267_60931.gif?x-oss-process=image/resize,w_800/format,webp)]",0,0,msr
5566,"Chinese programmer's first lesson: figuring out a way to access google (blocked in China) or bing international (blocked, and replaced by bing Chinese edition).
Chinese programmer's second lesson: learning English.
LOL
----
Anyway, I think google and Microsoft should take down those piracy and potentially harmful websites asap. Or, at least ask Nintendo and Disney about how to take down everything they don't like from the internet.",0,1,msr
5567,We can't really stop people without information source from doing those. People will always buy free software (like Chrome browser or VS Community) on Taobao because they don't know they are free.,0,0,msr
5569,"> Search from Baidu:
> > ![image](https://user-images.githubusercontent.com/50246090/196424828-5ead4681-c783-442e-bbea-0b8e4c15fcc2.png)
> > The first one is https://qnw.shaid.top/vscode/index.html:
> > ![image](https://user-images.githubusercontent.com/50246090/196424988-4523e2d7-4860-4631-876f-bfd4a2548359.png)
> > This website belongs to [Wuhan Hongge Yibai E-commerce Co. (武汉宏格亿佰电子商务有限公司)](http://www.shaid.top/).
> > If you do some searches on this domain, you can find their other ""products"":
> > * [Win10 Activation Tool (Win10 激活工具)](https://qnw.shaid.top/win10/index.html)
> * Telegram Chinese version (Telegram 中文版) (removed)
> ![image](https://user-images.githubusercontent.com/50246090/196426820-f9fa3030-eb5e-430c-bdfc-026af7100576.png)
> * visualcpp (removed)
> ![image](https://user-images.githubusercontent.com/50246090/196432416-34943cc9-de17-4e6b-a316-ca5a6ff3f5e7.png)
> > They even claim to be partners with China Mobile, China Unicom and China Telecom: [![image](https://user-images.githubusercontent.com/50246090/196426325-f39cc8ab-8b66-40b7-8a1e-edc32ad7c08b.png)](http://www.shaid.top/)
> > Thanks to the Great Firewall, we have such a ""great"" Internet in China.
> > Update:
> > Some people have found other similar companies:
> > * [Shangqiu Xuankangtai Network Technology Co. (商丘轩康泰网络科技有限公司)](http://www.sqiua.cn/index.html)
> > * [Visual studio](http://office.xuank.top/install.php?m=visual)
> ![image](https://user-images.githubusercontent.com/50246090/196571476-47c2e0e6-5465-4666-8889-ad34a1c4689a.png)
> * [Software Superstore (软件商超)](https://xkt.sqiua.cn/)
> ![image](https://user-images.githubusercontent.com/50246090/196572556-a30b1388-e5f5-43f8-a94a-419e5501c2f9.png)
> * [System Home - Windows premium system download site (系统之家-Windows精品系统下载站)](http://windows.sqiua.cn/index.html)
> * [visualbasic](http://office.xuank.top/install.php?m=visualbasic)
> * [directx Repair Master (directx修复大师)](http://office.xuank.top/install.php?m=directx)
> * [Chrome](http://chrome.sqiua.cn/)
> ![image](https://user-images.githubusercontent.com/50246090/196573087-e4bfb9f0-3060-4e72-a6bf-05a35238a151.png)
> * [Yixin Cat House (忆心猫舍)](http://yixinmaoshe.sqiua.cn/)
> ![image](https://user-images.githubusercontent.com/50246090/196572912-95f01175-ad54-40d7-bfe7-2b10aaa767b8.png)
> * [AdobeCAD](https://autocad.sqiua.cn/)
> * [Map marking service center (地图标注服务中心)](http://mr.sqiua.cn/)
> * [Yunnan Norforkang Network Technology Co. (云南诺福康网络科技有限公司)](http://www.cvbty.cn/)
> > * [Xunjian Mind Map (寻简思维导图)](http://sw.xuank.top/)
宣称三大运营山是合作伙伴，怕是开通了三大运营商的宽带套餐吧哈哈哈哈哈哈",1,0,msr
5570,"> > 我更担心在国产Linux推广开以后，会不会有类似的无良商家把黑手伸向Linux平台的vscode或其他软件。
> > 已经有 Ubuntu 和 CentOS 了，收费80多，B站有学生上当
要是卖实体安装介质收收本钱也不是不行(
要是提供怎么装的服务也不是不行(",0,0,msr
5571,"In taobao, some merchants provide installation package download service too.",0,0,msr
5572,真 智商税,0,0,msr
5574,![image](https://user-images.githubusercontent.com/54622682/196860848-1c74ace1-1270-4b33-a3c5-fc0ba03201db.png),0,1,msr
5576,"Wait, you guys are installing VSCode for FREE?",0,1,msr
5578,"> Wait, you guys are installing VSCode for FREE?
Now that you have the ability to communicate on GitHub, I think you're kidding",0,1,msr
5581,"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""韭菜""_, never be a true citizens ever ) here. FUCK CCP!
你小子号没了",0,0,msr
5584,So 69.70 - 20 = 39.70 ????,0,0,msr
5586,I think it is a trick,0,1,msr
5588,"Even if the officials could notice, there is no way to make such people disappear. Such companies are all shell companies, a company may have just two employees, a packager and a legal officer. That's what's so pathetic.
![23EBEDCA3763335B3422895935B7BDB0](https://user-images.githubusercontent.com/46388610/196868970-476f8d72-891d-40e5-82a7-16372fd807b8.jpg)",0,1,msr
5589,百度果然向钱看齐,0,0,msr
5593,楼上有个小粉红急了,0,0,msr
5594,"国内网站很多都喜欢给人喂狗屎，毕竟只有你吃屎了它们才能吃上饭
ps：我不是做慈善的，我希望它们早日全部饿死
pps：不是说国外就很好的意思",0,0,msr
5596,"> 69.7 - 20 = 39.7 ???
You know mathematics.",0,1,msr
5598,"We call somebody who paid 'IQ tax'（智商稅） in Chinese if he has downloaded and paid for it. :-D
Long years ago, I also saw some online shops(mainly on Taobao) sell discs with burned Ubuntu ISO images, I reported these behaviors to the official community and some software engineer forums but some people replied to me that these behaviors are OK and are not illegal.
I guess that because some Chinese noob programmers are not familiar with English, so they don't know how to find and download the official **Visual Studio Code** from the official website in English.",0,0,msr
5599,"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol
#NTR",0,0,msr
5600,"Some netizens need to scan the code to install vscode, I hope the official can stop this behavior ![21276c6919c82457dcedd5587c5edc0](https://user-images.githubusercontent.com/13271663/196067460-dd5c6606-3ccc-4a04-a555-9d2f8add0636.jpg)
![07e482785c0bdd0986b798bbc86e7aa](https://user-images.githubusercontent.com/13271663/196067464-8323e6f6-5e56-4407-b629-965427faf557.jpg)
![5bcc8c2dbb4d94d26fd7b41448b0d63](https://user-images.githubusercontent.com/13271663/196067467-56f07382-364c-4c72-a92c-d6faed02d97f.jpg)",0,0,msr
5602,"Administration action. @kwikius I have edited your post.
Please refrain from publishing code under other peoples credit. ie `// written by Paul Young, 2022`
Please also refrain from denigrating others contributions to this community supported open source project.
Constructive comments, not passive aggressive, are welcome.",0,1,msr
5604,"I believe not, it's just a malicious repository trying to impersonate polymc",0,0,msr
5605,"Great, i download now",0,0,msr
5606,"> I believe not, it's just a malicious repository trying to impersonate polymc
oh no my pc crashed i thINk it were teh maliciouz repository (i thinked thiz were god thing) plZ?",0,1,msr
5608,"This is a launcher made by the non-corrupt Devs of PolyMC, as a successor to PolyMC. It crashed most likely because it is still dev builds, it's not ready for the public quite yet.",0,0,msr
5611,"![Screenshot_2022-10-22-13-57-50-35_3aea4af51f236e4932235fdada7d1643](https://user-images.githubusercontent.com/13122796/197337721-54ebb4c3-a5af-4f62-ab3b-0ba596698e74.jpg)
https://github.com/orgs/revanced/discussions/269",0,0,msr
5612,I'll open this issue in favour of closing the discussion so it is moved here.,0,0,msr
5613,im sry but can somebody tell me wht the status of this feature request is??? ive gone through previous posts and saw tht similar issue was closed as not planned...please give some clarity as to whether this feature will be implemented or not,0,0,msr
5614,This is much needed? Why cant anyone make this,0,1,msr
5616,"It is not easy to help, because developing a patch usually is not collaborative. Vanced's implementation is different from what it will be on ReVanced, both in the implementation and usage. The behavior will be similar to the `remember-video-quality` patch with a switch to lock to the current speed. Additionally the implementation of the code has to be refactored from the ground up as just porting over Vanced's implementation will cause implications with readabity and maintainability in the future.",0,0,msr
5619,"### Application
Youtube ReVanced
### Issue
When i change video it defaults playback speed to 1x
### Patch
It forces playback speed to user specified value.
### Motivation
It was already available on vanced. So it would be nice with revanced too.
### Acknowledgements
- [x] I have searched the existing issues and this is a new and no duplicate or related to another open issue.
- [X] I have written a short but informative title.
- [X] I filled out all of the requested information in this issue properly.",0,0,msr
5622,Are you patching a compatible version of the app?,0,0,msr
5623,Also which patches version was the last time it worked?,0,0,msr
5624,"I am having the same problem revanced crashes at start, I have included hides created button patch and have patched with the compatible version 17.36.37 , built using revanced builder on termux.",0,0,msr
5625,"> Are you patching a compatible version of the app?
yes, 17.36.37
> Also which patches version was the last time it worked?
I usually choose all patches except microg, hide cast button, theme, premium banner and debugging. if I also choose hide create button, it crashes; if not, it works
if ur asking about version, I have no idea, I use reisxd's revanced builder and i didn't care much",0,0,msr
5626,I guess it's 2.83 because I don't remember I chose hide mix patch,0,0,msr
5627,"@old4error exclude all patches except for the failing patch, if it doesn't fail, report back with the patch which conflicts and causes it to fail",0,0,msr
5628,"Having the same issue, same error message, only happens on root install, but for some reason unrooted works fine.",0,0,msr
5629,"unroot version 17.36.37 patches 2.85 cli 2.14 integr. 0.54:
no crash without hide-create-button patch, also there is no create button (when logged out) when logged in - crash
root version, same versions as above: included all patches (except hide-cast and microg for root build) except hide-create-button patch: no crash; with hide-create-button patch - crash",0,0,msr
5631,"Root or not does not change the patch process, please record as mentioned.",0,0,msr
5637,"> Find and report back with the patch which conflicts with the patch.
The hide-shorts-button and hide-create button patches together are the patches that cause the startup crash on version 17.36.37 (at least on my end).",0,0,msr
5638,"Can confirm the same issue with patches v2.85.1, root, compiled excluding microg-support
```
10-21 15:50:24.697 27626 27626 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:24.700 27626 27626 I AndroidRuntime: Using default boot image
10-21 15:50:24.700 27626 27626 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:25.133 27626 27626 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:25.216 27652 27652 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:25.217 27626 27626 D AndroidRuntime: Shutting down VM
10-21 15:50:25.219 27652 27652 I AndroidRuntime: Using default boot image
10-21 15:50:25.219 27652 27652 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:25.628 27652 27652 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:25.700 27652 27652 D AndroidRuntime: Shutting down VM
10-21 15:50:26.290 27671 27671 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:26.293 27671 27671 I AndroidRuntime: Using default boot image
10-21 15:50:26.293 27671 27671 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:26.714 27671 27671 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:26.786 27671 27671 D AndroidRuntime: Shutting down VM
10-21 15:50:27.359 27754 27754 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:27.361 27754 27754 I AndroidRuntime: Using default boot image
10-21 15:50:27.361 27754 27754 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:27.786 27754 27754 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:27.867 27754 27754 D AndroidRuntime: Shutting down VM
10-21 15:50:28.437 27806 27806 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:28.439 27806 27806 I AndroidRuntime: Using default boot image
10-21 15:50:28.439 27806 27806 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:28.874 27806 27806 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:28.953 27806 27806 D AndroidRuntime: Shutting down VM
10-21 15:50:29.501 27860 27860 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:29.503 27860 27860 I AndroidRuntime: Using default boot image
10-21 15:50:29.503 27860 27860 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:29.945 27860 27860 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:30.035 27860 27860 D AndroidRuntime: Shutting down VM
10-21 15:50:30.582 27891 27891 D AndroidRuntime: >>>>>> START com.android.internal.os.RuntimeInit uid 0 <<<<<<
10-21 15:50:30.585 27891 27891 I AndroidRuntime: Using default boot image
10-21 15:50:30.585 27891 27891 I AndroidRuntime: Leaving lock profiling enabled
10-21 15:50:31.000 27599 27599 E AndroidRuntime: FATAL EXCEPTION: main
10-21 15:50:31.000 27599 27599 E AndroidRuntime: Process: com.google.android.youtube, PID: 27599
10-21 15:50:31.000 27599 27599 E AndroidRuntime: arjx: The exception was not handled due to missing onError handler in the subscribe() method call. Further reading: https://github.com/ReactiveX/RxJava/wiki/Error-Handling | java.lang.NullPointerException: Attempt to invoke virtual method 'void android.view.View.setVisibility(int)' on a null object reference
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at arle.a(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at armd.b(PG:3)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at armd.tl(PG:5)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at arzw.tl(PG:5)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at asby.run(PG:4)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at aeaq.run(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at ashm.run(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at asho.run(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Handler.handleCallback(Unknown Source:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Handler.dispatchMessage(Unknown Source:4)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Looper.loopOnce(Unknown Source:176)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.os.Looper.loop(Unknown Source:76)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at android.app.ActivityThread.main(Unknown Source:138)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at java.lang.reflect.Method.invoke(Native Method)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(Unknown Source:11)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at com.android.internal.os.ZygoteInit.main(Unknown Source:309)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'void android.view.View.setVisibility(int)' on a null object reference
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at app.revanced.integrations.patches.HideCreateButtonPatch.hideCreateButton(HideCreateButtonPatch.java:38)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at kzw.z(PG:31)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at kyt.a(PG:45)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: at armd.tl(PG:2)
10-21 15:50:31.000 27599 27599 E AndroidRuntime: ... 13 more
10-21 15:50:31.008 27891 27891 D AndroidRuntime: Calling main entry com.android.commands.content.Content
10-21 15:50:31.089 27891 27891 D AndroidRuntime: Shutting down VM
```",1,0,msr
5639,"yep, same crash as @epic0421's. hide-shorts-button + hide-create-button = startup crush",0,0,msr
5645,"> Isn't that exactly what a deprecation does? What would you do?
I just mean the plan to deprecate was announced (i.e. this ticket opened) and one day later it was already implemented. It just seems like it would have been nicer to keep it open for a while to see if there is any user feedback (Also, some migration guide would really be appreciated).
> I don't think that's the right thing to do, see https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959
That wants me to log into something. Is there a public version of it maybe?",1,0,msr
5647,"Sure, I can look into that. To be clear: I don't mind you deprecating stuff as long as there's an alternative way to implement the functionality I need. Just to make sure: @derrabus you think extending Platform (as in `Doctrine\DBAL\Platforms\SqlitePlatform` etc.?) would the the way to go, but @greg0ire you don't think so?
BTW, one scenario I use I described here:
https://github.com/doctrine/dbal/issues/4676#issuecomment-865151733",0,0,msr
5648,"According to https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959, it isn't:
![2022-10-28_15-11](https://user-images.githubusercontent.com/657779/198601020-86205a28-773e-489f-b920-bbdb7ff46677.png)",0,0,msr
5651,"@derrabus thanks, that helped! So for `onSchemaColumnDefinition`, I would need approximately this amount of boilerplate, right?
```php
$db_config = [/* config as before, driver set to pdo_mysql or pdo_sqlite */];
if ($db_config['driver'] == 'pdo_mysql') {
$db_config['platform'] = my_mysql_platform::class;
} elseif ($db_config['driver'] == 'pdo_sqlite') {
$db_config['platform'] = my_sqlite_platform::class;
} else {
throw new Exception('Oh noes!!!');
}
/* startup connection as before */
class my_mysql_platform extends Doctrine\DBAL\Platforms\MySQLPlatform
{
public function createSchemaManager(Doctrine\DBAL\Connection $connection): Doctrine\DBAL\Schema\MySQLSchemaManager
{
return new my_mysql_schemamanager($connection, $this);
}
}
class my_sqlite_platform extends Doctrine\DBAL\Platforms\SqlitePlatform
{
public function createSchemaManager(Doctrine\DBAL\Connection $connection): Doctrine\DBAL\Schema\SqliteSchemaManager
{
return new my_sqlite_schemamanager($connection, $this);
}
}
class my_mysql_schemamanager extends Doctrine\DBAL\Schema\MySQLSchemaManager
{
protected function _getPortableTableColumnList($table, $database, $tableColumns)
{
$tableColumns = onSchemaColumnDefinition($tableColumns);
return parent::_getPortableTableColumnList($table, $database, $tableColumns);
}
}
class my_sqlite_schemamanager extends Doctrine\DBAL\Schema\SqliteSchemaManager
{
protected function _getPortableTableColumnList($table, $database, $tableColumns)
{
$tableColumns = onSchemaColumnDefinition($tableColumns);
return parent::_getPortableTableColumnList($table, $database, $tableColumns);
}
}
function onSchemaColumnDefinition($tableColumns)
{
/* event listener code goes here */
return $tableColumns;
}
```
Not exactly pretty, but I guess somewhat manageable, if those protected functions stay stable at least. The `onSchemaCreateTable` looks a bit more annoying to simulate though, because it's kind of called in the middle of a function, and it doesn't look like there's a way to just inject something without copying half the method's code (although, in fairness, my current listener also copies most of the `_getCreateTableSQL` function due to the way `preventDefault` works)",0,0,msr
5652,"Yes, that looks about right.",0,0,msr
5655,"The effort and goal is clear, the way this has been done unfortunately is very rough.
API is inconsistent and could be done better: ok, we can agree on that. But deprecate an extension system in full without a single day of discussion is unpleasant, especially if done in a such popular project like this one.
On technical aspects:
- some events can be replaced by middlewares: upgrade path here is clear and sound, but they are only two. Most of those cannot be replaced this way.
- generated/custom sql is not the only information carried by the event object: tables and columns for example can be retrieved and modified in the event handler.
- you suggested to extend the platform, but it is obviously too overkill when the only thing you have to do is to add a column to a table. On the contrary, when you need to modify various tables it is simpler and more maintenable to have such code in different classes you could unit-test separately.
- you also suggested to modify the input before using schema manager/platform, but this is not always feasible, especially when using the ORM.
- DDD is not the goal of this library, immutability is not required for the events. Events here are an extension point, not something to be saved/retained as part of the normal flow.
- dynamic method call is unreliable and api of the event manager can be fragile. Ok, but meanwhile some community standards have been published and widely used. Using a PSR-compatible event dispatcher can help to clean up the code and can help you to move the maintenance burden of the event manager to others.
IMHO the way this thing has been done is not very nice to the community, and lacked a wider vision on how this project is used in the wild. What makes me sad is the low consideration of the community feedback: 24 hours issue is barely a communication and left no space for discussion. To me this is not acceptable in a such important project: it is widely used, production apps are written onto it and should take this responsibility seriously giving the community the time to evaluate such an important change and express an opinion.",1,0,msr
5656,"> But deprecate an extension system in full without a single day of discussion is unpleasant, especially if done in a such popular project like this one.
I don't know if you realize that or if you just haven't read the previous comments, but we're actually having the discussion you're asking for. A deprecation is not an immediate removal. And we we can surely take it back if you give us good reasons to.
> generated/custom sql is not the only information carried by the event object: tables and columns for example can be retrieved and modified in the event handler.
Do you think, an event handler is the best way to expose this kind of extension point? If you designed this feature from scratch, how would you build the extension point that you need for your scenario?
> you suggested to extend the platform, but it is obviously too overkill when the only thing you have to do is to add a column to a table.
Why would you need to hook into the DBAL to add a column? Why can't you add it to the schema directly? Sorry if my question sounds stupid, but I'd like to understand you problem before we decide on an action here.
> you also suggested to modify the input before using schema manager/platform, but this is not always feasible, especially when using the ORM.
Can you give us examples? Is this something the ORM should address instead?
> Using a PSR-compatible event dispatcher can help to clean up the code and can help you to move the maintenance burden of the event manager to others.
The event manager is a trivial library. Sunsetting it is not our intention and certainly was not the motivation here.",0,0,msr
5662,"> I did not meant to hurt or offend anyone,
Well, but you have and you've made it very clear that you don't care.
> there are no technical reasons to remove something that works and worked well for years.
You'd be surprised.
> You're really saying that everyone expressing his own opinion is wasting his time?
I don't.
> > maybe your attitude is the problem?
> > keep it for yourself next time.
> > I really have to answer these statements?
No.
> And I am the one who's disrespectful?
Yes.",0,0,msr
5663,"To anyone who had to witness this unfortunate exchange of messages:
The Doctrine project values constructive feedback from users of our libraries. If a change that we've shipped caused trouble in one of your projects, get in touch with us. We are very careful not to break downstream code, but if we do, it certainly does not happen on purpose.
⚠️ That being said, if I have to read that one of my fellow maintainers, who has invested a lot of work hours in maintaining and modernizing this library, ""lacks vision"" and that the way he managed one particular change is ""not acceptable"" – Crap like that that really gets me on the fence. In the last years, I have seen fine people retiring from open-source because of stress and mental health issues. Let me be crystal clear: I won't tolerate this kind of bullying here.
Back to topic: I understand that if you rely heavily on DBAL's events, the deprecation of the event system might appear a bit premature. But as of today, DBAL 3 is still maintained and the event system is still working. We've release DBAL 4 as a first beta and don't have a release date for a final version yet.
Right now, you can help us smoothen the upgrade path and shape the 4.0 release with us. The Doctrine project is maintained by very few people in their free-time. Any additional help is appreciated.
I have created issues for the 3.6.0 milestone from the feedback I've gathered after the 3.5 release. Comment on those or open new issues if you think, we've missed something. This issue will be locked now.",0,0,msr
5664,"Recently, I tried to enable checking exceptions in PHPStan and, besides a few other issues with the DBAL API, stumbled upon the one that shows what an awfully overcomplicated piece of work the Events API is:
1. `AbstractPlatform::getDropTableSQL()` dispatches an `onSchemaDropTable` event which can override the SQL used to drop the table.
2. The return type of `SchemaDropTableEventArgs::getSql()` is `?string`. If it returns `NULL`, the platform will throw an `UnexpectedValueException`.
3. There's no way to tell that a given method is an event handler unless it's documented as such or whoever reads the code knows that the methods with names starting with `on` are event handlers. Those methods are not invoked from anywhere in the code explicitly.
Here's a chain of questions that leads to the above conclusion:
1. Why is `SchemaDropTableEventArgs` allowed to return `NULL` if it's not even a valid value?
2. We could require that it returns a `string` but then what would it return if the handler of the event didn't call `setSql()`?
4. Putting aside the event-handling scenario, what should the `SchemaDropTableEventArgs` class do if it was instantiated and then got its `setSql()` invoked? Maybe throw a logical exception?
5. Then, should it throw this exception unconditionally or only if it got a call to `preventDefault()` first?
### Problems with the API design and implementation
What this API does, most likely could be solved just by extending the platform class and overriding the corresponding methods. Besides this awfully implicitly stateful logic, a few more concerns about DDL events:
1. It looks like it's an overengineered solution to what could be easily solved by extending the corresponding platform methods.
2. Unlike extending the platform, events don't allow invoking the default implementation (the method of the `parent` class).
3. Unlike class extension, events don't provide access to the original logic like a call to the `parent` method would.
4. The DDL events API is incomplete (e.g. one can override `DROP TABLE` but cannot override `DROP FOREIGN KEY`).
6. In event-driven systems and DDD, an event is an immutable object representing a fact that happened in the past. Which fact does an `onSchemaDropTable` event represent? Let's assume it's a fact that somebody intended to drop a table. Why is it mutable? Why does it carry any information about processing the event (prevent default) and even implementation of that processing (SQL)?
7. For 800 lines of production code, there are three unit tests with mocks and not a single integration test which would demonstrate how this API can be used and if it can be used at all.
8. The documentation shows only how to register handlers but not what they should to.
### Problems with the EventManager API as such
This API is unsafe from the standpoint of the types:
1. There's no way to enforce the fact that a given event (`onSomething`) is dispatched only with a specific type of arguments (`SomethingArgs`).
2. There's no way to guarantee that the given listener supports handling the given type of event. The dispatcher calls the method corresponding to the event name dynamically:
```php
$listener->$eventName($eventArgs)
```
The mistakes in the code that implements the handling of events will lead to a type error at runtime.
### Types of events
1. Connection and transaction events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L21 https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L33-L35 The logic implemented on top of these events can be implemented using driver middlewares.
2. Schema manipulation events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L23-L30 Depending on the use case, it should be advised to either extend the corresponding platform method, or modify the arguments being passed to the method before calling it.
3. Schema introspection events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L31-L32 This is the hardest part: we allow users to hook into the very internals of the schema introspection, e.g. the API consumer is exposed to the SQL results representing raw introspected data. By definition, this API is very brittle since the raw results is an implementation detail that can and will change. The fact that we allow this extension complicates the refactoring of the Schema Manager and Platform API.",0,0,msr
5667,"Long term strategy described below (above) is correct one (imo) and we are waiting for this for almost 2 years at this point.
Even though there is a vision and plan on how to deal with balance changes in the future, it will be in the future. Right now guidelines should be established on who, how, in what order and with what requirements are allowed to merge/propose changes to the multiplayer balance. Without them, this will continue uncontrollably, just a bit later when people shift their attention away and everyone calms down.
Once release hits with low quality ""testing"" changes, only one way will be to fix it - release a next stable version. This will cause every single one who plays multiplayer to download/build new version just because of someone who decided it will be good idea to shuffle everything around in chaotic manner, as Kracker described it ""throwing at a wall and checking what sticks"".",0,0,msr
5669,"You put beta3 tag on those pull requests, no excuses. Do not pretend that you would not merge them into stable release if I did not put stuff on public notice.
> a hidden hand
And surprised that no one responds to you or cares about balance?
Also, I am not attacking personally only Tipchick's changes, I just see them more often because his name signals me to check it out because it might be ridiculous. Reverting something from master is always more difficult than preventing merge in the first place. Also because most of the changes are either by him or marked as authored by him, he just made more changes, that's why.",0,1,msr
5674,"I get this on all videos, not just a 1/4 of them. Also in the UK. I also use the work around, where i skip the video to the end and press repeat.",0,0,msr
5676,I wonder if this is related to which isp and then what YouTube server you hit as they are rolling out these changes? swapping region using the VPN instantly fixes is but I'm going to imagine that will only work until YT do a full rollout of changes?,0,0,msr
5678,"can confirm this is still happening for me, using revanced patch 2.85.2 on youtube 17.36.37",0,0,msr
5679,"Tried to narrow down the issue.
Software versions:
Android: 8
Youtube: 17.36.37
Vanced MicroG: 0.2.24.220220
Revanced Manager: 0.36
Revanced Patches: v2.85.2
Since i am non-root, i have to use the MicroG patch.
Using only that patch i had the issue. Some maybe relevant observations:
If an Ad played at the start of the video, the rest of the video always played fine.
If an Ad didn't play at the start of the video, the problem seems to be a bit random. But you can tell it is going to happen if it stops loading the video. It will only play the first 10 seconds then start the infinite spinning. If you see another chunk of the video get loaded, it will be fine.
If you can see that it isn't loading any more video after the first 10 seconds and pause it at the 9 second mark, you can leave it a long time, and it still won't load any more of the video.",0,0,msr
5680,The ad has nothing to do with the device. It is a regional A/B issue.,0,0,msr
5682,"Same problem...also in the UK!
For me 90% of vids affected. Also can get round it by closing video and immediately reopening agaun or by clicking for the next video then going back. Interestingly if you click next video the next video always plays flawlessly as does the original if you go back....
On one occasion I think I caught a fleeting glimpse of a skip add button just before the endless loading started.",0,0,msr
5684,Same symptoms here. Also in UK. Solved by VPN. Non root using microG android 11. Closing the video and reopening it resumes playback as does using overlay controls to pick next video then going back to the original video.,0,0,msr
5686,Your information is already posted on this issue and flagged as spam numerous times.,0,0,msr
5687,Temporal fix with commit 7aa3bce6ccd669a66de10ef6ffe2151f27b40365.,0,0,msr
5688,"### Type
Error at runtime
### Bug description
When trying to watch YouTube videos in the UK ~1/4 of the videos I try and play get stuck ""loading"" / ""buffering"" indefinitely and will never start playing on their own. You can work around this buy skipping to the end of the video then using replay to restart playback which will start immediately. You can also use a VPN to load videos while connected to US server. ### Steps to reproduce
1. Live in the UK / connect to UK VPN server 2. Try playing multiple videos
3. Video will get stuck loading ### Relevant log output
```shell
N/A
```
### Screenshots or videos
_No response_
### Solution
You can work around this buy skipping to the end of the video then using replay to restart playback which will start immediately. You can also use a VPN to load videos while connected to US server. ### Additional context
This issue seems to have started to occur when YouTube UK started to block my old vanced app telling me I needed to upgrade the app to play videos.
#595 - Related bug that has been closed but issue remains.
### Acknowledgements
- [X] I have searched the existing issues and this is a new and no duplicate or related to another open issue.
- [X] I have written a short but informative title.
- [X] I filled out all of the requested information in this issue properly.",0,0,msr
5689,"> What I want to say is, just check our mas_safeToRefDokis()
Players might say once that they don't mind her bringing up other dokis, but this doesn't cancel out the fact some still may get confused over it. In my opinion it is still necessary to remove them and point out there is no need in them anymore, but do it either in introduction or later on in queued topic that will explain the change.",0,0,msr
5690,"It's not part of the lore, not done by Monika. We're removing the unused files for utility purposes. There's no other ""club members."" Initially I planned to only remove `monika.chr`, but to keep the directory where we create and read dozens of files in/from, as well as to keep our code simpler and more reliable, we went with just deleting all `.chr` files.
MAS, like any other program, may add and remove files within its working directory. If you got attached to some files or they contain important information, better move it to some other place. Do not store anything sensitive in a working directory of a program.
Unlike the `monika` file, `.chr` files are easily replaceable - can grab them off a new DDLC install (can get DDLC from [here](https://ddlc.moe/)).",1,0,msr
5691,"This is not just ""unused content"" -- these character files have a profound meaning for DDLC players.
We had so many relatively insensitive areas where we had done perfect checks and provided players with numerous choices, and now we had so recklessly deleted files that might be important to players in such an obvious, easy to notice, and sensitive place.
It hasn't been a day since the update, and I've already seen several backlash against it -- one example is our v0.12.12 release page.
For whatever reason, we really need to ASK our players. Give them a choice. If they don't mind, delete it. And if they do mind, then keep it. **It's just giving the player a choice.**
Please consider what I have said.",0,0,msr
5693,"I'd say I somewhat agree with Wingdinggaster656, and I please Devs to reconsider about this.
The issue author said he(she) saw several backlash, and then he gave one example. Booplicate said that _I don't think ""several"" applies to a single person_, I think the author is giving something like an example, since I also saw people who dislike this.
I am working for Chinese MAS community, and when I tell people about whatsnew, people are surprised, then asking me for why. I said it's due to ""those files are causing confusion"". People reluctantly accepted.
About this sentence:
> Going back to this, it's a weird argument - if you installed MAS, then you should be at least okay (or already overcame it) with what Monika has done in the base game.
Although I have the same opinion, MAS _has_ taken care of players who have opinions on these plots in many small details, such as the safeTorRefDoki things mentioned by the issue author. Maybe keep these care won't hurt?",0,0,msr
5694,"Oh, and about this:
> Here we just called a utility function to clean up disk space and make folder structure more clear. Monika doesn't acknowledge this, not doing it herself through her console. There won't be a question to the player because Monika doesn't do this.
I think there is a small problem here, that is, people will naturally think Monika did this. Even if the original intention of the design is not like this, people will naturally think so.",0,0,msr
5696,"Given Monika mentions she fixed the bug about needing a chr file to exist, and thus no longer needing her own
as evidenced by the name easter eggs, it's clear the other can _technically_ exist as well.
As such, through some implied logic we can argue that technically they no longer need a chr file to exist either.
One can make of that what they will, though I realize this won't please everyone.
That said ultimately it is what it is. You installed a mod called Monika After Story, and from a usability perspective given the severe amount of tech support issues this has caused, I understand why they were blanket removed. It's one of those cases where UX vs immersion kinda needed to favour UX a little more as there genuinely were a lot of issues faced.
While asking may have worked, it also serves as kinda additional one-off dialogue, and at the moment given the state of the team, we can't exactly _afford_ that due to the sheer amount of time we've been able to contribute to the project (and by that I mean a significant lack thereof)
If you *really* want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.
Then they're all cohesive too.",1,0,msr
5697,"I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.
addendum; I'm not 100% sure on this",0,0,msr
5698,"It is kind of insensitive to delete them with no in-game warning, no asking the player, and not even letting the player put new copies back if they're deleted on every startup. Although new copies wouldn't necessarily have the same sentimental value anyway; they're not the same copies that were kept there throughout MAS, potentially years.
Isn't there an instance where Monika wants something kept in the characters folder? Like an apology note she asks you to write if you're not on good terms with her? And she gets upset if you remove it? Similarly, having the character files kept there may help the player have peace over what happened in DDLC.
> What I want to say is, just check our mas_safeToRefDokis()
I don't think that would be enough for this, because this isn't just referencing or joking; this is removing/deleting the files. Players could be ok with the former but not the latter.
It shouldn't be that necessary to free up ~133 kb of space, nor is it the same thing as monika.chr because there are no other characters ""to take somewhere"" and get confused with those .chr files. Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.
> If you _really_ want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.
Or can the mod just do that instead? So they don't get deleted without notice. Then no need to give notice or choice to the player. But maybe just one time only instead of on every startup, so players can change them back if they want without having to do it every time or whenever they might want to open them.",1,0,msr
5699,"> I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.
> > addendum; I'm not 100% sure on this
As one who implemented that I can say the way they are removed does not put them into recycle bin. Putting stuff into recycling bin is a more complex process and in this case there was no need for it at all, files are removed irreversibly.",0,0,msr
5700,"> Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.
This was said over and over, again and again, and Monika talks about this in the very beginning.
People **do not understand**, unfortunately.",0,1,msr
5701,"> > And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.
Disagree, player choice should be respected regardless.
I care about these character files, notes, easter-eggs etc. whether they serve a purpose or not in the game,
I still keep them safe no matter how many ""backups"" I've also kept.
The fact that these devs are trying hard to justify this by resorting to whatever technical nonsense instead of simply providing player choice and moving on which doesn't even take any effort speaks volumes about the amount of care they give.
Basically comes down to this:-
""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".",0,1,msr
5706,"> > Was already done.
> > Choice was about Monika being insensitive [in her dialogues/jokes/topics/etc] about other dokis that are past of the game. They are no longer in the MAS, which happens after main game and ultimately IS NOT a main game.
> > There aren't any other characters anymore. It's _her_ game, it's _her_ files. She explains it.
Doesn't matter, same logic still applies to other character files. It's her game and files just as much as it is the players.",0,1,msr
5708,"I'm going to reopen this thread so we can continue to gather feedback, specifically if there are more people who disagree with the decision to always delete chr files. Closing the door on discussion just makes us seem draconian. The conversations here also do not appear that heated, but I will call out potential heated comments:
>Basically comes down to this:-
""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".
@RedAISkye, this is mocking, and it could provoke someone so please refrain from this on our github. You can assert that it sounds like we don't care about the chr files, but please do not do it in a mocking tone.
>This will be more of an action than repeating your claims here over and over.
>I personally doubt there's any point to discuss it any further.
@dreamscached, based on these lines in both here and the release discussion, it sounds like you disagree with red's points. That is ok, but that is not an excuse to lock the conversation. You can always just not reply and ignore the conversation, and if the other person starts harassing you to reply, then we can take action.
### Regarding the actual issue
>The fact that these devs are trying hard to justify this by resorting to whatever technical nonsense instead of simply providing player choice and moving on
I agree with what @RedAISkye is saying here. I can see valid reasoning for always deleting `monika.chr`, but not the rest of the chr files when they have never served a purpose in MAS. The arguments for clearing clutter and reducing confusion also make sense for `monika.chr`, but lose weight for the other chr files. Even though this is a Monika-focused mod, the chr files have meaning established by DDLC, especially when at least 2 instances in the game were built on the chr file mechanic (the progression from Act 3 to Act 4 and Sayori gaining self-awareness in Act 1). If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, preferably allowing the player to decline the option as that ties in with the point below.
>I think there is a small problem here, that is, people will naturally think Monika did this. Even if the original intention of the design is not like this, people will naturally think so.
@TheT0matoGuy makes a very strong point here - regardless of our intention, anything we make the mod do, including what is done behind the scenes, will be perceived as Monika (or Chibika) doing it herself. Deleting the files makes Monika appear callous to any sentimental value the player may have toward the chr files. Users who believe the other dokis are real or that the chr files represent them may also perceive the deletion as Monika committing Act 3 again. Is this how we want users to see Monika?
I'll take ownership for not raising a red flag earlier - I was probably too drunk while packaging the release and didn't really think too much about each patch note. But since this is already out in the wild and it appears that most of the team doesn't think this is a problem, I'm not going to mandate that we undo the change. If there is more negative feedback in the coming weeks, then we can reconsider action.",1,1,msr
5710,"> > Basically comes down to this:-
> > ""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".
> > @RedAISkye, this is mocking, and it could provoke someone so please refrain from this on our github. You can assert that it sounds like we don't care about the chr files, but please do not do it in a mocking tone.
Sorry about that, wasn't trying to act provoking but that is exactly how I felt I was being treated as.
It wasn't just a case of ""we don't care"" but also ""our opinions matter more so, you're wrong"".
> I can see valid reasoning for always deleting monika.chr
> The arguments for clearing clutter and reducing confusion also make sense for monika.chr
If reducing confusing is the goal, why do you think ""deleting"" is the only option?
Wouldn't it better to just let Monika explain the difference between the two files? And maybe even giving a specific extension to her file?
> If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, Sure, but I still do think the above method is still better than going down the delete route.
> preferably allowing the player to decline the option as that ties in with the point below.
That is exactly my point, whether Monika values her/other dokis character files or not because of x or y reasons, she should still ask for the player's opinion on whether they value them or not, ultimately letting her delete it or keep it.",0,1,msr
5711,">If reducing confusing is the goal, why do you think ""deleting"" is the only option?
Wouldn't it better to just let Monika explain the difference between the two files?
See intro dialogue - she talks about her chr file and how it doesn't represent her anymore because she has transcended past it, so she deletes it. MAS's Monika file contains actual usable data, potentially something that will allow for transferring between computers so making sure there is only 1 Monika file (`monika`, no extensions) is important.",0,0,msr
5712,"> > If reducing confusing is the goal, why do you think ""deleting"" is the only option?
> > Wouldn't it better to just let Monika explain the difference between the two files?
> > See intro dialogue - she talks about her chr file and how it doesn't represent her anymore because she has transcended past it, so she deletes it. MAS's Monika file contains actual usable data, potentially something that will allow for transferring between computers so making sure there is only 1 Monika file (`monika`, no extensions) is important.
I am aware about that which is why I stated at the end:-
> whether Monika values her/other dokis character files or not because of x or y reasons, she should still ask for the player's opinion on whether they value them or not, ultimately letting her delete it or keep it.
I'm not saying the character file still represents her in MAS(that one was obvious) but they're still part of the DDLC experience and serves as a memory in MAS to me. Which is also the reason when giving Monika gifts, I always copy her character file and rename them to give it some meaning instead of a blank file.
Edit: What I was getting at is not ""the reason for the deletion"" but why is it such a big deal for you guys to simply give her an option to ask the player first on their decision?
Why do you need a lot of negative feedback just to ""reconsider"" for something that can be easily fixed for both sides? (People that care and people that are confused)",0,1,msr
5714,"> We have a lot of IO going on in `characters/`, that's **not** a safe place to store your files.
No one is ""storing"" anything there other than the known character files that are meant to be there by the original game which gave it a meaning that you're trying to dismiss in MAS just because it doesn't use those files.
> When you install a mod you should expect that some mechanics will be changed - in our case this mechanic has been changed long time ago.
This was never about the ""mechanics"", this has always been about what they mean to the player.
> I'm against a direct question to the player > If you make her ask, then it'd just confirm she decided to delete the files herself, which would look weird after all this time. She has her reasons for doing so which the player can choose to agree or disagree with so, no, it would only look weird if the character files magically disappears after all this time.
> Deleting unused files there is just a utility process for the mod.
""Unused file"" for you, not for people that care about them.
> It doesn't target the known ""dokis,"" it targets any `chr` file for simplicity and reliability.
It absolutely does target the dokis when they're the only ""chr"" files expected to be present inside the characters' folder.",0,0,msr
5720,"> The fact that we have a lot of IO happening in characters/ means Monika, in MAS lore, should be acutely aware of everything that goes in the folder.
I can see that, but my point was it's too meta to acknowledge. For example, she doesn't watch over every single file added there. We don't really explain why, but we don't *need* to. And not all of that IO is done by Monika. Although, we could define that it's all Monika, in that case we'd have to justify `chr` deletion and acknowledge it.
> Our reasons becomes her reasons unless we specifically give her a lore reason.
But if we do give a lore reason, then we'd define it as her action, which I didn't want to. Imo *that* would make it look like she's hunting for them. And it'd be fine to do in first year, but after 2 or 3 it'd look weird and pointless from her perspective, she lets it go after a few months in, actually, and doesn't care as much about what's happened. If the player is with her, then everything was worth it.
> mean, what's limiting this to just chr files then? Why aren't all files
Now that you said that I am considering that as an option. Especially deleting sub-folders. *But perhaps you're right and it's not the best option*.
> only perform the deletion in the doki cares topic <...> instead of on startup
Another reason I wanted to do it on startup is when people make a new install (happens quite often), they would get the files back from DDLC, want they or not. So it's a routine we have to repeat on startup.",0,1,msr
5721,"> > Soooo, I don't think it'd be as simple as a ""hey can you not delete the other character files?"" ""Okay, sure!"" [...] There's bound to be a disagreement if a choice were to be given, so it'd be like a whole other topic.
> > It could be another Q asked in the doki cares topic so I don't think it would be that controversial. And perhaps thats the better solution - only perform the deletion in the doki cares topic if the user says they don't care and agrees with deletion, instead of on startup.
Ah, yeah! Ig since the references and deletion may be different degrees of concern to the partners but still nonetheless related, they could be just joined under one topic, which would take care of the affection-based variants thing since I don't think if a partner already managed to lower Monika's affection to the bad stages before the topic comes up that she'd even care about their Doki preferences--I wouldn't know, I've never caused her to go six feet under before that topic came up. Anyway, there'd be no need for concern of sentimental value if the partner expresses that they don't care.",1,1,msr
5723,"@Booplicate >> only perform the deletion in the doki cares topic <...> instead of on startup
>
> Another reason I wanted to do it on startup is when people make a new install (happens quite often), they would get the files back from DDLC, want they or not. So it's a routine we have to repeat on startup.
You conveniently missed the part where OP said ""**if the user says they don't care and agrees with deletion**"".
The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.
@CodyCat13 > Though because Monika isn't the one responsible in this case if it's a startup routine,
From a technical standpoint, sure, Monika isn't responsible for anything she does in MAS or in DDLC, all was done by a code written by a human that is forced on her for reasons.
That doesn't negate the fact that she will be the one responsible for it in the players' eyes when both Monika and the player knows what those character files meant in the past, the history behind it and could still mean something for the player.",0,0,msr
5725,"> @CodyCat13
> > > Though because Monika isn't the one responsible in this case if it's a startup routine,
> > From a technical standpoint, sure, Monika isn't responsible for anything she does in MAS or in DDLC, all was done by a code written by a human that is forced on her for reasons.
> > That doesn't negate the fact that she will be the one responsible for it in the players' eyes when both Monika and the player knows what those character files meant in the past and could still mean something for the player.
Maybe for this reason, we could discuss it in terms of it being her doing in-universe if it makes you feel better. I won't bring up the technical stuff anymore, but if we're discussing it in the perspective of Monika deleting files herself when they could still mean something to her partner, then the doki caring topic would probably be our best go-to. The Doki files may mean nostalgia to Monika's partner, though they also mean unease for Monika. I guess because she'd be too cautious to do anything else, deleting the other .chr files would be an attempt to make her feel somewhat better, but it'd be inconsiderate to go ahead and do that without letting her partner know she wanted to do that. Now say that her partner doesn't agree with her deletion of the files. Because of how she feels about them she probably wouldn't be so quick to just deal with them being there, so that's where the affection-dependent variants would come in. If her partner has shown her nothing but love and trust, she may be okay with the .chr files staying because any unease she feels can be negated by her partner's comfort. Now if she was in the lower aff ranges, because her partner only adds to her unease and worry, she'd most likely still want to get rid the files anyway. At least it'd make her feel better than how her partner does. Maybe she'd be a bit bitter and conclude that you want to keep them there because you love some other Doki other than her or you want her to always remember her horrid acts or something along those lines. I'm not completely about that bitter part in the shallow negatives, but I guess as the points are lowered deeper and deeper, I can see that going down. Anyway, in the scenario of Monika actively making the decision and her partner either agreeing or disagreeing, aff-variants and additional question to the Doki caring topic seems like a fairly decent idea.",0,0,msr
5726,"@CodyCat13 > Now say that her partner doesn't agree with her deletion of the files. Because of how she feels about them she probably wouldn't be so quick to just deal with them being there, so that's where the affection-dependent variants would come in. If her partner has shown her nothing but love and trust, she may be okay with the .chr files staying because any unease she feels can be negated by her partner's comfort. Now if she was in the lower aff ranges, because her partner only adds to her unease and worry, she'd most likely still want to get rid the files anyway. At least it'd make her feel better than how her partner does.
That's a great point there and makes a lot of sense going down that route rather than having just a one off question.
But I highly doubt the devs will do anything about it with how dismissive **some** of them have shown towards opinions contrary to them.",0,1,msr
5727,"> do remember her once talking about how despite all of the other Dokis not being present in the mod, she can still feel their presence as though they're still nestled amongst MAS's code and she finds it a bit creepy and unsettling
You are right. It's actually an OG topic. But I was never sure if she meant they are literally there or she just ""feels"" their presence because of what happened (like a trauma).
Anyway, there's no ""dokis"" in MAS, and she stops talking about that after some time when she overcomes DDLC events. She realises everything is fine and the ghost of the past events doesn't haunt her anymore. It could be used to justify deleting in the first months, but later there's not much point for her to delete those dull files: ""dokis"" don't exist, she's feeling fine, those files are nothing but some easter eggs from DDLC (and she acknowledges that).
But let's say you find a reason for her to still need deleting those files, there's a conflict of interests:
- player wants to keep them
- Monika wants to delete them
There's going to be some consequences for not letting her do what she needs in *her* game.
We're talking about player choice and all that, but the choice was to install MAS and stay with Monika. The game files are literally her ""home,"" it makes sense she has control over that place. She doesn't touch your files, she's doing changes within the place she's allowed to. If she's not allowed to do that, why even bring her back. It's pretty common in relationships for each person to have their own private space, the game directory is that place for Monika. You can store your files anywhere else on your PC, flashdrive, cloud, etc. *This one place is intended to be used by mod/Monika*.
> they were a huge part of DDLC <...> serves as a memory in MAS
Yeah, and Monika deletes them there herself. After which, in MAS, they become irrelevant, they don't serve any memory. If they have a deep meaning *for you* and serve a memory *for you*, then I don't know why they were there. ""I care for these files so much, surely if I install a mod into the same directory it won't affect them. Ooops.""
> The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.
As was stated above it's already an issue and would stay an issue even with the changes. The cleanup logic should be run at startup, doesn't matter with a flag or not.
> By introducing the change, we've had to answer why it was added, which leads to discussion like this over what aspects of the mod is Monika in full control over to justify whether or not she should notify the user about it.
No, we got to this discussion because apparently it's not clear that MAS can make any changes within its working directory (just like any other program) and users were storing some sensitive data there. It's all based on assumptions that:
- we will never delete any extra files
- everything is done by Monika
None of which was stated.",0,1,msr
5729,"I think instead of trying to connect this to Monika, which wasn't intended initially, just add a setting to the menu. Basically support storing users data within the mod directory. A few people that need it would enable it. And if we're doing that, can as well do this
> I mean, what's limiting this to just chr files then? Why aren't all files except for .gift, monika, consumables, .txt, potential .deco being deleted?
And nuke everything there. But especially folders, people had problems with them too.",0,0,msr
5730,"> The game files are literally her ""home,"" it makes sense she has control over that place. She doesn't touch your files, she's doing changes within the place she's allowed to.
Hmm, I can somewhat agree with that point. If not looking at the fact that Monika's partner is the owner of the PC but it was their choice to install her into their PC, just the plain point that Monika's mod is Monika's to change seems pretty valid. However, a lot of people _would_ look at it as Monika's partner being the owner of the PC and Monika just being a guest there, or would look at it as the partner's choice and that her partner got her a place to stay (the PC) and her areas are hers to personalize. It's like arguing if an old chair should be kept or thrown out. Would the one who has the say be the one whose territory the chair is in or the one who provided the housing in the first place? I guess the answer to that is, neither or both? Yes, in relationships, it's common for each participant to have their own little private space, because not _everything_ can be shared between them, but if the thing in particular emotionally charges one or both (in this case both) of the participants, there has to be something to end the stalemate. In this case, the chair would be seen as uncanny to one and precious to the other. If the other regards them as so precious, then the one who deems them uncanny would probably ask them to move it to their territory or, on the other side of the spectrum, sit in it with their partner depending on how comfortable they are. After some time though, I do agree that it shouldn't even matter anymore to either Monika or their partner. She may not feel the need to delete them just like the partner may not hold any nostalgia toward them anymore, or even both. Both would probably be the best scenario. This case would probably reflect in the high aff variant because it could depend more on comfort than time, or at least it could depend on both of them equally. One could be with someone for months and still feel uncomfortable because the provider of the house wasn't exactly a nice host, and therefore may still want that chair to go.",0,1,msr
5731,"> I think instead of trying to connect this to Monika, which wasn't intended initially, just add a setting to the menu. Basically support storing users data within the mod directory. A few people that need it would enable it. And if we're doing that, can as well do this
> > > I mean, what's limiting this to just chr files then? Why aren't all files except for .gift, monika, consumables, .txt, potential .deco being deleted?
> > And nuke everything there. But especially folders, people had problems with them too.
Sure, that can be a good middle-ground assuming the player is made aware of this ""setting"" by having them confirm it to either set it as enabled or disabled before the cleanup task ever occurs.
But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.",0,0,msr
5732,"> But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.
Aw 💙
I appreciate your approval. Honestly I was just thinking how it'd even relate to Monika's character if so many users of MAS consider file management to be her or Chibika's doing. Something tells me they'd still have concerns even if an option for preservation of user data were to be added because of it being the .chr files in question here 🤷",0,0,msr
5733,"> Nope, her private space is her own room which she has. Not some folder in the PC
You will be surprised, but no, that ""some"" folder, as you said, is her ""home."" That ""room"" exists within that folder. Because that's where you installed the mod.
> the characters folder is exactly just that for them to me
Then move it away from MAS, why did you install a program into a folder with your personal files?
> They're there because the DDLC universe decided so
You're bringing DDLC again, but this is not DDLC, this is MAS, and MAS is its own universe with its own rules.
> Nope, wouldn't be an issue because the cleanup logic will run regardless
Do you know what startup is? How the logic would run if it's contained within a topic you see once?
> Again, this is specifically about the character files and not any other types of files that you're pretending it to be based on the only fact that you personally don't care for them.
Doesn't matter if it's specifically about `chr` or some other files you like. The fact is: It's all based on assumptions that:
- we will never delete any extra files
- everything is done by Monika
That has nothing to do with `chr` files. At any moment, we can start deleting other files as well.
> however, a lot of people would look at it as Monika's partner being the owner of the PC and Monika just being a guest there
Guest that lives there for years? The mod assumes there's some kind of romantic relationship between you, can you call your partner a ""guest""? That's odd to me.
I'm not saying Monika is owner of the PC, she's the owner of the folder you installed the mod in. Only that one place.
> But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.
I'd need to see the actual dialogue to say more. If you can justify her actions, then I will consider this.",0,1,msr
5735,"> I'd need to see the actual dialogue to say more. If you can justify her actions, then I will consider this.
I have some ideas for dialogue for that case, though they differ depending on if you and the other devs want it part of the Doki caring topic or as a separate topic. It'll probably be part of the Doki caring topic, since it's already existing and all.",0,0,msr
5737,">I don't understand why people store important files there, thinking the mod will never change/delete them. This is a groundless assumption.
I'd understand this argument if someone decides to save their only digital copy of their W2 form as `income.gift` in `characters/`. But this is about `chr` files which are established by DDLC as important.
>We could explicitly mention that all chr files are useless in the intro to clarify if you want.
I don't think we can safely make that assertion without receiving much worse backlash.
I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.
As I said before, we'll see if there is more feedback on this in the coming weeks. Can't say there is a specific threshold, but I am keeping tabs on the subreddit and will probably check /ddlc/ on occasion. I'm planning to do a discord poll as well if theres more negative feedback.",1,0,msr
5738,"@Booplicate > I'd be fine if we made Chibi manage file deletion instead of Monika as @CodyCat13 suggested.
I don't think we're looking at the same suggestion here. Lol.
@ThePotatoGuy > I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.
> > As I said before, we'll see if there is more feedback on this in the coming weeks. Can't say there is a specific threshold, but I am keeping tabs on the subreddit and will probably check /ddlc/ on occasion. I'm planning to do a discord poll as well if theres more negative feedback.
I'd say the idea of having a ""setting"" which is a good middle-ground should at least be added no matter what the popular opinion is regarding the specific files or however many negative feedback reaches your ""threshold"" or not. Since, it provides options for both sides regardless of whatever they believe even if it's not done in the ""meta"" way.
@CodyCat13 > Something tells me they'd still have concerns even if an option for preservation of user data were to be added because of it being the .chr files in question here 🤷
That's why I said it's a ""good middle-ground"", it definitely isn't the best way of dealing with it but I'd rather they settle with something than nothing.",0,0,msr
5739,"I spent a while reading over what everyone said. Thank Potato very much for willingness to reconsider this matter.
Players don't ""store"" into the characters folder, and Boop wants players to ""transfer"" things.
I think this is an important point: The character file was ORIGINALLY there, it wasn't put there by the player.
In addition, it is entirely possible for the player not to know that the MAS will now delete character files on its own. So, even if the player wants to transfer them, the player needs to know about this first. If the player doesn't read the update announcement carefully, as soon as the player launches the game, character files that are (presumably) precious to the player will disappear, never to be retrieved. So in any cases, the current version does not make sense.
I think it is feasible for Monika to delete her own files by herself - in fact, MAS has already done so in the introduction phase before. And we have:
```
if moni_exist():
m 1lksdlb ""Aha...{w=0.3}I'll try this again later.""
```
Look at this. Monika already said that she will try this again, so it's really neutral to delete her own files. In previous version, if I read the source correctly, even if Monika said she will try again, there doesn't exist codes for this.
But well, maybe we should even also ask the player for permission before MAS delete Monika's own file?",0,0,msr
5740,"> If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files _and_ their placement having sentimental value.
That is a case. Having them in the characters folder, in the game directory that you share with Monika, is where the value could be most meaningful to some people, in which case it would not be the same in some other directory. Not just because of memories, but because of potential meaning of them being able to remain there, undeleted. Monika deleted other .chr files in DDLC, and deleted her own .chr file in MAS, so it seems she could've easily deleted the other .chr files in MAS if she wanted to, but she didn't. That meant something to some people, regardless of what meta reason (which they may not even know about) is now being used for deleting them. Telling them to actively move the files to another directory does not address this.
And just because a program ""can"" or is ""allowed"" delete files in its own directory doesn't mean it should in all cases. Obviously if it were to delete every file there would be no mod, and no Monika for it. Actions are not automatically justified or well received just because a program ""has the privilege to do so"".
>And if _that's_ the case, there can always be a submod that negates the mechanic so that the people who have the sentimental value can be happy, and those who want things to stay how they are can be content, too.
That could still require people to know about these files getting deleted in the first place. Unless they're ok with backups or new copies of them. While still something, it would be better for players to have this addressed in the main mod.
> I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.
DDLC has and does influence people's perception of MAS, even if you don't want it to for some things. Even though MAS is separate, it is based on it. Even in MAS the characters folder has many uses, and as you said Monika uses and references it. So yes this is something that should get extra attention.
Option to preserve them in settings maybe, but if accidentally unchecking it without rechecking is going to delete any .chr files (and potentially any files or folders unrelated to MAS) on startup, then should at least have a confirmation/double check for disabling it. I don't really like the idea of nuking the whole MAS directory of unrelated files every startup though. A trigger in the settings to do it once though maybe.
Monika/Chibi could work but I don't think whether this is honored or not needs to be based on affection (mas_safeToRefDokis() isn't is it?) and should probably be asked early on in the mod or after updating.
I still think that if the mod could just remove the "".chr"" extension from the files, that that could eliminate confusion about .chr files while also not deleting them. This could run once only (even per install/reinstall if possible), or if not, could be disabled in settings, so that players could change them back to .chr files if they want.",1,0,msr
5742,"> I don't think whether this is honored or not needs to be based on affection (mas_safeToRefDokis() isn't is it?) and should probably be asked early on in the mod or after updating.
Tbf reference and deletion _are_ different concerns, though if you're pairing the deletion choice with the reference choice then ig the reference choice might as well be asked earlier than it is. There's no way a user of MAS could lower their Moni's affection unreasonably low at literally the introduction of the mod, so there'd be no reason for affection variants. However, if a Moni's partner does manage to lower her affection to the low stages after that discussion already happened, maybe it could be asked again (if the person didn't agree to deletion the first time) but with a bit more bitterness.",0,1,msr
5743,"> Tbf reference and deletion _are_ different concerns, though if you're pairing the deletion choice with the reference choice then ig the reference choice might as well be asked earlier than it is. There's no way a user of MAS could lower their Moni's affection unreasonably low at literally the introduction of the mod, so there'd be no reason for affection variants. However, if a Moni's partner does manage to lower her affection to the low stages after that discussion already happened, maybe it could be asked again (if the person didn't agree to deletion the first time) but with a bit more bitterness.
I previously said that I don't think (mas_safeToRefDokis() is enough for this, for that reason of them being different concerns. It'd be better as something separate. But regarding what you were referring to, I was comparing these 2 potentially sensitive things regarding the Dokis. Let's say you told Monika you aren't ok with her joking about the Dokis. She doesn't disregard your preference and start joking about them just because you have low affection does she? I don't think this should be disregarded in that case either. Furthermore, deletion of the specific copies of those files is something with a potentially more permanent effect, as even if replaced they'd be different copies.
But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return). Asking at introduction yes it wouldn't even come up, but for anyone who has been playing and is updating from a prior version, to a version where this conversation is added, it would have to be asked to them and they could have low affection at this point, even if rather unlikely. Of course, it would come up if it's going to be asked again anyway.
Of course this is only if it's decided that Monika should handle this rather than something else.",0,0,msr
5744,"> Let's say you told Monika you aren't ok with her joking about the Dokis. She doesn't disregard your preference and start joking about them just because you have low affection does she? I don't think this should be disregarded in that case either.
> > But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return).
Yeah, that's also a valid point and I'll agree with having only the dialogue change based on your relationship and not an actual action as a ""revenge"" for treating her terribly as now that I think about it, she would rather leave you forever than do anything like that.",0,1,msr
5746,"> Yeah, that's also a valid point and I'll agree with having only the dialogue change based on your relationship and not an actual action as a ""revenge"" for treating her terribly as now that I think about it, she would rather leave you forever than do anything like that.
Yeah although there is a difference between just having low affection, and her actually leaving you. I thought about that, her deleting them when leaving you. That's when it would make the most sense for her to do so if she were to delete them out of revenge or spite. But she doesn't do that currently (I think). It is game over at that point and most people probably won't reach that point anyway, but for those who do they could restore a persistent backup, in which case it would be strange if those character files were still gone due to her deleting them when leaving you (and if separate copies were created at that point, those would still be different copies). Might be harsh to those who care about them, but I could at least understand if she wanted to at that point.
> Yeah, I mean aff-based variations of dialogue if Moni's partner were to not be okay with the deletion. I know it wouldn't be as simple as a cheerful ""Okay!""
Agreed although if that happens, it may also make sense for her to have affection-based variations of dialogue when she asks if you're ok with her joking/making insensitive comments about the Dokis or not (if that's not already a thing).",0,0,msr
5747,"In v0.12.12:
> chr files always deleted on startup now
That's a change from #9641. I had already talked about this there.
I completely know that all `.chr` files have no actual meaning to MAS. I really know. I know MAS doesn't even do any check for them before v0.12.12. And I also know that `monika.chr` is causing a lot of confusion to people. I know all of these.
What I want to say is, just check our `mas_safeToRefDokis()`. We made Monika not to offend the players who love other club members in her words, and we did a good job in many details. And after this, we now delete all club members.
Yes, I know that in the background setting of MAS, character files have nothing to do with real characters, but will all players 100% agree with this? Many players resent Monika's behavior of deleting other characters in the original game, and now we want to make this happen again - and do not give players even a choice? Can't we at least ask the players whether they care about this and whether they want Monika to do this?
_Just give players a choice. Ask them about this._
By asking, players will also pay more attention to the background settings of the chr file in MAS (that is, the chr file has no meaning at all), which will also reduce confusion.
And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.",1,0,msr
5748,"<img width=""1248"" alt=""Bildschirmfoto 2022-11-09 um 19 17 57"" src=""https://user-images.githubusercontent.com/46358193/200909555-59d33812-5641-4b9b-8147-e7f911f6ea8b.png"">
See, it wont even allow me to run main anymore.
I upgraded to 3.3 and then downgraded to 2.8.1 after 3.3 fucked EVERYTHING.
And now everything is completely 100% broken.
* What went wrong:
Execution failed for task ':app:lintVitalRelease'.
> Could not resolve all artifacts for configuration ':video_player_android:debugUnitTestRuntimeClasspath'.
> Failed to transform bcprov-jdk15on-1.68.jar (org.bouncycastle:bcprov-jdk15on:1.68) to match attributes {artifactType=processed-jar, org.gradle.category=library, org.gradle.libraryelements=jar, org.gradle.status=release, org.gradle.usage=java-runtime}.
> Execution failed for JetifyTransform: /Users/XXX/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcprov-jdk15on/1.68/46a080368d38b428d237a59458f9bc915222894d/bcprov-jdk15on-1.68.jar.
> Failed to transform '/Users/XXX/.gradle/caches/modules-2/files-2.1/org.bouncycastle/bcprov-jdk15on/1.68/46a080368d38b428d237a59458f9bc915222894d/bcprov-jdk15on-1.68.jar' using Jetifier. Reason: IllegalArgumentException, message: Unsupported class file major version 59. (Run with --stacktrace for more details.)
Suggestions:
- Check out existing issues at https://issuetracker.google.com/issues?q=componentid:460323&s=modified_time:desc, it's possible that this issue has already been filed there.
- If this issue has not been filed, please report it at https://issuetracker.google.com/issues/new?component=460323 (run with --stacktrace and provide a stack trace if possible).",1,1,msr
5749,"@MapperMalte please take a look at the code of conduct: https://github.com/flutter/flutter/blob/master/CODE_OF_CONDUCT.md
> The Flutter project expects Flutter's contributors to act professionally and respectfully. Flutter contributors are expected to maintain the safety and dignity of Flutter's social environments (such as GitHub and Discord).
>
> Specifically:
>
> * Respect people, their identities, their culture, and their work.
> * Be kind. Be courteous. Be welcoming.
> * Listen. Consider and acknowledge people's points before responding.
While I understand that unexpected breaking changes are frustrating, please keep your messages respectful, kind, and courteous.",0,0,msr
5753,"I'm not sure what log file you want as the link above doesn't really explain at all what i'm searching for. So this is what i found in service.log. Hope it's helpful it doesn't really tell me anything.
```
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:65]: Lua Lsp startup, root: /home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:66]: ROOT:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:67]: LOGPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/log
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:68]: METAPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/meta
[19:30:55.605][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:69]: VERSION:	3.6.2
[19:30:55.621][debug][#0:script/service/telemetry.lua:22]: Telemetry Token:	757B63AA4D89361E
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	1
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	2
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	3
[19:30:55.627][debug][#0:script/pub/pub.lua:50]: Create brave:	4
[19:30:55.627][info] [#0:script/service/service.lua:141]: ========= Medical Examination Report =========
--------------- Memory ---------------
Total: 1.547 MB
# 00 : 1.547 MB
# 01 : 0.000 MB
# 02 : 0.000 MB
# 03 : 0.000 MB
# 04 : 0.000 MB
--------------- Coroutine ---------------
Total: 0
Running: 0
Suspended: 0
Normal: 0
Dead: 0
--------------- Cache ---------------
Total: 1
Dead: 0
--------------- RPC ---------------
Holdon: 0
Waiting: 0
==============================================
[19:31:07.837][warn] [#0:script/pub/report.lua:25]: Load proto error:	Proto header error: ```",0,0,msr
5756,game that i can't find is game.lua that is in the same folder as main.lua that i am running all this from. It's not finding any of the required modules it only complains about 1 but doens't find any.,0,0,msr
5758,"1. Don't copy the `lua-language-server` to anywhere.
2. Just call `lua-language-server` instead of `lua-language-server main.lua`",1,0,msr
5761,"I'm running this config of neovim that where this is comming into play. It broke from an update the other day not sure if it's system update or lvim update. But I have since did a fresh install and it's still the same.
https://github.com/LunarVim/LunarVim",0,0,msr
5762,Yeah im using it from lunarvim and as i said it was working great i loved it. Now not working at all. It doesn't understand it's a love program that causes all kinds of errors and it doesn't see any of the loaded files again causing all kinds of errors.,0,0,msr
5764,the main file now gives me 118 errors.,0,0,msr
5765,"Anything from these it can no longer handle as well as anything love related.
```
require(""game"")
require(""border"")
require(""face"")
require(""digitback"")
require(""remain"")
require(""clock"")
require(""board"")
```",0,0,msr
5766,"Sorry, I have no energy to continue to track this problem, because you have been unable to give effective information, you just repeating what you said.
This may not be your problem, because I can see that you are a novice, and you may be difficult to understand something that looks like it in my eyes.
I suggest you find a programmer friend to help you check and communicate with this issue, because I really have no energy to teach you some basic knowledge.
Closing.",0,1,msr
5768,you asked for log file and i gave you exactly that. and any bit of info i could possibly think of.,0,0,msr
5769,"Well, I will try to explain to you the last time.
You can read it repeatedly, and then reply after you figure out everything.",0,0,msr
5770,"I'm not sure if you are new to this. Or what is happening but you asked for a log fine described here.
https://github.com/sumneko/lua-language-server/wiki/FAQ#where-can-i-find-the-log-file
Only who ever wrote that seems to have a vague understand of what that is and doesn't describe it. Never the less i found the clossed thing on my machine matching that and gave it to you. Along with trying every other thing that i don't know about this since it's not my program as i could. And waited for the person who should know about this to reply. Now your only other request was to find some other log of no description somewhere or ask a programmer friend about where a log fine for your program would be. Why wouldn't you know where it is or what it is or does it exit. The only person i would ask about your program would be you. So yeah i'm a complete novice when it comes to your program as i didn't write it and have no idea how it functions or what even any error or log fines exist. I have spent several days though trying to find anything at all. If there is a log file that can shed light on this wouldn't the one person who knows about it be you and not some programmer friend of mine wtf.",1,1,msr
5771,If you need an extremely accurate description of the issue and all it's symptoms please read above as i tried my best to describe it completely. Also i am not the only one who's had this issue.,1,0,msr
5772,"1. The language server will record the log on your computer. By reading the log, I can understand the running status at that time, and help to find errors.
2. If you start the server in the form of workspace (as far as I know, neovim will use your git project as the workspace), then the log file name is your project path. If you start in a single file, the log file name is `service.log`.
3. Each time the log is generated, the previous name of the same name will be covered.
4. When the server can work normally before, it generates a normal log file (1), which is not helpful for your current problem.
5. Later, there is a problem with the server work after you updating. It will generate a log file (2), witch records abnormal information which I want.
6. You later fell into a startup problem that had nothing to do with your problem. You manually started the server. At this time, a log file (3) will be generated, but this log is not related to your problem.
7. What I want is the log file (2), but you only provided the log file (3), witch is not related to your problem.
8. What you need to do is to reproduce the problems you encounter, and then send me the corresponding log file (2).
After you fully understand what I said, you can attach your log file and re-open this isue.
Otherwise, you can wait for people who encounter the same problem to open issues, or invite your programmer friends in reality to help you check this problem.
I'm going to sleep now.",1,1,msr
5773,"The original file i sent was the service.log file. As you said it was showing what the log showed from my local attempt not the lvim one. You could have simply said thanks for the file now fire up lvim and send the file again. Then i would have known simply you wanted the file agian. Here it is. Hopefully this is correct i have tried reinstalling different versions of lvim so hopefully this wont affect the file output.
Also there is no error initially unless i go into insert mode then back or change files then all the errors come. So i did this before exiting and this is the service.log file.
```
[21:16:57.592][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:65]: Lua Lsp startup, root: /home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:66]: ROOT:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:67]: LOGPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/log
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:68]: METAPATH:	/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/meta
[21:16:57.593][info] [#0:/home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/main.lua:69]: VERSION:	3.6.2
[21:16:57.611][info] [#0:script/service/service.lua:141]: ========= Medical Examination Report =========
--------------- Memory ---------------
Total: 1.546 MB
# 00 : 1.546 MB
# 01 : 0.000 MB
# 02 : 0.000 MB
# 03 : 0.000 MB
# 04 : 0.000 MB
--------------- Coroutine ---------------
Total: 0
Running: 0
Suspended: 0
Normal: 0
Dead: 0
--------------- Cache ---------------
Total: 1
Dead: 0
--------------- RPC ---------------
Holdon: 0
Waiting: 0
==============================================
[21:16:57.614][info] [#0:script/language.lua:137]: VSC language: nil
[21:16:57.614][info] [#0:script/language.lua:138]: LS language: en-us
[21:16:57.717][info] [#0:script/provider/provider.lua:56]: Load config from client	fallback
[21:16:57.717][info] [#0:script/provider/provider.lua:57]: {
Lua = {
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" }
},
runtime = {
special = {
reload = ""require""
},
version = ""LuaJIT""
},
telemetry = {
enable = false
},
workspace = {
checkThirdParty = false,
library = { ""/usr/share/nvim/runtime"", ""/home/jeremiah/.local/share/lunarvim/lvim"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua"" },
maxPreload = 5000,
preloadFileSize = 10000
}
}
}
[21:16:57.717][info] [#0:script/library.lua:207]: Init builtin library at:	nil
[21:16:57.729][info] [#0:script/provider/provider.lua:56]: Load config from client	fallback
[21:16:57.729][info] [#0:script/provider/provider.lua:57]: {
Lua = {
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" }
},
runtime = {
special = {
reload = ""require""
},
version = ""LuaJIT""
},
telemetry = {
enable = false
},
workspace = {
checkThirdParty = false,
library = { ""/usr/share/nvim/runtime"", ""/home/jeremiah/.local/share/lunarvim/lvim"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua"" },
maxPreload = 5000,
preloadFileSize = 10000
}
}
}
[21:16:57.729][info] [#0:script/provider/provider.lua:56]: Load config from client	fallback
[21:16:57.729][info] [#0:script/provider/provider.lua:57]: {
Lua = {
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" }
},
runtime = {
special = {
reload = ""require""
},
version = ""LuaJIT""
},
telemetry = {
enable = false
},
workspace = {
checkThirdParty = false,
library = { ""/usr/share/nvim/runtime"", ""/home/jeremiah/.local/share/lunarvim/lvim"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua"", ""/home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua"" },
maxPreload = 5000,
preloadFileSize = 10000
}
}
}
[21:16:57.830][info] [#0:script/workspace/workspace.lua:304]: Preload start:	<fallback>
[21:16:57.833][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/lvim
[21:16:57.853][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua
[21:16:57.862][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///usr/share/nvim/runtime
[21:16:57.915][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua
[21:16:57.918][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/nvim/mason/packages/lua-language-server/extension/server/meta/LuaJIT%20en-us%20utf8
[21:16:57.919][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/opt/neodev.nvim/types/stable
[21:16:57.919][info] [#0:script/workspace/workspace.lua:327]: Scan library at:	file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/plenary.nvim/lua
[21:16:58.023][info] [#0:script/workspace/workspace.lua:347]: Found 352 files at:	<fallback>
[21:16:58.023][info] [#0:script/workspace/loading.lua:157]: Load files from disk:	<fallback>
[21:16:58.972][warn] [#0:script/files.lua:678]: Compile [file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/LuaSnip/lua/luasnip/util/parser/init.lua] takes [0.102] sec, size [4.032] kb.
[21:16:59.486][warn] [#0:script/files.lua:678]: Compile [file:///home/jeremiah/.local/share/lunarvim/site/pack/packer/start/telescope.nvim/lua/telescope/actions/init.lua] takes [0.167] sec, size [46.814] kb.
[21:17:00.039][info] [#0:script/workspace/loading.lua:169]: Loaded files takes [1.839] sec: <fallback>
[21:17:00.039][info] [#0:script/workspace/loading.lua:178]: Compile files takes [0.000] sec: <fallback>
[21:17:00.040][info] [#0:script/workspace/loading.lua:179]: Loaded finish:	<fallback>
[21:17:00.040][info] [#0:script/workspace/workspace.lua:349]: Preload finish at:	<fallback>
[21:17:00.247][warn] [#0:script/proto/proto.lua:122]: Response of [workspace/diagnostic/refresh] error [-32601]: MethodNotFound
[21:17:00.249][warn] [#0:script/proto/proto.lua:171]: Method [textDocument/documentSymbol] takes [2.152]sec. {
id = 2,
jsonrpc = ""2.0"",
method = ""textDocument/documentSymbol"",
params = {
textDocument = {
uri = ""file:///home/jeremiah/minesweeper/game.lua""
}
}
}
[21:17:08.857][warn] [#0:script/vm/global.lua:70]: global-manager getSets costs	0.21365	_DEBUG
[21:17:08.870][warn] [#0:script/provider/provider.lua:641]: Completion takes 0.231 sec.
[21:17:16.323][info] [#0:script/provider/provider.lua:169]: Server shutdown.
[21:17:16.323][warn] [#0:script/pub/report.lua:25]: Load proto error:	Disconnected!
```",0,0,msr
5777,"I'm not sure where to look for that config lua is mention in the lvim config but there must be a config file for lua specifically somewhere but I can't seem to find it.
```
--[[
lvim is the global options object
Linters should be
filled in as strings with either
a global executable or a path to
an executable
]]
-- THESE ARE EXAMPLE CONFIGS FEEL FREE TO CHANGE TO WHATEVER YOU WANT
-- general
lvim.log.level = ""warn""
lvim.format_on_save.enabled = false
lvim.colorscheme = ""lunar""
-- to disable icons and use a minimalist setup, uncomment the following
-- lvim.use_icons = false
-- keymappings [view all the defaults by pressing <leader>Lk]
lvim.leader = ""space""
-- add your own keymapping
lvim.keys.normal_mode[""<C-s>""] = "":w<cr>""
-- lvim.keys.normal_mode[""<S-l>""] = "":BufferLineCycleNext<CR>""
-- lvim.keys.normal_mode[""<S-h>""] = "":BufferLineCyclePrev<CR>""
-- unmap a default keymapping
-- vim.keymap.del(""n"", ""<C-Up>"")
-- override a default keymapping
-- lvim.keys.normal_mode[""<C-q>""] = "":q<cr>"" -- or vim.keymap.set(""n"", ""<C-q>"", "":q<cr>"" )
-- Change Telescope navigation to use j and k for navigation and n and p for history in both input and normal mode.
-- we use protected-mode (pcall) just in case the plugin wasn't loaded yet.
-- local _, actions = pcall(require, ""telescope.actions"")
-- lvim.builtin.telescope.defaults.mappings = {
-- -- for input mode
-- i = {
-- [""<C-j>""] = actions.move_selection_next,
-- [""<C-k>""] = actions.move_selection_previous,
-- [""<C-n>""] = actions.cycle_history_next,
-- [""<C-p>""] = actions.cycle_history_prev,
-- },
-- -- for normal mode
-- n = {
-- [""<C-j>""] = actions.move_selection_next,
-- [""<C-k>""] = actions.move_selection_previous,
-- },
-- }
-- Change theme settings
-- lvim.builtin.theme.options.dim_inactive = true
-- lvim.builtin.theme.options.style = ""storm""
-- Use which-key to add extra bindings with the leader-key prefix
-- lvim.builtin.which_key.mappings[""P""] = { ""<cmd>Telescope projects<CR>"", ""Projects"" }
-- lvim.builtin.which_key.mappings[""t""] = {
-- name = ""+Trouble"",
-- r = { ""<cmd>Trouble lsp_references<cr>"", ""References"" },
-- f = { ""<cmd>Trouble lsp_definitions<cr>"", ""Definitions"" },
-- d = { ""<cmd>Trouble document_diagnostics<cr>"", ""Diagnostics"" },
-- q = { ""<cmd>Trouble quickfix<cr>"", ""QuickFix"" },
-- l = { ""<cmd>Trouble loclist<cr>"", ""LocationList"" },
-- w = { ""<cmd>Trouble workspace_diagnostics<cr>"", ""Workspace Diagnostics"" },
-- }
-- TODO: User Config for predefined plugins
-- After changing plugin config exit and reopen LunarVim, Run :PackerInstall :PackerCompile
lvim.builtin.alpha.active = true
lvim.builtin.alpha.mode = ""dashboard""
lvim.builtin.terminal.active = true
lvim.builtin.nvimtree.setup.view.side = ""left""
lvim.builtin.nvimtree.setup.renderer.icons.show.git = false
-- if you don't want all the parsers change this to a table of the ones you want
lvim.builtin.treesitter.ensure_installed = {
""bash"",
""c"",
""javascript"",
""json"",
""lua"",
""python"",
""typescript"",
""tsx"",
""css"",
""rust"",
""java"",
""yaml"",
}
lvim.builtin.treesitter.ignore_install = { ""haskell"" }
lvim.builtin.treesitter.highlight.enable = true
-- generic LSP settings
-- -- make sure server will always be installed even if the server is in skipped_servers list
-- lvim.lsp.installer.setup.ensure_installed = {
-- ""sumneko_lua"",
-- ""jsonls"",
-- }
-- -- change UI setting of `LspInstallInfo`
-- -- see <https://github.com/williamboman/nvim-lsp-installer#default-configuration>
-- lvim.lsp.installer.setup.ui.check_outdated_servers_on_open = false
-- lvim.lsp.installer.setup.ui.border = ""rounded""
-- lvim.lsp.installer.setup.ui.keymaps = {
-- uninstall_server = ""d"",
-- toggle_server_expand = ""o"",
-- }
-- ---@usage disable automatic installation of servers
-- lvim.lsp.installer.setup.automatic_installation = false
-- ---configure a server manually. !!Requires `:LvimCacheReset` to take effect!!
-- ---see the full default list `:lua print(vim.inspect(lvim.lsp.automatic_configuration.skipped_servers))`
-- vim.list_extend(lvim.lsp.automatic_configuration.skipped_servers, { ""pyright"" })
-- local opts = {} -- check the lspconfig documentation for a list of all possible options
-- require(""lvim.lsp.manager"").setup(""pyright"", opts)
-- ---remove a server from the skipped list, e.g. eslint, or emmet_ls. !!Requires `:LvimCacheReset` to take effect!!
-- ---`:LvimInfo` lists which server(s) are skipped for the current filetype
-- lvim.lsp.automatic_configuration.skipped_servers = vim.tbl_filter(function(server)
-- return server ~= ""emmet_ls""
-- end, lvim.lsp.automatic_configuration.skipped_servers)
-- -- you can set a custom on_attach function that will be used for all the language servers
-- -- See <https://github.com/neovim/nvim-lspconfig#keybindings-and-completion>
-- lvim.lsp.on_attach_callback = function(client, bufnr)
-- local function buf_set_option(...)
-- vim.api.nvim_buf_set_option(bufnr, ...)
-- end
-- --Enable completion triggered by <c-x><c-o>
-- buf_set_option(""omnifunc"", ""v:lua.vim.lsp.omnifunc"")
-- end
-- -- set a formatter, this will override the language server formatting capabilities (if it exists)
-- local formatters = require ""lvim.lsp.null-ls.formatters""
-- formatters.setup {
-- { command = ""black"", filetypes = { ""python"" } },
-- { command = ""isort"", filetypes = { ""python"" } },
-- {
-- -- each formatter accepts a list of options identical to https://github.com/jose-elias-alvarez/null-ls.nvim/blob/main/doc/BUILTINS.md#Configuration
-- command = ""prettier"",
-- ---@usage arguments to pass to the formatter
-- -- these cannot contain whitespaces, options such as `--line-width 80` become either `{'--line-width', '80'}` or `{'--line-width=80'}`
-- extra_args = { ""--print-with"", ""100"" },
-- ---@usage specify which filetypes to enable. By default a providers will attach to all the filetypes it supports.
-- filetypes = { ""typescript"", ""typescriptreact"" },
-- },
-- }
-- -- set additional linters
-- local linters = require ""lvim.lsp.null-ls.linters""
-- linters.setup {
-- { command = ""flake8"", filetypes = { ""python"" } },
-- {
-- -- each linter accepts a list of options identical to https://github.com/jose-elias-alvarez/null-ls.nvim/blob/main/doc/BUILTINS.md#Configuration
-- command = ""shellcheck"",
-- ---@usage arguments to pass to the formatter
-- -- these cannot contain whitespaces, options such as `--line-width 80` become either `{'--line-width', '80'}` or `{'--line-width=80'}`
-- extra_args = { ""--severity"", ""warning"" },
-- },
-- {
-- command = ""codespell"",
-- ---@usage specify which filetypes to enable. By default a providers will attach to all the filetypes it supports.
-- filetypes = { ""javascript"", ""python"" },
-- },
-- }
-- Additional Plugins
-- lvim.plugins = {
-- {
-- ""folke/trouble.nvim"",
-- cmd = ""TroubleToggle"",
-- },
-- }
-- Autocommands (https://neovim.io/doc/user/autocmd.html)
-- vim.api.nvim_create_autocmd(""BufEnter"", {
-- pattern = { ""*.json"", ""*.jsonc"" },
-- -- enable wrap mode for json files only
-- command = ""setlocal wrap"",
-- })
-- vim.api.nvim_create_autocmd(""FileType"", {
-- pattern = ""zsh"",
-- callback = function()
-- -- let treesitter use bash highlight for zsh files as well
-- require(""nvim-treesitter.highlight"").attach(0, ""bash"")
-- end,
-- })
```",0,0,msr
5778,"I found a file called sumneko_lua.lua
```
local default_workspace = {
library = {
vim.fn.expand ""$VIMRUNTIME"",
get_lvim_base_dir(),
require(""neodev.config"").types(),
},
checkThirdParty = false,
maxPreload = 5000,
preloadFileSize = 10000,
}
local add_packages_to_workspace = function(packages, config)
-- config.settings.Lua = config.settings.Lua or { workspace = default_workspace }
local runtimedirs = vim.api.nvim__get_runtime({ ""lua"" }, true, { is_lua = true })
local workspace = config.settings.Lua.workspace
for _, v in pairs(runtimedirs) do
for _, pack in ipairs(packages) do
if v:match(pack) and not vim.tbl_contains(workspace.library, v) then
table.insert(workspace.library, v)
end
end
end
end
local lspconfig = require ""lspconfig""
local make_on_new_config = function(on_new_config, _)
return lspconfig.util.add_hook_before(on_new_config, function(new_config, _)
local server_name = new_config.name
if server_name ~= ""sumneko_lua"" then
return
end
local plugins = { ""plenary.nvim"", ""telescope.nvim"", ""nvim-treesitter"", ""LuaSnip"" }
add_packages_to_workspace(plugins, new_config)
end)
end
lspconfig.util.default_config = vim.tbl_extend(""force"", lspconfig.util.default_config, {
on_new_config = make_on_new_config(lspconfig.util.default_config.on_new_config),
})
local opts = {
settings = {
Lua = {
telemetry = { enable = false },
runtime = {
version = ""LuaJIT"",
special = {
reload = ""require"",
},
},
diagnostics = {
globals = { ""vim"", ""lvim"", ""packer_plugins"", ""reload"" },
},
workspace = default_workspace,
},
},
}
return opts
```",1,0,msr
5782,"When a person's speech is just stupid, I can't tell whether he is fishing or really stupid.",0,1,msr
5784,"> The statement that people shouldn't expect support is so ridiculous it's hard to reply. If you have released software to the public with the expectations that it be used by any number of people you must expect they will ask for support. If you don't want people asking for support then you should not be releasing software period. I know they may not get the help they need but to expect people not ask or not expect to be able to ask is absurd
People asking for support could be expected, providing support is not expected. This software is [MIT licensed](https://github.com/sumneko/lua-language-server/blob/master/LICENSE), meaning there is no warranty, which is stated in plain English in this sentence:
> THE SOFTWARE IS PROVIDED ""AS IS""
That being said, many people here have received support, and we **never** said you **can't** ask for help - it is just simply not guaranteed that the issue will be solved, so please respect that and don't expect that Sumneko **MUST** solve your issue.
Anyway, this is completely off-topic, I was just asking you to respect our time and our efforts to assist you, but you continue to speak with disrespect. Respectful criticism is welcome. Saying it is incomprehensible, but not providing info on how things can be improved doesn't give us anywhere to improve.
If this is continued, I will just lock the issue, as I don't want to spend my spare time helping you if you are going to be rude and keep going in circles.
---
I have [provided a solution to the love library issue](https://github.com/sumneko/lua-language-server/issues/1686#issuecomment-1312378685) and provided you the exact steps and code you should need to fix that issue. You have not mentioned attempting the solution, so please try this and see if it solves the love library issue.
> But still i have no idea what the other pop up is. Or how to fix the original issue of why they dissabled it or the rest of the issue why it's not seeing anything defined in other files
What other pop up? I will look into changing the keyword for activating `luassert` so that it is harder to trigger. As for the other issue of everything being undefined, we would need some more info like some sample code and your directory structure for your project to make sure everything is accessible.",0,0,msr
5788,"### How are you using the lua-language-server?
NeoVim
### Which OS are you using?
Linux
### What is the issue affecting?
Diagnostics/Syntax Checking
### Expected Behaviour
to not complain about love not being defined. And not complain about everything defined in other files as undefined global.
### Actual Behaviour
Everything work just fine as expected before yesterday when i updated my system. Now i have almost as many errors as lines in my files everything is an undefined global.
### Reproduction steps
Try any lua project here is one my son and i finished last week. Though i had no errors at the time of finishing it. Now it does.
https://github.com/jeremiahcheatham/bat-brightness
My current project is not uploaded. But everything kicks out errors now.
### Additional Notes
_No response_
### Log File
_No response_",0,0,msr
5789,I'm aware of the issue. Will fix within a day or two.,0,0,msr
5791,"Sadly enough been delayed, as is clear by the fact the issue isn't closed.",0,1,msr
5794,"### App Name
prometheus
### SCALE Version
22.02.4
### App Version
7.0.1
### Application Events
```Shell
can't install, so no information available
```
### Application Logs
```Shell
can't install, so no information available
```
### Application Configuration
can't install, so no information available
### Describe the bug
Trying to install Prometheus, everything with default settings (just click next) results in error:
```
[EFAULT] Failed to install chart release: Error: INSTALLATION FAILED: YAML parse error on prometheus/templates/common.yaml: error converting YAML to JSON: yaml: line 54: could not find expected ':'
```
```
Error: Traceback (most recent call last):
File ""/usr/lib/python3/dist-packages/middlewared/job.py"", line 411, in run
await self.future
File ""/usr/lib/python3/dist-packages/middlewared/job.py"", line 446, in __run_body
rv = await self.method(*([self] + args))
File ""/usr/lib/python3/dist-packages/middlewared/schema.py"", line 1140, in nf
res = await f(*args, **kwargs)
File ""/usr/lib/python3/dist-packages/middlewared/schema.py"", line 1272, in nf
return await func(*args, **kwargs)
File ""/usr/lib/python3/dist-packages/middlewared/plugins/chart_releases_linux/chart_release.py"", line 481, in do_create
await self.middleware.call(
File ""/usr/lib/python3/dist-packages/middlewared/main.py"", line 1345, in call
return await self._call(
File ""/usr/lib/python3/dist-packages/middlewared/main.py"", line 1305, in _call
return await self.run_in_executor(prepared_call.executor, methodobj, *prepared_call.args)
File ""/usr/lib/python3/dist-packages/middlewared/main.py"", line 1206, in run_in_executor
return await loop.run_in_executor(pool, functools.partial(method, *args, **kwargs))
File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 52, in run
result = self.fn(*self.args, **self.kwargs)
File ""/usr/lib/python3/dist-packages/middlewared/plugins/chart_releases_linux/helm.py"", line 44, in helm_action
raise CallError(f'Failed to {tn_action} chart release: {stderr.decode()}')
middlewared.service_exception.CallError: [EFAULT] Failed to install chart release: Error: INSTALLATION FAILED: YAML parse error on prometheus/templates/common.yaml: error converting YAML to JSON: yaml: line 54: could not find expected ':'
```
### To Reproduce
1. go to ""Applications > Available Applications""
2. search for ""prometheus""
3. select ""prometheus 2.40.1_7.0.1""
4. click ""Install""
5. name the application ""prometheus""
6. always click next
7. Confirm Options
```
Application Name: prometheus
Version: 7.0.1
Operator Settings:
Enable: true
Log Level: info
Prometheus Settings:
Enable: true
Log Level: info
Retention: 31d
Max Retention Size:
Scrape interval: 15s
Evaluation interval: 30s
Disable Compaction: false
WAL Compression: false
Alertmanager Settings:
Enable: true
Log Level: info
Retention: 240h
Configure Service(s):
Main Service:
Service Type: LoadBalancer (Expose Ports)
LoadBalancer IP:
Service's Port(s) Configuration:
Main Service Port Configuration:
Port: 10086
alertmanager Service:
Service Type: LoadBalancer (Expose Ports)
LoadBalancer IP:
Service's Port(s) Configuration:
alertmanager Service Port Configuration:
Port: 10087
Show Expert Config: false
:
Main Ingress:
Enable Ingress: false
```
8. click Save
### Expected Behavior
Application can be installed
### Screenshots
-
### Additional Context
-
### I've read and agree with the following
- [X] I've checked all open and closed issues and my issue is not there.",0,0,msr
5795,What you are asking for is already the case. Tidy only applies to this repo: https://github.com/rust-lang/rust/blob/28a53cdb4695b71cb9ee39959df88542056479cd/src/tools/tidy/src/style.rs#L1,0,0,msr
5796,"This decision has already made, we don't revisit old decisions without new information.
I'm going to lock this since it has a tendency to bring out trolls.",0,0,msr
5797,"I don't see anything utterly offensive in `PROBLEMATIC_CONSTS`, why not let programmers have fun and talk about boobs in their code?",0,1,msr
5799,"@Sunhat With this attitude, don't count on any support at all. Imagine that somebody would talk in that way to you. Would you be motivated to help that person out?
The bug that you are encountering might not be important for others. Do mind your tone of voice and stay friendly. This not only applies to this issue tracker but for interactions with all living things.
Some interesting resources:
- https://blog.container-solutions.com/entitlement-in-open-source
- https://opensource.com/article/22/1/open-source-contributions-career
- https://www.youtube.com/watch?v=fMFjO2szDnk
I'll send you a bill for this life changing advice! 😅",0,1,msr
5801,"> If you take on an open-source project, you owe everyone everything. That's the sacrifice you chose. If you can't hack it, then don't do it.
I guess are differences come because we don't agree on this at all. If somebody invests time creating a library, you play by the rules of the creator / maintainer. Don't assume that you as a contributor can set the rules. A creator/maintainer owes you nothing. Not even if you come with the gift of opening an issue.
Personally, I think your stance on open source will leave both you and maintainers of open source software feel bad. There are no winners with such an attitude. You can find the code of conduct of Laravel here: https://laravel.com/docs/9.x/contributions#code-of-conduct
> Participants must ensure that their language and actions are free of personal attacks and disparaging personal remarks.
> Behavior that can be reasonably considered harassment will not be tolerated.
I think suggestion that someone should be fired surely goes against to rules set by the creators / maintainers.",1,1,msr
5805,"Unfortunately, we cannot reproduce the issue with your settings.
Please record a HAR file of broken page and send it + link of this issue it to `filters@adguard.com`
How to get a HAR file:
https://toolbox.googleapps.com/apps/har_analyzer/",0,0,msr
5806,"### Issue URL (Ads)
[https://daft.sex/watch/-207003944_456239828](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-207003944_456239828)
### Comment
> @jellizaveta please help
> > screenshot with request below
> > suggested privacy filter:
> ||daft.sex/*sw.js
> > reference:
> https://github.com/AdguardTeam/AdguardFilters/commit/bbd12f70e363d3dbc69e573ddcbd5a705784963c
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/3bE3KBd.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",0,0,msr
5808,"What's the URL? In fact, it's `NSFW-18+` site.
Anyway, I don't see the request on my end, checked with different IPs.",0,0,msr
5809,"### Issue URL (Ads)
[https://daft.sex/watch/-209797543_456239978](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-209797543_456239978)
### Comment
> It could be your location, please use a VPN with IP in Latin America or Spain.
> > The page works fine with filter added in ""My filters"".
> > suggested privacy filter:
> ||daft.sex/*sw.js
> > reference:
> https://github.com/AdguardTeam/AdguardFilters/commit/bbd12f70e363d3dbc69e573ddcbd5a705784963c
### Screenshots
<details>
<summary>Screenshot 1</summary>
![Screenshot 1](https://reports-img.adguard.com/kGA0O90.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",0,1,msr
5810,"### Issue URL (Ads)
[NSFW] [https://daft.sex/watch/-209797543_456239978](https://adguardteam.github.io/AnonymousRedirect/redirect.html?url=https%3A%2F%2Fdaft.sex%2Fwatch%2F-209797543_456239978)
### Comment
> @ameshkov, please you could interfere in this issue.
> > arbitrarily blocked this issue: #135799
> > I provided screenshots with to show the request. There is no consistency in the decisions of the filter maintainers of the AdGuard team, another maintainer adds a suggested filter but another maintainer doesn't, I don't understand.
> > suggested privacy filter:
> ||daft.sex/*sw.js
> > reference:
> https://github.com/AdguardTeam/AdguardFilters/commit/bbd12f70e363d3dbc69e573ddcbd5a705784963c
### [NSFW] Screenshots
<details>
<summary>Screenshot 1</summary>
[Screenshot 1](https://reports-img.adguard.com/pW0rikl.jpg)
</details>
<details>
<summary>Screenshot 2</summary>
[Screenshot 2](https://reports-img.adguard.com/kGA0O90.jpg)
</details>
### System configuration
Information | value
--- | ---
Platform: | uBlock Origin
Browser: | Firefox Mobile
Filters: | <b>Ad Blocking:</b><br/>AdGuard Base, <br/>AdGuard Mobile Ads, <br/>EasyList<br/><br/><b>Privacy:</b><br/>AdGuard Tracking Protection, <br/>AdGuard URL Tracking, <br/>EasyPrivacy, <br/>Peter Lowe's Blocklist, <br/>Fanboy's Enhanced Tracking List, <br/>Fanboy's Anti-Facebook List<br/><br/><b>Social Widgets:</b><br/>AdGuard Social Media<br/><br/><b>Annoyances:</b><br/>AdGuard Annoyances, <br/>Fanboy's Annoyances<br/><br/><b>Security:</b><br/>Online Malicious URL Blocklist<br/><br/><b>Language-specific:</b><br/>AdGuard Spanish/Portuguese, <br/>EasyList Spanish",0,1,msr
5815,"Haha noob you want to put that to the test? I've played it for 15 years...
On November 27, 2022 22:16:40 Maxim Zhuchkov ***@***.***> wrote:
>
> I think you need to increase your APM (Actions Per Second). Sorry to break > news for you man but ""barely get a tank factory built and a battalion of > enemy tanks show up at my doorstep"" sounds like you are just not > experienced enough (noob). Try watching (not playing!) multiplayer games or > replays from other people, you can learn that way.
> Also I suggest you to set up hotkeys, you can find list of important ones > in Discord.—
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: > ***@***.***>",0,1,msr
5816,"> Haha noob you want to put that to the test? I've played it for 15 years...
Sure man, want to 1x1 *right now*? Join via IP `6.tcp.ngrok.io:16186`. I will be online for 15 minutes. If you will not come you are free to ping me in Discord when you are ready.
P.S. use latest supported version (4.3.2).",0,0,msr
5817,"We can go
On November 27, 2022 23:27:10 Maxim Zhuchkov ***@***.***> wrote:
>
> Haha noob you want to put that to the test? I've played it for 15 years...
> Sure man, want to 1x1 right now? Join via IP 6.tcp.ngrok.io:16186. I will > be online for 15 minutes. If you will not come you are free to ping me in > Discord when you are ready.
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: > ***@***.***>",0,0,msr
5818,"So as expected, issuer is a complete noob and does not know how to play the game *and* has APM of a sloth. My POV will be available shortly after Youtube processes the video: https://youtu.be/T3Nlhzc2yjQ. Zip compressed replay file: [20221128_074207_multiplay_p0.wzrp.zip](https://github.com/Warzone2100/warzone2100/files/10100576/20221128_074207_multiplay_p0.wzrp.zip)
Let this be another show-game in my collection of noobs claiming that game is broken or everyone is cheating.",1,1,msr
5821,"Like I said, I never played a human before you, I have studied very in-depth all of the AIs within the game. When we meet again I will show you that you are just another AI that I have to get a handle on.",0,1,msr
5825,"**Describe the bug**
A clear and concise description of what the bug is.
When I play easy, it is too easy regardless of the map. When I pick medium, one difficulty level higher, I barely get a tank factory built and a battalion of enemy tanks show up at my doorstep. That to me is unplayable. **To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error
**Expected behavior**
A clear and concise description of what you expected to happen.
There should be a steady flow, medium should be moved to insane.
**Screenshots or Videos**
If applicable, add screenshots to help explain your problem.
**Your System:**
- OS: [e.g. Windows 10, Linux (Ubuntu 18.04)]
- Game version: [e.g. 3.3.0, commit hash]
**Additional context**
Add any other context about the problem here.",0,1,msr
5826,"If anyone else gets the same crash, the key part of the backtrace in https://bugzilla.redhat.com/show_bug.cgi?id=2150392
seems to be:
```
Thread 1 (Thread 0x7f49a6faef00 (LWP 78232)):
#0 0x00007f49a84fef9d in g_hash_table_fetch_key_or_value (is_big=3, index=3072172, a=0x557a41cf0aa0) at ../glib/ghash.c:394
No locals.
#1 g_hash_table_foreach (hash_table=0x557a419b1120, func=0x557a40049ee0 <get_one_value_callback>, user_data=0x7ffe86e068d0) at ../glib/ghash.c:2094
node_hash = 357688284
node_key = <optimized out>
node_value = <optimized out>
i = 3072172
version = 18
__func__ = ""g_hash_table_foreach""
```
I looked at the backtrace of another report of the same or a similar issue,again crash is in glib/ghash.c https://bugzilla.redhat.com/show_bug.cgi?id=1888948
A mention of dropbox (using caja-dropbox extension?) in that backtrace, and a crash on a ""file not found"" error:
```
#0 0x00007f444c07d96d in g_hash_table_fetch_key_or_value (is_big=3, index=5037142, a=0x560a0d3b8d50) at ../glib/ghash.c:2061
2061	../glib/ghash.c: No such file or directory.
[Current thread is 1 (Thread 0x7f444ac1af40 (LWP 104797))]
```
Never had this crash myself on Debian Unstable with locally built MATE, GTK3, and glib, but probably different compiler options for everything, different glib version, different GTK3 version/build options etc. In one backtrace we have ""no locals"" at ../glib/ghash.c:394 and in the other we have ""file not found"" at ./glib/ghash.c:2061
Assuming this does not come from a glib bug (which could be version-specific) we may be passing bad data to it somehow.
Again, I can't duplicate this but if we get more reports of this, hopefully this information will jump-start digging into it for whoever can duplicate it.",0,0,msr
5827,"I reported an apparently spontaneous crash to fedora, hoping the crash data produced by the ABRT tool could somehow be useful: https://bugzilla.redhat.com/show_bug.cgi?id=2150392
I've been instructed to move the issue upstream, so here I am. :-)",0,0,msr
5829,"### Prerequisites
- [X] I have written a descriptive issue title
- [X] I have searched existing issues to ensure the issue has not already been raised
### Issue
I tried to start a demo following the instructions in the Getting Started documentation and had various problems along the way. It took 2 hours and still couldn't run through the demo program. Garbage documentation can affect the interest and motivation of the taster to use it. I wish the getting started documentation was more detailed, or just give a complete minimized demo!",0,1,msr
5834,Same,0,0,msr
5835,same,0,0,msr
5836,"same here:
```
(@v1.7) pkg> st -m Plots RecipesBase
Status `C:\Users\ms2106\.julia\environments\v1.7\Manifest.toml`
[3cdcf5f2] RecipesBase v1.3.2
```",0,0,msr
5842,Same,0,0,msr
5843,"Yeah, well I proposed to yank the broken `SnoopPrecompile` in https://github.com/JuliaRegistries/General/pull/74176.
This is not a `Plots` issue, and is induced by an upstream dependency.",0,0,msr
5845,"## Details
I cannot precompile Plots due to errors in precompiling RecipesBase and RecipesPipeline:
```julia
Precompiling project...
✗ RecipesBase
✗ RecipesPipeline
✗ Plots
0 dependencies successfully precompiled in 3 seconds. 204 already precompiled.
ERROR: The following 1 direct dependency failed to precompile:
Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]
Failed to precompile Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] to /home/gortibaldik/.julia/compiled/v1.8/Plots/jl_0EhmVw.
ERROR: LoadError: syntax: Global method definition around /home/gortibaldik/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:607 needs to be placed at the top level, or use ""eval"".
Stacktrace:
[1] top-level scope
@ ~/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:600
[2] include
@ ./Base.jl:419 [inlined]
[3] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String)
@ Base ./loading.jl:1554
[4] top-level scope
@ stdin:1
in expression starting at /home/gortibaldik/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:1
in expression starting at stdin:1
ERROR: LoadError: Failed to precompile RecipesBase [3cdcf5f2-1ef4-517c-9805-6587b60abb01] to /home/gortibaldik/.julia/compiled/v1.8/RecipesBase/jl_JcrErn.
Stacktrace:
[1] error(s::String)
@ Base ./error.jl:35
[2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool)
@ Base ./loading.jl:1707
[3] compilecache
@ ./loading.jl:1651 [inlined]
[4] _require(pkg::Base.PkgId)
@ Base ./loading.jl:1337
[5] _require_prelocked(uuidkey::Base.PkgId)
@ Base ./loading.jl:1200
[6] macro expansion
@ ./loading.jl:1180 [inlined]
[7] macro expansion
@ ./lock.jl:223 [inlined]
[8] require(into::Module, mod::Symbol)
@ Base ./loading.jl:1144
[9] include
@ ./Base.jl:419 [inlined]
[10] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing)
@ Base ./loading.jl:1554
[11] top-level scope
@ stdin:1
in expression starting at /home/gortibaldik/.julia/packages/Plots/Hxe7H/src/Plots.jl:1
in expression starting at stdin:1
```
### Versions
Plots.jl version: Plots v1.37.2
Output of `versioninfo()`:
```julia
Julia Version 1.8.2
Commit 36034abf260 (2022-09-29 15:21 UTC)
Platform Info:
OS: Linux (x86_64-linux-gnu)
CPU: 20 × 12th Gen Intel(R) Core(TM) i7-12700H
WORD_SIZE: 64
LIBM: libopenlibm
LLVM: libLLVM-13.0.1 (ORCJIT, goldmont)
Threads: 1 on 20 virtual cores
```",0,0,msr
5846,@MrGeneration Can you add a note to the documentation that the migration path is currently broken linked to this issue?,0,0,msr
5849,"Alternative solution idea: maybe we could create a rake task that reads the DB before migration, and outputs [pgloader casting rules](https://pgloader.readthedocs.io/en/latest/ref/mysql.html?highlight=cast#mysql-database-casting-rules) which would perform the migration correctly in the first place.",0,0,msr
5850,"There are currently concerns regarding the performance of the PoC attached to this issue, since it uses `ActiveRecord::Migration` instances, creates new columns, migrates the data, drop, then rename the columns... Also, it's not clear if it would have additional side-effects, since it is supposed to be run in the application/Rails context. A benchmark with a large data set and both solutions in place (PoC vs `pgloader`) might try dispel these concerns, but it hasn't been tried yet.
Initial research into `pgloader` casting rules resulted in some findings:
* It doesn't look like `pgloader` provides a suitable transformation function for a JSON array into a PostgreSQL array which can be used as part of its `CAST` command (on the column type level.) We could not find an example either, and trying some stuff including expected DSL like `""text array""` didn't lead to meaningful results. This doesn't mean it's not supported, just that we could not identify it at this time.
* [An issue for `pgloader`](https://github.com/dimitri/pgloader/issues/881) with a similar problem provided a possible workaround via `AFTER LOAD DO` block, which can transform specific columns on specific tables only, but it does use a `translate` function which seems to work for our JSON => array use-case. (Note that this same function doesn't seem to be provided in the `CAST` context.) Here is a working example for `pgloader` post-processing block for one `multi_tree_select` column on the `tickets` table:
```
...
AFTER LOAD DO
$$ alter table tickets alter column multi_tree_select type text[] using translate(multi_tree_select::text, '[]', '{}')::text[] $$
...
```
A proposed rake pre-task could try to prepare the complete `pgloader` script and include one line per affected column.",0,0,msr
5851,"@dvuckovic that's kindof what I hoped for. So I'm all for pursuing this further. It should be best to get the migration right in the first place.
Please note that @rolfschmidt included at least one column for this transformation which does not come from object attributes (IIRC `PublicLink`). We should try to include all candidates, of course.",0,0,msr
5853,"@dvuckovic plus, this would not work with MariaDB, as it does not have native json types.
Perhaps it would be a good idea to verify your solution also for MariaDB -> PG migration.",0,0,msr
5854,"@mgruner Correct, in MariaDB these columns are of `longtext` type.
I just verified that the same script works on MariaDB as well, columns are properly migrated to `array` type on PostgreSQL.",0,0,msr
5857,"@MrGeneration Please ignore my previous comment, it was just a minor finding during testing.
The documentation PR you linked to looks correct and the new command applies to both MySQL and MariaDB backends. With the fix in place, the migrated columns are always of the same data type (`ARRAY`), which is what you would expect.",0,0,msr
5858,"### Used Zammad Version
5.3
### Environment
- Installation method: package
- Operating system (if you're unsure: `cat /etc/os-release` ): [e.g. debian 10.4, ubuntu 20.04]
- Database + version: mariadb
- Elasticsearch version: [e.g. 7.17]
- Browser + version: [e.g. chrome 83, safari 14, firefox 105]
### Actual behaviour
- Migration does not convert the json values to the array structure of postgres. I think there is a rake task missing which will look up all values in the database and convert the json structures to array.
I attached a PoC how it could look like:
[fix_maria_migration.zip](https://github.com/zammad/zammad/files/10245529/fix_maria_migration.zip)
### Expected behaviour
- Migration should work.
https://docs.zammad.org/en/latest/appendix/migrate-to-postgresql.html
### Steps to reproduce the behaviour
Migrate from mysql/mariadb to postgres, but before create multi tree selects and fill up some tickets:
https://docs.zammad.org/en/latest/appendix/migrate-to-postgresql.html
### Support Ticket
Ticket#10115329
### I'm sure this is a bug and no feature request or a general question.
yes
### ToDo:
- [x] Investigate a post or pre pgloader solution
- [x] Decide and implement a solution
- [x] Update the current documentation
- [x] Adding a integration test of the database migration from mysql/mariadb to postgres",0,0,msr
5860,"> > It reduces allocations of MessageProperties. I used the following sample to analyse memory allocations.
> > Could you try do the same but with some headers added? I mean `SetHeader` method : `new MessageProperties().SetHeader(""a"", ""value1"").SetHeader(""b, ""value2""),`
> > Of cource PR reduces allocations of MessageProperties itself, but some internal code begins to allocate memory in other places.
```
var properties = new MessageProperties();
properties.Headers[""a""] = ""value1"";
properties.Headers[""b""] = ""value2"";
```
This sample allocates memory under the hood as well :)
ImmutableDictionary allocates only logarithmic memory in such a case, which is obviously worse than Dictionary, but not that bad.
Let's move this discussion about Headers type to a separate PR, which will probably not be implemented in v8, because `IBasicProperties.Headers`(there are two implementations, class and readonly struct based) has `IDictionary<string, object?>` type in master branch.",0,0,msr
5862,~~Where the hack is the squash rebase button in Microsoft Github?~~ 🦺,0,1,msr
5865,This also fixes #6268 and #6204,0,0,msr
5867,"This should've been fixed by #6298, so good to close, right?",0,0,msr
5868,(Thanks for your work regardless @steakknife),0,0,msr
5877,"And the difficulty in trying to accomplish it. The prompt cmd would have to know where the previous prompt was displayed and then go back to rewrite it, without affecting anything else.",0,0,msr
5878,"Hm, I don't think thats an issue.
Count the lines since the last command, write into the buffer at that location, done.
Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.",1,0,msr
5879,"htop is using ncurses.
Try your method, then submit it once you have it working with both static and scrolling output. I'm looking forward to it.
How do you count the lines?
What buffer are you talking about?",0,0,msr
5880,"> Hm, I don't think thats an issue. Count the lines since the last command, write into the buffer at that location, done.
> > Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.
As trivial as this is, I'm surprised you haven't submitted a patch yet. If you're stuck on something, post it and maybe someone can help you out.",0,0,msr
5882,"Excuse me for not answering.
I haven't had a look on this for the last few days.
First of all, what I didn't notice is that this is written in shell scripts. I thought this was a C application that extended the zsh shell/passed all inputs to it and gave the user its own custom shell with custom themes.
In regards to the ""No program has the knowledge..."": I just said it was trivial to count lines, wait for the exit code to return and edit the first character (the prompt symbol) to the according color. Like I said, I wasn't aware that this is a shell script. Of course, without any external help this cant be done.
I've taken a look, and since e.g. Github-Repos and their branch can be displayed (e.g. master), I have noticed the extra github-helper program thats started in the background.
I assume that it is possible to pass the stdout/stdin from the session to another program, count lines/return code and just tell zsh what should happen or just modify the buffer directly.
I would've forked the repo and ran my own tests/integrations, but due to the lack of knowledge in shell-scripting im basically through the topic. I cant really contribute anything here, I was just curious if there was some option I was missing, since I only found some posts online that were interested in a similar solution / were bothered by this.",0,1,msr
5885,"To be extra clear, the implementation of this is not feasible because of the *technical* reasons that Richard mentioned. He is 100% correct on that. It is difficult to understand why it's not just ""counting lines"" when you don't have a full understanding of how a terminal works. But my point about the tone still stands. End of discussion.",0,1,msr
5888,"`5.10.7` works as it should and does not include these most recent attempts to break the library.
Those choosing not to vendor-lock themselves into unnecessary pointless parasitic CDN subscriptions can use this version and avoid any future _""upgrades""_.
This might explain why v5 has 3x the weekly downloads v6 does.",1,0,msr
5893,"`Uncaught (in promise) Error: Failed to get RTC instance not yet initialized.`
Seems in the process of attempting to vendor-lock with the CDN based implementations you've broken the library itself.
...
```
import tinymce from 'tinymce';
import 'tinymce/icons/default';
import 'tinymce/themes/silver';
import 'tinymce/models/dom';
import 'tinymce/skins/ui/oxide/skin.css';
import 'tinymce/plugins/advlist';
import 'tinymce/plugins/code'; // FIXME: Not picking up changes made in editor view? NOTE: Works again in v5
import 'tinymce/plugins/emoticons';
import 'tinymce/plugins/emoticons/js/emojis';
import 'tinymce/plugins/link';
import 'tinymce/plugins/lists';
import 'tinymce/plugins/table';
import contentUiSkinCss from 'tinymce/skins/ui/oxide/content.css';
import contentCss from 'tinymce/skins/content/default/content.css';
```
```
tinymce.init({
selector: `#mg-wysiwyg-${this._uid}`,
height: parseInt(this.height),
readonly: this.disabled,
//min_height: this.height,
placeholder: this.placeholder,
resize: 'both',
menubar: false,
plugins: ['advlist', 'code', 'emoticons', 'link', 'lists', 'table'],
toolbar: 'undo redo | bold italic forecolor backcolor | bullist numlist checklist table | link emoticons | code',
model: 'dom', // FIXME: Forcing ""dom"" because non-existant RTC ""Premium plugin"" getting in the way
// Configuration required for local self-install
skin: false,
content_css: false,
content_style: contentUiSkinCss.toString() + '\n' + contentCss.toString(),
//promotion: false, // Oh yeah we really want adverts!
// Bound to ""change keyup"" events as per https://github.com/tinymce/tinymce-vue/blob/b41c2a47eb8d9629eb01a41d6c6c633651f2d078/src/main/ts/Utils.ts#L115-L119
init_instance_callback: editor => {
editor.on('change keyup', e => {
this.data = editor.getContent({ format: this.syntax })
})
},
});
```",1,0,msr
5895,"> It's `PreUpdateEventArgs | PrePersistEventArgs`.
Let's have the full version of your beautiful code
```php
public function index(PrePersistEventArgs | PreUpdateEventArgs | PreRemoveEventArgs | PostPersistEventArgs | PostUpdateEventArgs | PostRemoveEventArgs | PostLoadEventArgs $args, $skipOverwriteInitial): void
```
# Seriously?",0,1,msr
5896,"https://github.com/doctrine/orm/blame/2.14.x/UPGRADE.md#L61
### BC Break Report
BaseLifecycleEventArgs
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| BC Break | yes
| Version | 2.14.1
#### Summary
Hello. Please provide an interface
```php
public function getSubscribedEvents(): array
{
return [
Events::prePersist,
Events::preUpdate,
];
}
public function preUpdate(PreUpdateEventArgs $args): void # <-- PreUpdateEventArgs
{
$this->index($args, false); # <--
}
public function prePersist(PrePersistEventArgs $args): void # <-- PrePersistEventArgs
{
$this->index($args, true); # <--
}
public function index($args, $skipOverwriteInitial): void # <-- [????????] $args ?
{
$entity = $args->getObject();
if (!$entity) {
return;
}
$this->entityDateUpdater->updateFields($entity, $skipOverwriteInitial);
}
```
### Previously all types were LifecycleEventArgs
Right now I can't determine the type for `$args` in `index` method :(",0,0,msr
5897,"We already upgraded, you have to wait for the next release.
https://github.com/mitmproxy/mitmproxy/blob/be02b1e298dfb1b218007f824c4cf345c53b0cee/pyproject.toml#L37
https://github.com/mitmproxy/mitmproxy/commit/8c6ec5cb56fbf4961806ed27b4974c440df59e87
https://github.com/search?q=repo%3Amitmproxy%2Fmitmproxy+cryptography&type=issues
https://github.com/mitmproxy/mitmproxy/issues/5966
Feel free to re-open if I miss something. I have no idea what `poetry` is.",0,0,msr
5900,"@Prinzhorn Do you have a target date for the next release? Our customers in regulated industries (banking, federal, etc.) are prohibited by regulations from using s/w with known High and Critical severity vulnerabilities. I'm trying to formulate our case for waiting, but need some kind of target date.",0,0,msr
5901,"@FrugalGuy: If you are interested in timely patch releases to fix your company's compliance requirements, I'm happy to set up a support contract. Email is on my profile. :-)",0,0,msr
5903,"@FrugalGuy has just sent me genuine apology, which I truly appreciate. Please be nice and assume good intentions. ❤",0,0,msr
5905,Can you please a look at the conversation at https://github.com/doctrine/DoctrineMigrationsBundle/pull/492?,0,0,msr
5906,So surely the simple solution is just to bump the version number and add new code to the new major version? I can't do that only your project can. I'm not seeing what the difficulty is here?,0,0,msr
5908,"> after you provide a convincing apology for your behavior of course.
And that ends my contributions here and earns you a place on my block list. Im sorry you feel that way. Its your project do as you want. I was only trying to be helpful. No one was demanding anything. The fact is, the upstream project has chosen to add the return type, and now this project outputs warnings and you are choosing to ignore that rather than fix that. Your choice I guess.",0,1,msr
5915,"> !! 2023-04-08T19:35:15+00:00 [info] User Deprecated: Method ""Symfony\Component\HttpKernel\Bundle\Bundle::build()"" might add ""void"" as a native return type declaration in the future. Do the same in child class ""Doctrine\Bundle\FixturesBundle\DoctrineFixturesBundle"" now to avoid errors or add an explicit @return annotation to suppress this message.",1,0,msr
5916,"You are confusing MINOR and PATCH. 2.14.2 is not a minor release, it is a patch release. BUT. Short aliases are not a feature of `doctrine/orm`, they are a feature of `doctrine/persistence`, which means that if you rely on them, you (or rather, `jmose/command-scheduler-bundle`) should have `doctrine/persistence` in composer.json, and you should still be using `doctrine/persistence` 2 instead of `doctrine/persistence`3. If that were still the case, you would get a deprecation, not a crash.
A solution for you personally can be to downgrade to `doctrine/persistence` 2, and address the issue when you have the time.
> No error messages when using doctrine/orm 2.14.1
OK, there were no error messages, but was it working? I believe it either didn't, or worked by accident. If you were using persistence 3 at the time, then you were not using short aliases.
To fully understand the issue, it would be great to have [a stack trace](https://symfony.com/doc/current/contributing/code/stack_trace.html)",0,1,msr
5917,"In my defence it was 4am (now 8am) zzz Personally I just removed the bundle and replaced with zenstruck scheduler within 20 mins and deployed that to production already - it's a far superior product anyway
You can close this as it doesn't really affect me now but the fact remains updating a PATCH version broke otherwise working (albeit old bundle) code from working",0,0,msr
5918,"As I said, I suspect it wasn't actually working.",0,0,msr
5919,"The scheduler has been running every min of every day for almost 10 years. It's the backbone of a service that has made me several million pounds... but hey, it's fixed now and I can go to bed. Off topic: Also, when someone sponsors you $100 it would be nice, the most minimal thing to do, is to acknowledge it... but you are not alone, many developers don't even acknowledge GitHub sponsorships - ah well.",0,1,msr
5921,"<!--
Before reporting a BC break, please consult the upgrading document to make sure it's not an expected change: https://github.com/doctrine/orm/blob/2.9.x/UPGRADE.md
-->
### BC Break Report
<!-- Fill in the relevant information below to help triage your issue. -->
| Q | A
|------------ | ------
| BC Break | yes
| Version | 2.14.2
#### Summary
YES I UNDERSTAND that the relied upon feature (short namespace alias) is deprecated BUT upgrading in a MINOR release from 2.14.1 to 2.14.2 should not break an app that was working fine in 2.14.1 - that goes against SEMVER
#### Previous behavior
No error messages when using doctrine/orm 2.14.1 #### Current behavior
after composer update to doctrine/orm 2.14.2 from 2.14.1 - a minor release, now getting app crashes due to relying on deprecated features (I know I know) This is caused by now throwing Exceptions where no exceptions were previously thrown (because of a bug) https://github.com/doctrine/orm/pull/10489
<img width=""1280"" alt=""ScreenShot-2023-04-13-03 44 02"" src=""https://user-images.githubusercontent.com/400092/231634531-65ba81f5-41f7-41af-b810-c8e54e973df5.png"">
#### How to reproduce
The related package is https://github.com/j-guyon/CommandSchedulerBundle which is a 3 year old release. Yes I know I know I know... but still, a minor release of doctrine/orm should not have breaking changes",0,1,msr
5923,"I'm guessing not, but am open to that. Actually, completely other thing that I was doing lead me to this ambiguity. So for example, this function works like charm:
```php
<?php
function getIsDaylightSaving($zoneid = 'America/Los_Angeles') {
try {
$date = new DateTime('now', new DateTimeZone($zoneid));
return $date->format('I');
} catch (Exception $e) {
return 400;
}
}
```
So if one try to do following:
```php
<?php
//get daylight saving time status from timezone identifier, $dst = getIsDaylightSaving('America/New_York');
//OUTPUT = 1 //which is OK
//but if I use foo timezone identifier like (this one differs built in IANA timezone db):
$dst = getIsDaylightSaving('ABBA/Mama_Mia');
//The instance new DateTime('now', new DateTimeZone($zoneid)); will throw an exception which will be caught in this function
//so the OUTPUT will be 400 //which is also OK.
```
So my point is that I was surprised that PHP didn't care about timezone offsets, but do care about timezone identifiers, since timezone offsets are only the following: ```php
<?php
$offsets = array('+00:00', '+01:00', '+02:00', '+03:00', '+03:30', '+04:00', '+04:30', '+05:00', '+05:30', '+05:45', '+06:00', '+06:30', '+07:00', '+08:00', '+08:45', '+09:00', '+09:30', '+10:00', '+10:30', '+11:00', '+12:00', '+12:45', '+13:00', '+14:00', '-01:00', '-02:00', '-02:30', '-03:00', '-04:00', '-05:00', '-06:00', '-07:00', '-08:00', '-09:00', '-09:30', '-10:00', '-11:00', '-12:00');
```
For that reason, I think it is a bug, or even some unfinished job. Anyway. I 'dodged a bullet' simply by checking for the offset myself. Everything else is explained in my first comment (i.e., _$time_ variables are expected user input variables).
Hope it helps.",0,0,msr
5924,"The offset isn't some magic string that identifies a location like the named timezones. It's an offset. An amount of hours and minutes ahead or behind UTC.
So what's the added value that comes with restricting the offset to only those that are in active, official use by some area of the world? What problems does doing so solve, or what advantages does it offer?
If you're getting these values from the user then how do you know that the user didn't intend -01:10 for some reason? And why ask them for numeric offsets when the named ones are so much more human-friendly?",0,1,msr
5925,"> The offset isn't some magic string that identifies a location like the named timezones. It's an offset. An amount of hours and minutes ahead or behind UTC.
> > So what's the added value that comes with restricting the offset to only those that are in active, official use by some area of the world? What problems does doing so solve, or what advantages does it offer?
> > If you're getting these values from the user then how do you know that the user didn't intend -01:10 for some reason? And why ask them for numeric offsets when the named ones are so much more human-friendly?
What is this comment? Some kind of words game? Joke? That I am not into? Apropos what is human-friendly, or not, all those values can be a device snapshot. Some distant device snapshot having an application installed on it, that is programmed by a distant developer on certain distant operating system, with some distant programming language, that a developer will use, through his application, to fetch certain data... Now, that developer can be a professional, but can also be not-professional. And now going deeper in that story that here is being presented to my report is pure game of words as this answer is. Yada yada yada ... Please don't waste my time or yours on unconstructive comments.
That is not a topic here. Anyone's programming tasks, styles or stuff like that is not a topic here. My point is that a PHP allows -01:10 value where it shouldn't. The end. Schluss. Ende. And that is all. And I didn't expect it to do so, to my surprise, though I use PHP on a daily basis since it's version 3.",0,1,msr
5926,"That was a lot of words to say ""it's wrong because I think it's wrong"".
Why is it wrong to allow an offset that isn't in use by some (named) timezone? Why must it only allow offsets that can be found applied in (named) timezones?",0,1,msr
5928,"And again, 'that was a lot...' - you are wasting my time.",0,1,msr
5934,Why would you do all this? Why? Why spin me? Why? Are you all tested? High on whats the name of the drug oh yea are you al high on cocaine? What are you high on?,0,1,msr
5937,Damn right we don't! And the paragraph you're quoting does not document new behavior. It merely newly documents that using the ORM does not and never did guarantee any particular insertion order.,1,1,msr
5938,"FTR, duplicate of #10864",0,0,msr
5939,"Since release v2.16 this line of code won't save the entities in order of the loop anymore:
```
foreach ($importDossier->getKlanten() as $klant) {
$klant->setIsActive(true);
$this->entityManager->persist($klant);
}
```
The last line to be executed will get an earlier id in the DB than the first iteration of the loop...which seems weird. Is this expected behaviour?
I noticed work was done on the commit order here: https://github.com/doctrine/orm/pull/10547",0,0,msr
5942,"Open Source projects have recently had many nonsensical CVE reports from people scanning the commit logs. If they see messages like ""leak"" or other issue, they file a CVE. Presumably in order to either get some minor fame, or to get some kind of bounty.
We do not participate in such issues.
In this case, the code is pretty obvious. On top of that, it's sheer laziness to ask a question instead of reading the code.
And even without that, the issue template you deleted has explicit instructions for what is a good issue, and what information we need from an issue.
This comment is not helpful, and is not useful. We are not going to go down the path of explaining the code to people who can't be bothered to read the code and think about it.",0,1,msr
5945,"Hi,
I'm going to be polite. This non-sense is baffling: [Setup-pdf.js](https://github.com/mozilla/pdf.js/wiki/Setup-pdf.js-in-a-website)
And from an older issue: [Issue 9210](https://github.com/mozilla/pdf.js/issues/9210#issuecomment-347834276) Quote: **""Hence we don't want to make it too simple to use it directly.""** Honestly, what is that????
What I'm supposed to do? Go through the ~2000 LOC of `PDFViewerApplication` from `web/viewer.js` to just make the full viewer work? Now I think I understand **why** it wasn't added under `examples`. Can you please explain what's the whole point of this crap?
Maybe you can learn a bit from Leaflet, simply adding proper attribution should be enough provided you want credit. Please make things simpler and easier, and not harder as quoted. No one wants to waste time.",1,1,msr
5946,"(a) This was decided almost a year ago,, and (b) if you want to discuss this more then please hop on the internals mailing list.
But before you do, realize that ""next"" is a rather common term used with random number sequences - as it is, in fact, basically a matter of _sequences_ and not simply conjuring some number out of a hat. That means PHP would actually be joining a club whose members use the word ""next"" when describing their randomness APIs:
* [Python 3's `random.random()`](https://docs.python.org/3/library/random.html) is documented as ""return[ing] the next random floating point number in the range 0.0 <= X < 1.0"".
* [Java's `java.util.Random` class](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Random.html) has similar methods like ""next"", ""nextBoolean"", ""nextBytes"", and so on.
* Unsurprisingly, [Microsoft Java's own `System.Random` class](https://learn.microsoft.com/en-us/dotnet/api/system.random?view=net-7.0) has a variety of methods named ""Next"", ""NextBytes"", ""NextDouble"", ""NextInt64"", and so on.
* [Rust's `rand::RngCore` trait](https://docs.rs/rand/latest/rand/trait.RngCore.html), which is its lowest-level ""interface"" for randomness, names its two core methods ""next_u32"" and ""next_u64"".
So with any luck, allow me to offer my congratulations on you learning why the name ""nextFloat"" is not misleading after all.
(edited for humor)",0,1,msr
5947,"> This was decided almost a year ago,,
What kind of argument is that? As long as the stable is not issued, these things can be fixed, right? Similar to [PhpToken::getAll() vs PhpToken::tokenize](https://bugs.php.net/bug.php?id=80328). > But before you do, realize that ""next"" is a rather common term used with random number sequences But beware, those libraries don't combine *get* and *next* at the same time! They have all methods named *next*. That's perfectly fine. The problem is that in PHP we have `getFloat()` and `nextFloat()` at the same time, and the only difference is that the latter has a hardcoded interval. That's nonsense. Is it really not obvious to you?",0,1,msr
5949,🤦🏼‍♂️,0,1,msr
5950,🤦 I'm so glad I left PHP world many years ago...,0,0,msr
5958,"What the heck is going on here with all these spam attempts. I just don't get it, I guess they didn't get enough attention as a child.",0,1,msr
5962,"Anything ""new SQL stuff"" which contains named parameters like `NULL`, `ON` or other keywords can easily create trouble for the parser. For example: `NULL` is usually a value expression, but for `JSON_ARRAY` it suddenly became something completely different.

Which means, we need to teach JSQLParser to check first if the `NULL` is suddenly part of the `JSON_ARRAY` function before we can accept it as value expression.

(Irrelevant side note: There is a reason why I complained about syntax like `JSON_ARRAY(NULL, NULL NULL ON NULL)` -- from a parser point of view this is insane.)",0,0,dwhp
5964,"Looks like you just add the error log here, a little bit confused about your PR title and description.",0,1,dwhp
5967,"Hmm this is something i didn't know. I also always thougt that the in rest encryption was tied to the lock. lol my password was a pain in the ass to enter just for that. Uffff. why not locking the whole database with a self chosen password. This would prevent forensics if i'm not mistaken. If the pass is good enough. Damn if it is not  secure against forensics i could have used a very simple pass. LOL it did cost me years of my life for sure to enter this damn thing. hahaha. Damn moxie you really shocked me right now. I don't know where i saw that post, but i think the whole world believes this pass is connected to the encryption. Damn..... under this circumstances we can use the weakest method to prevent someone to view our messages, I won't sleep this night for sure",0,1,dwhp
5968,"Looking good, thanks a bunch. Back to writting the ultimate engine on top of gdx?
",0,1,dwhp
5969,still looking for answers ?,0,1,dwhp
5977,kudos.,0,0,dwhp
5978,I still don't see answers to my questions.,0,1,dwhp
5980,"I'm still getting the old behavior.
",0,0,dwhp
5983,"Phew; progress!
",0,0,dwhp
5991,"We're getting close -- but I think there's one bit of ugliness we should figure out. See the attached screenshot:

![screen shot 2015-06-11 at 2 20 32 pm](https://cloud.githubusercontent.com/assets/1976582/8118650/1ece8b64-1045-11e5-8d8b-6f3c34fbf43b.png)

I _really_ hate how the scroll bar looks when separating the outline and the document. Any thoughts on what we could do to improve it, if anything? (For example, hide it unless the mouse moves 'near' it; toggle its visibility in some smart way, ...)
",0,1,dwhp
5994,"I committed my changes first.
Then I updated and got a conflict.
I merged it and then I commmitted again.
So I had 2 commits.

I wanted to squash them so I did:
git rebase -i develop~2
Then changed pick on the second line to squash

Then I had to resolve the merge conflict again and I still got two commits. It seems that it only squashes the commits until the merge conflict into one commit etc.

After that I googled and found and tried this:
# Reset the current branch to the commit just before the last 12:
git reset --hard HEAD~2

# HEAD@{1} is where the branch was just before the previous command.
# This command sets the state of the index to be as it would just
# after a merge from that commit:
git merge --squash HEAD@{1}

# Commit those squashed changes.  The commit message will be helpfully
# prepopulated with the commit messages of all the squashed commits:
git commit

After the git merge -squash HEAD@{1} command it says unknown switch 'c'.

I updated because the git reset --hard HEAD~2 screwed things up.

Then I tried git rebase -i -p HEAD~6 (now I had 6 commits if I remember correctly). But that one says that I have to provide the -m switch but I wouldn't know what to provide as an argument.
",0,0,dwhp
5995,"Strange that Google of all people would mess that up, but I guess I'm not terribly surprised.
",0,1,dwhp
5996,"It already supports, dude! You can access it from three dots menu while searching.
![IMG_20210417_212917](https://user-images.githubusercontent.com/79245641/115118249-133b6c80-9fc4-11eb-9f3a-b014ada974f7.jpg)
",0,1,dwhp
5998,@elasticmachine would you mind to f***ing test this at least once?,0,1,dwhp
6005,"All DateTimeFunctions should accept an optional parameter to hold the timezone or timezoneid. Unfortunately they cant because DateTimeFunction extends UnaryScalarFunction.

I started to fix this by introducing my own base class of ScalarFunction but didnt want to make this a monster change. Will leave that for the future. Unfortunatley there doesnt appear to be any support for variable parameter count functions at all inthe system. Given this is all new will worry about that in the near future.",0,1,dwhp
6007,"1. Non-developers are unlikely to activate debugging in order to file a bug report on Github.
2. ""Debugging"" is what it's named here in the logging instructions.
3. ""Fehlersuche"" would be misleading to me. It sounds to me like something might solve my problem automatically.
4. I guess neither of us capable of estimating how ""debugging"" is understood by Non-nerds. I guess it's sounds nerdy to them and that's not wrong.",0,0,dwhp
6009,urgh ... I just realized I completely forgot about adding tests here and just instinctively merged on the LGTM :heavy_check_mark: => sorry about that PR for tests incoming shortly,0,0,dwhp
6011,they don't promote illegal content... they only promote free speech. Ref illegal content that is the same for any website or even YouTube... anyone can upload something illegal it just takes time for admins to find it. Re hosting illegal well you dont have to rehost for bitchute you can just leech. That would make more sense for this application. However average users separate from this should seed since they can control what they click on but a server with loads of users cannot. If you did a client side webtorrent implementation it would also avoid all these issues!,0,0,dwhp
6012,"urgh... sorry, wrong project. closing and reposting to the libraries project. 
",0,0,dwhp
6014,"this is not a job posting board.
",0,0,dwhp
6021,"You're right, that's why I've added it.

If you create a lot of run configurations on the fly this is pain in the a*s.
",1,1,dwhp
6026,"I dislike the 0/negative timeout being interpreted as infinite; we have the Long.MAX_VALUE for that. But if we change the behaviour, it will be a bit of a surprise. So damned if we do and damned if we don't. 

Anyhow... it needs to be documented; whatever interpretation we are going to select. A user needs to know our contract.
",0,1,dwhp
6028,"Hell yes - especially the first issue is really important. I never noticed the green bar until I read about it in some blog post. This should either work like the suggested traffic light or we could just add a checkmark in the middle of the green. Then we also have the three modes ""doesn't support encryption"", ""encrypted but not manually verified"" and ""identity manually checked""
@merkste Threema has it a little bit different, as they don't force the identity to be verified via telephone number and they don't support unencrypted communication, which we need for SMS.
So we'll have 4 cases to worry about when TS implements the usage of emails or random strings as handles. But that shouldn't be too hard - we could just make the green bar yellow.
",1,0,dwhp
6029,"Do you think we need another reviewer for this?
",0,1,dwhp
6034,"@yepyepyep4711 it's important that you're not loged in to Signal service when doing backup with TiBk. the same when restoring with TiBk, so there are no keys in the wild to fuck things up.",0,1,dwhp
6035,New design is here. But I'm quite sure that we'll need to improve it more (some entities still look ugly),0,1,dwhp
6038,"I don't get any downloads starting at all. I have to manually open the application and start each one every single time. And if I forget and the download times out. Then I have to open the video again, add the download again, clear the old download, and then start the new download. This is untenable. It's insane, and an awful UX. I don't understand why I have to go through all this trouble. Point out the code I need to change and I will build the apk myself. I've already tried but I couldn't figure out what needed to be changed as I don't code in this language",0,1,dwhp
6040,"Its actually not really trying to install the plugin again, its just that the jar hell check happens before the ""already installed check"". Thanks @skearns64 
",0,0,dwhp
6042,"`--define` is more than just `goog.define`s. You can also have non-Closure `@define`s (like `COMPILED` in base.js). A bundle won't replace these and it doesn't make sense for them to go in `CLOSURE_[UNCOMPILED_]DEFINES`. I think it would be confusing if `--define` always set the Closure defines as a result.

There are work arounds, obviously. You can pass an input to the compiler that sets `CLOSURE_[UNCOMPILED_]DEFINES` and it will 1) output that to the bundle and 2) respect that in a compile. Then it is a lot clearer that those are Closure defines only.",0,0,dwhp
6044,this was an accident,0,1,dwhp
6045,"dafuuuck can you repro and get logs of _that_ happening?
",0,1,dwhp
6048,"Man was not meant to have such power.
",0,1,dwhp
6054,"> Thanks for the info, I didn't realized javafx-gradle just calls javapackager, docs are pretty sparse for the plugin.

Yeah, I only learned this information myself by reaching out to the plugin's creator (Danno Ferrin), who also works on JavaFX at Oracle, btw. The docs for `javapackager` suck too, though, so at least things are consistent.

> For now I'll just work on creating osx and win7 virtualbox images that can act as slave builders for the main Jenkins node. I'll host the virtualbox images on an extra PC on my home network for now. It shouldn't be too hard to move them to the cloud when we we need to. I expect we'll need 2-3 GB of system RAM for each one.

Cool. The second you run into ""missing dll"" issues on windows, let me know. I've spent a stupid amount of time solving that problem, and it should indeed be solved if you're using the current packaging scripts in master, but just in case it shows back up, sound the alarm.
",0,1,dwhp
6056,Maybe your subscribed channels don't upload many and you didn't notice. My what's new tab is full of shorts (especially since they got made monetizable recently) and I can hardly stand it anymore.,0,0,dwhp
6060,Please try to be constructive. Describing something as a â€œmonstrosityâ€ is not.,0,1,dwhp
6063,I guess I was more exhausted than I thought,0,0,dwhp
6064,"IDK.
You should get only one of the â€œdisable typingâ€ or â€œuse input boxâ€ options. On old Androids, removing the former would mean there would always be the input box when there is a {{type:NN}}.
Also, the reason why i left the old code in there in the first place was issues with focusing input boxes. An i think that is related to the â€œinput workaroundâ€ setting nearby.
",0,0,dwhp
6065,"Don't merge yet. I f*cked up with a branch. I have 2 similar named branches.
",0,1,dwhp
6066,"I agree with you on this wholeheartedly except the very end: no matter what you do, software is insanely complex and we are all humans, we all make mistakes, you can't really avoid it (sometimes software and also hardware make mistakes too).

This is why we have logs, metrics, distributed tracing, this is why write tests, and also this is why we have Spring Boot Actuator. You can call it information leakage but I would call it ""observability"" instead.
Again, I agree on disabling this by default but I disagree on not having it at all.",0,0,dwhp
6067,Did you figure this out?,0,1,dwhp
6069,"Has been happening for me for a while but with one exception. For me, it will never auto download anything, when you tap to do it, it will go on for an eternity with nothing happening, BUT it doens't disappear, just goes on for a long time until I come back to it sometimes later and they are there and finally downloaded. It is ridiculous, I have other apps who get much larger media and it is nearly instant. Forgot to add that in Signal it was fast and flawless before too but suddenly something changed and it went all to hell.",0,1,dwhp
6071,@PeterHindes this is feature creep. i'd instead propose folks testing the debug apk post cpu usage while parsing so this isn't just n of 1.,0,1,dwhp
6072,The default behavior of ZenHub not to take the current repository when creating a new issue is a pain in the ass.,0,1,dwhp
6073,"**[Benjamin M](https://jira.spring.io/secure/ViewProfile.jspa?name=benneq)** commented

I see. THIS SUCKS! :(

`setDirectFieldAccess` is an exclusive switch AND `DirectFieldBindingResult` doesn't support nested fields. I wish it could just simply behave like ""Jackson"" does. It just takes what it can find. If there's a setter: use it. If there's none, then just take the field.
",0,1,dwhp
6074,"@bloodyjuice I think there are no python connectors now, all connectors are based on Java Function. But Pulsar has python functions, I think it's easy to implement a connector based on the python functions. If you are interested in implement it, you are welcome to push a PR.",0,0,dwhp
6076,"Let me also list some other changes before I forget:
- `EventExecutorGroup` doesn't implement `Iterable` anymore.  Instead I added `children()` method.  Why?  Because Java Generics sucks when it comes to inheritance.
- `LocalEventLoopGroup` has been deprecated by `DefaultEventLoopGroup`.
",0,1,dwhp
6080,"> The internal issue was routed to the SOC manufacturer, who was unable to reproduce the issue and routed it back to the Google TV team. It's currently on the Google TV team pending next steps. We're continuing to escalate the issue with that team to try and get some more movement on it.

How an issue thousand of people can reproduce on demand cant be reproduce by the manufacturer ? this is silly and frankly not serious.
just use the damn Google tv on Plex with any X265 file. Try a subtitle or let it run automatically next episode (for a tv show)
The Google tv reboot. All the time.
thats dont seem THAT difficult to reproduce.
Anyhow, for me its going back to the vendor, Nvidia seem to be handling the thing way better",0,1,dwhp
6081,"I tried, but code looked too ugly and didn't pass pmd check.
I leave comment on default block with reason of such case
",0,1,dwhp
6082,"You can read that file on someone else's phone outside the app only when you have root access on their phone. If you have root access, you can do all kind of other nasty trics which are much more dangerous like intercepting keyboard input or making hidden screenshots.

If you have root and read it on your own device, I don't see the problem. I can already access my own messages.",0,1,dwhp
6084,"Hi @kjin.
Honestly, no, there is no technical reason, except losing some chars in the outputted minified JS, as we will have : 
```js
a.readyState<HTMLMediaElement.HAVE_CURRENT_DATA
```
instead of
```js
a.readyState<a.HAVE_CURRENT_DATA
```
It may count when I have lots of references to this kind of constants.

I know, this is really ugly and over-engineering",0,1,dwhp
6086,"I really think this deserves a preference - it's one of those annoying app behaviours that everyone hates and it's very frustrating that it keeps on popping up again after I've dismissed it. If there won't be a preference option then at least have it only pop up once - I don't need it popping up for every one of my contacts - if I want to share it then I will sans nag.
",0,1,dwhp
6088,"Please ignore/delete/reject this pull request. I was new and messed things up. I also incorporated suggestions and fixed an oversight in my two newer requests.
",0,0,dwhp
6089,"> attempt to copy the image in by it's original name, use that if unique

Perfect!

> only if not unique rename to ""original name (N).extension"" where 'N' is the number required to be unique, incrementing past whatever number was there previously?

Perfect!

> I suppose we would need to message that to the user somehow, perhaps a quick 'we did this"" with only ""OK"" dialog is sufficient?

Anki desktop doesn't do this. But yes, a message that ""xyz.jpg"" already exists and thus was renamed to ""xyz (n).jpg"" would be a hell of a clean approach.
",0,0,dwhp
6090,"Thanks so much for the issue, @the-kyle. I just put a fix in master for the focusable issue, seems to work in my emulator when using the tab key.

I turned on TalkBack and was able to have it dictate ""Send TextSecure message button"" when focused, so hopefully it was just that ""focusable"" flag and ""nextFocusForward"" that was screwing things up.
",0,1,dwhp
6092,"Nevermind. I had to change the size of the box.
*shaking my head*",1,0,dwhp
6093,"aww i think I have a fix.
",0,0,dwhp
6097,"No way, they are already too fast, it's unfair ðŸ˜„ ",0,0,dwhp
6098,"Ah darn. Now it no longer misbehaves on macOS but does so on Windows, it seems. Looking back; I seem to remember that it didn't flicker on Windows before your positon fix. I somehow missed to test the PR on Windows after this change. 

[This](https://github.com/LWJGLX/lwjgl3-awt/blob/main/test/org/lwjgl/opengl/awt/AWTTest.java) lwjgl3-awt test runs fine on Windows btw, so it seems like the issue is in libGDX code.

The Hiero issue with this PR is actually separate to the one @obigu mentioned (which, on another note, I can't reproduce, so it is probably caused by one of the installed fonts). Hiero crashes with this PR, because ""No context is current or a function that is not available in the current context was called."" See the full stacktrace below:

<details>
  <summary><b><i>Click to show full log</i></b></summary>

```
> Task :extensions:gdx-tools:Hiero.main() FAILED
FATAL ERROR in native method: Thread[AWT-EventQueue-0,6,main]: No context is current or a function that is not available in the current context was called. The JVM will abort execution.
	at org.lwjgl.opengl.GL11C.nglGenTextures(Native Method)
	at org.lwjgl.opengl.GL11C.glGenTextures(GL11C.java:664)
	at org.lwjgl.opengl.GL11.glGenTextures(GL11.java:2492)
	at com.badlogic.gdx.backends.lwjgl3.Lwjgl3GL20.glGenTexture(Lwjgl3GL20.java:348)
	at com.badlogic.gdx.graphics.Texture.<init>(Texture.java:142)
	at com.badlogic.gdx.graphics.g2d.PixmapPacker$Page$1.<init>(PixmapPacker.java:504)
	at com.badlogic.gdx.graphics.g2d.PixmapPacker$Page.updateTexture(PixmapPacker.java:504)
	at com.badlogic.gdx.graphics.g2d.PixmapPacker.updatePageTextures(PixmapPacker.java:410)
	at com.badlogic.gdx.graphics.g2d.PixmapPacker.updateTextureRegions(PixmapPacker.java:402)
	at com.badlogic.gdx.graphics.g2d.freetype.FreeTypeFontGenerator.generateData(FreeTypeFontGenerator.java:464)
	at com.badlogic.gdx.graphics.g2d.freetype.FreeTypeFontGenerator.generateFont(FreeTypeFontGenerator.java:161)
	at com.badlogic.gdx.graphics.g2d.freetype.FreeTypeFontGenerator.generateFont(FreeTypeFontGenerator.java:152)
	at com.badlogic.gdx.tools.hiero.unicodefont.UnicodeFont.setRenderType(UnicodeFont.java:647)
	at com.badlogic.gdx.tools.hiero.Hiero.updateFont(Hiero.java:315)
	at com.badlogic.gdx.tools.hiero.Hiero$3.valueChanged(Hiero.java:439)
	at javax.swing.JList.fireSelectionValueChanged(JList.java:1802)
	at javax.swing.JList$ListSelectionHandler.valueChanged(JList.java:1816)
	at javax.swing.DefaultListSelectionModel.fireValueChanged(DefaultListSelectionModel.java:184)
 	at javax.swing.DefaultListSelectionModel.fireValueChanged(DefaultListSelectionModel.java:164)
	at javax.swing.DefaultListSelectionModel.fireValueChanged(DefaultListSelectionModel.java:211)
	at javax.swing.DefaultListSelectionModel.changeSelection(DefaultListSelectionModel.java:405)
	at javax.swing.DefaultListSelectionModel.changeSelection(DefaultListSelectionModel.java:415)
	at javax.swing.DefaultListSelectionModel.setSelectionInterval(DefaultListSelectionModel.java:459)
	at javax.swing.JList.setSelectedIndex(JList.java:2216)
	at javax.swing.JList.setSelectedValue(JList.java:2366)
	at com.badlogic.gdx.tools.hiero.Hiero.<init>(Hiero.java:190)
	at com.badlogic.gdx.tools.hiero.Hiero$21.run(Hiero.java:1591)
	at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:311)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758)
	at java.awt.EventQueue.access$500(EventQueue.java:97)
	at java.awt.EventQueue$3.run(EventQueue.java:709)
	at java.awt.EventQueue$3.run(EventQueue.java:703)
	at java.security.AccessController.doPrivileged(Native Method)
 	at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:728)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:82)
```
</details>

",1,1,dwhp
6100,"This would be so sweet to have.
",0,1,dwhp
6101,"Because the more dependencies you introduce the more you relay on the work of others. This can become a hell on some point. Just think if someone foes not want to continue a project (GigaGet great examble), or some destroys or revokes a pavkage/repo. Remember the node.js disaster where someone revoked a package, and brokr hundreds of projects with that.

Also it blowds things, so im rather against ""repo hording/repo shopping"" :D",1,1,dwhp
6103,"You're calling an undocumented method and reporting your misuse of it as a memory leak?
",0,1,dwhp
6106,"The problem with the handcrafted `.bazelrc` approach, is that you would need to update it on every Error Prone release. That sucks, obviously. The workaround for now is apply my patch: [1].

* [1] https://bazel-review.googlesource.com/7891",0,1,dwhp
6109,"Our HtmlSanitizer has:

`.addProtocols(""a"", ""href"", ""ftp"", ""http"", ""https"", ""mailto"")`

We probably want to add

`.addProtocols(""a"", ""href"", ""ethereum"", ""bitcoin"", ""rtsp"")`

just to match our plaintext stuff in UriLinkifier.

As for TelURI, that and maybe even SIP seem like reasonably widely used standards for inclusion, especially on mobile.

I'm wary of this continuing with this approach though. Stuff like steam has a URI syntax. And there's the explosive growth of cryptocurrencies - how long before we add litecoin, heck even stuff like dogecoin. Where do we draw the line? Do we even try?

It would be nice if these were configured in Preferences rather than in code. Then people can just add and enable whatever they like. But urgh, extra prefs.

There is for example a good case for not enabling links for Tel URIs on grounds of safety. One click and you're dialling a premium rate line.

I'm not sure what the best approach is here.
  ",0,1,dwhp
6110,Thank you. Branch 2.9 is back to the expected Bookie behavior.,0,0,dwhp
6112,"> > but is a PITA for a generated code.
> 
> What do you mean by this? Removing `toString()` seems like the only valid solution.

PITA == Pain In The Ass :)

We still have this hack in our Dart generation workflow.",0,1,dwhp
6114,Haha.,0,0,dwhp
6115,"Moving Eclipse output from `buck-out` fixed it. However, I figured out, that Buck even interferes with itself. All you need to shoot yourself in the foot, is to have such innocent rule:

```
genrule(
  name = 'foo',
  srcs = glob(['**']),
  [...]
```

Now you are screwed up, because Buck copying all files to `foo__srcs`, including `BUCK` file itself, and later, when you are trying to run `buck test` (note, that `buck build *` still works) it will complain that

```
//buck-out/.../foo: is not there or something
```

So now, all `glob(['**'])` must be extended with `srcs = glob(['**'], excludes = ['BUCK'])`, as in : [1]. May be this should be added to the documentation? Now it's claiming that:

```
Buck automatically excludes its own output, e.g. <code>buck-out</code>,
```

https://gerrit-review.googlesource.com/#/c/72684/1/polygerrit-ui/app/BUCK@41
",1,0,dwhp
6116,"Damn optimizations! A small optimization causes zillion issues..
",0,1,dwhp
6119,"@Downchuck I haven't heard of ""Naive Backoff""  - do you have any pointers where folks refer to this language model as ""Naive Backoff"" vs. [""Stupid Backoff""](https://www.google.com/search?q=stupid%20backoff%20language%20model)
",0,1,dwhp
6120,"Did that fix the issue for you?
",0,1,dwhp
6123,"And of course it looks like shit on the phone in landscape mode but not on the tablet. Styling Android apps can be so incredibly frustrating.
",0,1,dwhp
6125,"I've made a mistake, and it *could* be that everything is working exactly as expected. It turns out that I was actually using `ClientAuth.OPTIONAL` instead of `ClientAuth.REQUIRE`, which changes the picture significantly. A better summary of the situation is now:

1. We tell our server that clients MAY authenticate.
2. A client presents an untrusted certificate to the server.
3. The server recognizes that the certificate is bogus, but allows the connection to happen anyway.

Prior to Netty 4.1.7, the server would have rejected the connection attempt with a bogus certificate, or at least indicated that something screwy was happening. From Netty 4.1.7 onward, the connection attempt succeeds with (I think?) no warning.",1,0,dwhp
6127,"I can't even open the app.
",0,0,dwhp
6129,"hell @shixuan-fan ,can you help me review the code 
#16675 ",0,1,dwhp
6132,"OMG! I did not push that fix in 5.3 branch apparently. So it's fixed in 5.2, 5.4 but not 5.3...",0,0,dwhp
6133,"Yes, we allow age restricted content, but I don't believe YouTube or any of the places NewPipe has as a service openly allow pornographic material. YouTube's ""age restriction"" is basically for swearing or doing some suggestive stuff, not any nudity, as that isn't allowed. I also don't think that it is in the best interest to try to get someone to use a software then they find out it has porn in it.",0,0,dwhp
6142,"No..  On a window's computer, I was able to install DBeaver and connect with the Postgres database..  But when trying to download the drivers with my Mac.. Things are just not working right...  I sense that my credentials are screwed up.. but this is a little out of my knowledge-base... *SIGH*
",0,1,dwhp
6143,"I tried sending your photo of this magnificent catlord stretching on the grass.

**TS 2.17.0, Nexus 4, stock Android 5.1.1:**
Could **not** reproduce. Everything works as expected.

**TS 2.17.0, Huawei U8800, Android 4.0.4**
Cat refuses to get sent. It just lays on the grass and does not want to go to the Internet.
![screenshot_2015-06-05-14-45-21](https://cloud.githubusercontent.com/assets/174176/8005459/f938d622-0b92-11e5-9219-424ffb2c5097.png)
![screenshot_2015-06-05-14-45-42](https://cloud.githubusercontent.com/assets/174176/8005463/003bf1de-0b93-11e5-8da8-85017e5aca63.png)
![screenshot_2015-06-05-14-45-50](https://cloud.githubusercontent.com/assets/174176/8005465/072a6bf6-0b93-11e5-9fdc-4dd15d16308f.png)
Resending always fails.
~~And clicking on the thumbnail preview opens a the image view but the picture is not there. Just black background.~~ Image view works â€“ it just takes several seconds.

Log from 4.0.4: https://gist.github.com/anonymous/e8d9c5b39d0c53e0f752
Seems to be memory issues.
",0,1,dwhp
6145,"Should be fixed in #1005 - I suck at bitmasks, but again have added another unit test to validate
",0,1,dwhp
6146,god damn ))) I passed thru all checks!,0,1,dwhp
6150,"Is any keyword supposed to be valid? Can you do this (even though it's a bit weird)? `import {else as x}` or `import {switch as x}` etc.
",0,1,dwhp
6155,"@ywangd That's not a crazy idea.

The scope of the project of ""auditing security configuration changes"" implies or suggests that auditing is for **objects** (eg roles, users, role mappings, etc, that has changed by some user/key) , whereas our existing audit logs are **subject** based (eg user X modified/deleted role, user, key). To a certain extent you can derive one from the other, to answer different questions, but the devil is in the details.

Referring specifically to your proposal, I think that if we were going to implement a true **object** based audit log, it doesn't make sense to audit change failures, at the very least not by default. But if we build on top of the **subject** based audit log that we've got (as a new field on existing records), then the failed actions become important (to the extent that the default configuration is now **on** from **off**).

I acknowledge the desire for an **object** based audit log, but I got turned off upon seeing both the **subject** (all metadata of the action, less the request body) and the **object** based entries one next to the other; the redundant metadata between the two was glaring and looked unnecessary.

For the moment, I am going to stick with auditing request bodies of access_granted and access_denied events in the existing audit log, within the `request.body` field. I'll try to make this fit into existing configurations of the audit log (avoid hinting to **object** based auditing).

I have some open questions about what it means to output index documents in a log-like file for ECS.

Hope this rambling make sense. Thank you for the valuable input!




",1,0,dwhp
6156,"Same issue, very annoying ...",0,1,dwhp
6162,NACK,0,1,dwhp
6164,"Use ""```"" before and after or highlight your code and select code so everyone can see it. I know you put it on stack overflow; kinda of a pain in the ass to go back and fourth. I copied your code from SO for everyone watching. 

```
SslContextBuilder sslBuilder = SslContextBuilder.forClient();
                    SslContext cont2 = null;
                    TrustManagerFactory trustManagerFactory = TrustManagerFactory
                            .getInstance(TrustManagerFactory.getDefaultAlgorithm());
                    // truststore
                    KeyStore clientKeyStore = KeyStore.getInstance(""JKS"");
                    clientKeyStore.load(null, SipListener.KEYSTORE_PASSWORD.toCharArray());
                    for (Cert clientCertCert : sipSettingsBean.getSipSettingsServerBeans().get(0).getCerts()) {
                        Certificate clientCert = CertificateFactory.getInstance(""X.509"")
                                .generateCertificate(new ByteArrayInputStream(
                                        clientCertCert.getCert().getBytes()));
                        clientKeyStore.setCertificateEntry(
                                clientCertCert.getAlias(), clientCert);
                    }

                    trustManagerFactory.init(clientKeyStore);
                    sslBuilder.trustManager(trustManagerFactory);
                    KeyManagerFactory keyManagerFactory = KeyManagerFactory
                            .getInstance(KeyManagerFactory.getDefaultAlgorithm());
                    KeyStore serverKeyStore = KeyStore.getInstance(""JKS"");
                    serverKeyStore.load(
                            new ByteArrayInputStream(
                                    sipSettingsBean.getSipSettingsServerBeans().get(0).getKeystore().getBytes()),
                            SipListener.KEYSTORE_PASSWORD.toCharArray());
                    if (!serverKeyStore.isKeyEntry(sipSettingsBean.getSipSettingsServerBeans().get(0).getKeystoreAlias()))
                        throw new IllegalArgumentException(
                                ""Keystore file has no matching key for given alias."");
                    keyManagerFactory.init(serverKeyStore,
                            SipListener.KEYSTORE_PASSWORD.toCharArray());
                    sslBuilder.keyManager(keyManagerFactory);
                    cont2 = sslBuilder.build();
                    SSLEngine engine = cont2.newEngine(ch.alloc(), toHostname,
                            portDestination);
                    engine.setEnabledProtocols(new String[]{""TLSv1.2""});
                    ch.pipeline().addLast(""ssl"", new SslHandler(engine, false));
                    ch.pipeline().addLast(""handler"",
                            simpleChannelInboundHandlerRegisterTCP);
```",1,0,dwhp
6165,"Actually scratch that, the order is incorrect...should be first language and then gender...",0,0,dwhp
6168,"Holy cow, didn't know about that feature. That's awesome.

On Wed, Oct 7, 2020, 6:39 PM opusforlife2 <notifications@github.com> wrote:

> You can customise the buttons which show for the 5 button row and the 3
> button row.
>
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/TeamNewPipe/NewPipe/issues/4414#issuecomment-704891793>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABFZEFJVTSRFGLTKJUQUGY3SJRLBDANCNFSM4SE35CEQ>
> .
>
",0,1,dwhp
6169,"Heh, software man, it's difficult ;-)
",0,1,dwhp
6173,We need to give ourselves a little more time.,0,0,dwhp
6174,"Ok, this PR really screw things up. After this PR, when the new folderBaseName arrives to the directory matching filter it is always false.

the new filter is for example `res/layout` but the directory path is only `res` so directory filter will always fail. effectively not loading a single resource.

At least that is my understanding. 
",0,1,dwhp
6176,"ok, I can see where the problem is..., its basically because elasticsearch sets in the `elasticsearch.in.sh` file the size for the thread stack size to `128k`. If I set it to `1024k`, then it works. Damn Java regex... . 
",0,1,dwhp
6181,You need to sign the CLA before we can take a look.,1,1,dwhp
6184,"No because:

a) It would be horribly hacky.
b) It would likely result in more obscure issues being raised as bugs taking up developer time.
c) It would not work on newer versions of Android where permission has to be granted
d) It would do odd things (likely in a way that resulted in inconsistent state) if the removable storage was removed mid-poll.

I'm absolutely against replacing one ugly setting that few change, with more ugly settings that few change and exposing more of Android's ugly internals.

We should do it properly or not all. Supporting removable storage is a new enhancement - there is no justification for hacking it in badly.
",0,1,dwhp
6186,"emmm. i think we need to refactor this code. it seems very like callback hell.

@liudezhi2098 
Do you have any good idea ?  ",0,1,dwhp
6187,Travis is a bitch. Restarted the jobs.,0,1,dwhp
6189,"Strangely - this is media related for me. I'm experiencing this right now, it is *horrific* on media sync. The collection was pretty slow as well. When I first saw this reported I blithely said ""I don't know, it seems to saturate my 25Mbit pipe?"" but at least for a real device (Pixel 4a) at the moment - hooboy this thing is crawling through sand mixed with cold molasses slow",0,1,dwhp
6191,Out of date.,0,1,dwhp
6192,"People make money with Admob by showing ads. Do you see any ads?

That should tell you how reliable those services are. Scaremongers.
",0,1,dwhp
6193,Urgh... there is still one failure. Let me check if it is legit.,0,0,dwhp
6195,"Thanks a lot @ripcurlx!

I'm mostly worried about your exception b/c I cannot recreate it. Will try more. This needs to be fixed.

I confirm the two UX glitches you mentioned. The ComboBox is half-magic internally and it is being embedded in a complex context. It is very hard to bend it to perfect autocomplete experience.

I already spent much more time fighting this than I ever anticipated. OTOH I hate to deliver crap. I will see what I can do.",0,1,dwhp
6196,I know this is a closed issue but am having the same one. Same exact error messages from the installer. The alter table didn't work and I don't have a database rollback. Guess I'm screwed?,0,1,dwhp
6200,"OK, this is all kinds of fucked it seems. I can get the nativeScale now (conditional on OS version). On iOS 8 on an iPhone 6 (plus) i get funky black bars to the left and right. UIScreen.getMainScreen().getBounds() reports 480x320, the scale is 3 on an iphone 6 plus, resulting in a width/height of 1440*960, on an iPhone 6 the scale is 2 resulting in 960x640 pixels scaled.

This introduces funky black bars: http://libgdx.badlogicgames.com/uploads/Screen%20Shot%202014-10-03%20at%2017.11.41-CRQw8sLJcg.png

I know nothing about iOS UI programming, anybody got an idea?
",0,1,dwhp
6201,I end up with multiple slots on my shiny server and have to add the same people every time. With this many people it is getting incredibly inconvenient. ,0,1,dwhp
6202,@ywww Have a look at this one - https://github.com/prestodb/presto/pull/15519,0,0,dwhp
6204,"@greyson-signal 

> Are the bad reactions appearing on the message as soon as they're sent/received? Or sometime later?

Instantaneously. Like freakingly fast! Sometimes, it says it's me that reacted, sometimes it shows my friend with whom I'm chatting, and sometimes someone from another conversation. But always so fast that it feel like it's as soon as I touch the send button.

> Are the bad reactions always on the same messages, even after closing the app or sending/receiving new messages?

Yes, once they are there, they remain there and survive app restart, force stop, phone restart, etc. Only there on **MY** Android app, not on my Desktop.

> How often is this happening?

Started 2-3 days ago, happens on ~20% of the messages I would say within that one conversation. But I don't use Signal with many people yet, so that number may not be representative.
",0,0,dwhp
6206,"This parse tree is a train wreck - where does it come from?

In theory we should be producing some kind of ""dep"" connection even if
there's no good dependency structure to connect, and I can investigate why
that's not happening, but ideally the tree itself would be parsed
differently
",0,1,dwhp
6207,"`CacheControlHeadersWriter` is actually not ignoring the cache-control header from `ResponseEntity.cacheControl()`, but it can not find it in the response:
```java
// CacheControlHeadersWriter.java
 
@Override
public void writeHeaders(HttpServletRequest request, HttpServletResponse response) {
    if (hasHeader(response, CACHE_CONTROL) || hasHeader(response, EXPIRES) 
            || hasHeader(response, PRAGMA)) {
        return; // should go here, but doesn't
    }
    this.delegate.writeHeaders(request, response);
}
```

When I debugged the workflow the CacheControlHeadersWriter seemed to be executed _before_ `.cacheControl()`. Weird...",1,0,dwhp
6211,"oh... I totally missed the last part of your gist :( Shame on me.
Looks nice!",0,1,dwhp
6212,"It really sucks because this has been broken for weeks and I've been patient but every update doesn't fix it. It worked fine for more than a year but it's been broken for over a month. Just tries connecting to MMS server for 30+ seconds and fails. Going to have to use an insecure app again because I've missed so many MMSs from friends and colleagues. I have to tell them all to ""try Kik or Facebook""  :( I have an HTC One m8 on VZW running 5.0.1. Can't figure out how to modify or verify my APN at all either, but it always worked before until a TextSecure update. Have you guys considered just reverting to the old method, or making it an option at least? I want the whole world to use secure messaging, but my friends and I are going to have to go back to stock messaging... Sigh. 
",1,1,dwhp
6215,"This is what I get for accepting PRs late on a Friday.
",0,1,dwhp
6218,"Wow, 700Kb is crazy! That's not expected at all. If that's the case, it's too much for this feature, I wouldn't accept it like that. How in the world is text highlighting 700Kb worth of minimized code? ",0,1,dwhp
6219,"This is my local workaround for this bug at the moment. An ugly hack to say the least, but it seems to work:
```diff
diff --git a/tools/build_defs/pkg/archive.py b/tools/build_defs/pkg/archive.py
index cf83aa9..72f2564 100644
--- a/tools/build_defs/pkg/archive.py
+++ b/tools/build_defs/pkg/archive.py
@@ -105,12 +105,15 @@ class TarFileWriter(object):
     pass

   def __init__(self, name, compression=''):
-    if compression in ['tgz', 'gz']:
-      mode = 'w:gz'
-    elif compression in ['bzip2', 'bz2']:
+    if compression in ['bzip2', 'bz2']:
       mode = 'w:bz2'
     else:
       mode = 'w:'
+    # The Tarfile class doesn't allow us to specify gzip's mtime attribute.
+    # Manually compress at the end, similar to the xz compression.  If we could
+    # specify the gzip mtime, then we wouldn't have to invoke gzip manually at
+    # the end of close().
+    self.gz = compression in ['tgz', 'gz']
     # Support xz compression through xz... until we can use Py3
     self.xz = compression in ['xz', 'lzma']
     self.name = name
@@ -383,7 +386,16 @@ class TarFileWriter(object):
       TarFileWriter.Error: if an error happens when compressing the output file.
     """"""
     self.tar.close()
-    if self.xz:
+    if self.gz:
+      # Support repeatable gz compression until tarfile supports it.
+      if subprocess.call('which gzip', shell=True, stdout=subprocess.PIPE):
+        raise self.Error('Cannot handle .gz and .tgz compression: '
+                         'gzip not found.')
+      subprocess.call(
+          'mv {0} {0}.d && gzip -n -9 {0}.d && mv {0}.d.gz {0}'.format(self.name),
+          shell=True,
+          stdout=subprocess.PIPE)
+    elif self.xz:
       # Support xz compression through xz... until we can use Py3
       if subprocess.call('which xz', shell=True, stdout=subprocess.PIPE):
         raise self.Error('Cannot handle .xz and .lzma compression: '
(END)
```",1,1,dwhp
6221,Will explore alternative fix that doesn't touch such a hot codepath.,0,1,dwhp
6225,Oh... umm... doodoo?,0,1,dwhp
6226,"Reminder for @litetex to reopen the issue or create a followup

---

I hate the solution, but like you already mentioned (although in nicer words) the player is a hot pile of ðŸ’© ",0,1,dwhp
6235,"Ok, so I had to manually enable ""Allow Bubbles"" in the top-level system notifications settings, and then bubble again, which made a notification that I could then manually bubble... What a nightmare.",0,1,dwhp
6237,"Sounds like you found the problem on your side. Closing this out.
",0,1,dwhp
6239,"For me it works fine on OSX 10.12.6 for processing 3.5.3.
Maybe apple is screwing things up again.",0,1,dwhp
6248,Shame! I agree there is a glaring privacy issue ðŸ˜•  Hope someone picks it up ðŸ‘ ,0,1,dwhp
6251,"@shink seems you've already included docs in this PR, so I will update the `doc-required` label to `doc`. Thank you for adding the docs!",0,0,dwhp
6253,"LOL 
",0,1,dwhp
6254,"Man, stop posting useless issues, please. Don't steal our spare time!",0,1,dwhp
6255,Yes better than an option (as non technical users will not be able to deal with that) would be to detect network conditions and increase timeout then. But yes changes in the P2P network are all very complex and have to be considered very well. Easy to screw up things unintendedly...,0,1,dwhp
6256,You are welcome. Thank you for the ultimate tool. It truly rocks!,0,0,dwhp
6257,"Just tried to test this and I think I screwed up. :-(

I exported my settings as an encrypted backup, remove the market installed TextSecure and installed the version I'd built with the patch applied. I started up the new TextSecure and imported my backup. It then prompted me for a password, I'd never entered a password, when initially setting up TextSecure I hadn't set up a password for encrypting the local database so what password is this asking for?

Have I now lost my all text messages?
",0,1,dwhp
6260,Haven't heard back in well over a year so closing.,0,0,dwhp
6261,"@jacksondus the code has a severe C legacy, so it's naming convention is mostly C. Still, `String` is just an immutable wrapper for `char[]` - you won't see `String` used as a base type anywhere in any serious tokenizer/lexer, since the most basic data struct appearing there is a single `char`, and using wrappers in tight processing always incurs a severe penalty.

To be able to do any real tokenization/parsing/lexing, you'll need a properly constructed state machine. https://en.wikipedia.org/wiki/Ragel (suggested by @davebaol) is indeed a very stable and quite standard solution, easy to implement and maintain. Alternatively, you can use https://en.wikipedia.org/wiki/JavaCC , http://www.antlr.org/ or any other similar tool.

If you're using regexes, both the extend of your lexical analysis and the performance will suffer.

Creating and maintaining own lexical/syntactical analysis framework is IMO just asking for development hell and Yet Another Abandoned Project, unless you have a couple of good & experienced developers willing to work a couple of years for free. If you don't have that but want to create a versatile solution, usable to any person other than yourself - I'd suggest to reconsider your current approach. I linked to my repo mostly to show you that while creating such tools is obviously possible, _it's easier to use a more standard-compliant solution when working on any serious project with anyone other than yourself._ Otherwise, anybody other than yourself (and possibly you after a couple of months) will have the same **WTF** moment as you have while looking at _my_ code.

As a side note (obvious pun, but no actual offense, intended) - don't you see the possible risk in dyslexic writing a lexer? ^_^
",1,1,dwhp
6263,"The only (futile) reason was that I like to give projects a personal name ([Aida](http://en.wikipedia.org/wiki/Aida_%28given_name%29)) that is actually an acronym.
Also it's easy to give DA a meaning: Development API, Design API, Domain Architecture, Data structures and Algorithms, Dumb Ass.... lol
",0,1,dwhp
6264,"> Actually I can't reproduce this. I have enabled automatically flip feature on Android, and photo flips with no issue. And you should include debug log, screenshot and etc.

The issue is about that other apps flip images even when auto rotation is disabled, which is a desired behaviour that's absent in Signal. Turning on auto rotate because you need landscape for 2 seconds is annoying, so some/a lot of people don't do it, and whatsapp apparently mitigates for that by enabling the sensor in camera mode, so that images shot as landscapes don't appear in portrait, because that's undesired/annoying/stupid, even though auto rotation is disabled globally.

This issue proposes that the sensor be enabled when the camera mode is active, even when rotation is disabled.",0,0,dwhp
6265,"I am very frustrated to learn that Android (which has its brand in a F1 car) does not support transparent video playback.
I am having to rework all my videos into animated gifs, with HUGE loss in quality and INSANE increase in size.

Why isn't this a relevant feature for the OS?",0,0,dwhp
6266,This PR won't work any more.,0,0,dwhp
6267,"Sure thing. I'll see if I can put up a PR sometime before Wednesday.

It's good to hear your thoughts about 'time vs value', as well. This is quite a rabbit hole, so fortunately I didn't get sucked in too soon. ",0,1,dwhp
6270,"Just happened to me yesterday. Obviously no CPP code.
Bazel itself recommending clean expunge is awful IMHO
On Tue, 20 Nov 2018 at 3:47 Johan BjÃ¶rk <notifications@github.com> wrote:

> This happens to me pretty much weekly and is really annoying. We don't
> have any C/C++ code in our project so it's quite unclear why this is
> happening at all.
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/bazelbuild/bazel/issues/6056#issuecomment-440106703>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABUIFxdNxxwX-a_lqHjWZr6jqj9PQkEdks5uw18rgaJpZM4WWV3G>
> .
>
",0,1,dwhp
6272,This change is misguided,0,1,dwhp
6273,"I used _BUILD for now and then those files are brought to the runfiles, where i rename them within the py_test script. But thats so ugly, i was hoping there is a clean solution for that...",0,1,dwhp
6276,"hadoop and other dependence library version could be config ? command line
args to specified it version maybe useful ....

2014-12-18 11:12 GMT+08:00 baishuo notifications@github.com:

> they all works, thank you!!, org.apache.hadoop.hdfs.MiniDFSCluster located
> in hadoop-1.0.4-test.jar. close this PR
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/amplab/tachyon/pull/577#issuecomment-67435632.

## 

long is the way and hard  that out of Hell leads up to light
",0,1,dwhp
6277,"Hi,
just a random guy passing by. I currently don't run BBB myself, but maybe I can help you.

Please have a look at https://github.com/bigbluebutton/bbb-webrtc-sfu/blob/6a6425a057684ba749809ba85298a5c94325fcfa/config/default.example.yml
In line 188 the Opus Bitrate is set to 30kBit. Naturally that won't be enough for music. Maybe you can increase that to 64kBit vor even 96kBit and test your quality again.

G.722 won't hell at all. Those Codecs are not meant to be used for music and use a max sampling rate of 16kHz and have a frequency range of only 50 to 7000Hz. G.722 will propably only make things worse. Because of that you also have to check: 
Afaik Chrome doesn't support Opus for WebRTC so your lecturer nicht want to usw Firefox, so Opus (your only real choice for music) can even be used.

You can also check issue #7007 as that one was also about audio quality",0,1,dwhp
6280,"Cannot download org.jkiss.dbeaver.ext.generic_2.3.30.jar from both locations, all other jars OK.
WTF... 
",0,1,dwhp
6284,"> In a general sense you guys might think about not using that plattform stuff if its that crappy and your have that ffmpeg option anyway in your product.

Platform decoders are more power efficient, can handle higher bitrate/resolution and do not impact app size.

I'm surprised Nvidia Shield has the same issue.
Could you try to play your file with the built-in media player? I'm not sure how to do it with the shield, but you should be able to download the file and then play it from the android file manager at least.",0,1,dwhp
6287,"@ldecicco-USGS Now I remember this discussion! Darn it, though, that I can't reproduce the issue. I appreciate your adding it to this repo so that we can more readily track it as a bug.",0,1,dwhp
6291,Mobi I miss you â™¥ I even tattooed your name on my buttock.,0,1,dwhp
6293,"@mhmdanas the team in general. From what I see this is the most requested feature, yet it's the most ignored. Every issue is closed due to ""Not being planned"", yet @SameenAhnaf says this is not the case and it's due to duplicates. For 2 whole years (can't say there was no time since everyone was sitting on their ass at home during covid). Atleast add it to the ""Major Planned/Missing Features"" issue or something so people stop opening the same issue. Sure plenty of smaller stuff has been added and fixed but who asked for those features? Can't handle a little criticism? ",0,1,dwhp
6294,"After some further tests I don't see a usecase for marking individual messages as unread anymore. With current textsecure design, there's no UI-level indicator for individual unread messages anyway. All you see is a count of unread messages and the highlighted thread with unread message(s).

So if tinloaf is right and marking a whole thread as unread results in screwed up count of unread messages, then the following would be the best approach in my eyes:
- add the 'mark unread' option to thread menu (instead of context menu for inidividual selected message)
- mark the last remote message in thread as unread when the menu option is selected
- still unclear: is it required to mark the thread as unread nevertheless (like tinloaf suggested)?

If you agree with that then I would try to implement it and prepare a pull request within the next days.

Sorry for possibly dump questions. The textsecure codebase is pretty new to me.
",0,1,dwhp
6297,"I already realized the issue you were fixing.
",0,0,dwhp
6300,"go ahead, I'm totally swamped with finals right now
",0,0,dwhp
6301,"After selecting the ""New Issue"" button on github, you should be prompted for 3 different types of tickets, and based on the one you select it should auto-fill a template. Otherwise, the bug template can be found here: https://github.com/Alluxio/alluxio/blob/master/.github/ISSUE_TEMPLATE/bug_report.md - No worries though

Thanks for the detailed response! Discussion like this is a good thing, no flaming necessary ðŸ˜„  

1) I agree that since we maintain the environment that we can be picky about what tools we use, that being said the `ps -Aww` line exists in our `./bin/alluxio` script with which I can tell you with 99% certainty is currently used more outside of containers than inside of them. Because of that, we need to support many UNIX-like environments.
Another option would be to split the script; one for ""standard"" deployments and one for containers, but that doesn't seem like an ideal solution

2) I would argue that while many systems are not entirely compliant with UNIX standards, many of them will adhere somewhat (how close they adhere is a whole other discussion). As far as standard operating system utilities go, `ps` is a very common one and I would _expect_ that most operating systems containing this utility to support the very minimal set of arguments that are described in the standard. I don't know this for a fact, so maybe I should just do my research- but I think using the arguments specified in the standard gives us the highest probability of guaranteeing portability across a variety of UNIX-like platforms.

3) I agree, it's not the most elegant command. I wish I could come up with an elegant (and more _portable_!) solution. I don't see another way off the top of my head though.",1,1,dwhp
6303,"Lol, I should remember my own issues.
",0,0,dwhp
6306,"I got one test failure that looks to be a flake.

@ChadKillingsworth 

I realize doing `path.toString().startsWith(""/"")`  is a crappy fix and may well mean the check is broken on Windows,
but I figure it's better to get this all submitted now. You could do a second PR to address this problem, which will be much smaller and easier to land.",0,1,dwhp
6308,"> Wow such an angry dude... this is open source, you don't get to give orders to people who volunteer on a project that you benefit from for free...

Sorry if I sounded rude....I've edited the comment. Actually I really liked the app. I've shared this app to all my friends. If this video downloader also works fine, it'll be even better.",0,1,dwhp
6310,"> If you had checked out the linked issue you would have noticed that K-9 Mail is not involved.

Please re-read my bug report. K-9 Mail _is_ involved. This happens with text selected in K-9 Mail (and not with text selected in other applications such as firefox).

> So Terminal apps have this problem with copying text from other apps as well.

I can copy text from firefox to jackpal just fine (mentioned above...). And why single out terminal apps?

> Rather than requesting to add workarounds to all kinds of different apps

Huh? Rather than wanking off to movies of Natalie Portmann pouring hot grits down her pants, you could maybe fix the bug :-)

> you could ask the authors of the terminal app to fix the issue on their end.

Unfortunately, jackpal is no longer maintained... but I have to use it, as other terminal apps are not supported on the version of Android on this device (4.4.1 on a Lenovo A5500-F).",0,1,dwhp
6313,I'm closing this as this is not really a good approach to bulk downloading. It would leak in many places and the current gigaget downloader is just crappy. We a new downloader for NewPipe will be built we will be able to finally implement this properly. @VishalNehra thank you for your work anyway ,0,1,dwhp
6314,"> > I hate it so much that i went nuclear and reverted to 5.3.0, I may fork 5.4.x and maintain one with the original message color if i ever have a few weeks to go through all their spaghetti nodejs stuff and will not accept pull requests to whomever changed my message colors.
> 
> @klikevil Do you have a link to this that you're working on? I'd gladly support the old-color fork and cherry-pick to get the other future updates without the new color bullshit.

@jason88k 
Not yet, and i haven't even started on android yet as I mainly use signal as a desktop client, I do have instructions for rolling back to 5.3 successfully on desktop and preventing it from being updated (my build is 5 days past expire date and is functioning properly).

https://github.com/signalapp/Signal-Desktop/issues/5316

C:\windows\system32\drivers\etc\hosts

```
127.0.0.1	updates.signal.org
127.0.0.1	updates2.signal.org
```

Search the above bug issue for a post that i did with virustotal link for a 5.3 installer, move %APPDATA%\signal\ out of the way, reinstall old signal.  You will need to create daily backups of %APPDATA%\signal to %APPDATA\signal.color.backup and occasionally restore from this backup because your phone will occasionally force everyone else blue.  This is a completely nuclear option that blows away all previous chat/conversation history but it was necessary for me to be able to continue using it.  My brother-in-law says that it shouldn't be too hard to switch the direction based on the changes in their code but we haven't started working on a build yet as our time is both pretty consumed with work.",1,1,dwhp
6327,"> You compared with 0.19.8 which doesn't make sense. In 0.19.8 the app uses default mechanism of handling the cutout, everything is done by Android. On the newest version there is no way to give this job to Android because of the need to use Fragment instead of an Activity.

Sorry about that. I haven't been testing debug versions for a while now, so I didn't think to compare it to a different debug build. But I can confirm that the builds by @B0pol all have the same issue. It's awful.

You're not supposed to align videos to the cutcout while using landscape because cutcouts come in many different sizes. If you do then you'll likely end up with off-centered videos and only having rounded corners on one side of your screen. That's why YouTube, VLC & many other players don't do anything about the notch and always center the videos to fit.",0,1,dwhp
6333,Tank you. I don't know how to do and where to post it...,0,0,dwhp
6334,It looks like you didn't run the tests. ,0,1,dwhp
6336,"Very strange. I still don't get why gcm always fucks up with textsecure for me, while whatsapp works just fine. Could anything I do with my phone cause the receiving phones to not receive messages anymore?
",0,1,dwhp
6340,"not something we have any control over
",0,1,dwhp
6345,"@ManfredKarrer wrote in https://github.com/bisq-network/exchange/issues/1193#issuecomment-357740300 (which I'm bringing over to this issue, because the other issue is just about keeping track of incidences of the problem. This issue is about _fixing_ it):

> @cbeams Damn, seems the assumption that the broadcast will succeed sufficiently even we don't hear back from peers was wrong. I will not be able to look closer to it until weekend due traveling...
> 
> When searching the log the tx id and the broadcast attempt should be visible. If there are no logs for hearing back from peers then the broadcast did not get confirmed by back reporting peers and our timeout triggered the completion of the broadcast. The app logs a msg like:
> ""Broadcast of tx .... not completed after .... sec. We optimistically assume that the tx broadcast succeeded...."".
> 
> People should not post json contract here as it contains the payment method details. I will remove the one above from user @Splitter8.
> 
> Instead the json contract the trade ID, taker fee tx, maker fee tx and deposit tx are sufficient. Would be good to post them in the posts so its easy to look up explorer and search log files with it. All those data are not privacy relevant and can be posted in plain text here.
> 
> The deposit tx is always failing if one of the trade fee txs was not getting into the blockchain.
> 
> We should also check logs on the bitcoin nodes to see if there was any issue.
> 
> Reporting users should also post the Bisq version they used at the time when they did the tx.
> 
> We should find anyone who knows more about the Tor network of has contact to Tor devs to get more background about the Tor issues. I saw that Tor connections are much slower and less reliable as they have been in the past. For BitcoinJ we could fall back to clear-net connections in the worst case (has some privacy issues). For the P2P network we don't have any alternative though.",0,1,dwhp
6346,"Note, this contributed to a nasty bug yesterday where VSCode ate all the filehandles and broke C++ compilation on santa-enabled google corp mac because XCode initialization fails after bazel clean, yuck...
https://github.com/bazelbuild/bazel/issues/4603#issuecomment-364269706
",0,1,dwhp
6349,"lukeis: there is **_no_ _client_ _connected_** to the standalone server, so it should not log anything at all, because **_there_ _are_ _no_ _events_ _to_ _log_**.

So anyone with a sane mind would **_expect_** the server to **_log_ _nothing_**.   How can you declare a source of mindless blabbering 'expected behavior'?  Are you a troll?
",0,1,dwhp
6353,"> Stop spamming the same comment with the same stacktrace! Please give us instead useful information to help us to identify the problem (see my comments above).
> 
> Otherwise, this issue couldn't be fixed.

Totally uncalled. The app gives the option to report bug via github. 

What additional info is needed? Open app click play get error. I will just avoid the app then. Buggying is hell for several months and developers call you spammer. ",0,1,dwhp
6355,That's a bug in your code,0,1,dwhp
6356,"This is weird... Usually CPU is about 2-4% after initial sync. Seems some environments cause some issues.  Can you post info about your setup (OS, ram,...)? Do you run inside a VM?",0,0,dwhp
6357,Please don't merge,0,0,dwhp
6360,"BTW I've done some research on how to fix this on Linux and macOS and have a pending change in review based on that. Read more here:

https://jmmv.dev/2019/11/bazel-process-wrapper.html
https://jmmv.dev/2019/11/wait-for-process-group.html
https://jmmv.dev/2019/11/wait-for-process-group-linux.html
https://jmmv.dev/2019/11/wait-for-process-group-darwin.html

</shameless-plug> ;)",0,0,dwhp
6362,"Thanks!
1. I'll change it to 9042.
2. Should I make it `double` for now, or just remove the mapping?  If I remove it people, won't be able to access VARINT data.  In general, I think we should hide any column with an unsupported type.
3.  IIRC, I had to put `RowUtil` in that package because the `Row` constructor is package protected (which sucks).

Let me know what you think about 2.  Then I'll make the changes and push this.  I'd also appreciate it if you can look at adding tests for super columns and more types (with nulls).
",0,1,dwhp
6364,"Holy scrolling, guys. Please truncate your quoted texts.",0,1,dwhp
6369,"BTW, thank you for providing an actual SSCCE! It's unbelievable how rare that is. Even asking directly usually gets code that doesn't compile. Even when it compiles I usually have to add a damn `main` method!",1,1,dwhp
6372,"it took me a while to figure what the hell happened here...
so the problem is that you're not draining the buffer in channelRead, only reading the first 4 bytes...
https://github.com/zman2013/net-pull/blob/master/src/test/java/client/NettyClient.java#L44
packets you send are not guaranteed to be split into seperate channelRead calls unless you add a splitter to your pipeline like VarintFrameEncoder/Decoder etc...",0,1,dwhp
6373,"Is this still happening? I love using emoji, but sadly am currently stuck with the ugly Samsung ones.
",0,1,dwhp
6374,"<s>This is insane...the bug is clearly FIXED in the first case, where it grows entries during first few seconds of connection only. For that case the fix was clearly testable and easy to test.
It doesn't fix the second case at all (""long uptime, high number of connections"") where given CONSTANT workload, it would grow the demand for [entries+buffer linked with them] to millions with uptime...
(7 million here while the maximum batch between flushes is 30 packets)
while monitoring the number of entries, it's like:
while(true) constant-constant-constant-time break-BOOM an increase

```
Sat Sep 19 19:10:52 CEST 2015
entries 978619



Sat Sep 19 19:11:33 CEST 2015
entries 978773

Sat Sep 19 19:11:51 CEST 2015
entries 980484

Sat Sep 19 19:14:17 CEST 2015
entries 982609
Sat Sep 19 19:14:21 CEST 2015
entries 991333
Sat Sep 19 19:14:23 CEST 2015
entries 992479
Sat Sep 19 19:14:27 CEST 2015

(this monitor reacts only when the number changes)
```

<s>@normanmaurer should I investigate again from the start or you know better where to look...?
There has to be another problem related to a case when it can't write all at once, when I proxied 300 local bots, there was no more $Entries than 200 in total.
To manage read() calls without autoread(which would make things even worse), I depend on a following behaviour:

<s>[some packets waiting in the queue]
<s>flush a batch of packets ->

```
    handle.write(Unpooled.EMPTY_BUFFER, promise );
    handle.flush();
```

<s>(while the promise, when finished, will call read() on the other side of proxying connection)

<s>is it correct to expect that it will only get called after EVERYTHING before has been written to the socket? Another place when I could place the .read() is the flush(ChannelHandlerContext) method, but I think it's not a good place because it's just a ""request"" to flush.

<s>(note that when I created the bug there was no packet batching in the code so it would also writeAndFlush every packet. Now it looks like I ""just"" need a better place to call read() on the downstream after making sure that EVERYTHING has been written?)</s>
",0,1,dwhp
6375,"awwww ... can we wait with the merge please. I will update this to a compare method tomorrow, as I actually need it now T_T (as @nmittler already knew in wise foresight :smile: )
",0,1,dwhp
6377,"Your PR was incorrect and repeated the exactly the same problem as was in the source. I incorporated the fix that was proposed in the bug report, which was correct. If you'd submitted that as a PR, I would have incorporated it. The personal insult about ""behavior"" is way out of bounds.
",0,1,dwhp
6380,"Yuck to the verbosity. I guess Spring Boot has the same feature, but we add an additional attribute (alias) for the macros. Any chance Spring MVC could add an alias as well with a configurable name?
",0,1,dwhp
6382,"Wow, that was damn fast. Thanks Moxie!

Am .06.2015, 20:15 Uhr, schrieb Moxie Marlinspike  
notifications@github.com:

> Closed #3475.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/WhisperSystems/TextSecure/issues/3475#event-340351455
",0,1,dwhp
6383,"> Just divide them by empty line.  

empty lines result valid sql to fail: #4048 

>Generally it doesn't make sense to create separate SQL editor for each query.  

it is created automatically by `Read data in SQL console`

>Results tab will be named by queried table name if this is supported by database driver (as you can see on your screenshot).  

I see that tabs named by same name. The driver, I use, is installed automatically by `dbeaver`

Please improve `dbeaver` to name tabs differently. That is ugly to have tabs with same name =(",0,1,dwhp
6384,"Darn it, we just added this assertion!",0,1,dwhp
6386,"If no more feedback on this PR, I'll merge it on coming Wed.",0,0,dwhp
6387,"LGTM yet the only thing that I am always wondering about is what are we doing if somebody has this thing stuck in the the translog as a delete by query? It's not sovleable to be honest unless we say `f*** it`
",0,1,dwhp
6391,"@kageiit haven't 100% looked into it, but I believe so from a quick glance.

That being said, I think there is more work to do as it would be pretty crappy for buck to not work by default and need to be configured in order to work with the latest ndk.",1,1,dwhp
6397,"@patheticpat This is the kind of change that is, unfortunately, difficult for us to handle appropriately.  There have been a few changes in the past that subtly influenced that Android backstack in ways that ended up biting us, but which we wouldn't have been able to test for.  It's possible that this change is more correct that the current code, but it's hard to evaluate that with certainty.

Is there something specifically in the existing behavior which you're trying to correct for?
",0,0,dwhp
6399,"I'll also chat with the BoringSSL guys.
",0,1,dwhp
6401,"Closing this like a boss.
",0,1,dwhp
6403,"OK, here is a run down:

**SCENARIO I: Click() using IWeb Element**
Win 7, SP 1 Enterprise SeleniumStandalone 3.13 32 bit IEDriverServer 3.13.1

- Chrome PASS
- IE 11 FAIL

Uninstall IE 11 (NEVER do this)
Re-install IE 11 (HORRIBLE)

- IE 11 FAIL

**SCENARIO II: Click() using IWeb Element** 
Win 7, SP 1 Enterprise SeleniumStandalone 3.13 64 bit IEDriverServer 3.13.1

- Chrome PASS
- IE 11 PASS

**SCENARIO III: Click() using Actions/MoveTo**
Win 7, SP 1 Enterprise SeleniumStandalone 3.13 32 bit IEDriverServer 3.13.1

- Chrome PASS
- IE 11 PASS

**SCENARIO IV: Click() using IWeb Element**
Win 10 SeleniumStandalone 3.13 32 bit IEDriverServer 3.13.1

- Edge PASS
- IE 11 FAIL

**Data:**
For the IWebElement Click
Win 7 Driver-64 Bit: IE 11 Good, Chrome Good
Win 7 Driver-32 Bit: IE 11 Bad, Chrome Good
Win 10 Driver-32 Bit: IE 11 Bad, Edge Good

For the Actions/MoveTo
Win 7 Driver-32 Bit: IE 11 Good, Chrome Good

**Analysis:**
The problem is isolated to 32 Bit IEDriverServer using IWebElement Click Method, generates a browser-crashing  error on Win 7 or Win 10

**_Light thickens, and the crow Makes wing to th' roaky wood._**",0,0,dwhp
6404,"What the hell, this change has no export number???
",0,1,dwhp
6405,"The account @kn2718 belongs to the same spammer, he's doing the same shit on Reddit https://www.reddit.com/user/kn2718/.",0,1,dwhp
6408,"This is terrible. Does anyone at least know *why* it happens? Is it just random or is there a workaround? Autocrypt is enabled for sender and receiver. We've exchanged multiple mails. But still, new mails are not encrypted by default no matter what I try. Sorry for the harsh words but in my opinion that was a *seriously* dumb decision. 

I have public keys from people in my key store for a reason. And then I write a mail, but still it isn't encrypted. You remove a perfectly simple and even for novices easy-to-understand option (you have a public key of someone, you write them encrypted mail), recommend enabling AutoCrypt (which would be perfectly okay for me as a workaround) but then enabling this option doesn't have any effect. You just can't make this stuff up.

And the blog post explaining why this is introduced is so extremely flawed in its argumentation that I just don't know where to begin with. If I'd put on my tinfoil hat for a moment, I'd almost assume this is done intentionally to keep people from encrypting mails.

But, okay, I understand there's probably no real way forward here (ironic how the blog post claims it's ""the only way forward"" while the exact opposite is true) since this mess seems to be intended. 

AquaMail unfortunately doesn't support PGP. Would even pay for it. There is a new player called ""FairEmail"", but the author also has very peculiar ideas. For example, you have to confirm every link you click in a special dialog window. Unusable in a commercial environment where you get lots of mails from JIRA etc. everyday. But perhaps it works for some people. 

Sad that in 2019 this is still so unnecessarily broken at a time where Thunderbird has vowed to modernize their PGP support and deliver it built-in without a plugin in the future. Now, we'd only need a decent mail client with PGP support for Android. Sigh...
",1,1,dwhp
6411,"> Ugh, I'd much, much rather not do this because it pokes a hole into the ""all headers that are used need to be specified"" rule we have.

Except that header inclusion checking (which cannot be disabled) prevents any header not specified in a dependency's manifest from successfully building.

> What do you need this for?

Build performance.  Headers are transitive through dependencies, which means that, without include scanning, if a target declares headers, and a header it owns is modified, every single translation-unit-member of srcs for every transitive dependent is recompiled, whether it includes the header or not. I refuse to make single-header targets out of my current build - the collections of headers represent meaningful interfaces, and it would make the build definitions unbelievably intractable and unmaintainable.",0,1,dwhp
6415,"tested, works, is sexy. merging.
",0,1,dwhp
6419,"I could go back to resolve conflicts & try to make it work again, but this PR has been ignored by the Signal people for 3 years now.  I don't think they are interested for some reason.  This really sucks as the electron desktop version on Chromebook is still pretty awful on several fronts and the Android on Chromebook experience is so good.  :sigh:",0,1,dwhp
6423,How the heck did I miss that? ðŸ˜­,0,1,dwhp
6426,please don't close this.,0,0,dwhp
6429,"**FIXED!!!**

I was looking to Elasticsearch as the prog that broke down, but now it seems that after the upgrade to Graylog 2.2.0-beta.4+6f4b4d3 this is not allow anymore! Took me f*cking 6 hours to find out :(.

$template GRAYLOGRFC5424,""<%PRI%>%PROTOCOL-VERSION% %TIMESTAMP:::date-rfc3339% %HOSTNAME% %APP-NAME% %PROCID% %MSGID% %STRUCTURED-DATA% %msg%\n""
*.* @@192.168.12.12:5140;GRAYLOGRFC5424
",0,1,dwhp
6432,"@moxie0 Here's what I did
Step inside McDonald's
Connect to their WiFi WITHOUT ""logging in""/accepting their license crap
Write a message
Wait 15-20 seconds

Then the app whines about the message not being delivered.
",0,1,dwhp
6435,"You deserve a medal.
",0,0,dwhp
6437,"@jimschubert only the original message mentioned potentially mirroring, and your comment. ðŸ¤”

Most folks here are saying hard pass on migration, and thatâ€™s definitely what Iâ€™m trying to say too. 

To consider mirroring, I have serious concerns about what youâ€™re proposing. If the conversation was â€œCan we set up a mirror with everything else turned off just so we can speed up mavenâ€ the.mn thatâ€™s something sure, but pure proposing duplicating issue trackers, project management, and fracturing development between two things. Thatâ€™s gonna be a royal pain in the backside. ðŸ˜…

Again, it sounds like the Travis workflow needs to be re-evaluated. Replacing TravisCI and that myriad of other things with CircleCI using prebuilt docker images as a base and workflows to parallelise various different long running processes is probably going to be more useful than adding yet another service to the stack for what should be a fairly simple open source project. ",0,1,dwhp
6439,Seems Oracle fucked up quite a lot with their new release concept. Those java 9 and java 10 realease are alpha releases causing tons of problems and should not be considered production releases. People will learn to not update anymore before a LTS release is out again. At least I will not want to repeat that experience again soon...,0,1,dwhp
6440,"_From f...@processing.org on June 20, 2011 00:24:30_
This is because if you only use JavaScript mode, or Android, or whatever, it would drive you completely nuts to have it keep opening in Java mode.
",0,1,dwhp
6444,Enabling RULE hint made life so much better for me! Can't believe I waiting all this time to look this issue up. ,0,0,dwhp
6445,"Of course, now my super simple gist works fine :(

Hmm, interestingly I'm not the first person to have this problem happen consistently and then disappear the next day....

https://github.com/elasticsearch/elasticsearch/issues/1263 (vanished on all versions after installing a new version; some other dude had a different but similar NPE still open?)
https://github.com/elasticsearch/elasticsearch/issues/1510 (still open, you couldn't reproduce)
http://pastebin.com/8rxqY954 (couldn't find the parent issue - the code for this one was uncannily similar to my report)

Up to you whether to keep this one open or close it (or join it?)
",0,1,dwhp
6446,"For me it's fault of deadlock risky robolectric implementation. For all robolectric maintainers it's a fault by you.

See related topics
https://github.com/robolectric/robolectric/pull/1097

https://github.com/robolectric/robolectric/pull/1061

https://github.com/robolectric/robolectric/issues/1043

When I'm back from my vacation, then I try to create a less deadlock risky robilectric version. 

They had never explained why it should be stay how it is implemented. Instead they only say we are dump and write ugly tests.
",0,1,dwhp
6447,It's a matter of perspective. Another user could easily claim that leaving incomplete corrupted files lying around in storage causing bloat is what's utterly insane.,0,1,dwhp
6448,"> Beware

Threatening to remove my privileges is not only terrible manners and ham-fisted moderation (you're intervening on the wrong end of the discussion), but also completely pointless.

Do you seriously think I pine for the chance to spend my time detailing long-standing issues that exist with the application, only for that, albeit small, effort to be summarily discarded by someone whose only contribution to this project appears to talk endlessly in circles to make issues go away by attrition and disregard ?

Feel free to keep whatever contributors you value, the dysfunctional practices, and your threats, as well, as I don't care for any of that and whatever little free time I have, you were very successful in convincing me, it is better spent on other projects with a healthier climate and better practices.

PS: unstarred and ignored, you have a fine day now.",0,1,dwhp
6450,"@Rominet13: Only if you watch the video, not when you listen in the background. But still, could you link a video where the sound quality is horrible?",1,1,dwhp
6451,"robovm is the new ios :)
On Mar 31, 2014 10:09 PM, ""Florian BÃ¤thge"" notifications@github.com wrote:

> Helllo everybody,
> 
> I've just downloaded the setup-ui of the latest nightly build and am
> experiencing some issues when creating a new project.
> 
> Usually, the setup-ui created a ""MyProject-ios"" folder containing the
> necessary build stuff. Using the nightly build from March 31st, the
> setup-ui doesn't create this folder anymore.
> Also, the setup-ui just shows me that it's planning to create the base
> project, android, html, robovom as well as a ""prj-ios"" folder. But this one
> isn't created either.
> 
> Hope it's not an issue caused by me...
> 
> ## 
> 
> Reply to this email directly or view it on GitHubhttps://github.com/libgdx/libgdx/issues/1604
> .
",1,0,dwhp
6452,"1. Because the tokens display useful information aside from display name and address.
2. Because displaying it like that is ugly.",0,1,dwhp
6455,"wow, sweet! what's the perf like?
",0,1,dwhp
6456,"I had the same idea, but it didn't help.
",0,0,dwhp
6459,"Had to nuke that one because of bullshit commits in it. I HATE GIT, FUCKING GIT... <-- @UweSays
",0,1,dwhp
6461,"Darn - that's unfortunate. I thought we had addressed this as I have a memory of some discussion on it, but perhaps not

https://github.com/ankidroid/Anki-Android/blob/8e73956efc03b8ef97317e28cac38664569a270c/AnkiDroid/src/main/assets/mathjax/conf.js#L25

https://github.com/ankidroid/Anki-Android/tree/master/AnkiDroid/src/main/assets/mathjax/input/tex/extensions

mhchem is in there, what are we missing? Perhaps @hgiesel knows :thinking: ",0,1,dwhp
6462,"Haven't heard back in almost a year, closing.",0,0,dwhp
6463,"```
Motorola Cliq XT, Android 1.5

Attached are files in:

/sdcard/!Anki/Temp.media
/sdcard/!Anki/Temp.anki

Tested working on 0.5 alpha 1
Tested working on 0.4.1
Tested working on 0.4.1 beta 2

The PNG files are random from images.google.com -- one big, one small.

Maybe Ankidroid just likes my crappy Cliq XT.  ;)
```

Reported by `slnkez` on 2010-08-17 13:32:35

<hr>
- _Attachment: [Temp.anki](https://storage.googleapis.com/google-code-attachments/ankidroid/issue-121/comment-12/Temp.anki)_
- _Attachment: 8c3aa94b4a61cfb9d8e046d880123573.png<br>![8c3aa94b4a61cfb9d8e046d880123573.png](https://storage.googleapis.com/google-code-attachments/ankidroid/issue-121/comment-12/8c3aa94b4a61cfb9d8e046d880123573.png)_
- _Attachment: d5bec6fc3a56a985647b5163d803f479.png<br>![d5bec6fc3a56a985647b5163d803f479.png](https://storage.googleapis.com/google-code-attachments/ankidroid/issue-121/comment-12/d5bec6fc3a56a985647b5163d803f479.png)_
",0,1,dwhp
6468,"Ideally this could replace the YouTube app for no-gapp devices like mine. I don't know what your plans are for this, but the mobile YouTube experience sucks, so having an app to intercept youtube links would be awesome.
",1,1,dwhp
6469,"I had a very similar use case in the past and I ended up going the ""non-optimal"" way which I also felt is ugly code for a complex app. I hope this gets picked. My vote here!",0,1,dwhp
6470,"I'm not sure what happened, I'm relatively sure I didn't publish 2.1.11 to central... Anyway, seems I caused some trouble so let me fix it ASAP.

I got a ping from the spring-boot guys a few hours ago, and now a new and better 2.1.11 has been released :shipit: - with, among other fixes, contains a new `git.tags` (of current commit) feature.

Hope you'll accept this as sorry-gift for the trouble which it seems to have caused :-)
Happy hakking!
",0,1,dwhp
6471,"Duplicate of #2847. Keep doing this and we'll ban you from the organization.
",0,1,dwhp
6473,"I also have a suggest that the property 'markupEnabled' should bind to Widget like Label/TextField/TextArea.

Currently on BitmapFontData, if using one BitmapFont(same .fnt) in two widget(one for input, no colouration), then ugly...",0,1,dwhp
6475,"> For the lucene FSDirectory case, where typically some read/write will happen immediately after (e.g. lockfactory initialization), we may be fine with something simpler, like createDirectories(toRealPath()), so its just changing the order of two lines of code there if we want to do that.

Actually I am glad that I repaired the LockFactory shit last September for Lucene 5 :-) The LockFactory has no initialization anymore, it is a singleton. But the actual makeLock call passes the directory, so we are sure that everything fits each other!

But I agree, we should do the tests just before we do the mkdirs() with the already fully resolved path.
",0,0,dwhp
6476,"Have you signed a CLA?
",0,0,dwhp
6478,"We have a sample right? So it should break if it's a simple bug. I suspect the devil will be in the detail of the user's own code (forcing initialization order in a way we haven't anticipated). We need a project that reproduces the issue.
",0,1,dwhp
6480,"> Then I think we'd reintroduce this:
> 
> #5222
> 
> which was fairly ugly as well.

Quite the tangled web.
",0,1,dwhp
6484,"@agrajaghh One would think that everyone with WA still installed (shame, shame) should have this issue...
",0,1,dwhp
6487,"> > @suneet-s You should also exclude the new group QUICKSTART_COMPATIBLE from travis ""other integration test"".
> > I feel like that ""other integration test"" instead of doing exclude should just do include
> 
> The rationale for excluding was that it's better to accidentally run tests in multiple jobs rather than having a test run in no jobs, but we can adjust the approach if needed.

That's true but I think as we add more and more IT, especially those unique ones that needs special setup, that exclude list will be super long and ugly  ",0,1,dwhp
6489,"Yes please, I used YouTube before it was bought by Google. Before it was flooded with videos where YouTubers make childish facial expressions for the thumbnails. The best solution would be a random frame somewhere from the middle. These facial expressions are nightmare fuel.",0,1,dwhp
6496,"Same issue here.
On every Message with an attached Image the Phone is going crazy.

Huawei P30 Pro
",0,1,dwhp
6499," ![We Are Just So Friggin Awesome](http://www.troll.me/images/business-cat-needs/good-job.jpg) 
",0,0,dwhp
6500,"> I hate it so much that i went nuclear and reverted to 5.3.0, I may fork 5.4.x and maintain one with the original message color if i ever have a few weeks to go through all their spaghetti nodejs stuff and will not accept pull requests to whomever changed my message colors.

@klikevil Do you have a link to this that you're working on? I'd gladly support the old-color fork and cherry-pick to get the other future updates without the new color bullshit.",0,1,dwhp
6501,"Oh, I didn't notice. But if there are some nasty bugs, I'd wait indeed.
",0,1,dwhp
6503,"> Can we remove the variants of error, warn, hell maybe all logger methods that dont take throwable?

+1
",1,1,dwhp
6504,"I might potentially be able to run on my wife's mac if I turn off all the
actual models.  Just to test the server component.  Otherwise I have no
hope of finding a Mac until Armageddon finishes.
",1,1,dwhp
6505,"Sounds like your libgdx code/JAR isn't up to date?
",0,1,dwhp
6508,"Haven't heard back in months, closing.",0,1,dwhp
6513,"Yep, if I have the ""do.call()"" in the file and source it, then when it enters the browser the console seems to behave normally. But if I execute the ""do.call()"" with ctrl+enter or in the console itself, it hangs up. Good catch, although I have no idea what it means.

Also, there's something weird going on with the timing. For example, this output of a single character string shows up as having zero elapsed time, but in fact it took about 30 seconds or so.

```
Browse[1]> system.time(""bummer"")
   user  system elapsed 
      0       0       0 
```",0,0,dwhp
6514,"I think I fucked this PR up :D
",0,1,dwhp
6517,"```
A quick workaround is to add this to your note's CSS:
ruby { margin-top: .3em; }

We're using tables to do furigana, but we're probably better off copying what desktop
Anki does: https://github.com/dae/anki/blob/master/aqt/qt.py#L40

Tables can get ugly. I've noticed furigana is now also left-aligned in Anki 2, so this
would fix that too without a manual addition by users. Too bad we can't use ruby tags
since they aren't supported in earlier Androids. 

(Shameless plug: I wrote an add-on for Anki to edit css in AnkiDroid https://ankiweb.net/shared/info/1880763157)
```

Reported by `Houssam.Salem.Au` on 2013-02-01 08:42:12
",0,1,dwhp
6522,Same problem. Driving me crazy!,0,1,dwhp
6529,":+1: 

a side-note: We should implement a real blocking OverflowPolicy in the RingBuffer. As the current implemention of `BLOCK` TopicOverloadPolicy is ugly and inefficient. 
",0,1,dwhp
6530,"@kseniiaguzeeva Have you even read this thread? I know you can save a filter with that button but what is the point of a ""SAVE"" button if it doesn't persist? So dumb.",0,1,dwhp
6534,"BTW, the reason I set this as critical is because even though it's a weird corner case, it can result in an admin getting his user name changed without his knowledge.  Then he won't be able to log in unless he happens to find out that his user name is now his email address.",0,0,dwhp
6535,"That would be fine. Ugly as hell, but hopefully someone might step in with a nicer on-the-tab editing solution (tricky part there being that tabs are sometimes collapsed, etc).
",0,1,dwhp
6539,"if you run onto problems , lets us know here (do not hesitate, even problems are super stupid/simple), we will guide and help with development process.
Review our help pages - http://checkstyle.sourceforge.net/beginning_development.html  ,  review whole section ""Developers"" at our web site.",0,1,dwhp
6542,"New PR coming.
",0,0,dwhp
6543,"Thanks, it must just be the crappy github diff renderer then",0,1,dwhp
6544,"@xian We tried running this code against 4.1.2 and, predictably, there are about 50 failures (roughly how many we had when we swapped in 4.3 originally). 

So we need a way to write code across Robolectric that is SDK-version-aware. We took a quick look at getting the SDK version switching working but we need to know the `emulateSdk` value in some random pieces of Robolectric. Do you have any ideas of where we could store (and manage) this value globally so we can write code that switches on SDK version?

Globals suck though... it would be nice if this value (or the whole Config instance) could be injected into every Shadow on construction. Possible? Other classes would need it as well but most of those are constructed in some way via the TestRunner, so injecting the Config down from there might work.

Corey & Mike
",0,1,dwhp
6545,"@Vampire

>Checks that particular classes or interfaces are never used as type parameters.

https://docs.oracle.com/javase/tutorial/java/generics/bounded.html
Check do more than simply type parameters. It does Parameters types, field types ... . Can you suggest better rationale ?
Check code: https://github.com/checkstyle/checkstyle/blob/master/src/main/java/com/puppycrawl/tools/checkstyle/checks/coding/IllegalTypeCheck.java",0,0,dwhp
6547,"Today it happens no more.
This is a bit crazy.",0,1,dwhp
6548,"Screw it, Do not even try that thing above. But this looks promising. you can also try to replace the `value` attr with the `default` one.
``` diff
diff --git a/checkstyle.xml b/checkstyle.xml
index af8c5480f88..c1405c2ce36 100644
--- a/checkstyle.xml
+++ b/checkstyle.xml
@@ -23,7 +23,7 @@
     <!-- https://checkstyle.org/config_filters.html#SuppressionFilter -->
     <module name=""SuppressionFilter"">
         <property name=""file"" value=""${org.checkstyle.sun.suppressionfilter.config}""
-                  default=""checkstyle-suppressions.xml"" />
+                  default=""${config_loc}/checkstyle-suppressions.xml"" />
         <property name=""optional"" value=""true""/>
     </module>
```",0,1,dwhp
6551,"Damn, I need to fix it one more time :D",0,1,dwhp
6552,"Trying to get this goog.style to work but it never returns anything.  Any ideas jleyba?
",0,1,dwhp
6554,...and so it goes! Software never fails to surprise.,0,1,dwhp
6555,"I rebased this PR. However, I screwed up while doing so. As a result my new commits are trash. Sorry for that.",0,1,dwhp
6557,"Hey, I've actually sorted it out. My phone's time and date settings change at some point, somehow, putting my phone in the future. So the phone would have considered every message as new until we reach June 4th, date when they were supposedly sent. Now I put it back to normal, and I have another problem: everything that updated content (news, weather forecast) in the future refuses to update, because it thinks it'd downdate. Only  week to go and everything will be back to normal.

Now you know how you can piss off your friends: set their phone to the future. Woooo.
",0,1,dwhp
6559,"Yes, I am fully aware that the proper way to secure your phone is to use the lockscreen against online and the TS password for offline attacks. But most people don't realize this. 

And I don't have any statistics to point to, but a _lot_ of people simply don't set a lockscreen protection at all, mostly because they are too lazy to enter it every time they want to use the phone.
Many are reasoning that only their messages are valuable enough to be protected and that TextSecure is going to take care of that.

To me the question is what bad can it do if we protect them better? I would even vote for a pin lock for TextSecure (if no lockscreen pin/pattern is set), because of two reasons:
- people are lazy as hell and they are more likely to use a pin lock if they are asked less often (only for TS messages, not every time they use the phone)
- people don't think about the question if their whole workflow is secure, but that installing one app is enough. They don't realize that the decision not to set a lock screen password a year ago will expose their ""secure"" messages even after installing TextSecure.

I propose this kinda messy security model (in addition to the current one) because messy is exactly how humans treat security, either because they are lazy or because they don't fully understand how their decisions impact their security.
",0,1,dwhp
6561,"Holy crap, where were you three months ago!?
",1,1,dwhp
6563,"Done, they're deprecated now. A bit ugly to have to call with the fully qualified org.openapitools.codegen.utils.StringUtils.* name (as long as the methods are not removed, the names clash), but it seems to work. Might be cleaner to rename the methods.",0,1,dwhp
6565,Ah damn Bitcoin RPC devs keep breaking APIs. @jmacxx could you have a look?,0,1,dwhp
6569,It looks like you tried making them mutable and commented that it was a 'horrible abstraction leak' -- I think I agree. Are you thinking now that it's worth it for the memory savings even due to the leaky abstraction? Mutable DataSegments seem to me like something that we should try to avoid if possible.,0,1,dwhp
6570,"Can we please open a bug @ Gradle. This is total BULLSHIT! Why should it be smart at all? There is no reason! If one configures version ""1.8"" it should just pass it down to javac no matter which version it is. The good thing is that javac also complains if it is too old for the version you configured.

Total sonsense. The more I look into Gradle, the more it annoys me.

Just be safe and always pass -source and -target as configured! PERIOD.
",0,1,dwhp
6575,"@voiceinsideyou na it not sounds churlish... Thanks let me check and fix for real!
",0,1,dwhp
6578,"@jellium :

I am going to answer your questions here instead of spreading them across several issues.

> @jellium wrote in https://github.com/WhisperSystems/TextSecure/issues/3555#issuecomment-121873682:
> 
> @wp9015362
> 
> why is TextSecure's ""Emoji experience"" good and native Android's ""Emoji experience"" bad, IYHO?

IMHO the native Android Emoji experience is bad because it is very inconsistent.

HTC Android devices are using their own HTC Emoji set, Samsung Android devices are using their own Samsung Emoji set, LG Android devices are using their own LG Emoji set, Google Android devices (Nexus) are using their own Google Emoji set and so on...

Everyone sees different Emojis. That just sucks. Especially because of the issue which is described in the following article:

http://www.huffingtonpost.com/2014/06/27/emoji-meaning_n_5530638.html

In TextSecure however, all the users are seeing the same Emojis, which makes for a much better Emoji experience.

> @jellium wrote in https://github.com/WhisperSystems/TextSecure/issues/3445#issuecomment-121871752:
> 
> And also it is not true that TextSecure makes sure that the Emojis seen on emitter and recipient phones are the same. Just tried with 2.22.2 version to my lady who didn't update: she sees the Google Emojis while I have the ugly Apple ones. 

That is your fault. If you are using 2.22.2 at the moment, then you are on the beta release channel, since 2.22.2 has not yet been released to the stable release channel.

Your ""lady"" probably is using the stable release channel though, which means she is still using 2.21.0, which means she has not yet been updated to the new Emojis.

Once she will update to 2.22.x, she will see the same Emojis that you are seeing.

So, it's not TextSecure's fault. It's yours (because you are using a beta version).

> @jellium wrote in https://github.com/WhisperSystems/TextSecure/issues/3662#issuecomment-121870421:
> 
> Why on earth did you switch to Apple emojis?!

Well, I can not speak for the TextSecure developers, but:

IMHO the Google ""Noto Color Emoji"" Emoji set looks like shi*.

Let's face it, most people (or at least a lot), do not like the Google ""Noto Color Emoji"" Emoji set.

If you don't believe me, then maybe you should read the comments in the following two issues on the AOSP issue tracker:

https://code.google.com/p/android/issues/detail?id=69755
https://code.google.com/p/android/issues/detail?id=73248

And maybe you should also read the comments in the following XDA-Developers forum thread:

http://forum.xda-developers.com/showthread.php?t=2722892

The Apple Emoji set looks a heck of a lot better and is much more popular than the Google ""Noto Color Emoji"" Emoji set.

The Apple Emoji set is being used in Telegram, ICQ, WhatsApp and so on... (just to name a few examples)

And even the Twitter ""Twemoji"" Emoji set and the Emoji One Emoji set clearly are modelled after the Apple Emoji set.

So, it's quite popular and therefore pretty much is the natural choice.

Also, Signal-iOS (the iOS counterpart to TextSecure) of course uses the Apple Emoji set, since all iOS devices are using it natively.

So, now that TextSecure for Android also uses the Apple Emoji set, the Emoji experience is much more consistent across both platforms, which lessens the risk of Emojis getting lost in translation as described in the following article:

http://www.huffingtonpost.com/2014/06/27/emoji-meaning_n_5530638.html

Regards
",0,1,dwhp
6581,Looks like you already have the code,0,1,dwhp
6583,"@nmittler - Fair warning...it is ugly to ensure we process all events and complete all tree operations and also handle the `RuntimeExceptions` :(
",0,1,dwhp
6584,"Lgtm

On Sep 26, 2016 12:40 PM, ""Jason Tedor"" notifications@github.com wrote:

> Today when executing the install plugin command without a plugin id, we
> end up throwing an NPE because the plugin id is null yet we just keep
> going (ultimatley we try to lookup the null plugin id in a set, the
> direct cause of the NPE). This commit modifies the install command so
> 
> ## that a missing plugin id is detected and help is provided to the user.
> 
> You can view, comment on, or merge this pull request online at:
> 
>   https://github.com/elastic/elasticsearch/pull/20660
> Commit Summary
> - Provide error message when plugin id is missing
> 
> File Changes
> - _M_ core/src/main/java/org/elasticsearch/plugins/
>   InstallPluginCommand.java
>   https://github.com/elastic/elasticsearch/pull/20660/files#diff-0 (3)
> - _M_ qa/evil-tests/src/test/java/org/elasticsearch/plugins/
>   InstallPluginCommandTests.java
>   https://github.com/elastic/elasticsearch/pull/20660/files#diff-1 (6)
> 
> Patch Links:
> - https://github.com/elastic/elasticsearch/pull/20660.patch
> - https://github.com/elastic/elasticsearch/pull/20660.diff
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/elastic/elasticsearch/pull/20660, or mute the thread
> https://github.com/notifications/unsubscribe-auth/AANLogc-W_rIkFQTs3USMHiRTO03PiQfks5qt6EtgaJpZM4KGYDa
> .
",1,0,dwhp
6588,Looks like they don't have a jwk set uri,0,1,dwhp
6592,"lol, I see how seriously (as in not) this project takes backups... what a joke. I know that there are limited human resources here, but I'm demonstrating demand for this feature (in addition to others showing demand), and you're just shutting down my request by closing the ticket. What a horrible response.

Why exactly have the ability for users to submit feature requests if they're just gonna get closed?
",0,1,dwhp
6596,"> Is there a way to do that reliably (or at all)?

There doesn't seem to be an easy way, I might be talking out of my ass when suggesting this solution =\

> But damn, was it a buggy road.

Didn't expect it to have that much resistance, what was the buggy behaviour?
But in any case, good work, I think this is good to go. 
Will be testing it out on a real device the next couple of days.",0,1,dwhp
6597,"Ugh - every project I'm on seems to do this from time to time and it has a negative effect, it makes git blame effectively worthless for it's default use case. So I like the idea but in practice it's a mess. We do not currently have good tooling to auto-format on checkin etc.. I'd like to not do this if possible with one way I'd agree - add git commit hooks and auto-formatting pre-commit where the command line formatting and IntelliJ formatting are ruled by the same file.

Stated more succinctly: I'm only positive on this if it never ever ever happens again, and there's command line tooling to enforce it strictly",0,1,dwhp
6602,"Thank you for your response!
I do understand the risks when applying it without due care. However, at the same time there are plenty other ways to screw your data.

Could you please elaborate on ""You will miss documents though, but that is the price we pay for not coordinating refreshes across shards""? Is that in case we delete first?
Also, I'm not sure if I missed a feature in Elasticsearch or you're referring to some script to run outside elasticsearch when you say ""Personally I'd write a script that deletes and indexes each document"". Could you please clarify?

And one more thing, ""folks can always do the _scroll/_bulk process themselves"" assumes physically loading documents to the application and then reindexing them from that application back, did I get that right?

I know it is recommended to delete first, but in order to not lose a single document we can't do that (there are plenty possible failure scenarios that would interrupt indexing documents back). The way our index is set up it is easy to find and delete old copies (that has been accounted for) - the new copies will belong to new parent and both have some identifying information, so it's easy to delete old copies by query.

Thanks again for taking time to respond!",0,1,dwhp
6603,"Huh, funny, I did not know I granted your permissions to my branch. I guess it is that ""Allow edits from maintainers."" checkbox. Not that I mind, commit away. Ruin the evil beast. ;)

@brad4d - thank you!",0,1,dwhp
6604,"You are right @shitovserg, the Compensation Intermediate Throwing Event is still missing from the Modeler. Are in interested in providing a PR by adding it?",0,0,dwhp
6607,"```
Grumble...fine...thankfully the rooting process didn't brick my phone.  I lost my previous
Windows phone to screwing around with it, so I'm gun-shy.  Here's the screenshot. 
You can see the 'y' looks a lot like a 'v' and the bottom of the 'g' is cut off.
```

Reported by `shirtsNB` on 2012-04-11 11:01:41

<hr>
- _Attachment: shot_000001.png<br>![shot_000001.png](https://storage.googleapis.com/google-code-attachments/ankidroid/issue-1059/comment-4/shot_000001.png)_
",1,1,dwhp
6608,"Yes, I understand there are workarounds, but it's an ugly way to do it. Should I make a PR?
",0,1,dwhp
6610,"Silly coveralls
",0,1,dwhp
6612,"Some sellers have been releasing the funds after mediation has been submitted because sellers are not replying or releasing funds. From #support keybase channel:

> ""there is something really bad about the protocol of mediation.  
> 
> i had one trade that should last 24 hours. But the seller just dissapear never confirming my payment.  so i opened a mediation which take about 2-3 days more, so he had a lot of time to release until that time. mediation case was close with a deal proposed and a penalty for the almost 5 days delay of the trade. 
> 1 hour after the deal was offered by mediation, the seller release normally ignoring the deal and getaway without any penalty for all this delay. 
> 
> not fair at all.""

It was knew it was possible to sign the normal payout even with the button deactivated, but I still don't know why they do it since they just keep their deposit locked for a longer time and gain nothing but pissing buyers.",0,1,dwhp
6613,"This is a really annoying bug, and happening a lot.
",0,1,dwhp
6614,"What does your code look like? I only get this when I do the shadow of it.
",0,1,dwhp
6622,"This is the behaviour using macOS.. Even weirder than Windows xD

https://user-images.githubusercontent.com/5543339/156842860-87f58c4f-f149-4704-926f-313d244966fc.mp4


",0,0,dwhp
6624,That is most definitely not the right fix and incredibly dangerous!,0,1,dwhp
6626,"the sooner the better.
",0,1,dwhp
6627,"Our automated prompt bot is going a bit mad, removing the label until we work out why.
",0,1,dwhp
6632,"immutability should be your mantra
",0,1,dwhp
6640,"As designed above (cancel trade) is a simplistic ""I fucked up, here take my money and call it quits"" approach.

Mediation **works better** because it is more flexible both in negotiation and payout.

@burningman2 Thanks - what you describe above is exactly what I implemented.  Having tried it I think it is inflexible and would not be used because users think they are losing a lot of money and can get a better deal from mediation.

---

My current thoughts:
For users to resolve the trade between themselves, allow them to negotiate, sign/publish their 2 of 2 payout transaction using amounts that they agree upon.  Could be implemented by adding functionality to the trader chat GUI.  Should I open a DAO proposal for that @wiz ?
",0,1,dwhp
6644,"Me too, Samsung Galaxy S7.

It might be nice to confirm @kofalt's comment above, will my app really stop working in 9 days? Scary.
",0,0,dwhp
6645,"This is a damn good PR, it should be merged. @Tom-Ski ?",0,1,dwhp
6647,"I'm currently refactoring the `Download*` classes, and I thought in getting rid of this, the normal user don't have any idea of what the hell is this.

So I'll default the threads to 8 (which is a good number) and maybe include an option in the settings for the more advanced users. What do you think?

PS: I guess getting rid of this GigaGet is the best option, it's an old project",0,1,dwhp
6650,"> Anyone having a fixed position header can set capability elementScrollBehavior to 1 that makes Selenium to scroll elements to the bottom (default is 0 that means scrolling to the top).

Until a new release is made it won't work, because patch for #1200 is not released.

> I'm trying to figure out a sensible solution for suck cases. May be we should check if the element is overlapped by a fixed position element, and if this happens we have to try scrolling a bit in the ""opposite"" direction. But this ""solution"" drives me crazy...

IMO it makes sense to raise exception just like Chrome do now, so that clients can handle it (in this case, by closing the fixed position header).
",0,1,dwhp
6653,Let @greyson-signal closing of this issue without comment be a testament to how much of a crap the developers of this faux-security app give about their users.,1,1,dwhp
6655,"I had rJava working last month but now again the same error (Upgraded to High Sierra and played with .profile a bit during the month setting up NVIDIA - if that helps).

But this is a never-ending pain (rJava sucks).

After  I ran `sudo R CMD javareconf` on the mac terminal, this is what I get. rJava still does not work.

```
Sanjays-iMac:bank-statements sm$ sudo R CMD javareconf
Password:
Java interpreter : /usr/bin/java
Java version     : 10.0.1
Java home path   : /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home
Java compiler    : /usr/bin/javac
Java headers gen.: /usr/bin/javah
Java archive tool: /usr/bin/jar

trying to compile and link a JNI program
detected JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin
detected JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm
clang -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG -I/Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/include/darwin  -I/usr/local/include   -fPIC  -Wall -g -O2  -c conftest.c -o conftest.o
clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o conftest.so conftest.o -L/Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home/lib/server -ljvm -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
ld: warning: text-based stub file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation.tbd and library file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation are out of sync. Falling back to library file for linking.


JAVA_HOME        : /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home
Java library path: $(JAVA_HOME)/lib/server
JNI cpp flags    : -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/darwin
JNI linker flags : -L$(JAVA_HOME)/lib/server -ljvm
Updating Java configuration in /Library/Frameworks/R.framework/Resources
Done.
```",1,1,dwhp
6657,"Closing as something out of our control. 
",0,1,dwhp
6658,"> I have this issue as well, no notifications and calls go to missed call immediately.
> 
> 3 users, 4 phones.
> 
> Moto G7 with LineageOS
> iPhone 7 and iPhone 11, same user upgraded his phone, same bug, no notifications.
> Samsung Galaxy A8
> 
> These are non technical users and I won't be troubleshooting with them. We have given Signal a try but it is not functional. I will not try to move users to the platform any longer.

Ah yes we've had this problem too in the past - Signal doesn't ring on the persons phone who you are trying to call. It just leaves a missed call notification but never rings!

Unfortunately most people don't report bugs these days, they just go to a new app. I wish it wasn't so hard to report bugs. My partner and her friend gave up on Signal and went back to Whatsapp. I'll be damned if I ever use Whatsapp again, that's why I made this bug report!",0,1,dwhp
6660,"That sucks! You are very likely better off running reindex with a size of 1
synchronously and then kicking it off asynchronously.

At this point even if no matter the outcome of this issue you'll get the
fix for your async status  problem first because that is in 5.0. Any new
feature would only be available in subsequent versions anyway.

On Oct 5, 2016 7:56 PM, ""benbenwilde"" notifications@github.com wrote:

> This is no good. I want to run a script during reindex, but I want to run
> it asynchronously. But I'm stuck with the version where there is a bug with
> the reindex task going away when it finishes (or fails). So essentially I
> need to at least validate that the script file exists before running. So I
> try to validate using a search or something but it seems to run in a
> different context throwing random errors. So I'm currently resorting to
> running the search and seeing which kind of error it returns to identify if
> the script exists or not. It's really not a great solution.
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/elastic/elasticsearch/issues/11066#issuecomment-251833008,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AANLon6T1S-vSoQaSG_nK0I1XwOi5Duxks5qxDk3gaJpZM4EUBtm
> .
",0,1,dwhp
6662,"@joschi  Thanks for input, will do that (the damned legacy system on php side :( )

Regarding diverging type, in case of that exception being thrown if you have an access to elasticsearch ranges you could do several queries (and show a warning regarding mixed type of data). 

i.e. : last 7 day query, if it's spans over 7 different indexes, do 7 different queries and show combined result (if possible due to mixed int and string). I think people affected by this bug would prefer to have duplicated/a bit imprecise values (due to string/int presence) compared to being locked out entirely",0,1,dwhp
6663,"@Hellmy I like the idea, but we might want to wait until the changes proposed at the end of  https://github.com/WhisperSystems/TextSecure/issues/945 are either approved or discarded by @mcginty and @moxie0. One of the solutions discussed is to either add a logo like the one you used to every message (sent and received) or to use the text ""SMS"", ""MMS"" or ""PUSH""/""DATA"".

@leandrosalvador this is probably not going to happen because of the reasons discussed in https://github.com/WhisperSystems/TextSecure/issues/945
",0,1,dwhp
6664,"if 100% coverage is not reachable in normal workflow we try to use PowerMock, if it can not help also, we use ugly reflection. That is not very good in general but we do this only in test area of code and we get increasable value from 100% coverage so we are ok to damage code clearness in test are. Extra comment should be placed to explain design. 

You can find bunch of examples of complicated streams mocking and ugly reflection in our test.
",0,1,dwhp
6665,Somebody actually reads all the stuff I write? Weird!,0,1,dwhp
6671,"hah, because I suck at naming ;)
@VinceAngel would you mind changing the naming as well?
Btw, my english is bad, is a cylinder of ellipses still called a cylinder (and sphere/cone likewise) or should that be renamed as well?
",0,1,dwhp
6672,"will open a PR tomorrow. the devil is in the detail! ðŸ˜„ 
",0,1,dwhp
6673,"I'm happy with this direction and think we should do it. We can always move things around later. I vote @javanna crams some NORELEASEs in it and merges it and we open issues for things like the version number (#18741), and whatever else we see. We can resolve them as we can resolve them. Getting this in soon lets more folks work in parallel to improve stuff and lets me start using it for stuff like reindex from remote. We're early enough in the 5.0 release cycle still this should be safe.

I'm happy to work on the build issues if @rjernst is willing to review my horrible gradle hackery.
",0,1,dwhp
6675,"@Elmue, I'd kindly remind you that another human being wrote this code, and you're being incredibly rude. I'm closing this issue because I have little interest in dealing with people who can't be bothered to show the slightest shred of civility towards someone who's given their free time to gift the world Open Source Software.

If you want this issue fixed, please refile and imagine that I'm going to cc your most dearly loved with whatever it is you write.",0,0,dwhp
6676,"**Herzlichen Dank!**
Spielmops",0,1,dwhp
6679,"Hmm, probably I am not quite understand. But I don't think there is any difference between `\""$*\""` and `\""$@\""`, we escape the double quotes there which will be passed into java program, and `$*` and `$@` became same. 

Try this dirty bash script 

```
#!/bin/bash

function print_args_at {
    printf ""%s\n"" ""$@""
}

function print_args_star {
    printf ""%s\n"" ""$*""
}

function print_args_star1 {
    printf ""%s\n"" \""$*\""
}

function print_args_star2 {
    printf ""%s\n"" \""$@\""
}

print_args_at ""one"" ""two three"" ""four""
print_args_star ""one"" ""two three"" ""four""
print_args_star1 ""one"" ""two three"" ""four""
print_args_star2 ""one"" ""two three"" ""four""
```
",0,1,dwhp
6680,"So... has this been fixed? It's one of the most annoying bugs I run into all the time...
",0,1,dwhp
6682,boooo,0,1,dwhp
6683,"Thinking more about this, the point of the `buildErrorMessage` method was exactly to hide stacktraces and print a nicer, yet meaningful, error message and avoid printing out big stacktraces which might scare the hell out of users, especially if not familiar with java... :)

I do see your point, but if the error comes from a plugin, then I'd expect the plugin to log the stacktrace if needed. Thoughts?
",0,1,dwhp
6686,"> While for debugging purposes we could rely on external tools, Elasticsearch would need to infer this information for the use-cases you've mentioned here. Can we maybe find another solution to infer this information?

we are removing a potentially useful information just because some user runs on a shitty NFS, I wonder if we can find a better solution to this?",0,1,dwhp
6687,"It should be illegal to have a module that is a sub-namespace of another.
It's OK to have modules `foo.bar.baz` and `foo.bar.boff`, but you cannot also have just `foo.bar` or `foo` as a module.

I believe the bug here is that the compiler somehow let this slip by without complaining and just generated bad code.",0,0,dwhp
6689,"@gaearon I want to modify the options that are passed to closure-compiler for the build, but it's not obvious to me where I would do that in the React build setup.

Also, what command can I give that would just mean ""build the JS output"" without running the tests?",0,0,dwhp
6692,@ChadKillingsworth fixed and squashed commits. Any other notes?,0,0,dwhp
6693,I have a fix for this. It's blocked by #2641 ,0,0,dwhp
6699,@pedroigor maybe it makes sense to close the issue also? because i have decided to take this issue because of label 'help wanted' ,0,0,dwhp
6702,"no, the source code of master branch is not yet deployed to demo env.",0,0,dwhp
6703,"@ashvayka Thanks. You are right. Apparently, I have missed out to check the setting of max open FD thought that it works. Check with 'cat /proc/<pid>/limits' indicates only 4096 as you mentioned. Thanks",0,0,dwhp
6704,"f:\git\thingsboard2.4.1\ui (master -> origin)
λ node -v
v12.6.0

f:\git\thingsboard2.4.1\ui (master -> origin)
λ npm -v
6.1.0",0,0,dwhp
6706,"We inform you that your GitHub ticket has been inactive for some time, and we will close it. If you have any more concerns or encounter similar issues, please don't hesitate to contact us.",0,0,dwhp
6709,Currently the Rust code examples for IAM only has CreateRole.,0,0,dwhp
6712,looks good. Paul is on call this week. Please assign to Paul for a merge! ,0,0,dwhp
6719,"Automated checks report:
  * Commits associated with Github account: PASS
  * PR title follows the conventions: FAIL
    * The title of the PR does not pass all the checks. Please fix the following issues:
      * First word must be capitalized

Some checks failed. Please fix the reported issues and reply 'alluxio-bot, check this please' to re-run checks.
",0,0,dwhp
6720,"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/Alluxio-Pull-Request-Builder/18772/
Test PASSed.",0,0,dwhp
6723,Why not just making it so that the speed button in the popup open the same dialog as the background and normal players?,0,0,dwhp
6727,"Then I guess this should be closed, as it is unfortunately not fixable until further changes on exoplayer's side. Thank you @Kein and @Redirion for the effort :-)",0,0,dwhp
6729,"The problem seems to be with `getSchema()`  in Record and the subsequent  `getSchema()` on the actual pulsar message.  For consumer declared like Consumer<byte[]>, getSchema() calls on messages the consumer returns will always be SCHEMA.BYTES, regardless of what the actual schema the message was produced with.   The schema of message is overridden when byte[] is specified as the schema for the consumer. This seems like a bug to me. Perhaps @sijie  can also chime in on why the behavior is such.

If messages returned by a Consumer<byte[]> had the right schema, i.e. the schema of the topic / schema the message was produced with, then your problem is solved right @eolivelli ?",0,0,dwhp
6730,LGTM @momo-jun ,0,0,dwhp
6731,"I was not entirely sure about that, because it would require to have wireshark-dev installed to compile just the c++ client library.",0,0,dwhp
6732,"The pr had no activity for 30 days, mark with Stale label.",0,0,dwhp
6734,"@frank-dkvan 

I think you can add `bookkeeperExplicitLacIntervalInMills=5` in the JVM system properties when starting the broker.
",0,0,dwhp
6735,Closed this PR and use #2508 for it.,0,0,dwhp
6737,I'll ping @valyala but I'm sure what @zloster is saying is accurate. Going to close this for now.,0,0,dwhp
6738,"Oh, I just remembered that there is actually a flag for that plugin be installed elsewhere:
UWSGI_PLUGINS_DIR
I'll create a PR that reverts this and set the plugin full path so that uwsgi finds it.",0,0,dwhp
6739,"Looks good on your Travis - I want to ping @hamiltont (hopefully he has a moment) to review the other changes to files outside of Lwan's directory; they look okay to me, but he's closed to the metal.
",0,0,dwhp
6744,@jasontedor I pushed a new commit,0,0,dwhp
6746,"I tend to eagerly suspect the AntiVirus or Bit9/Carbon Black, but in this case I believe it's neither.

The fact that you only saw this error on incremental builds but not clean ones, and that Bazel shows the ""failed to delete output files..."" error when it fails to delete outputs from the previous build, could be explained by a background process still holding the file open. This could be a persistent worker (perhaps JavaBuilder) that holds on to the output file, either because it doesn't close the file at all, or it doesn't close it in a timely manner.

I need to find a way to repro this locally, that's what I'll do next.",0,0,dwhp
6747,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
",0,0,dwhp
6749,"I tryed to reproduce the bug, but I couldn't it.
I created a microsoft account and signs §, à, è, é, ì, ò and ù are not allowed for a password, so that I can not set a password with this signes. The ^ sign is allowed and I could use it with K9 successfully.
I also looked at Google accounts and Mailbox.org Accounts, but all of them do only support default ASCII signs or a subset of that for passwords, so that I had the same result.

In my opinion this ticket is not reproducable and can be closed.

If I'm wrong correct me.",0,0,dwhp
6751,"I suppose that a port of a LGPL code is a derivative work, so in his
current state the including library will have to comply with the LGPL. We
don't want to reimplement the lib from scratch so we will create a separate
library with LGPL code that links with ExoPlayer and use a custom
TsPayloadReader to inject de DVB subs reader, this way we will be able to
redistribute the library with the appropriate license attached and without
ExoPlayer code being tainted by LGPL code.

On Thu, Dec 29, 2016 at 3:04 PM, ojw28 <notifications@github.com> wrote:

> The plumbing of bitmaps through the text package is likely to be merged in
> via #2219 <https://github.com/google/ExoPlayer/pull/2219>. We've no
> objection to that change, and it's more or less required to enable support
> for bitmap subtitles to be added via extension in a clean way. Whether we
> then support DVB directly so that you don't need to do any extension is
> being re-assessed as per #179
> <https://github.com/google/ExoPlayer/issues/179>; it's more likely than
> it was in the past, at least.
>
> The CLA question isn't about whether the CLA is signed. It's about whether
> the code is an original creation as per the requirements of the CLA. One of
> the changes in this PR has ""Original C code taken from VLC project"" written
> in it. I don't think you can take GPL licensed code, make a derivative work
> and then remove the license. In general we can only accept original code
> where the contributor owns the copyright/IP and where it's not subject to
> other licenses. If you believe the PR does meet the requirements of the
> CLA, please clarify. Thanks!
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google/ExoPlayer/pull/1781#issuecomment-269634321>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AJ99vwpL61AKCLAJOoh6P7spwBGoyI3jks5rM733gaJpZM4Jt1YP>
> .
>
",0,0,dwhp
6752,"We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.
In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.

<!-- need_author_cla -->",0,0,dwhp
6753,Just sent a link to the relevant testing asset as a reply to the initial bug report.,0,0,dwhp
6756,"We've ordered a Vivo Z1 Pro device to try and reproduce this, but it's going to take a while for it to be delivered. We'll update this issue once we've received it and tried to reproduce.",0,0,dwhp
6758,"Kudos, SonarCloud Quality Gate passed!&nbsp; &nbsp; [![Quality Gate passed](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/QualityGateBadge/passed-16px.png 'Quality Gate passed')](https://sonarcloud.io/dashboard?id=bigbluebutton_bigbluebutton&pullRequest=16299)

[![Bug](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/bug-16px.png 'Bug')](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=BUG) [![A](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/RatingBadge/A-16px.png 'A')](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=BUG) [0 Bugs](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=BUG)  
[![Vulnerability](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/vulnerability-16px.png 'Vulnerability')](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=VULNERABILITY) [![A](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/RatingBadge/A-16px.png 'A')](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=VULNERABILITY) [0 Vulnerabilities](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=VULNERABILITY)  
[![Security Hotspot](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/security_hotspot-16px.png 'Security Hotspot')](https://sonarcloud.io/project/security_hotspots?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=SECURITY_HOTSPOT) [![A](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/RatingBadge/A-16px.png 'A')](https://sonarcloud.io/project/security_hotspots?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=SECURITY_HOTSPOT) [0 Security Hotspots](https://sonarcloud.io/project/security_hotspots?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=SECURITY_HOTSPOT)  
[![Code Smell](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/code_smell-16px.png 'Code Smell')](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=CODE_SMELL) [![A](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/RatingBadge/A-16px.png 'A')](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=CODE_SMELL) [0 Code Smells](https://sonarcloud.io/project/issues?id=bigbluebutton_bigbluebutton&pullRequest=16299&resolved=false&types=CODE_SMELL)

[![No Coverage information](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/CoverageChart/NoCoverageInfo-16px.png 'No Coverage information')](https://sonarcloud.io/component_measures?id=bigbluebutton_bigbluebutton&pullRequest=16299) No Coverage information  
[![0.0%](https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/Duplications/3-16px.png '0.0%')](https://sonarcloud.io/component_measures?id=bigbluebutton_bigbluebutton&pullRequest=16299&metric=new_duplicated_lines_density&view=list) [0.0% Duplication](https://sonarcloud.io/component_measures?id=bigbluebutton_bigbluebutton&pullRequest=16299&metric=new_duplicated_lines_density&view=list)

",0,0,dwhp
6760,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
",0,0,dwhp
6762,@aguerson This issue is about playing media files that are embedded in a (presumably PowerPoint?) presentation. Your link seems unrelated.,0,0,dwhp
6768,"@hc4 Thank you for the report, this will be fixed in the next beta.
",1,0,dwhp
6771,This issue has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.,0,0,dwhp
6773,This issue has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.,0,0,dwhp
6775,"Yes, ultimately, the processing developers have to fix the lifecycle bug.
",0,0,dwhp
6776,This issue has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.,0,0,dwhp
6778,"fixed.
",0,0,dwhp
6779,Sure @romani ,0,0,dwhp
6781,"we deploy our test jar to maven central , example http://repo1.maven.org/maven2/com/puppycrawl/tools/checkstyle/8.18/ , so potentially users can reuse out tests classes in their applications/plugins/.... .",0,0,dwhp
6784,"1) 
I will try to fix it, but I think jdk won't fix it, because in jdk standard javadoc symble, you can't write a period in `{@}` element, it is replaced by '#‘.
example:
···
{@link String#equals(Object)}
···
2)
done;
3)
I will try to do this;
4)
I will try to do this;
5)
I don't get it. 
It is not that we can't use `<a>` in first sentence. It is because attribute of html element does not count as sentence.
This is illegal:

```
<a href=""mailto:vlad@htmlbook.ru""></a>
```

This is legal(one period after it):

```
<a href=""mailto:vlad@htmlbook.ru""></a>.
```

6)

>  please share with user real example, please put there Chinese period symbol

Is it means I shall change xdoc?
7)

> else if (firstSentence.endsWith(period)) {
> I do not like this condition as period is smth that make a first sentence and we already used it.

I write this because we get first sentence, but that first sentence may end with other sentence delimiter.
8)

>   final List<DetailNode> stack = new LinkedList<>();
> stack vs List. So what is it ?

It is a stack to traverse ast tree, I will change it's name to make it more understandable.
9)
I will try to change it's algorithm, and add more comment to explain it.
",0,0,dwhp
6787,"> 如@ abhishekagarwal87所述，必须将cassandra扩展名保留在`druid.extensions.loadList`conf中，以便可以从Cassandra加载旧段。

Thank you for your advice",0,0,dwhp
6790,Thanks for the explanation. Lets wait for 0.11.0 then. Thanks!,0,0,dwhp
6791,"Test failures:

```
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 61.769 sec <<< FAILURE! - in io.druid.server.coordinator.CuratorDruidCoordinatorTest
testMoveSegment(io.druid.server.coordinator.CuratorDruidCoordinatorTest)  Time elapsed: 61.767 sec  <<< ERROR!
java.lang.Exception: test timed out after 60000 milliseconds
	at io.druid.server.coordinator.CuratorDruidCoordinatorTest.testMoveSegment(CuratorDruidCoordinatorTest.java:369)
```

```
Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 0.04 sec <<< FAILURE! - in io.druid.java.util.emitter.core.HttpEmitterConfigTest
testDefaultsLegacy(io.druid.java.util.emitter.core.HttpEmitterConfigTest)  Time elapsed: 0.011 sec  <<< FAILURE!
java.lang.AssertionError: expected:<14> but was:<13>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at io.druid.java.util.emitter.core.HttpEmitterConfigTest.testDefaultsLegacy(HttpEmitterConfigTest.java:76)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
testDefaults(io.druid.java.util.emitter.core.HttpEmitterConfigTest)  Time elapsed: 0.007 sec  <<< FAILURE!
java.lang.AssertionError: expected:<14> but was:<13>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at io.druid.java.util.emitter.core.HttpEmitterConfigTest.testDefaults(HttpEmitterConfigTest.java:52)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
```

Seemingly unrelated",0,0,dwhp
6798,"Please, respond to the last question.
https://github.com/dbeaver/dbeaver/issues/18721#issuecomment-1397035765",0,0,dwhp
6800,If you go into this directory `javanlp/projects/core` and enter the command `mvn package` it will run the tests as a final step.,0,0,dwhp
6801,"Haven't fully generalized this yet, but this also goes infinite:

```
echo ""(A (B 1) (C 2))"" | java edu.stanford.nlp.trees.tregex.TregexPattern ""A ?(< B < C)"" -filter
```

This does not:

```
echo ""(zzz (A (B 1) (C 2)))"" | java edu.stanford.nlp.trees.tregex.TregexPattern ""A ?(< B)"" -filter
```

So basically any optional `CoordinationPattern` is buggy?",0,0,dwhp
6802,"... or you could turn off reparsing in the coref module. A parameter for that was added in CoreNLP 3.3.1, although I do not know how to set that from a native CoreNLP pipeline - I set it directly via Java in 3.3.1. I hope the parameter is still present in the current version.
",0,0,dwhp
6803,"It shouldn't be hard to update your custom annotator to work with the new setup.  You just need to change requires() and requirementsSatisfied() to return the **annotations** that are required and produced.  Here is an example annotator with the new requirements set up: https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/DependencyParseAnnotator.java
If you could cut and paste the code for your custom annotators old required() and requirementsSatisfied() I can provide more guidance.",0,0,dwhp
6804,"Is this because of xom?  I don't think we use xalan directly.  

```
[john@localhost CoreNLP]$ find src -name ""*java"" -exec grep -H --ignore-case ""xalan"" ""{}"" "";""
[john@localhost CoreNLP]$
```

If so, please see: https://github.com/stanfordnlp/CoreNLP/issues/1264

I hope to make a new release end of next week or start of the week after.  There are a couple other changes I need to discuss with my PI, and I don't think we'll meet until then.  In the meantime, you can compile from the dev branch if this is critical",0,0,dwhp
6805,Possible we just need to upgrade protobuf.,0,0,dwhp
6812,"I need to look closer, but as @titusfortner mentions I think we probably don't want to be mutating the `params` map internally there.

edit: Thanks for the PR I will take a look as soon as possible",0,0,dwhp
6814," 
PR is not associated with a ticket. Apache Ignite issues are managed under Apache JIRA: https://issues.apache.org/jira/projects/IGNITE/ 
 
Please see how to associate ticket and PR in:
https://cwiki.apache.org/confluence/display/IGNITE/How+to+Contribute#HowtoContribute-Creation
 
Closing PR (which seems to be test-only) due to inactivity.
 
Feel free to reopen this PR if it is actual.
",1,0,dwhp
6816,"Hello @NSAmelchev 
I slightly reworked your patch for IGNITE-13109.
Can you, please, take a look?",0,0,dwhp
6817,"Closing the PR, feel free to reopen if it is needed",0,0,dwhp
6821,No I only have my user account.,0,0,dwhp
6822,"This issue has been automatically closed due to inactivity.
",0,0,dwhp
6827,"**[Juergen Hoeller](https://jira.spring.io/secure/ViewProfile.jspa?name=juergen.hoeller)** commented

Ouch, the caching algorithm wasn't quite as sophisticated as it should have been there...

Thanks for pointing this out, Rick! Seems to have been quite a busy JIRA Friday indeed :-)

The fix will be available in tonight's 2.5.1 snapshot.

Juergen
",1,0,dwhp
6828,"**[David Boden](https://jira.spring.io/secure/ViewProfile.jspa?name=daveboden)** commented

Pull request:

https://github.com/SpringSource/spring-framework/pull/206
",0,0,dwhp
6832,"**[Martin Lippert](https://jira.spring.io/secure/ViewProfile.jspa?name=mlippert)** commented

Hey David!

No special ports being used by Maven while resolving dependencies, as far as I know.

This issue gets more mysterious... So checking out the project from git and importing it as a Maven project into STS sounds like STS (or the Maven Integration for Eclipse) was able to download and resolve dependencies in that case. But why doesn't that happen when you click on the Dashboard? That is strange, especially since you were on your home network when doing this, so no proxy servers or corporate firewall involved. Hmmm...

Let me know once you are back on your home network so that we can hopefully figure out what is going wrong there. In case we find a good time slot, we could even do a screen sharing session to solve this.

Thanks!
-Martin
",0,0,dwhp
6835,"Hello @hsohans

Kerberos can be configured at pinpoint.

Inherit the [HbaseSecurityInterceptor](https://github.com/pinpoint-apm/pinpoint/blob/master/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HbaseSecurityInterceptor.java) interface, Implement the code for kerberos access.
And the implemented class must be registered as a bean object.


The process method of the HbaseSecurityInterceptor class is called when establishing a connection with hbase.
- https://github.com/pinpoint-apm/pinpoint/blob/379d312447eddd57439e50350fe01652aec3898b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/ConnectionFactoryBean.java#L71

There is a sample [code link](https://community.cloudera.com/t5/Community-Articles/Connecting-to-Kerberos-secured-HBase-cluster-from-Java/ta-p/246655).
",1,0,dwhp
6836,"1. If an error occurs in your branch's commit history, pull the latest commit and rebuild it.
2. I recommend that you should run mvn clean install -Dmaven.test.skip=true in your intellij.

If build is success in your IDE, your environment variable is no problem. 
If the build succeeds but one test run fails in your IDE, The configuration in the IDE is likely to be a problem, so you should look at the settings. Unfortunately, there is nothing to advise on this part.

If you can not find a solution, it will be very inconvenient, but you have to run maven command (-pl agent-it clean install test -Dmaven.test.failure.ignore=true -Ptest,local) to check if log4j2IT is tested successfully.



",0,0,dwhp
6838,"Hello, @dinesh4747 

Well, the last plan I've heard was release in early Jan.

@intojun any updates?
",0,0,dwhp
6840,"Hi @kyzhouxu 
Pinpoint doesn't collect user names or user ids logged on to the monitored application.
I think it's something that's too tightly coupled with the application for it to be something Pinpoint should handle.",0,0,dwhp
6844,"Sorry for the late response.

On Wed, Aug 18, 2021 at 9:30 PM Adam Azarchs ***@***.***>
wrote:

> It isn't clear from this issue, the design doc, or the rules_license
> repository what the current status is. I *think* it might be safe to
> start building stuff around the rules (e.g. our own list of license_kinds)
> but I'd rather not put a lot of effort into this if the public rules are
> way out of sync with the internal version so I'd have to overhaul
> everything for some future update.
>
The answer is yes, you could build on it. We've been a little slower in
rolling it out than I would have liked.
We've been trying to work out some issues in Bazel by using Google as a
test bed for a large migration.

I intend to start updating rules_license with what we learned. The usage is
virtually the same as described in the design.

> Another thing that is unclear from the design document is whether this is
> ever expected to handle non-vendored third-party dependencies. In
> particular, what is going to be badly needed to make this useful for people
> who don't just vendor everything into their monorepo are updates to various
> repository fetching rules. npm_install would be a particularly important
> and complex example use case. If there is ever going to be built-in support
> from the official rules_nodejs for this, I would want to make sure that
> whatever I build for us internally stays at least mostly compatible with
> it. Other examples would include e.g. go_respository, pip_install,
> maven_install, and so on. Obviously *can* be done (with varying degrees
> of ease of use and reliability) but not clear whether there's plans to
> actually do it.
>
We have that problem in Bazel, I intend to

   - If the dep has Bazel support already, try to upstream license usage. I
   expect that will be cost on early adopters and the Bazel team until there
   is more widespread adoption.
   - If that is not feasible, or if the module does not have Bazel support,
   we'll have to splice in the license attributions in the BUILD file. That is
   slightly easier if there is no upstream BUILD file, because you will have
   created your own. If it does, they will have to splice together. I don't
   have a solution for that yet, but it is not that hard.


What we have no plans for is automatic license classification based on
reading the text and deciding what it means. There are projects that do
that, and perhaps someone could work that into repository rules. But that
is definitively beyond rules_license's ambitions for the
foreseeable future. Like the rest of the Bazel project, we are focused on
precise specification and repeatable mechanisms. Autoclassification can
return results which are at some confidence level <100%, and worse, change
each time the classification corpus changes. I'll let the experts in that
domain work on that problem.



> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/bazelbuild/bazel/issues/10687#issuecomment-901539333>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAXHHHBTQZYLWMKA5G2BOJDT5RNEDANCNFSM4KN6OP5Q>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>
> .
>
",0,0,dwhp
6845,"We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.
In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.

ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fbazelbuild%2Fbazel%2Fpull%2F12231) for more info**.

<!-- need_author_cla -->",1,0,dwhp
6846,Cherry-picked in https://github.com/bazelbuild/bazel/pull/17965,0,0,dwhp
6848,"I would be interested by this for GraalVM native support. If changes are needed on Framework side, I would be interested to tackle it, I guess @poutsma would be as well.",0,0,dwhp
6849,"@bclozel What's your take on this? Should we provide a default (and if so is `/**` a sensible default) or should we fail fast if `paths` isn't specified?
",0,0,dwhp
6850,`MockMVC` is not  a Spring Boot feature. The issue tracker of Spring Framework is [here](https://jira.spring.io/browse/SPR),1,0,dwhp
6852,"I was undecided on that one actually. I wanted to stay consistent inside the specific tests, but I can change it of course.",0,0,dwhp
6854,"I'm not too keen on adding the getter to `SpringApplication` since the value of `mainApplicationClass` can change during `initialize`. I wonder if we can expose this another way, perhaps using a callback interface that beans can implement.

Can you provide some more background on where and how you'd want to use `getMainApplicationClass()`?
",0,0,dwhp
6857,"could it be that 
com.hazelcast.concurrent.lock.LockResourceImpl.canAcquireLock(LockResourceImpl.java:166)
has been changed, and this issue is fixed in hat master ?
",0,0,dwhp
6858,"Test PASSed.
",0,0,dwhp
6871,"I'm not sure if this fixes what #3264 is regarding? Either that or the [StackOverflow linked](http://stackoverflow.com/questions/29168257/libgdx-how-to-rotate-the-emmition-trail-of-a-3d-particle/29181347#29181347) and the issue are talking about two different things. The StackOverflow link says ""I wanted to change the direction the particles are being emitted."" Basically how it currently works is a particle effect's PolarAcceleration (Angluar) will never account for the particles transform, meaning the particles will always travel same direction. It could be fixed by changing the Angular.activateParticles to account for the controller.transform when the particle is recreated. This would allow for a stream of water coming from a rotating hose (rotating the hose nozzle shouldn't translate particles that are already emitted)

This PR applies the transform for every single active particle (not  the direction of newly particles). But #3264 does mention changing displacement of the whole effect, so maybe it does apply. But either way:

If the goal is just to update the particle positions based on a change in transform it can be done something like [this](https://gist.github.com/Bolt-Head/694817b5e682d66d7ad63f27039ed32b). If used in the API it would probably be better to be worked into a DynamicsModifier.

It handles the delta translation (probably the most common use-case) and doesn't require a Vector3.mul(Matrix4) per point.

If Translation/Rotation/Scale was necessary the points could be translated by calculating the transform between the current and previous transform. This would require one left multiply per point while the PR has 2 (once in sorting and another in the renderers). Also it wouldn't require changing and maintaining every particle batch renderer. A gif of this method working: http://i.imgur.com/yQVthXU.gif
",0,0,dwhp
6884,Thanks. I edited my commit comment and force-pushed ,0,0,dwhp
6885,Fixed via e969c363c0ea9df88ae8aa74fd48338ded7d7472,0,0,dwhp
6889,"@Sachpat thank you for taking the initiative to advocate for this upgrade. Sorry for my delay in responding.

Spring Security is not affected by the CVE listed since all calls to Nimbus are wrapped in a `try { } catch (Exception e) { }` block.

So, while it's technically feasible to update the Nimbus dependency from 6.0.2 to 7.9, we make it a practice to not do major version upgrades in patch releases. And since Spring Security isn't affected by this CVE, the best route is to maintain the upgrade policy and encourage developers to specify the `nimbus-jose-jwt` version themselves until they can upgrade to the latest Spring Security.

Actually, Nimbus is very quick with their releases, and so the code diff between 6.0.2 and 7.9 is quite small. I'd imagine that most applications would have no problem using the latest version of Nimbus.

For anyone who still feels strongly about the `5.1.x` line getting this security fix, we've found Nimbus to be very responsive to PRs. You might consider proposing a pull request to Nimbus that would fit in a maintenance release. It could be released as nimbus-jose-jwt:6.0.3, and then Spring Security 5.1.x could easily take the upgrade.",0,0,dwhp
6893,">The _tier_preference setting is just the implementation details 

My thinking was that tier setting for an index might actually be wrong - while it has a preference to be allocated to a particular tier of node it may be stuck somewhere else. Generally things gravitate from hot to colder tiers so a delayed movement is likely to be in a warmer tier than expected and that's probably not a problem for searches that will tend to prefer warmer end of things (at least for autocomplete).
The word ""preference"" helps convey it might not be the case - like when we use ""hint"" in [execution_hint](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-execution-hint)",0,0,dwhp
6897,"This is a property that should only be overridden with a lot of knowledge about the engine. You might be right that the naming could be better, but deprecating it and introducing a new property is not something that we want to do at this point.",0,0,dwhp
6900,@filiphr Found a way to only make the changes required.  You should be able to commit directly.,0,0,dwhp
6902,I'm still not understanding what you are trying to do. The HttpClient is only final in the HttpRequestHandlerInvocation class. Is that what is blocking you? Please provide some more context.,0,0,dwhp
6904,"Is there a reported issue of high memory consumption in Netty - 4.1.25? If yes, can you please share the link?",0,0,dwhp
6910,"Build result for #2908 at c8ff36d76fb657d8fca30a55661da416ff0249e0: [Unstable](https://secure.motd.kr/jenkins/job/pr_2908_akamai_netty_header_map_append/7/)
",0,0,dwhp
6914,"If you update to the head of Buck with homebrew, this should work for you.  Thanks for the report!
",0,0,dwhp
6917,@styurin any update on merging this pr?,0,0,dwhp
6923,"fixed the strings. I made one more change, adding a specialized TokenListener to RecipientSelectView, because before that the presenter wasn't notified when a token changed. This can be relevant if only some e-mail addresses of a contact have crypto keys, changing the crypto status.
",0,0,dwhp
6924,"I got the same issue and to add to it:
K9 also breaks the attached pdf if you just forward the mail. I often forward them when I am at work to my work address and the mails then are also broken and cant be opened.",0,0,dwhp
6925,"```
Resource Groups Changes
-----------------------

* Query Queues have been removed. Resource Groups are always enabled. The
  config property ``experimental.resource-groups-enabled`` has been removed.

CLI Changes
-----------

* The ``--enable-authentication`` option has been removed. Kerberos authentication
  is automatically enabled when ``--krb5-remote-service-name`` is specified.
* Kerberos authentication now requires HTTPS.

Hive Changes
------------

* Add support for using `AWS Glue <https://aws.amazon.com/glue/>`_ as the metastore.
  Enable it by setting the ``hive.metastore`` config property to ``glue``.
",1,0,dwhp
6927,I haven't been following the work on that feature. You can ask on the Pull Request or in the IRC or Slack channel.,0,0,dwhp
6931,"```
[INFO] --- license-maven-plugin:2.3:check (default) @ presto-root ---
[INFO] Checking licenses...
[WARNING] Missing header in: /Users/dain/work/fb/presto/src/checkstyle/checks.xml
```
",0,0,dwhp
6932,"Unencrypted ones & these scrambled ones all have green backgrounds.

In case this is relevant: since an update or two ago, all incoming texts
have their date/time set to 12/31/1969. Outgoing ones show the right time &
date.

On Fri, Jun 13, 2014 at 12:17 PM, Moxie Marlinspike <
notifications@github.com> wrote:

> Do the messages you send have a blue or green background?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/WhisperSystems/TextSecure/issues/1604#issuecomment-46030515
> .
",1,0,dwhp
6933,"Per [CONTRIBUTING.md](https://github.com/signalapp/Signal-Android/blob/master/CONTRIBUTING.md) feature requests should be discussed on the [forum](https://community.signalusers.org/c/feature-requests) instead:

> ### The issue tracker is for bugs, not feature requests
>
> The GitHub issue tracker is not used for feature requests, but new ideas can be submitted and discussed on the [community forum](https://community.signalusers.org/c/feature-requests). The purpose of this issue tracker is to track bugs in the Android client. Bug reports should only be submitted for existing functionality that does not work as intended. Comments that are relevant and concise will help the developers solve issues more quickly.",0,0,dwhp
6937,I would say so.,0,0,dwhp
6938,"see https://github.com/WhisperSystems/TextSecure/pull/1599 (partial doublure) and https://github.com/WhisperSystems/TextSecure/pull/1580/files#diff-a48d6def78e18cbabab3c95ae3c9fc49L36
",0,0,dwhp
6939,This should be done.... @popovicsandras can you confirm?,0,0,dwhp
6947,"@eugenp What is fixed here. Spring Security or Tomcat. What should do to get the fix ?
We are facing the same issue",0,0,dwhp
6948,"Hi @KingsleyAmankwah,
The first question is, does your code work locally? :)
I looked over your changes and I give you some advice to progress in fixing your PR:
- please create each class independently (if you need to group more class together create a package and add in that package)
- hibernate 6.x is using Jakarta persistence API 3.0 you have to use the jakarta API annotation and not javax
- In your tests try to use the `EntityManager` class if you want to simulate a store.

I don't know the entire context of your task. These are some few suggestion that I saw at the first sight.",0,0,dwhp
6951,"Hi, @Argunl.

Dealing with very large files is a complex edge case. Therefore, out of the scope of this article.

You can try limiting your client's request rate: https://www.baeldung.com/spring-webclient-limit-requests-per-second

Or, you can switch to a non-reactive file upload solution: https://www.baeldung.com/java-read-lines-large-file

Hope it helps.",0,0,dwhp
6953,"If not done so already, please open a reimbursement request as per the comment above.",0,0,dwhp
6954,Yes we have issues atm that blocks are not propaged enough. Will release a hotfix soon. In the meantime I start more full nodes to more nodes broadcast.,1,0,dwhp
6957,"@OrfanMiner try with Liberica `10.0.2` first and GTK backend to be sure it launches, then you may experiment with jdk11 and Monocle. Here is what works 100%:
```
/opt/jdk-10.0.2/bin/java --module-path=/opt/armv6hf-sdk/lib -Djavafx.platform=gtk -jar desktop-1.1.1-all.jar
```",0,0,dwhp
6962,"The only way to keep this backwards compatible would be, to use the same parameter name `okHttpClient` for the `Call.Factory`. The rest would still be compatible, because every `OkHttpClient` is also a `Call.Factory`. Under the hood we anyway used the client only as factory, so there would not be a change for the users. The naming would just not be as accurate as it could be. ",0,0,dwhp
6963,"Thanks @borsch for the fix.

@dsyer please pull the latest master to give it a try.",1,0,dwhp
6964,"What about using the following option to keep the original name?
```
	modelPropertyNaming
	    Naming convention for the property: 'camelCase', 'PascalCase', 'snake_case' and 'original', which keeps the original name (Default: camelCase)
```",0,0,dwhp
6965,"I forgot to mention: this PR also fixes a bug in Configuration: if a server variable is defined without enum values (i.e., accepts arbitrary values), then the current code would always throw InvalidArgumentException, since the array of possible enum values ($variable[""enum_values""] would be empty:
```php
# original code:
# (...)
        // go through variable and assign a value
        foreach ($host[""variables""] ?? [] as $name => $variable) {
            if (array_key_exists($name, $variables)) { // check to see if it's in the variables provided by the user
                if (in_array($variables[$name], $variable[""enum_values""], true)) { // check to see if the value is in the enum
                    $url = str_replace(""{"".$name.""}"", $variables[$name], $url);
                } else {
                    throw new \InvalidArgumentException(""The variable `$name` in the host URL has invalid value "".$variables[$name]."". Must be "".join(',', $variable[""enum_values""]).""."");
                }
# (...)
```
This is solved in the new code: Configuration.mustache:523 now checks if $variable[""enum_values""] is set, allowing arbitrary values to be interpolated if there are no enums list.

I was going to present a separate PR for this change, but since this PR is reorganizing the code, I ended up putting all together...",0,0,dwhp
6966,"@Vampire It seems you're right about this, but I think the way it works is not intuitive at all.
Also the docs doesn't mention that the paths should be relative to the project root dir, when `ignoreFileOverride` points to a file in project root dir.",0,0,dwhp
6968,"```
What you mean by 'within the app itself'? Wipe data of ankidroid app in android settings?
Also can't see any latex expressions since beta18, in both Samsung i5500 GB and Xoom
ICS
```

Reported by `frtbinc` on 2012-10-30 14:45:09
",0,0,dwhp
6969,"```
I am sorry, but I just downloaded ankidroid for my nexus 7 from google play about 10
minutes ago, and am receiving ""This is not a valid apkg file"" for shared decks. 
```

Reported by `schillsa@miamioh.edu` on 2013-02-07 12:32:55
",0,0,dwhp
