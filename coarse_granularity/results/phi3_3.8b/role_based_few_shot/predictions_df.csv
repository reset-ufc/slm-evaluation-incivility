index,message,prediction,actual,source
0,"I noticed that Prepared Statements seem to be emulated client-side by escaping certain characters.
Any plans to fully support service-side Prepared Statements? This can be done via the binary protocol, but there's a slower SQL-based approach available for non-binary clients:
http://dev.mysql.com/doc/refman/5.5/en/sql-syntax-prepared-statements.html",0,0,msr
1,"Yes, prepared statements are on my todo list. I don't need them myself, so unfortunately they kind of linger at the bottom of the list unless somebody wants to sponsor some of my time to work on the feature.
That being said, the SQL based approach looks interesting as a stop-gap solution for the short term.",0,1,msr
2,"The only downside with the SQL-based approach is that you probably still end up needing to do client-side escaping. Still it does offer a little bit more structure, so it might still buy some protection. Depending on how you do it, it might also simplify the escaping part.
Unless I'm mistaken, you are already implementing the actual protocol at the lower levels of your driver. I wonder how much more you need at that level to finish?
http://dev.mysql.com/doc/internals/en/command-packet-details.html",0,0,msr
3,"Prepared statements use a range of additional packets that are currently not implemented by my driver:
- http://dev.mysql.com/doc/internals/en/prepared-statement-initialization-packet.html
- http://dev.mysql.com/doc/internals/en/parameter-packet.html
- http://dev.mysql.com/doc/internals/en/long-data-packet.html
- http://dev.mysql.com/doc/internals/en/execute-packet.html
I have not yet analyzed how much work it would be to implement them, but my gut feeling is ~5 days of work.",0,0,msr
4,"How does https://github.com/sidorares/nodejs-mysql-native handle this? Any reason not to just borrow parts of the way it's done over there?
I'm still somewhat struggling with the number of different MySQL drivers for Node.JS. I think Node makes it way too fun to write network protocol code. :P Maybe in a year or so the community will have coalesced around one or two really solid libraries.",0,1,msr
5,"> How does https://github.com/sidorares/nodejs-mysql-native handle this?
It seems to implement the parts of the protocol that are required for prepared statements.
> Any reason not to just borrow parts of the way it's done over there?
Yes, I didn't have the time to work on this yet. I'm also not in the business of copying code unless it's up to my personal coding standards. So even with good inspiration like this, it will still take me some time.
> Maybe in a year or so the community will have coalesced around one or two really solid libraries.
This library is solid. It just does not implement all features.",0,1,msr
6,Couldn't we just prepare and execute statements using SQL instead of raw packets?,0,1,msr
7,"@dresende the SQL method still winds up tampering with values to make them safe (escaping quotes, etc), whereas the protocol method explicitly separates query from values so tampering is not necessary. To be fair, as long as its impossible to smuggle a query in as a value, the driver is plenty secure enough. I suppose I'm just being a nitpicky ex-PHP developer who wants everything to be conceptually elegant. :P",0,1,msr
8,"I'm not sure if this is officially supported by Doctrine, since it is not documented, but it works - with an error notice.
For example, if I have documents A and B where document B has an embedded document, E, I can design the mapping as follows:
``` php
/** @Document */
class A {
/** @ReferenceMany(targetDocument=""B"", mappedBy=""foo.bar"") */
private $whatever = [];
}
/** @Document */
class B {
/** @EmbedMany(targetDocument=""E"") */
private $foo = [];
}
/** @EmbeddedDocument */
class E {
/** @ReferenceOne(targetDocument=""A"", inversedBy=""whatever"") */
private $bar;
}
```
However, reading A::$whatever generates the following notice:
> Notice: Undefined index: foo.bar in mongodb-odm/lib/Doctrine/ODM/MongoDB/Persisters/DocumentPersister.php on line 709
Despite this, the document collection is retrieved correctly.",0,0,msr
9,"This doesn't work if class `B` has a binary ID and `$bar` is a discriminator reference. I'm not sure what the culprit is here, but the symptomns are that `count(A::$whatever)` will never be > 1. That is, at most only one item is loaded into the collection on the field with an embedded document mapping.",0,0,msr
10,This issue has been automatically marked as stale because it has not had any recent activity. It will be closed in a week if no further activity occurs. Thank you for your contributions.,0,0,msr
11,Imagine thinking anyone cares about design flaws.,0,1,msr
12,Imagine thinking anyone cares about Open Source being a joint effort.,1,1,msr
13,Imagine thinking open source means other people work for you for free.,0,1,msr
14,"If you set up some task this way:
before 'deploy', 'some_task', And in some_task you simply read the ""latest_release"" variable (in a puts line for instance), then:
Current results:
a) If it's the first deploy ever in capistrano (the www folder doesn't exist), capistrano breaks completely, it cannot deploy.
b) If it's not the first deploy, then it will make capistrano change its behaviour with regards to the ""current"" symlink, because it will point to the previous release instead of the last one (after the deploy happened).
Expected results:
a) It should work.
b) It should point current to the latest release.
This is a big fuckup IMHO.",0,1,msr
15,"> This is a big fuckup IMHO.
Profanity aside, you may have a point. I'll take a patch.",0,0,msr
16,Why do you close the issue?,1,0,msr
17,"Because you don't offer a patch, and profanity really pisses me off.",1,1,msr
18,"Mutable programming pisses me off even more.
Anyway, the point of github issues is to have a bug tracker.
The contributions comes in the pull-requests tab.",0,1,msr
19,"Indeed, in short - if there's no release, you are doing something wrong if you are calling `latest_release`. It's not safe for use when it's a cold deploy. _It's that simple._
There are other variables which are safe for use throughout the deploy process.
Capistrano, the 2.0.x branch mainline is effectively on the verge of being deprecated, send a patch or things won't be fixed. And even if you do send a patch, changing the behaviour of core variable lookups is not something I approach lightly.
I'd rather invest that time into the v3 branch which will coincide with the Rails 4 release in the coming weeks, and have a stable version which has fewer insane design decisions behind it.",0,0,msr
20,"As for your second point, you may be onto something there, but it's too late to change it, it's unreasonable overhead to call back to the server to calculate that every time, the results are cached for a reason. If you populate the cache with bad data, then you can't really effectively clear it. I accept that it's confusing, but I won't change it.",0,0,msr
21,"I understand that latest_release may not have the proper value when it's queried at the wrong time... but CHANGING BEHAVIOUR???? Sorry for the caps, but really finding out this bug has put me on my nerves.
In regards to your backwards-compatibility policy: fair enough, I understand if you prefer to accept only a patch to fix this in master rather than the 2.0.x branch, but:
a) I never proposed to fix it in a branch.
b) This bug is not fixed yet on any branch, so it should be reopened. Otherwise people confused about it will not be able to find it.
This is, my friend, how bug trackers work.",0,1,msr
22,"I won't fix it, and it's been the way it is for five years without anyone running into problems. I can't afford the time to test it, fix it and make sure it's safe before releasing it. That's what it boils down to. Those variables are a source of a lot of confusion, and they are fragile, and have different meanings wherever you call them, anyway.
I'd like to be able to fix it, your profane issue report aside; but the reality is, I simply can't.",0,0,msr
23,Then leave this issue open until the end of times.,1,1,msr
24,"Otherwise it would be the first time in my entire life in which I see a bug closed as WONTFIX.
I mean, I've seen feature requests closed as WONTFIX status. But bugs? It's like denying to recognize that there is a bug.",0,0,msr
25,"Just for the sake of people being able to find the issue (and be enlightened by the work-around: ""not read a variable""), this issue should remain open.",0,0,msr
26,Agreed. I've re-ragged with v2.,1,0,msr
27,"Why did you close this? People are getting here from this SO question: http://stackoverflow.com/questions/3141454/deploysymlink-on-capistrano-points-the-current-directory-to-previous-release/16043844#16043844 and if they find this issue closed, they might be misled into thinking that it is fixed.",0,0,msr
28,@knocte did you reproduce it on v3?,0,0,msr
29,"Oh, when was that released?",0,0,msr
30,"See rubygems, early september
On 2 Nov 2013 20:30, ""Andres G. Aragoneses"" notifications@github.com
wrote:
> Oh, when was that released?
> > —
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630493
> .",0,0,msr
31,"Ok, then I have not tested that. And I'm afraid I will not be able to test it very soon.
Just one advice: if you want to close issue in order to request feedback from users about new versions, add a comment explaining why you are closing the issue.
Thanks",0,0,msr
32,"We have more than 100 obsolete issues to close, and past experience
suggests the OPs never reapond
On 2 Nov 2013 20:36, ""Andres G. Aragoneses"" notifications@github.com
wrote:
> Ok, then I have not tested that. And I'm afraid I will not be able to test
> it very soon.
> > Just one advice: if you want to close issue in order to request feedback
> from users about new versions, add a comment explaining why you are closing
> the issue.
> Thanks
> > —
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630609
> .",0,0,msr
33,That doesn't respect the Robust principle ;) http://en.wikipedia.org/wiki/Robustness_principle,0,1,msr
34,"People are still upvoting my stackoverflow answer linked above, which seems to hint that this bug is still present in 3.x.",0,0,msr
35,thanks for checking the so answer upvote rates @knocte,1,0,msr
36,"Running into this issue currently I think.
How does one call the current release path that is right now being built?
I have a task running at `after :updated, :build do` and I'm trying to change directories and then run composer install, but changing directories using release_path is giving me the previous release, not the one currently being built.
Is that this issue? If not I'm sorry but the stackoverflow question led me here and I'm going kind of crazy trying to figure out how to change directories and then run a command, without it doing it in the release that's about to be replaced.",0,0,msr
37,"This issue is more than five years old, and refers to a long obsolete version of Capistrano, please open a new issue.",0,0,msr
38,"Can someone link to this ""new"" issue?",0,0,msr
39,"This issue is very old and the discussion was not very productive, so I will lock it. If you are a Capistrano v3 user running into an error using the `latest_release` setting please open a new issue.",0,0,msr
40,"@batmanbury there is no ""new"" issue; please open one if you are running into this problem.",0,0,msr
41,"The only query / issue I have is the big lines it draws blue (or themed borders) around countries.
There is a method to remove them?",0,0,msr
42,"Pass the options parameter:
http://leafletjs.com/reference.html#polygon
http://leafletjs.com/reference.html#path",0,0,msr
43,"It's a shame you couldn't provide an answer. Pointing to a documentation page that has no response to the original question isn't useful at all. Equally, it's pretty useless having something that draws a map with a big blue border around every country and no explanation as to how to remove it.",0,1,msr
44,"@abrice This kind of passive-aggressiveness is disrespectful towards maintainers.
I suggest reading [""How To Ask Questions The Smart Way""](http://www.catb.org/esr/faqs/smart-questions.html) and [""How to Report Bugs Effectively""](http://www.chiark.greenend.org.uk/~sgtatham/bugs.html) so you can make good bug reports in the future.",0,1,msr
45,Because most of the projects require Composer I have a hard time installing PHP application. This is because I don't have SSH access to my shared hosting. Sadly Composer makes PHP projects for only rich people who can buy hostings with SSH access. Can you please make Composer so that it does not require SSH. http://stackoverflow.com/questions/20894518/how-do-i-install-composer-on-a-shared-hosting,0,0,msr
46,Write a PHP script that you can hit in your browser that executes the `composer.phar` on your server?,0,0,msr
47,"@milesj, any tutorial on it?",1,0,msr
48,"No tutorial really, just execute shell commands or something similar. http://php.net/shell_exec
`shell_exec('php /path/to/composer.phar install');`
Never really tried it however.",0,0,msr
49,"Unfortunately, `shell_exec` and similar functions are also disabled on most shared hosting that don't support ssh login.
The current solution is to run composer locally and upload the vendor dir with the rest of your files.
As a gift, you can use [composer as a service](http://composer.borreli.com/) by @pborreli to download a zip of all your dependencies. It must be possible to create a simple PHP page that upload your `composer.json`, download the zip and extract it.",0,0,msr
50,"@GromNaN there are plans to have it done this via a composer plugin or extension or configuration (via composer as a service future api), that hopefully soon :baby:",0,0,msr
51,@cordoval The composer plugin is not enough as the cli of composer cannot be run on these environments. A webpage is necessary. But that will be easy.,0,0,msr
52,"If you don't have a CLI access on these systems, there is a high probably than other tools used by composer are not available either. In such case, the proper workflow would indeed be to run composer locally and to upload the code including the vendor folder, a suggested by @GromNaN",0,0,msr
53,WordPress can download and modify files from internet. I'm sure it is possible to make a similar installer (dependency-manager) with user interface.,0,0,msr
54,"@ilhanyumer it is indeed possible to build an app performing the composer job, but composer itself relies on git, svn and hg (depending of the VCS being used) to install from source. And it is not always possible to install from an archive. Shared hostings don't offer them generally.
Thus, it relies on `proc_open` to call these tools, which is often disabled on these shared hostings",0,0,msr
55,"https://github.com/CurosMJ/NoConsoleComposer
This can sort this thing out, hopefully.",0,0,msr
56,"We have created a client for the Contao CMS to use composer directly from the CMS Backend.
https://github.com/contao-community-alliance/composer-client
The client can run composer in 3 modes:
- inline: directly within the web servers php process
- process: start a process with `proc_open` within the web servers php process
- detached: start and fork composer into background process",0,0,msr
57,"This isn't really an issue with composer and should be closed. > Sadly Composer makes PHP projects for only rich people who can buy hostings with SSH access.
That's entirely untrue as Digital Ocean sells VPS for $5/month and Linode sells them for $10/month.",0,0,msr
58,"Hi, im running node-mysql latest on node-latest.
Somebody using the acunetix vulnerability scanner has triggered this error:
UNKNOWN COLUMN '$acunetix' IN WHERE CLAUSE.
The query: SELECT id, email FROM accounts WHERE username = ?
How is this possible? Its very dangerous to our application, please respond quickly.",0,1,msr
59,"The problem seems to be about params that are not strings. Although I'll continue to sanitize all my user inputs (to avoid username impersonation attacks like `admіn` posing as `admin`), I'd expect the query engine to convert any param to a string if it should have been one in the first place. If it already was, `String(param)` should be of low cost.",0,0,msr
60,@thekiur @mk-pmb can you post code samples?,1,0,msr
61,"We can confirm that the problem is caused by passing objects to the query call.
The objects come from the express bodyParser middleware.
We were simply passing req.body.username as the parameter for that query.
The acunetic vulnerability tester injected an object there.
We are not sure on the severity of this issue, but its unexpected to say atleast.
As we experienced, this can crash a live application in production mode if you dont expect any db errors.
There is no code to show: its as simple as passing a req.body.something to the .query call of node-mysql when using express with the bodyparser middleware. Running the vulnerability scanner against https://gist.github.com/ssafejava/9a2d77704712a8769322 causes the exception to be thrown.",0,0,msr
62,"This is not an issue with escaping with this library; this library is properly escaping all values and column names. The security issue is just with the way you are combining express and this library, such that you were expecting to get a string from express, so you were only expecting the `?` to expand according to string rules.
`req.body` properties can be anything with `bodyParser` and as such you need to at least verify what you are using is a string before passing to your query.",0,0,msr
63,"I consider prepared statements as intended to mitigate lack of input validation in the params in general. Therefor, limiting it to the case where input has already been validated as being a string, in my opinion misses the point.
Yours, MK",0,0,msr
64,"These are not prepared statements, they are done client-side and have various rules for how `?` is replaced depending on the data type, which is documented. If you want to be sure you are using the string-based `?` replacement though the API, you have to give the API a string. If you don't want to validate at all, you can use the `String()` function:
`conn.query('SELECT * FROM user WHERE username = ?', [String(req.body.username)]')`
The _purpose_ if it doing stuff different for objects is to help people who want to easily use `SET`:
`conn.query('UPDATE user SET ? WHERE id = ?', [{username: 'bob', password: '1234'}, 43])`
Please see the ""Different value types are escaped differently, here is how:"" section in https://github.com/felixge/node-mysql#escaping-query-values",0,0,msr
65,"I see. Looks like an unlucky case of embrace and extend. I wish you had opted for something like `??` in that case. Probably too late to change the interface?
Edit: Not really embrace and extend, as you wrote they aren't prepared statements. Rather just a pitfall for people who learn from tutorials and conclude topical similarity from visual similarity.
Edit 2: I see, `??` is already used for column names.",0,0,msr
66,I can't see how it's the type system's fault when programmers assume that a mechanism that looks like prepared statements will defuse any data they pass in. Let's at least blame it at the programmers for trusting visual similarity instead of reading the manual thoroughly.,0,1,msr
67,"@mk-pmb sure, though this module only has a small Readme, which has all the `?` stuff explained (https://github.com/felixge/node-mysql#escaping-query-values), so it's not even some weird hidden feature. Unfortunately if people on the Internet are writing tutorials about this module and giving incomplete or wrong information, it's hard for us to even try to police that.",0,0,msr
68,"@mk-pmb it's the programmers role to understand the libraries he/she is using at least to the extend they are documented before including them in any production environment. If the library isn't fully documented, that's on the creator, but since this is an open-source world you can't really blame somebody for dedicating their time towards creating something for free.
Inferring functionality from syntax is useful, but think rationally: if the `?` operator accepts strings, would it only accepts strings? What if it accepted other data types? Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.
Libraries and languages that make it easier to start developing are extremely useful, but I fear it gives a novice developer a misplaced sense of confidence. It's easy to build a small application, and when it ""just works"" assume nothing could possibly go wrong.",0,0,msr
69,"> Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.
I agree with that. And still, lots of people do it. So for all software that I manage, I'll try and have it be compatible with everyday flawed humans, in hopes to lessen the risk and impact of errors in software based on mine, written by fallible humans.
BOfH would ship a GNU/Linux distro where the default shell acts fully like bash, just that on every line starting with an uppercase letter, the meaning of `&&` and `||` is swapped. Might even document it properly. You'd read the manual and probably wouldn't use it. However, if the next day a toy drone crashes into your car because it's pilot didn't read the manual as thoroughly as you did, your expectations of how humans should act had much less impact than how they really do act. And I'd still partially blame that BOfH.
Update: Thanks for making it opt-in.",0,0,msr
70,"Please, this issue doesn't need any more comments. It is still open as a tracking issue for me. There are coming changes that will affects this module and even things like `express` which will make any kind of ""shoot yourself in the foot"" operations opt-in. As an example, for this module `?` really should strictly only result in a single entry in the SQL (i.e. numbers, strings, null, etc.). Anything over that should be opt-in (on the connection-level or one-off on the query level to reduce accidental exposure.
These are changes that are coming I listed, not speculation. Please just know that this issue is taken seriously.",0,0,msr
71,"Are there any circumstances where this would lead to an injection attack?
As far as I can work out so far this appears to only ever result in syntax errors.",0,0,msr
72,"@SystemParadox: I don't think so. The report seems to be badly explained and seems to be related to constructing SQL based on user input without any check.
Good usage:
``` js
db.query(""SELECT * FROM users WHERE id = ?"", [ +req.params.id ], next);
```
No harm on that, casting forces it to be a number. Even if it wasn't a number and the `+` was omitted, it's just fine (or else you would have problems when UPDATing columns with binary data - there's tests for that).
The problem here seems to be with something more like:
``` js
// BAD! BAD!
db.query(""SELECT * FROM "" + req.params.table + "" WHERE ...."", next);
```",0,0,msr
73,@SystemParadox yeah I just took a look at the formatting and escaping code. I don't see any way that passing unvalidated data to be interpolated into the query could result in an injection vulnerability. Without validation you can easily get a syntax error.,0,0,msr
74,"Could you please implement xBR shader or xBRZ filter or both in GSDX plugin. It would be very beneficial for both PS2 and PSX 2D and sprite-based games.
xBR and xBRZ are pixel art scaling algorithms ,they give best results in 2D/sprite based games with low resolution textures and games with pre-rendered backgrounds which dont upscale well with higher internal resolutions but they also give good results in 3D games. xBR/xBRZ are already used with good results in emulators like Retroarch , Higan, Desmume and PPSSPP.
Here is explanation:http://code.google.com/p/2dimagefilter/wiki/ImageScaling#xBR
http://www.vogons.org/viewtopic.php?t=34125
Here is newest xBR source code including hybrid variants: https://github.com/libretro/common-shaders
Source code for xBRZ is in source code of HqMAME : https://sourceforge.net/projects/hqmame/ ,http://sourceforge.net/projects/hqmame/files/xBRZ.zip/download, Spline36: http://code.google.com/p/remote-joy-lite-fix/source/browse/trunk/RemoteJoyLite_pc/spline36.psh
https://github.com/xbmc/xbmc/tree/master/xbmc/cores/VideoRenderers/VideoShaders
Here is comparison for 3D graphics:http://blog.metaclassofnil.com/?p=306
Here is official tutorial about xBR: http://www.libretro.com/forums/viewtopic.php?f=6&t=134
http://forum.zdoom.org/viewtopic.php?f=19&t=37373&sid=57269f5e32514a88a5d5252839c9ff6a&start=45
Some 2D graphics of old version of xBR: http://imgur.com/a/ZZiiH
I also found interesting algorithm Libdepixelize: http://bazaar.launchpad.net/~vinipsmaker/libdepixelize/trunk/revision/184
http://vinipsmaker.wordpress.com/tag/libdepixelize/
https://sourceforge.net/projects/inkscape/
There is also ''Ours'' but I cant find source code for it anywhere: http://research.microsoft.com/en-us/um/people/kopf/pixelart/supplementary/
But Libdepixelize and Ours both use Kopf-Lischinski algorithm so they should have similiar effects.
http://www.mediafire.com/download/22o6ahnchkbzhef/Shaders.rar
http://www.mediafire.com/download/86bo6bl66cnwv2j/chromaNEDI.rar
https://github.com/jpsdr/NNEDI3
http://forum.doom9.org/showthread.php?t=170727",0,0,msr
75,"It would work pretty well as a texture scaler too.
PPSSPP already does this.",0,0,msr
76,I think best option would be to use xBR in hardware mode and xBRZ can work in both software and hardware modes.,0,0,msr
77,"Instead of asking, try submitting a patch with your desired changes.",0,0,msr
78,Unfortunately I am not programmer and I am not skilled enough to implement it myself.,1,0,msr
79,"This is something i would like to see in the future, the way its implemented in PPSSPP is great. Although really not very necessary for PS2 emulation just a nice extra.",0,0,msr
80,It would be great for textures.,1,0,msr
81,"It wouldn't actually... since its design/function is for sprites, not textures.",0,0,msr
82,Its possible to use it for textures as well.,1,0,msr
83,PPSSPP is using xBRZ for textures and it looks very good.,0,0,msr
84,yes but one more option that the team don't wan't to do is mipmap at hardware mode in gsdx but i think the mipmap are better than xBRZ,0,0,msr
85,"Mipmapping would be useful but is pretty basic xBRZ may be better. Techniques which are clearly better are Tessellation, Displacement Mapping and Parallax Occlusion Mapping.",0,0,msr
86,"xBRZ makes textures and sprites ugly, I rather have Nearest.",0,1,msr
87,"xBR/xBRZ looks ugly only in your subjective opinion. Nearest-neighbor is primitive technique which looks blurry and pixelated,",0,1,msr
88,"xBR/xBRZ are ugly because computers are not artists, they ruin the artwork of games.",0,1,msr
89,"I am not sure what you mean but not everyone wants to use emulators with ugly native graphics.
PCSX2 already has option to increase internal resolution but xBR/xBRZ would be very useful for 2D sprites, pre-rendered backgrounds and textures.",0,1,msr
90,yes that's true ratchet and clank games have this problem and are very ugly only works in software mode and it's freaking me out with a amd fx 8350 ¬¬ a points of fps of 20 or 30,0,1,msr
91,"You mean Native graphics, most of which created professionally by artist then you want to ruin it with an over-exaggerated interpolation.",0,1,msr
92,"Ratchet & Clank was my first PS2 game I ever owned, i have being waiting years for it to be fixed in PCSX2",0,0,msr
93,Developers wanted games to look like they look in high resolution but were limited by underpowered hardware.,0,0,msr
94,Developers certainly not want to apply a silly image interpolation like xBR to ruin all their artwork,0,1,msr
95,Prove they dont.,1,1,msr
96,"If you take a SNES Game like Super Mario World, the developers created all the artwork pixel by pixel.. xBR simply adds unwanted pixels everywhere, ruining the sharp-crisp sprites, it's a silly gimmicky interpolation .",0,1,msr
97,Snes games have huge blocky pixels because hardware was primitive and not because developers intended it to look that way. xBR is advanced upscaling algorithm not ''silly gimmicky interpolation'' show little respect to shader/filter developers.,0,1,msr
98,"xBR is silly gimmicky interpolation because computers are not artists, you need an Artist to recreate the sprites and textures.",0,1,msr
99,You are obsessed about computers not being artists but they dont need to be. Shader developers being artists is enough. Artists already created xBR/xBRZ.,0,1,msr
100,xBR destroys Pixel art,0,1,msr
101,Pixel art is term invented by nostalgia fetishists no such thing exists chunky pixels are result of low resolution forced by weak hardware.,0,1,msr
102,"You continue to prove that you have no idea what you're talking about Monochrome,
xBR is not a texture filter, it is a sprite filter.",0,1,msr
103,At least xBRZ can be used for textures. PPSSPP is using it for textures so it can be done.,0,1,msr
104,"I prefer Nearest over xBRZ, I don't like my games looking like playdoh",0,1,msr
105,Nearest-Neighbor is primitive garbage you may like what you want but I prefer games to not look like blurry blobs.,0,1,msr
106,"This would be an interesting enhancement, in certain instances it can help reduce scaling artifacts in some PSP games, so it could possibly help games like FFX.
https://i.imgur.com/as4jG1S.jpg
https://i.imgur.com/8p76qGp.jpg
Just a few things about this issue thread:
1. Ratchet and Clank HD collection uses a similar smoothing filter for textures:
https://imgur.com/a/Kc52r
2. Squall, you're - like always - wrong, it can be used to filter textures - or any pixel based medium.
Whether or not xBR texture filtering looks better is subjective, so there's no point arguing either way. Adding this would be nice for those who like it, it could help scaling artifacts, and implementing a way to add xBR filtering could mean other texture filters could be added too, so people could have a variety and go with what they like.",0,1,msr
107,"SonofUgly,
You're wrong for one key reason.
xBR and xBRZ are per pixel filters, they can only operate on the inside of sprite if the renderer operates AT the pixel level.
In the case of PS2 sprites, or textures, it will only filter the edges and no more.",0,1,msr
108,"What are you on about. Are you talking specifically about how GSdx currently handles textures, now? Your previous condescending remarks sounded like you didn't think xBRZ could be used on textures at all - which you do realize PPSSPP uses it, right?
""In the case of PS2 sprites, or textures, it will only filter the edges and no more.""
http://i.imgur.com/p6OEJSp.png",0,1,msr
109,"Arguing aside, PPSSPP has it i think it works well (that's IMO). Something to consider if there is enough of a demand for it. Also i believe PPSSPP De-posterizes the textures as well which helps with the colour banding.",0,0,msr
110,"Your argument about XBRZ being good or not is irrelevant. In the case of a github feature request, nobody cares about your points about XBRZ being wrong because it's not a forum discussion thread. The fact is that there is a huge amount of emulation scene fans that really enjoy XBRZ texture scalling... ...and the context ends here. With PPSSPP leading the way and lots of other emulators now following these steps (PS1, N64...) it's safe to say that everybody would turn to see if PCSX2 would follow on a such well recieved feature. It's not a leage of who likes it or not, making points etc, it's a polite-asking feature request and it's actually possible. Now it's up to somebody with the interest and skill to pick it up. If closed source plugins like Pete's OpenGL2 could be modified to achieve such effect then I bet PCSX2/GSDx can aswell. Let's just not argue if it's possible or not.",0,0,msr
111,nobody cares,1,1,msr
112,#800,1,0,msr
113,"@LasagnaPie But older games were designed to be displayed on CRTs, where the sprites aren't sharp or crisp. :^)",0,1,msr
114,"It would be great for games like metal slug, megaman, etc.",1,0,msr
115,are we there yet ? xD,1,0,msr
116,"I would like to have a deeper knowledge to implement it, unfortunately not.
However I have an idea how this goes, bilinear filtering for pixel art just like Xbrz, so if it can be implemented, it goes right here. We need an expert developer on the subject.
![image](https://user-images.githubusercontent.com/46450049/97360816-afce4e80-186c-11eb-9d76-8f4c1bc0f4fc.png)",0,0,msr
117,"Perhaps someone on PPSSPP, it works great there.
Best link other issue too #3755.",0,0,msr
118,"@MRCHEESE97 https://github.com/stenzek/duckstation/commit/6f250a4ff7ffc06c68ed9d48ee7b97f925d354d8
https://github.com/flyinghead/flycast/pull/57/files
https://github.com/stenzek/duckstation/commit/62892b02d11893bc8079523da079ac04960f79f6
https://github.com/stenzek/duckstation/commit/5635ee1d7c521e1a1cd590bf804287d30c901948
https://github.com/stenzek/duckstation/commit/ae1e4b1b8fd52690ecda600232141e6ed1df1d4d",0,0,msr
119,"Just wanted to say that I am very much looking forward to this feature being implemented and not having to choose between blocky or blurry 2D assets.
Also for those that argue that ""developers did not intended for the graphics to look that way"" well, they did not intend for their ps2 games to be played on a PC either, but here we are...",0,0,msr
120,then I would like to add bicubic,1,0,msr
121,"the reason why xbrz is highly desired is because it looks great and is the
closest thing to high rez textures. it has to be internally coded for it to
work properly. all the other shaders can be acquired thru 3rd party means.
xbrz is technically a scaler.
On Tue, Feb 16, 2021, 12:34 PM Papermanzero <notifications@github.com>
wrote:
> then I would like to add bicubic
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/PCSX2/pcsx2/issues/10#issuecomment-779999104>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AA6LIM2IELRTWRRL2CMK2PTS7KUBPANCNFSM4ANWZOUQ>
> .
>",0,0,msr
122,"![2021-04-06_17h16_08](https://user-images.githubusercontent.com/42075250/113734722-dafe7900-96fb-11eb-91a4-aeb572ca14c1.png)
Which is better ? Xbr or nearest neighbor ?",0,0,msr
123,biliear and/or bicubic 😅,1,0,msr
124,"> ![2021-04-06_17h16_08](https://user-images.githubusercontent.com/42075250/113734722-dafe7900-96fb-11eb-91a4-aeb572ca14c1.png)
> Which is better ? Xbr or nearest neighbor ?
😻😻😻❤❤❤",0,0,msr
125,"> xbr or nearest neighbor ?
nearest neighbor. your mistaking the smooth look of xbr with an HD upscale. what it is, is a smoothed over approximation of nearest neighbor. i don't think any game studio would have intentionally made a game that looks like the xbr. it some how looks blockier and chunkier than nearest neighbor.",0,0,msr
126,"Xbrz forever, I really love this filter in all emulators, and for those who don't want this feature implemented, nobody is forcing you to enable this option in case someone implements the filter on the pcsx2.",0,0,msr
127,"> > xbr or nearest neighbor ?
> > nearest neighbor. your mistaking the smooth look of xbr with an HD upscale. what it is, is a smoothed over approximation of nearest neighbor. i don't think any game studio would have intentionally made a game that looks like the xbr. it some how looks blockier and chunkier than nearest neighbor.
It literally does not matter what game studios would or would not have their games look like.
xBR is not perfect, but it is the closest we are going to get in real time to high resolution 2D assests (short of texture replacement, wich not all games will have; and it fills disc space, wich might not be ideal to some people, depending on the amout of games they own).
If you prefer nearest neighbor, nice, luckily that option is already available to you.
This is not a discussion of wich is better (wich is highly subjective); this is about having the option to use a feature that is already available in other emulators, that a number of people enjoy.",0,0,msr
128,"Locking as the discussion here hasn't been very productive.
If you're interested in implementing these shaders please open a pull request.
If you're wondering about the status of this feature request, check if there is a linked pull request.",0,0,msr
129,"Closing as low priority, and what fobes said.",0,0,msr
130,"```
SPL: Loaded module v0.6.3-1
ZFS: Loaded module v0.6.3-1, ZFS pool version 5000, ZFS filesystem version 5
```
doing nothing but some basic rsync's with moderate sizes 4-10G result always in having approx 1MB/sec throughtput (very slow) on a up-date 16G RAM HP server - CentOS 6 with OpenVZ and selfcompiled modules for ZFS - dmesg:
```
INFO: task rsync:6517 blocked for more than 120 seconds.
Tainted: P --------------- 2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
rsync D ffff8804029bafb0 0 6517 6516 0 0x00000080
ffff88001e55da18 0000000000000086 0000000000000003 00000003d2dad320
000000000001ec80 0000000000000001 ffff88001e55dab8 0000000000000082
ffffc900105a3428 ffff8803fe837d60 ffff8804029bb578 000000000001ec80
Call Trace:
[<ffffffff810a1dfe>] ? prepare_to_wait_exclusive+0x4e/0x80
[<ffffffffa019fb35>] cv_wait_common+0x105/0x1c0 [spl]
[<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
[<ffffffffa019fc45>] __cv_wait+0x15/0x20 [spl]
[<ffffffffa02ae1fb>] txg_wait_open+0x8b/0x110 [zfs]
[<ffffffffa027194e>] dmu_tx_wait+0x29e/0x2b0 [zfs]
[<ffffffff81530bfe>] ? mutex_lock+0x1e/0x50
[<ffffffffa0271a41>] dmu_tx_assign+0x91/0x490 [zfs]
[<ffffffffa027fab7>] ? dsl_dataset_block_freeable+0x27/0x60 [zfs]
[<ffffffffa02e8d3e>] zfs_write+0x43e/0xcf0 [zfs]
[<ffffffff8100bc4e>] ? apic_timer_interrupt+0xe/0x20
[<ffffffff811c5e5c>] ? core_sys_select+0x1ec/0x2d0
[<ffffffffa02fd354>] zpl_write_common+0x54/0xd0 [zfs]
[<ffffffffa02fd438>] zpl_write+0x68/0xa0 [zfs]
[<ffffffff811ac798>] vfs_write+0xb8/0x1a0
[<ffffffff811ad091>] sys_write+0x51/0x90
[<ffffffff810f4dee>] ? __audit_syscall_exit+0x25e/0x290
[<ffffffff8100b102>] system_call_fastpath+0x16/0x1b
INFO: task txg_sync:876 blocked for more than 120 seconds.
Tainted: P --------------- 2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
txg_sync D ffff880405d28640 0 876 2 0 0x00000000
ffff8803fe839b70 0000000000000046 0000000000000001 ffff880405101430
0000000000000000 0000000000000000 ffff8803fe839af0 ffffffff81060032
ffff8803fe839b40 ffffffff81054939 ffff880405d28c08 000000000001ec80
Call Trace:
[<ffffffff81060032>] ? default_wake_function+0x12/0x20
[<ffffffff81054939>] ? __wake_up_common+0x59/0x90
[<ffffffff8152f833>] io_schedule+0x73/0xc0
[<ffffffffa019fadc>] cv_wait_common+0xac/0x1c0 [spl]
[<ffffffffa02f53e0>] ? zio_execute+0x0/0x140 [zfs]
[<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
[<ffffffffa019fc08>] __cv_wait_io+0x18/0x20 [spl]
[<ffffffffa02f561b>] zio_wait+0xfb/0x1b0 [zfs]
[<ffffffffa02867e3>] dsl_pool_sync+0xb3/0x440 [zfs]
[<ffffffffa029a67b>] spa_sync+0x40b/0xae0 [zfs]
[<ffffffffa02aebb4>] txg_sync_thread+0x384/0x5e0 [zfs]
[<ffffffff8105b309>] ? set_user_nice+0xc9/0x130
[<ffffffffa02ae830>] ? txg_sync_thread+0x0/0x5e0 [zfs]
[<ffffffffa01978e8>] thread_generic_wrapper+0x68/0x80 [spl]
[<ffffffffa0197880>] ? thread_generic_wrapper+0x0/0x80 [spl]
[<ffffffff810a1596>] kthread+0x96/0xa0
[<ffffffff8100c34a>] child_rip+0xa/0x20
[<ffffffff810a1500>] ? kthread+0x0/0xa0
[<ffffffff8100c340>] ? child_rip+0x0/0x20
```",0,0,msr
131,"I hit something similar but maybe different last night. I believe it occurred while KVM was copying disk blocks from another server to this one. This ended up causing actual corruption on at least one of the zvols (as seen by the VM). ```
[381360.908047] INFO: task zvol/0:331 blocked for more than 120 seconds.
[381360.908147] Tainted: PF O 3.13.0-32-generic #57-Ubuntu
[381360.908240] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[381360.908352] zvol/0 D ffff88062fc54440 0 331 2 0x00000000
[381360.908357] ffff88060796dbe0 0000000000000002 ffff880609aa17f0 ffff88060796dfd8
[381360.908360] 0000000000014440 0000000000014440 ffff880609aa17f0 ffff880609a5eb30
[381360.908362] ffff880609a5e9f8 ffff880609a5eb38 0000000000000000 0000000000000002
[381360.908365] Call Trace:
[381360.908374] [<ffffffff817200d9>] schedule+0x29/0x70
[381360.908396] [<ffffffffa00c0485>] cv_wait_common+0x105/0x1a0 [spl]
[381360.908402] [<ffffffff810aaf00>] ? prepare_to_wait_event+0x100/0x100
[381360.908408] [<ffffffffa00c0535>] __cv_wait+0x15/0x20 [spl]
[381360.908459] [<ffffffffa0209abb>] txg_wait_open+0x8b/0x110 [zfs]
[381360.908476] [<ffffffffa01cd83b>] dmu_tx_wait+0x29b/0x2a0 [zfs]
[381360.908492] [<ffffffffa01cd8cc>] dmu_tx_assign+0x8c/0x460 [zfs]
[381360.908520] [<ffffffffa025a8c7>] zvol_write+0xa7/0x480 [zfs]
[381360.908527] [<ffffffffa00bab27>] taskq_thread+0x237/0x4b0 [spl]
[381360.908530] [<ffffffff81097508>] ? finish_task_switch+0x128/0x170
[381360.908534] [<ffffffff8109a800>] ? wake_up_state+0x20/0x20
[381360.908539] [<ffffffffa00ba8f0>] ? taskq_cancel_id+0x1f0/0x1f0 [spl]
[381360.908543] [<ffffffff8108b3d2>] kthread+0xd2/0xf0
[381360.908545] [<ffffffff8108b300>] ? kthread_create_on_node+0x1d0/0x1d0
[381360.908548] [<ffffffff8172c5bc>] ret_from_fork+0x7c/0xb0
[381360.908550] [<ffffffff8108b300>] ? kthread_create_on_node+0x1d0/0x1d0
```
This was repeated for `zvol/0` through `zvol/9`. There are 31 zvols on this system.
```
[ 4.995804] SPL: Loaded module v0.6.3-1~precise
[ 5.130074] ZFS: Loaded module v0.6.3-2~precise, ZFS pool version 5000, ZFS filesystem version 5
```
I tried migrating a VM again today and all hell broke loose but I did not get these errors. The system load was in the 100's on an 8 core system. Major I/O wait time. I killed the migration but ended up having at least 3 corrupt zvols anyways.",0,0,msr
132,"i do not have any corruption or problem - the system is stable and running - also the rsync tasks are done 100% perfect - but very slow 1M/second write performance - and while running the whole system is not responding fast - but no errors and no corruption - these ""blocked for more than 120 seconds"" dmesg's come in pairs for rsync and txg_sync once a day",0,0,msr
133,"not similar but related message:
Aug 29 05:37:06 morpheus kernel: [46185.239554] ata6.00: configured for UDMA/133
Aug 29 05:37:06 morpheus kernel: [46185.239562] ata6: EH complete
Aug 29 05:53:40 morpheus kernel: [47179.890587] INFO: task txg_sync:1462 blocked for more than 180 seconds.
Aug 29 05:53:40 morpheus kernel: [47179.890589] Tainted: P O 3.16.0_ck1-smtnice6_BFQ_integra_intel #1
Aug 29 05:53:40 morpheus kernel: [47179.890590] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Aug 29 05:53:40 morpheus kernel: [47179.890591] txg_sync D 0000000000000006 0 1462 2 0x00000000
Aug 29 05:53:40 morpheus kernel: [47179.890594] ffff8805fbda9e40 0000000000000046 ffff88020931a9b0 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890596] 0000000000000066 ffff8807fb2a0f20 000000000000b020 0000000000013100
Aug 29 05:53:40 morpheus kernel: [47179.890597] ffff88065b6103a0 00002992b81d47dd ffff880617373fd8 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890599] Call Trace:
Aug 29 05:53:40 morpheus kernel: [47179.890605] [<ffffffff82a3eaa8>] ? io_schedule+0x88/0xd0
Aug 29 05:53:40 morpheus kernel: [47179.890613] [<ffffffffc028b536>] ? __cv_timedwait+0x96/0x110 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890616] [<ffffffff820fa7b0>] ? finish_wait+0x90/0x90
Aug 29 05:53:40 morpheus kernel: [47179.890623] [<ffffffffc03c38cb>] ? zio_wait+0xeb/0x1a0 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890631] [<ffffffffc03553da>] ? dsl_pool_sync+0xaa/0x450 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890639] [<ffffffffc036d4c3>] ? spa_sync+0x483/0xb20 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890642] [<ffffffff820f7c3d>] ? default_wake_function+0xd/0x20
Aug 29 05:53:40 morpheus kernel: [47179.890644] [<ffffffff821168bd>] ? ktime_get_ts+0x3d/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890652] [<ffffffffc037ce8a>] ? txg_sync_start+0x6ea/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890655] [<ffffffff820013ca>] ? __switch_to+0x2a/0x560
Aug 29 05:53:40 morpheus kernel: [47179.890662] [<ffffffffc037cb60>] ? txg_sync_start+0x3c0/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890664] [<ffffffffc0286f75>] ? spl_kmem_fini+0xa5/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890667] [<ffffffffc0286f00>] ? spl_kmem_fini+0x30/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890669] [<ffffffff820ea5dc>] ? kthread+0xbc/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890670] [<ffffffff82a40000>] ? __ww_mutex_lock_slowpath+0x8c/0x2cc
Aug 29 05:53:40 morpheus kernel: [47179.890672] [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 05:53:40 morpheus kernel: [47179.890674] [<ffffffff82a4242c>] ? ret_from_fork+0x7c/0xb0
Aug 29 05:53:40 morpheus kernel: [47179.890675] [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 06:28:01 morpheus kernel: [49241.775276] usb 1-1.1: USB disconnect, device number 3
this seems to occur from time to time with a rather slow USB3.0 powered 4TB hdd (Touro Desk 3.0, HGST5K4000) in an external case during rsync & transferring of large files (several GiB)",0,0,msr
134,"clarification for above:
I had this happen with the above mentioned drive - of which I still don't know what causes it (but I suspect it could be related either to the chipset in the external harddrive enclosure where the drive sits in or powersaving features of the XHCI driver & hardware, which I already had issues with in the past)
another drive showed this behavior (a seagate ST3000DM001) which likely underwent a headcrash and did reallocate several sectors (<10). It had been placed in an external enclosure by fantec [db-f8u3e with an incompatible chipset against smartctl] that had shown in the past to have a life & mind of its own: it would occasionally turn off during transfers and causing trouble with other filesystems, on ZFS, however, the files so far seemed fine. The day before yesterday I placed the drive in another external enclosure and it worked well during backups (only transferring several hundreds of MiB of data per backup job incrementally via rsync) until I decided to run a scrub and check everything: after several hours the drive again screamed and made hearable noises of a head-crash and/or sector re-allocation (had those in the past) and access wasn't possible anymore to the drive
that's where the above posted message occured again
so at least in the second case (ST3000DM001) ZFS seemingly showed indirectly that something was wrong with the hardware/harddrive
so when encountering this message - make sure to double- or triple-check that it's not a hardware-issue instead a ""software""- (ZFS-related) problem",0,0,msr
135,@wankdanker I think that your issue is separate. It might have been caused by the zvol processing occuring inside an interrupt context. Pull request #2484 might resolve it.,0,0,msr
136,"@freakout42 Would you tell us more about your pool configuration? Also, do you have data deduplication enabled on this pool?",0,0,msr
137,"```
[root@blood ~]# zpool status
pool: tank
state: ONLINE
scan: scrub repaired 0 in 0h53m with 0 errors on Tue Sep 9 12:45:13 2014
config:
NAME STATE READ WRITE CKSUM
tank ONLINE 0 0 0
mirror-0 ONLINE 0 0 0
sda4 ONLINE 0 0 0
sdb4 ONLINE 0 0 0
errors: No known data errors
[root@blood ~]# zpool get all
NAME PROPERTY VALUE SOURCE
tank size 824G -
tank capacity 34% -
tank altroot - default
tank health ONLINE -
tank guid 3198719639486948540 default
tank version - default
tank bootfs - default
tank delegation on default
tank autoreplace off default
tank cachefile - default
tank failmode wait default
tank listsnapshots off default
tank autoexpand off default
tank dedupditto 0 default
tank dedupratio 1.00x -
tank free 538G -
tank allocated 286G -
tank readonly off -
tank ashift 0 default
tank comment - default
tank expandsize 0 -
tank freeing 0 default
tank feature@async_destroy enabled local
tank feature@empty_bpobj active local
tank feature@lz4_compress enabled local
```",0,0,msr
138,"Had similar failures, which occurred during heavy rsync pulls from remote machine. I was able to make it happen very quickly by starting up the remote pull. Did this three times in a row and caused the fault every time. The symptom was that any userland zfs/zpool commands hang, but the machine was still responsive to other commands. I set the parameter spl_kmem_cache_slab_limit=0 (it had been spl_kmem_cache_slab_limit=16384), and the problem seems to be gone, or not easliy triggered.
Part of the process which triggers this includes snapshot renaming, **but no zvols** are involved in this process, although the pool has some.
The pool is a raidz1 pool, and there are no hardware issues on the server.
```
Sep 23 16:17:51 zephyr kernel: [ 27.497382] SPL: Loaded module v0.6.3-1~precise
Sep 23 16:17:51 zephyr kernel: [ 27.515752] ZFS: Loaded module v0.6.3-2~precise, ZFS pool version 5000, ZFS filesystem version 5
Sep 26 00:11:23 zephyr kernel: [200583.071397] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Sep 26 00:11:23 zephyr kernel: [200583.093284] txg_sync D ffffffff8180cbe0 0 4178 2 0x00000000
Sep 26 00:11:23 zephyr kernel: [200583.093291] ffff8807d9fd1aa0 0000000000000046 00000000000000ff ffffffffa026c280
Sep 26 00:11:23 zephyr kernel: [200583.093297] ffff8807d9fd1fd8 ffff8807d9fd1fd8 ffff8807d9fd1fd8 00000000000139c0
Sep 26 00:11:23 zephyr kernel: [200583.093303] ffff8807f3d6ae20 ffff8807e0189710 00000000ffffffff ffffffffa026c280
Sep 26 00:11:23 zephyr kernel: [200583.093309] Call Trace:
Sep 26 00:11:23 zephyr kernel: [200583.093333] [<ffffffff8169f099>] schedule+0x29/0x70
Sep 26 00:11:23 zephyr kernel: [200583.093338] [<ffffffff8169f35e>] schedule_preempt_disabled+0xe/0x10
Sep 26 00:11:23 zephyr kernel: [200583.093343] [<ffffffff8169df77>] __mutex_lock_slowpath+0xd7/0x150
Sep 26 00:11:23 zephyr kernel: [200583.093350] [<ffffffff8169db8a>] mutex_lock+0x2a/0x50
Sep 26 00:11:23 zephyr kernel: [200583.093395] [<ffffffffa0230f19>] zvol_rename_minors+0x79/0x180 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093432] [<ffffffffa01ad349>] dsl_dataset_rename_snapshot_sync_impl+0x189/0x2c0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093460] [<ffffffffa01ad52f>] dsl_dataset_rename_snapshot_sync+0xaf/0x190 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093491] [<ffffffffa01bde52>] dsl_sync_task_sync+0xf2/0x100 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093521] [<ffffffffa01b63b3>] dsl_pool_sync+0x2f3/0x420 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093554] [<ffffffffa01cc4f4>] spa_sync+0x414/0xb20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093590] [<ffffffffa01db021>] ? spa_txg_history_set+0x21/0xd0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093625] [<ffffffffa01de335>] txg_sync_thread+0x385/0x5d0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093659] [<ffffffffa01ddfb0>] ? txg_init+0x260/0x260 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093672] [<ffffffffa00d3fc8>] thread_generic_wrapper+0x78/0x90 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.093682] [<ffffffffa00d3f50>] ? __thread_create+0x300/0x300 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.093688] [<ffffffff81077ee3>] kthread+0x93/0xa0
Sep 26 00:11:23 zephyr kernel: [200583.093694] [<ffffffff816a9b24>] kernel_thread_helper+0x4/0x10
Sep 26 00:11:23 zephyr kernel: [200583.093700] [<ffffffff81077e50>] ? flush_kthread_worker+0xb0/0xb0
Sep 26 00:11:23 zephyr kernel: [200583.093704] [<ffffffff816a9b20>] ? gs_change+0x13/0x13
Sep 26 00:11:23 zephyr kernel: [200583.093737] INFO: task zfs:22581 blocked for more than 120 seconds.
Sep 26 00:11:23 zephyr kernel: [200583.104867] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Sep 26 00:11:23 zephyr kernel: [200583.126712] zfs D ffffffff8180cbe0 0 22581 22253 0x00000000
Sep 26 00:11:23 zephyr kernel: [200583.126717] ffff880182f2f958 0000000000000082 ffff8807eee9c530 ffff8807deda2550
Sep 26 00:11:23 zephyr kernel: [200583.126722] ffff880182f2ffd8 ffff880182f2ffd8 ffff880182f2ffd8 00000000000139c0
Sep 26 00:11:23 zephyr kernel: [200583.126728] ffff8807f3f04530 ffff8807eee9c530 0000000000000292 ffff8807deda2550
Sep 26 00:11:23 zephyr kernel: [200583.126733] Call Trace:
Sep 26 00:11:23 zephyr kernel: [200583.126740] [<ffffffff8169f099>] schedule+0x29/0x70
Sep 26 00:11:23 zephyr kernel: [200583.126753] [<ffffffffa00dbd8d>] cv_wait_common+0xfd/0x1b0 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.126768] [<ffffffff810787c0>] ? add_wait_queue+0x60/0x60
Sep 26 00:11:23 zephyr kernel: [200583.126779] [<ffffffffa00dbe95>] __cv_wait+0x15/0x20 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.126813] [<ffffffffa01c4aec>] rrw_enter_read+0x3c/0x130 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126849] [<ffffffffa01c4c70>] rrw_enter+0x20/0x30 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126878] [<ffffffffa01b51cd>] dsl_pool_config_enter+0x1d/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126915] [<ffffffffa01b709a>] dsl_pool_hold+0x4a/0x60 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126927] [<ffffffffa019957b>] dmu_objset_hold+0x2b/0xb0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126929] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.126942] [<ffffffffa01b8a6f>] dsl_prop_get+0x3f/0x90 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126944] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.126956] [<ffffffffa01b8ade>] dsl_prop_get_integer+0x1e/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126970] [<ffffffffa022ef6f>] __zvol_create_minor+0xbf/0x630 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126985] [<ffffffffa0230bc7>] zvol_create_minor+0x27/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126998] [<ffffffffa0230bee>] zvol_create_minors_cb+0xe/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127010] [<ffffffffa0197e6e>] dmu_objset_find_impl+0x37e/0x3f0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127023] [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127034] [<ffffffffa0197cae>] dmu_objset_find_impl+0x1be/0x3f0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127048] [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127061] [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127072] [<ffffffffa0197f32>] dmu_objset_find+0x52/0x80 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127074] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.127088] [<ffffffffa0230d53>] zvol_create_minors+0x33/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127102] [<ffffffffa0202a69>] zfs_ioc_snapshot+0x259/0x2a0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127116] [<ffffffffa0206530>] zfsdev_ioctl+0x180/0x500 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127118] [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.127131] [<ffffffff8119ae9a>] do_vfs_ioctl+0x8a/0x340
Sep 26 00:11:23 zephyr kernel: [200583.127135] [<ffffffff8119b1e1>] sys_ioctl+0x91/0xa0
Sep 26 00:11:23 zephyr kernel: [200583.127141] [<ffffffff816a8829>] system_call_fastpath+0x16/0x1b
```",0,0,msr
139,"@ColdCanuck Your comments regarding `spl_kmem_cache_slab_limit` piqued my interest in this issue which until now, I've not had time to follow. In fact, a quick skim of this whole issue makes me wonder how related each of the problem reports are and whether there's too much issue creep.
Back to the point at hand: I'm posting this followup because there have been a disturbing number of seemingly otherwise unrelated problems sporadically seemingly caused by using the Linux slab. Although I've not been able to spend the time on it I've wanted to, I've been rather knee deep investigating the series of issues related to Posix ACLs and SA xattrs and have seen at least one report (#2701) and, more interestingly #2725 which makes me think there may be a tie-in to our use of the Linux slab for <= 16KiB objects. I don't have any other brilliant observations offer at the moment other than to raise concern there may be problems realted to using the Linux slab and to ask @behlendorf, @ryao et al. what your thoughts are on this (particularly given the last few comments in #2725).",0,0,msr
140,"just posting what comes to mind:
could scheduling a regular cronjob which compacts memory via
echo 1 > /proc/sys/vm/compact_memory
change things (provided slab issues and timeouts are related to memory fragmentation)",0,0,msr
141,"I'm systematically having this issue when trying to RSync when using the latest ZFS from Arch: zfs-git 0.6.3_r170_gd958324f_3.18.2_2-1
```
[46181.967521] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
[79468.027144] INFO: task txg_sync:583 blocked for more than 120 seconds.
[79468.027287] Tainted: P O 3.18.2-2-ARCH #1
[79468.027363] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[79468.027468] txg_sync D 0000000000000000 0 583 2 0x00000000
[79468.027476] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[79468.027483] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[79468.027489] ffff8801591955b8 ffffffff00000000 ffff880159195570 00000000025e92cf
[79468.027494] Call Trace:
[79468.027509] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[79468.027518] [<ffffffff81550b59>] schedule+0x29/0x70
[79468.027524] [<ffffffff81550e38>] io_schedule+0x98/0x100
[79468.027547] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[79468.027554] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[79468.027565] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[79468.027588] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[79468.027616] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[79468.027625] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[79468.027656] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[79468.027663] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[79468.027694] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[79468.027724] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[79468.027734] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[79468.027742] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[79468.027749] [<ffffffff81090e0a>] kthread+0xea/0x100
[79468.027755] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[79468.027761] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[79468.027767] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120097] INFO: task txg_sync:583 blocked for more than 120 seconds.
[82589.120252] Tainted: P O 3.18.2-2-ARCH #1
[82589.120347] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[82589.120479] txg_sync D 0000000000000001 0 583 2 0x00000000
[82589.120489] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[82589.120497] ffff88021ce03fd8 0000000000013640 ffff8802240ceeb0 ffff88021cd5eeb0
[82589.120505] ffff88022363f7b0 ffff88022363f798 0000000000000000 0000000000000003
[82589.120512] Call Trace:
[82589.120529] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[82589.120540] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[82589.120549] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[82589.120558] [<ffffffff81550b59>] schedule+0x29/0x70
[82589.120564] [<ffffffff81550e38>] io_schedule+0x98/0x100
[82589.120591] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[82589.120599] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[82589.120613] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[82589.120641] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[82589.120676] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[82589.120689] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[82589.120727] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[82589.120735] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[82589.120774] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[82589.120811] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[82589.120823] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[82589.120834] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[82589.120842] [<ffffffff81090e0a>] kthread+0xea/0x100
[82589.120849] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120858] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[82589.120864] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.468460] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89311.468644] Tainted: P O 3.18.2-2-ARCH #1
[89311.468741] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89311.468872] txg_sync D 0000000000000000 0 583 2 0x00000000
[89311.468882] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89311.468890] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89311.468897] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89311.468905] Call Trace:
[89311.468922] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89311.468933] [<ffffffff81550b59>] schedule+0x29/0x70
[89311.468940] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89311.468968] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89311.468976] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89311.468989] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89311.469017] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89311.469052] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89311.469065] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89311.469103] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89311.469112] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89311.469150] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89311.469187] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89311.469199] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89311.469211] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89311.469218] [<ffffffff81090e0a>] kthread+0xea/0x100
[89311.469226] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.469234] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89311.469241] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
```
```
hinoki% free -h
total used free shared buff/cache available
Mem: 7.8G 4.3G 533M 1.1M 3.0G 702M
Swap: 4.0G 0B 4.0G
```",0,0,msr
142,"Still trying to run rsync:
```
[89431.510429] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89431.510571] Tainted: P O 3.18.2-2-ARCH #1
[89431.510647] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89431.510753] txg_sync D 0000000000000000 0 583 2 0x00000000
[89431.510762] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89431.510769] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89431.510774] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89431.510780] Call Trace:
[89431.510796] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89431.510805] [<ffffffff81550b59>] schedule+0x29/0x70
[89431.510811] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89431.510836] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89431.510842] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89431.510853] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89431.510877] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89431.510906] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89431.510915] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89431.510946] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89431.510953] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89431.510984] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89431.511014] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89431.511024] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89431.511033] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89431.511039] [<ffffffff81090e0a>] kthread+0xea/0x100
[89431.511045] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89431.511052] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89431.511058] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.552404] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89551.552565] Tainted: P O 3.18.2-2-ARCH #1
[89551.552660] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89551.552792] txg_sync D 0000000000000000 0 583 2 0x00000000
[89551.552803] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89551.552812] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89551.552819] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89551.552826] Call Trace:
[89551.552844] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89551.552854] [<ffffffff81550b59>] schedule+0x29/0x70
[89551.552862] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89551.552890] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89551.552898] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89551.552911] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89551.552940] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89551.552975] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89551.552987] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89551.553025] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89551.553034] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89551.553073] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89551.553110] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89551.553122] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89551.553133] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89551.553140] [<ffffffff81090e0a>] kthread+0xea/0x100
[89551.553148] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.553156] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89551.553163] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.594371] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89671.594507] Tainted: P O 3.18.2-2-ARCH #1
[89671.594605] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89671.594711] txg_sync D 0000000000000000 0 583 2 0x00000000
[89671.594720] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89671.594727] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89671.594733] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89671.594738] Call Trace:
[89671.594753] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89671.594762] [<ffffffff81550b59>] schedule+0x29/0x70
[89671.594768] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89671.594792] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89671.594798] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89671.594809] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89671.594832] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89671.594860] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89671.594870] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89671.594901] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89671.594908] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89671.594939] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89671.594969] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89671.594979] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89671.594988] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89671.594994] [<ffffffff81090e0a>] kthread+0xea/0x100
[89671.595000] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.595007] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89671.595013] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636364] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89791.636507] Tainted: P O 3.18.2-2-ARCH #1
[89791.636583] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89791.636688] txg_sync D 0000000000000000 0 583 2 0x00000000
[89791.636697] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89791.636703] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89791.636709] ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89791.636715] Call Trace:
[89791.636729] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89791.636738] [<ffffffff81550b59>] schedule+0x29/0x70
[89791.636744] [<ffffffff81550e38>] io_schedule+0x98/0x100
[89791.636767] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89791.636774] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89791.636785] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89791.636808] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89791.636836] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89791.636852] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89791.636883] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89791.636890] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89791.636921] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89791.636951] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89791.636960] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89791.636969] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89791.636975] [<ffffffff81090e0a>] kthread+0xea/0x100
[89791.636981] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636989] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89791.636994] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90105.367103] systemd[1]: systemd-journald.service stop-sigabrt timed out. Terminating.
[90105.490377] systemd[1]: Listening on Journal Audit Socket.
[90105.490436] systemd[1]: Starting Journal Service...
[90151.762415] INFO: task kswapd0:31 blocked for more than 120 seconds.
[90151.762569] Tainted: P O 3.18.2-2-ARCH #1
[90151.762663] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.762795] kswapd0 D 0000000000000000 0 31 2 0x00000000
[90151.762805] ffff8802235a3908 0000000000000046 ffff880224225a90 0000000000013640
[90151.762814] ffff8802235a3fd8 0000000000013640 ffffffff81818540 ffff880224225a90
[90151.762821] ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.762828] Call Trace:
[90151.762845] [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.762856] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.762865] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.762893] [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.762902] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.762915] [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.762956] [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.762984] [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763011] [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763040] [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763049] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763075] [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763083] [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763090] [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763097] [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763106] [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763115] [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763123] [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763131] [<ffffffff8116e0e9>] kswapd_shrink_zone+0x129/0x1d0
[90151.763138] [<ffffffff8116eb7a>] kswapd+0x54a/0x8f0
[90151.763147] [<ffffffff8116e630>] ? mem_cgroup_shrink_node_zone+0x1c0/0x1c0
[90151.763155] [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.763162] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763171] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.763178] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763190] INFO: task systemd-journal:138 blocked for more than 120 seconds.
[90151.763340] Tainted: P O 3.18.2-2-ARCH #1
[90151.763433] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.763563] systemd-journal D 0000000000000000 0 138 1 0x00000004
[90151.763571] ffff88022310b128 0000000000000082 ffff880222c25080 0000000000013640
[90151.763577] ffff88022310bfd8 0000000000013640 ffff88021cd5e4a0 ffff880222c25080
[90151.763583] ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.763590] Call Trace:
[90151.763599] [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.763607] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.763614] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.763627] [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.763635] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.763647] [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.763683] [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.763710] [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763737] [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763765] [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763774] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763799] [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763805] [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763812] [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763819] [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763827] [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763834] [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763842] [<ffffffff811c1523>] ? mem_cgroup_iter+0x2f3/0x4d0
[90151.763850] [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763858] [<ffffffff8116da45>] do_try_to_free_pages+0x365/0x4e0
[90151.763866] [<ffffffff8116dc71>] try_to_free_pages+0xb1/0x1a0
[90151.763873] [<ffffffff81160ca7>] __alloc_pages_nodemask+0x697/0xb50
[90151.763884] [<ffffffff811a6e9c>] alloc_pages_current+0x9c/0x120
[90151.763891] [<ffffffff811afba5>] new_slab+0x305/0x370
[90151.763899] [<ffffffff811b2175>] __slab_alloc.isra.51+0x545/0x650
[90151.763907] [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763914] [<ffffffff814380f4>] ? raw_pci_write+0x24/0x50
[90151.763923] [<ffffffff812e8be6>] ? pci_bus_write_config_word+0x66/0x80
[90151.763949] [<ffffffffa004ed7f>] ? ata_bmdma_start+0x2f/0x40 [libata]
[90151.763960] [<ffffffffa01121ad>] ? atiixp_bmdma_start+0x9d/0xe0 [pata_atiixp]
[90151.763969] [<ffffffff811b5445>] __kmalloc_node_track_caller+0xa5/0x240
[90151.763976] [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763984] [<ffffffff81445a11>] __kmalloc_reserve.isra.38+0x31/0x90
[90151.763990] [<ffffffff81445acb>] ? __alloc_skb+0x5b/0x210
[90151.763997] [<ffffffff81445af9>] __alloc_skb+0x89/0x210
[90151.764004] [<ffffffff81445de4>] alloc_skb_with_frags+0x64/0x1e0
[90151.764011] [<ffffffff8143efb9>] sock_alloc_send_pskb+0x219/0x290
[90151.764020] [<ffffffff810136fb>] ? __switch_to+0x1fb/0x600
[90151.764029] [<ffffffff814f886d>] unix_dgram_sendmsg+0x18d/0x690
[90151.764037] [<ffffffff8143ac39>] sock_sendmsg+0x79/0xb0
[90151.764045] [<ffffffff810986da>] ? finish_task_switch+0x4a/0xf0
[90151.764051] [<ffffffff815504c8>] ? __schedule+0x3e8/0xa50
[90151.764059] [<ffffffff8143c76c>] ? move_addr_to_kernel+0x2c/0x50
[90151.764066] [<ffffffff8144a3c7>] ? verify_iovec+0x47/0xd0
[90151.764074] [<ffffffff8143b928>] ___sys_sendmsg+0x408/0x420
[90151.764083] [<ffffffff812149d0>] ? ep_read_events_proc+0xe0/0xe0
[90151.764089] [<ffffffff81440030>] ? sk_prot_alloc.isra.33+0x30/0x130
[90151.764097] [<ffffffff811b291a>] ? kmem_cache_alloc+0x16a/0x170
[90151.764104] [<ffffffff811d100c>] ? get_empty_filp+0x5c/0x1c0
[90151.764112] [<ffffffff8126e496>] ? security_file_alloc+0x16/0x20
[90151.764118] [<ffffffff811d1084>] ? get_empty_filp+0xd4/0x1c0
[90151.764126] [<ffffffff811d118f>] ? alloc_file+0x1f/0xb0
[90151.764134] [<ffffffff8143d651>] __sys_sendmsg+0x51/0x90
[90151.764142] [<ffffffff8143d6a2>] SyS_sendmsg+0x12/0x20
[90151.764149] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[90151.764186] INFO: task txg_sync:583 blocked for more than 120 seconds.
[90151.764325] Tainted: P O 3.18.2-2-ARCH #1
[90151.764418] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.764547] txg_sync D 0000000000000000 0 583 2 0x00000000
[90151.764554] ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[90151.764560] ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[90151.764566] ffff88018de384b8 ffffffff00000000 ffff88018de38490 00000000025e92cf
[90151.764572] Call Trace:
[90151.764581] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[90151.764588] [<ffffffff81550b59>] schedule+0x29/0x70
[90151.764594] [<ffffffff81550e38>] io_schedule+0x98/0x100
[90151.764607] [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[90151.764615] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.764627] [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[90151.764651] [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[90151.764686] [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[90151.764698] [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[90151.764735] [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[90151.764744] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[90151.764781] [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[90151.764818] [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[90151.764830] [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[90151.764841] [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[90151.764847] [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.764855] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.764862] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.764869] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90195.648689] systemd[1]: systemd-journald.service stop-sigterm timed out. Killing.
[90195.649942] systemd[1]: Starting Journal Service...
```",0,0,msr
143,"This might be caused by #2523, can you verify you have the fix to the SPL applied: zfsonlinux/spl@a3c1eb77721a0d511b4fe7111bb2314686570c4b",0,0,msr
144,"More of the same
```
[ 1320.575152] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1320.575263] Tainted: P O 3.18.2-2-ARCH #1
[ 1320.575314] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1320.575384] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1320.575390] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1320.575395] ffff88021ba43fd8 0000000000013640 ffff88021dee2840 ffff8800cbfc4670
[ 1320.575399] ffff880131dcc138 ffffffff00000000 ffff880131dcc130 00000000b3f1aac7
[ 1320.575402] Call Trace:
[ 1320.575414] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1320.575420] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1320.575424] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1320.575442] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1320.575447] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1320.575453] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1320.575470] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1320.575490] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1320.575496] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1320.575517] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1320.575522] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1320.575543] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1320.575563] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1320.575569] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1320.575575] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1320.575579] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1320.575583] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1320.575588] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1320.575592] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.661841] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1560.661995] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.662090] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662221] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1560.662232] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1560.662240] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1560.662247] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1560.662255] Call Trace:
[ 1560.662272] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1560.662283] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1560.662292] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1560.662301] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.662307] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1560.662335] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1560.662343] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.662355] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1560.662383] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1560.662418] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1560.662430] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1560.662468] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1560.662477] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1560.662516] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1560.662552] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1560.662564] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1560.662575] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1560.662583] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1560.662590] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662598] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1560.662605] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662617] INFO: task java:1218 blocked for more than 120 seconds.
[ 1560.662753] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.662846] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662975] java D 0000000000000001 0 1218 655 0x00000000
[ 1560.662983] ffff8800c17f7ac8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1560.662990] ffff8800c17f7fd8 0000000000013640 ffff8802240ceeb0 ffff8800c33e1420
[ 1560.662997] 0000008000000000 ffff88002041b7a0 0000000000000000 ffff88002041b778
[ 1560.663003] Call Trace:
[ 1560.663027] [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1560.663049] [<ffffffffa0261eca>] ? dbuf_read+0x8da/0xf20 [zfs]
[ 1560.663061] [<ffffffffa01c8901>] ? kmem_asprintf+0x51/0x80 [spl]
[ 1560.663068] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663080] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663088] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663099] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663126] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663153] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663161] [<ffffffff810e1006>] ? getrawmonotonic+0x36/0xd0
[ 1560.663186] [<ffffffffa0268e1c>] dmu_free_long_range+0x1ac/0x280 [zfs]
[ 1560.663217] [<ffffffffa02daf1c>] zfs_rmnode+0x6c/0x340 [zfs]
[ 1560.663244] [<ffffffffa02fe141>] zfs_zinactive+0xc1/0x1d0 [zfs]
[ 1560.663273] [<ffffffffa02f6974>] zfs_inactive+0x64/0x200 [zfs]
[ 1560.663281] [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[ 1560.663307] [<ffffffffa030ef18>] zpl_vap_init+0x838/0xa10 [zfs]
[ 1560.663315] [<ffffffff811eab68>] evict+0xb8/0x1b0
[ 1560.663322] [<ffffffff811eb405>] iput+0xf5/0x1a0
[ 1560.663330] [<ffffffff811df7f2>] do_unlinkat+0x1e2/0x350
[ 1560.663337] [<ffffffff811d4839>] ? SyS_newstat+0x39/0x60
[ 1560.663345] [<ffffffff811e02c6>] SyS_unlink+0x16/0x20
[ 1560.663353] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1560.663371] INFO: task sshd:1381 blocked for more than 120 seconds.
[ 1560.663508] Tainted: P O 3.18.2-2-ARCH #1
[ 1560.663601] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.663791] sshd D 0000000000000001 0 1381 764 0x00000004
[ 1560.663798] ffff880113ad7b38 0000000000000086 ffff8800a7356eb0 0000000000013640
[ 1560.663804] ffff880113ad7fd8 0000000000013640 ffff8802240ceeb0 ffff8800a7356eb0
[ 1560.663810] ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000280 0000000000000001
[ 1560.663816] Call Trace:
[ 1560.663825] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663839] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663847] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663858] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663885] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663912] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663940] [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1560.663951] [<ffffffffa01c8956>] ? kmem_free_debug+0x16/0x20 [spl]
[ 1560.663962] [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1560.663972] [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1560.663998] [<ffffffffa02fe288>] ? zfs_tstamp_update_setup+0x38/0x1c0 [zfs]
[ 1560.664026] [<ffffffffa02f0cfe>] ? zfs_read+0x39e/0x460 [zfs]
[ 1560.664049] [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1560.664056] [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1560.664082] [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1560.664106] [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1560.664114] [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1560.664122] [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1560.664128] [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1560.664137] [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1560.664145] [<ffffffff81554f20>] int_signal+0x12/0x17
[ 1680.702704] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1680.702864] Tainted: P O 3.18.2-2-ARCH #1
[ 1680.702958] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1680.703089] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1680.703100] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1680.703108] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1680.703116] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1680.703123] Call Trace:
[ 1680.703141] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1680.703152] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1680.703161] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1680.703169] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1680.703176] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1680.703203] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1680.703211] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1680.703224] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1680.703252] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1680.703287] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1680.703299] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1680.703338] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1680.703346] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1680.703385] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1680.703422] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1680.703433] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1680.703444] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1680.703452] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1680.703459] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1680.703467] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1680.703474] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.742712] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1800.742869] Tainted: P O 3.18.2-2-ARCH #1
[ 1800.742964] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1800.743095] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1800.743106] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1800.743114] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1800.743121] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1800.743129] Call Trace:
[ 1800.743146] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1800.743157] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1800.743165] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1800.743174] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1800.743180] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1800.743206] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1800.743214] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1800.743227] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1800.743256] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1800.743290] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1800.743302] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1800.743341] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1800.743349] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1800.743388] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1800.743425] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1800.743437] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1800.743448] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1800.743455] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1800.743462] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.743471] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1800.743477] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782195] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1920.782354] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.782449] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.782580] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 1920.782590] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1920.782599] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1920.782606] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1920.782614] Call Trace:
[ 1920.782631] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1920.782641] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1920.782650] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1920.782658] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.782665] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1920.782693] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1920.782701] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.782714] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1920.782742] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1920.782777] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1920.782789] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1920.782827] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1920.782836] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1920.782874] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1920.782911] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1920.782923] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1920.782934] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1920.782942] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1920.782949] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782957] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1920.782964] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782976] INFO: task java:1218 blocked for more than 120 seconds.
[ 1920.783136] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.783236] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.783366] java D 0000000000000000 0 1218 655 0x00000000
[ 1920.783373] ffff8800c17f7ab8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1920.783381] ffff8800c17f7fd8 0000000000013640 ffffffff81818540 ffff8800c33e1420
[ 1920.783387] ffff8800c17f7a08 ffffffffa025a872 0000000000000000 ffff8800235be4a0
[ 1920.783393] Call Trace:
[ 1920.783416] [<ffffffffa025a872>] ? arc_buf_eviction_needed+0x82/0xc0 [zfs]
[ 1920.783439] [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1920.783476] [<ffffffffa02b634d>] ? bp_get_dsize+0xad/0xf0 [zfs]
[ 1920.783503] [<ffffffffa02770e4>] ? dmu_tx_callback_register+0x324/0xab0 [zfs]
[ 1920.783512] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.783524] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.783532] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.783544] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.783570] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.783597] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.783627] [<ffffffffa02810e5>] ? dsl_dataset_block_freeable+0x45/0x1d0 [zfs]
[ 1920.783653] [<ffffffffa0276fb9>] ? dmu_tx_callback_register+0x1f9/0xab0 [zfs]
[ 1920.783681] [<ffffffffa02f74a0>] zfs_write+0x3c0/0xbf0 [zfs]
[ 1920.783688] [<ffffffff810a9dee>] ? enqueue_entity+0x24e/0xaa0
[ 1920.783696] [<ffffffff8109a1f0>] ? resched_curr+0xd0/0xe0
[ 1920.783705] [<ffffffff810ec497>] ? wake_futex+0x67/0x90
[ 1920.783711] [<ffffffff810ef856>] ? do_futex+0x8f6/0xae0
[ 1920.783736] [<ffffffffa030d2cb>] zpl_putpage+0x21b/0xd50 [zfs]
[ 1920.783744] [<ffffffff811cf1d7>] vfs_write+0xb7/0x200
[ 1920.783752] [<ffffffff811cfd29>] SyS_write+0x59/0xd0
[ 1920.783760] [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1920.783778] INFO: task imap:1389 blocked for more than 120 seconds.
[ 1920.783916] Tainted: P O 3.18.2-2-ARCH #1
[ 1920.784009] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.784138] imap D 0000000000000000 0 1389 770 0x00000000
[ 1920.784145] ffff88000f97fb38 0000000000000082 ffff880222d3bc60 0000000000013640
[ 1920.784151] ffff88000f97ffd8 0000000000013640 ffff88009d290a10 ffff880222d3bc60
[ 1920.784161] ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000fd8 0000000000000001
[ 1920.784168] Call Trace:
[ 1920.784176] [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.784190] [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.784197] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.784209] [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.784236] [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.784263] [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.784292] [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1920.784303] [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1920.784314] [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1920.784338] [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1920.784345] [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1920.784372] [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1920.784395] [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1920.784403] [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1920.784411] [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1920.784417] [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1920.784426] [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1920.784434] [<ffffffff81554f20>] int_signal+0x12/0x17
[ 2280.901736] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 2280.901897] Tainted: P O 3.18.2-2-ARCH #1
[ 2280.901992] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 2280.902124] txg_sync D 0000000000000000 0 583 2 0x00000000
[ 2280.902134] ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 2280.902143] ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 2280.902150] ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 2280.902157] Call Trace:
[ 2280.902175] [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 2280.902186] [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 2280.902194] [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 2280.902203] [<ffffffff81550b59>] schedule+0x29/0x70
[ 2280.902210] [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 2280.902237] [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 2280.902245] [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 2280.902258] [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 2280.902286] [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 2280.902321] [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 2280.902333] [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 2280.902371] [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 2280.902380] [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 2280.902419] [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 2280.902455] [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 2280.902467] [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 2280.902478] [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 2280.902486] [<ffffffff81090e0a>] kthread+0xea/0x100
[ 2280.902493] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 2280.902502] [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 2280.902508] [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[38275.738573] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
```
Running spl versions from arch:
```
hinoki% pacman -Q | grep spl
spl-git 0.6.3_r54_g03a7835_3.18.2_2-1
spl-utils-git 0.6.3_r54_g03a7835_3.18.2_2-1
```",0,0,msr
145,"Free memory:
```
hinoki% free -h
total used free shared buff/cache available
Mem: 7.8G 3.6G 691M 1.0M 3.6G 872M
Swap: 4.0G 0B 4.0G
```",0,0,msr
146,I'm getting this every couple of days now since upgrading to the latest Debian build (which according to the linked bug has the fix in it). Oops messages more or less the same as those already posted.. The zvols lock up and load average climbs into the hundreds. I've never had an issue prior to this.,0,0,msr
147,"I'm seeing similar symptoms after running a `zfs rollback` on a SMB shared dataset. After seeing load quickly rise I've stopped the SMB service and will give the rollback command some time to see if it recovers.
From dmesg, on Ubuntu Server 14.04.2 LTS, ZoL 0.6.3-5 from the Ubuntu PPA:
```
[3764100.795021] smbd D ffff88010b433480 0 34303 60686 0x00000000
[3764100.795025] ffff8800ae491d68 0000000000000082 ffff8800cf951800 ffff8800ae491fd8
[3764100.795029] 0000000000013480 0000000000013480 ffff8800cf951800 ffff8800e48dc3d0
[3764100.795032] ffff8800e48dc3a0 ffff8800e48dc3d8 ffff8800e48dc3c8 0000000000000000
[3764100.795037] Call Trace:
[3764100.795052] [<ffffffff817251a9>] schedule+0x29/0x70
[3764100.795067] [<ffffffffa006b7b5>] cv_wait_common+0x125/0x1c0 [spl]
[3764100.795073] [<ffffffff810ab0b0>] ? prepare_to_wait_event+0x100/0x100
[3764100.795091] [<ffffffffa006b865>] __cv_wait+0x15/0x20 [spl]
[3764100.795130] [<ffffffffa017019b>] rrw_enter_read+0x3b/0x150 [zfs]
[3764100.795181] [<ffffffffa01bf65d>] zfs_getattr_fast+0x3d/0x180 [zfs]
[3764100.795230] [<ffffffffa01d81fd>] zpl_getattr+0x2d/0x50 [zfs]
[3764100.795234] [<ffffffff811c2829>] vfs_getattr_nosec+0x29/0x40
[3764100.795237] [<ffffffff811c28fd>] vfs_getattr+0x2d/0x40
[3764100.795240] [<ffffffff811c29d2>] vfs_fstatat+0x62/0xa0
[3764100.795244] [<ffffffff811c2e5f>] SYSC_newstat+0x1f/0x40
[3764100.795248] [<ffffffff811cdc99>] ? putname+0x29/0x40
[3764100.795252] [<ffffffff811bcfe8>] ? do_sys_open+0x1b8/0x280
[3764100.795256] [<ffffffff811c30ae>] SyS_newstat+0xe/0x10
[3764100.795260] [<ffffffff8173186d>] system_call_fastpath+0x1a/0x1f
[3764100.795263] INFO: task smbd:34313 blocked for more than 120 seconds.
```",0,0,msr
148,"I can trigger the same error by using rsync from two different pools. The rsync process is hanging and can not be killed.
If I won't stop the rsync from the cli, the <my target pool> will endup into a fault status. The fault status is gone after a reboot.
```
May 07 22:43:27 <my host> kernel: INFO: task txg_sync:1408 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel: Tainted: P O 4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: txg_sync D ffff880028b63a28 0 1408 2 0x00000000
May 07 22:43:27 <my host> kernel: ffff880028b63a28 ffff88018c5f3cc0 ffff880070a16540 0000000000000000
May 07 22:43:27 <my host> kernel: ffff880028b63fd8 ffff88021fc93e00 7fffffffffffffff ffff8801b1ccae08
May 07 22:43:27 <my host> kernel: 0000000000000001 ffff880028b63a48 ffffffff8156fa87 ffff88008e7bdcb0
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel: [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel: [<ffffffff8157246c>] schedule_timeout+0x1bc/0x250
May 07 22:43:27 <my host> kernel: [<ffffffff8101f599>] ? read_tsc+0x9/0x10
May 07 22:43:27 <my host> kernel: [<ffffffff810e6757>] ? ktime_get+0x37/0xb0
May 07 22:43:27 <my host> kernel: [<ffffffff8156ef9a>] io_schedule_timeout+0xaa/0x130
May 07 22:43:27 <my host> kernel: [<ffffffffa034daa0>] ? zio_taskq_member.isra.6+0x80/0x80 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa0192248>] cv_wait_common+0xb8/0x140 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel: [<ffffffffa0192328>] __cv_wait_io+0x18/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa034f943>] zio_wait+0x123/0x210 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02d6be1>] dsl_pool_sync+0xc1/0x480 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02f1f80>] spa_sync+0x480/0xbf0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b36>] ? autoremove_wake_function+0x16/0x40
May 07 22:43:27 <my host> kernel: [<ffffffffa0303e06>] txg_sync_thread+0x386/0x630 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff8156fd62>] ? preempt_schedule_common+0x22/0x40
May 07 22:43:27 <my host> kernel: [<ffffffffa0303a80>] ? txg_quiesce_thread+0x3a0/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa018d561>] thread_generic_wrapper+0x71/0x80 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa018d4f0>] ? __thread_exit+0x20/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff81093338>] kthread+0xd8/0xf0
May 07 22:43:27 <my host> kernel: [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel: [<ffffffff81573718>] ret_from_fork+0x58/0x90
May 07 22:43:27 <my host> kernel: [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel: INFO: task rsync:10064 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel: Tainted: P O 4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: rsync D ffff8801a4633a58 0 10064 1 0x00000004
May 07 22:43:27 <my host> kernel: ffff8801a4633a58 ffff880216186f60 ffff880070a56540 ffff8801a4633a68
May 07 22:43:27 <my host> kernel: ffff8801a4633fd8 ffff8800da2a4a20 ffff8800da2a4ae0 ffff8800da2a4a48
May 07 22:43:27 <my host> kernel: 0000000000000000 ffff8801a4633a78 ffffffff8156fa87 ffff8800da2a4a20
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel: [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel: [<ffffffffa019229d>] cv_wait_common+0x10d/0x140 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel: [<ffffffffa01922e5>] __cv_wait+0x15/0x20 [spl]
May 07 22:43:27 <my host> kernel: [<ffffffffa030327b>] txg_wait_synced+0x8b/0xd0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c078c>] dmu_tx_wait+0x25c/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c096e>] dmu_tx_assign+0x9e/0x520 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02c8e10>] ? dsl_dataset_block_freeable+0x20/0x70 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa02be639>] ? dmu_tx_count_dnode+0x59/0xb0 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffffa033f5de>] zfs_write+0x3ce/0xc50 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff81515821>] ? unix_stream_recvmsg+0x701/0x7e0
May 07 22:43:27 <my host> kernel: [<ffffffffa03558cd>] zpl_write+0xbd/0x130 [zfs]
May 07 22:43:27 <my host> kernel: [<ffffffff811d56b3>] vfs_write+0xb3/0x200
May 07 22:43:27 <my host> kernel: [<ffffffff811d624c>] ? vfs_read+0x11c/0x140
May 07 22:43:27 <my host> kernel: [<ffffffff811d6399>] SyS_write+0x59/0xd0
May 07 22:43:27 <my host> kernel: [<ffffffff815737c9>] system_call_fastpath+0x12/0x17
May 07 22:44:27 <my host> kernel: WARNING: Pool '<my target pool>' has encountered an uncorrectable I/O failure and has been suspended.
May 07 22:44:27 <my host> zed[12566]: eid=2302 class=data pool=<my target pool>
May 07 22:44:27 <my host> zed[12569]: eid=2303 class=io_failure pool=<my target pool>
```
```
# zpool status -v
pool: <my target pool>
state: ONLINE
scan: scrub repaired 0 in 9h15m with 0 errors on Thu May 7 09:01:37 2015
config:
NAME STATE READ WRITE CKSUM
<my target pool> ONLINE 0 0 0
<my target pool>-crypt ONLINE 0 0 0
errors: No known data errors
pool: <my source pool>
state: ONLINE
scan: scrub repaired 0 in 6h3m with 0 errors on Thu May 7 05:49:33 2015
config:
NAME STATE READ WRITE CKSUM
<my source pool> ONLINE 0 0 0
mirror-0 ONLINE 0 0 0
crypt-<my source pool>-00 ONLINE 0 0 0
crypt-<my source pool>-01 ONLINE 0 0 0
errors: No known data errors
```
```
#modinfo zfs | head
filename: /lib/modules/4.0.1-1-ARCH/extra/zfs/zfs.ko.gz
version: 0.6.4.1-1
license: CDDL
author: OpenZFS on Linux
description: ZFS
srcversion: 8324F6AEA2A06B2B6F0A0F5
depends: spl,znvpair,zunicode,zcommon,zavl
vermagic: 4.0.1-1-ARCH SMP preempt mod_unload modversions ```
```
#modinfo spl | head
filename: /lib/modules/4.0.1-1-ARCH/extra/spl/spl.ko.gz
version: 0.6.4.1-1
license: GPL
author: OpenZFS on Linux
description: Solaris Porting Layer
srcversion: 8907748310B8940C9D0DCD2
depends: vermagic: 4.0.1-1-ARCH SMP preempt mod_unload modversions ```
```
#uname -a
Linux <my host> 4.0.1-1-ARCH #1 SMP PREEMPT Wed Apr 29 12:00:26 CEST 2015 x86_64 GNU/Linux
```
Fingers crossed I've provided good information. I'm running an arch linux with demz repo.",0,0,msr
149,"I got the same issue with high-loaded mongodb on ZFS.
```
free -m
total used free shared buffers cached
Mem: 7928 7422 506 76 192 1024
-/+ buffers/cache: 6205 1723
Swap: 0 0 0
```
```
# modinfo spl|head
filename: /lib/modules/4.0.0-1-amd64/updates/dkms/spl.ko
version: 0.6.4-2-62e2eb
license: GPL
author: OpenZFS on Linux
description: Solaris Porting Layer
srcversion: 3F1EAF06925B312A0B3F767
depends: vermagic: 4.0.0-1-amd64 SMP mod_unload modversions parm: spl_hostid:The system hostid. (ulong)
parm: spl_hostid_path:The system hostid file (/etc/hostid) (charp)
```
```
# modinfo zfs|head
filename: /lib/modules/4.0.0-1-amd64/updates/dkms/zfs.ko
version: 0.6.4-16-544f71
license: CDDL
author: OpenZFS on Linux
description: ZFS
srcversion: 29BA21B62706579B75D5974
depends: spl,znvpair,zunicode,zcommon,zavl
vermagic: 4.0.0-1-amd64 SMP mod_unload modversions parm: zvol_inhibit_dev:Do not create zvol device nodes (uint)
parm: zvol_major:Major number for zvol device (uint)
```
```
# uname -a
Linux dan-desktop 4.0.0-1-amd64 #1 SMP Debian 4.0.2-1 (2015-05-11) x86_64 GNU/Linux
```
```
# lsb_release -a
No LSB modules are available.
Distributor ID: Debian
Description: Debian GNU/Linux unstable (sid)
Release: unstable
Codename: sid
```",1,0,msr
150,"Same for me on Debian Jessie with Linux 3.16.0-4-amd64 and zfs 0.6.5.2-2.
Happened on a postgresql server, no load at that time.
Jan 22 07:04:42 db04 kernel: [5056080.684110] INFO: task txg_sync:378 blocked for more than 120 seconds.
Jan 22 07:04:43 db04 kernel: [5056080.684128] Tainted: P O 3.16.0-4-amd64 #1
Jan 22 07:04:43 db04 kernel: [5056080.684134] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Jan 22 07:04:43 db04 kernel: [5056080.684142] txg_sync D ffff8800f96bc7e8 0 378 2 0x00000000
Jan 22 07:04:43 db04 kernel: [5056080.684153] ffff8800f96bc390 0000000000000246 0000000000012f00 ffff8800f707ffd8
Jan 22 07:04:43 db04 kernel: [5056080.684165] 0000000000012f00 ffff8800f96bc390 ffff8800ff3137b0 ffff880048db7570
Jan 22 07:04:43 db04 kernel: [5056080.684176] ffff880048db75b0 0000000000000001 ffff8800f8eb9000 0000000000000000
Jan 22 07:04:43 db04 kernel: [5056080.684187] Call Trace:
Jan 22 07:04:43 db04 kernel: [5056080.684201] [<ffffffff8150e159>] ? io_schedule+0x99/0x120
Jan 22 07:04:43 db04 kernel: [5056080.684220] [<ffffffffa00f8760>] ? cv_wait_common+0x90/0x100 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684232] [<ffffffff810a7a40>] ? prepare_to_wait_event+0xf0/0xf0
Jan 22 07:04:43 db04 kernel: [5056080.684264] [<ffffffffa02b679b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684287] [<ffffffffa02447ca>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684313] [<ffffffffa025e6b6>] ? spa_sync+0x366/0xb30 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684339] [<ffffffffa0270131>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684363] [<ffffffffa026fd60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684374] [<ffffffffa00f3deb>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684388] [<ffffffffa00f3d80>] ? __thread_exit+0x20/0x20 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684398] [<ffffffff81087f7d>] ? kthread+0xbd/0xe0
Jan 22 07:04:43 db04 kernel: [5056080.684406] [<ffffffff81087ec0>] ? kthread_create_on_node+0x180/0x180
Jan 22 07:04:43 db04 kernel: [5056080.684417] [<ffffffff81511618>] ? ret_from_fork+0x58/0x90
Jan 22 07:04:43 db04 kernel: [5056080.684428] [<ffffffff81087ec0>] ? kthread_create_on_node+0x180/0x180",0,0,msr
151,"@andreas-p please update to 0.6.5.4\* if available
alternatively: you can build your own latest zfsonlinux packages:
http://zfsonlinux.org/generic-deb.html",0,0,msr
152,"Unfortunately, debian8 is DKMS still on 6.5.2, no update in the last three months. Any clue when this gets resolved?",0,1,msr
153,"@andreas-p sorry, no idea,
but there's always the option of building the packages on your own - which is some effort but you'll know that you can trust those instead of having to rely on third-party repositories, etc.",0,0,msr
154,"Got the very same problem with 0.6.5.4 on a different machine with Debian8, zfs built from source. The stack trace shows exactly the same positions as the trace from Jan 18.
Virtual machine: 6GB RAM, 1.5GB swap, 4 AMD CPU cores.
12:54 Starting an rsync from a 2.5TB xfs to 4TB zfs partition, memory rising from 2GB to 5GB within 5 minutes.
13:09 CPU load/1m steps up from 1 to 10, CPU utilization around 50%, mostly system
13:11 txg_sync hung_timeout first appearance, CPU load/1m steps to 16, committed memory drops to 0, CPU utilization 85% system.
13:15 CPU utilization 95% system, load 16, need reboot.",0,0,msr
155,"same problem here with Debian 8, ZoL 6.5.2 from the official package repository.",0,0,msr
156,"Exactly the same block with 0.6.5.5:
Mar 24 11:13:06 db04 kernel: [4746960.372343] [<ffffffff815107c9>] ? io_schedule+0x99/0x120
Mar 24 11:13:06 db04 kernel: [4746960.372358] [<ffffffffa0198572>] ? cv_wait_common+0x92/0x110 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372370] [<ffffffff810a7ba0>] ? prepare_to_wait_event+0xf0/0xf0
Mar 24 11:13:06 db04 kernel: [4746960.372400] [<ffffffffa05410ab>] ? zio_wait+0x10b/0x1e0 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372423] [<ffffffffa04ce75a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372447] [<ffffffffa04e8706>] ? spa_sync+0x366/0xb30 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372472] [<ffffffffa04fa181>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372496] [<ffffffffa04f9db0>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372507] [<ffffffffa0193cab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372518] [<ffffffffa0193c40>] ? __thread_exit+0x20/0x20 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372529] [<ffffffff8108805d>] ? kthread+0xbd/0xe0
Mar 24 11:13:06 db04 kernel: [4746960.372547] [<ffffffff81087fa0>] ? kthread_create_on_node+0x180/0x180
Mar 24 11:13:06 db04 kernel: [4746960.372558] [<ffffffff81513c58>] ? ret_from_fork+0x58/0x90
Mar 24 11:13:06 db04 kernel: [4746960.372567] [<ffffffff81087fa0>] ? kthread_create_on_node+0x180/0x180
I got two consecutive ""txg_sync blocked for more than 120 seconds"", then it went back to normal.",0,0,msr
157,"I'm suffering from the similar problem. `rsync` reads/writes are extremely slow ~3M. After struggling like this ~24h (it's a multi-million file dataset) machine gets bricked. Please advice.
---
```
# uname -a
Linux ip-172-30-0-118 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
```
```
# lsb_release -a
No LSB modules are available.
Distributor ID: Debian
Description: Debian GNU/Linux 8.5 (jessie)
Release: 8.5
Codename: jessie
```
```
# dpkg -l '*zfs*' | grep ii
ii libzfs2linux 0.6.5.7-1 amd64 OpenZFS filesystem library for Linux
ii zfs-dkms 0.6.5.7-1 all OpenZFS filesystem kernel modules for Linux
ii zfs-zed 0.6.5.7-1 amd64 OpenZFS Event Daemon
ii zfsutils-linux 0.6.5.7-1 amd64 command-line tools to manage OpenZFS filesystems
```
```
# dpkg -l '*spl*' | grep ii
ii spl 0.6.5.7-1 amd64 Solaris Porting Layer user-space utilities for Linux
ii spl-dkms 0.6.5.7-1 all Solaris Porting Layer kernel modules for Linux
```
```
# zpool status
pool: zfs-backup
state: ONLINE
scan: none requested
config:
NAME STATE READ WRITE CKSUM
zfs-backup ONLINE 0 0 0
xvdf ONLINE 0 0 0
logs
xvdg ONLINE 0 0 0
errors: No known data errors
```
```
# zpool iostat -v
capacity operations bandwidth
pool alloc free read write read write
---------- ----- ----- ----- ----- ----- -----
zfs-backup 564G 380G 538 0 1.48M 2.42K
xvdf 564G 380G 538 0 1.48M 2.11K
logs - - - - - -
xvdg 0 1.98G 0 0 471 312
---------- ----- ----- ----- ----- ----- -----
```
```
# zpool get all
NAME PROPERTY VALUE SOURCE
zfs-backup size 944G -
zfs-backup capacity 59% -
zfs-backup altroot - default
zfs-backup health ONLINE -
zfs-backup guid 2876612074418704500 default
zfs-backup version - default
zfs-backup bootfs - default
zfs-backup delegation on default
zfs-backup autoreplace off default
zfs-backup cachefile - default
zfs-backup failmode wait default
zfs-backup listsnapshots off default
zfs-backup autoexpand off default
zfs-backup dedupditto 0 default
zfs-backup dedupratio 1.00x -
zfs-backup free 380G -
zfs-backup allocated 564G -
zfs-backup readonly off -
zfs-backup ashift 0 default
zfs-backup comment - default
zfs-backup expandsize - -
zfs-backup freeing 0 default
zfs-backup fragmentation 44% -
zfs-backup leaked 0 default
zfs-backup feature@async_destroy enabled local
zfs-backup feature@empty_bpobj active local
zfs-backup feature@lz4_compress active local
zfs-backup feature@spacemap_histogram active local
zfs-backup feature@enabled_txg active local
zfs-backup feature@hole_birth active local
zfs-backup feature@extensible_dataset enabled local
zfs-backup feature@embedded_data active local
zfs-backup feature@bookmarks enabled local
zfs-backup feature@filesystem_limits enabled local
zfs-backup feature@large_blocks enabled local
```
```
Jun 15 16:55:40 ip-172-30-0-118 kernel: [113640.618167] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:55:41 ip-172-30-0-118 kernel: [113640.621686] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.625572] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.629691] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:55:44 ip-172-30-0-118 kernel: [113640.633524] Call Trace:
Jun 15 16:55:45 ip-172-30-0-118 kernel: [113640.634740] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:55:47 ip-172-30-0-118 kernel: [113640.637492] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:55:48 ip-172-30-0-118 kernel: [113640.640409] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.643375] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.646118] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:55:50 ip-172-30-0-118 kernel: [113640.649130] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:55:51 ip-172-30-0-118 kernel: [113640.651910] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.655111] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.658303] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:55:54 ip-172-30-0-118 kernel: [113640.661609] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:55:55 ip-172-30-0-118 kernel: [113640.664528] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:55:56 ip-172-30-0-118 kernel: [113640.667024] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.670190] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.672778] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:57:39 ip-172-30-0-118 kernel: [113760.681975] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:57:40 ip-172-30-0-118 kernel: [113760.685521] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:57:41 ip-172-30-0-118 kernel: [113760.689295] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:57:42 ip-172-30-0-118 kernel: [113760.693211] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:57:43 ip-172-30-0-118 kernel: [113760.697110] Call Trace:
Jun 15 16:57:44 ip-172-30-0-118 kernel: [113760.698425] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:57:45 ip-172-30-0-118 kernel: [113760.701106] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:57:47 ip-172-30-0-118 kernel: [113760.704131] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:57:48 ip-172-30-0-118 kernel: [113760.707157] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:57:49 ip-172-30-0-118 kernel: [113760.709944] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:57:51 ip-172-30-0-118 kernel: [113760.713084] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:57:52 ip-172-30-0-118 kernel: [113760.715920] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:57:54 ip-172-30-0-118 kernel: [113760.719014] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:57:55 ip-172-30-0-118 kernel: [113760.722283] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.725571] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.728485] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:57:58 ip-172-30-0-118 kernel: [113760.730974] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:58:00 ip-172-30-0-118 kernel: [113760.734102] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:58:02 ip-172-30-0-118 kernel: [113760.736819] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:38 ip-172-30-0-118 kernel: [113880.747394] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.750796] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.754658] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:59:41 ip-172-30-0-118 kernel: [113880.758412] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:59:42 ip-172-30-0-118 kernel: [113880.762234] Call Trace:
Jun 15 16:59:43 ip-172-30-0-118 kernel: [113880.763454] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:59:44 ip-172-30-0-118 kernel: [113880.766117] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.769024] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.772106] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:59:46 ip-172-30-0-118 kernel: [113880.775310] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:59:48 ip-172-30-0-118 kernel: [113880.778331] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:59:50 ip-172-30-0-118 kernel: [113880.781198] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:59:51 ip-172-30-0-118 kernel: [113880.784324] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:59:52 ip-172-30-0-118 kernel: [113880.787500] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:59:54 ip-172-30-0-118 kernel: [113880.790757] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:59:55 ip-172-30-0-118 kernel: [113880.793707] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:59:56 ip-172-30-0-118 kernel: [113880.796081] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:57 ip-172-30-0-118 kernel: [113880.799104] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:59:58 ip-172-30-0-118 kernel: [113880.801724] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:41 ip-172-30-0-118 kernel: [114000.817906] txg_sync D ffff880079c259c8 0 1361 2 0x00000000
Jun 15 17:01:42 ip-172-30-0-118 kernel: [114000.822256] ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 17:01:43 ip-172-30-0-118 kernel: [114000.827952] 0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.833092] ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.837533] Call Trace:
Jun 15 17:01:47 ip-172-30-0-118 kernel: [114000.838994] [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 17:01:48 ip-172-30-0-118 kernel: [114000.842069] [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 17:01:49 ip-172-30-0-118 kernel: [114000.845833] [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 17:01:50 ip-172-30-0-118 kernel: [114000.849362] [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 17:01:51 ip-172-30-0-118 kernel: [114000.852630] [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 17:01:52 ip-172-30-0-118 kernel: [114000.856037] [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 17:01:53 ip-172-30-0-118 kernel: [114000.859185] [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 17:01:54 ip-172-30-0-118 kernel: [114000.862633] [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 17:01:55 ip-172-30-0-118 kernel: [114000.866301] [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 17:01:56 ip-172-30-0-118 kernel: [114000.870041] [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.873530] [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.876425] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:59 ip-172-30-0-118 kernel: [114000.879915] [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 17:02:00 ip-172-30-0-118 kernel: [114000.882644] [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
```",0,0,msr
158,"@narunask could you please post output of ```
cat /proc/spl/kstat/zfs/arcstats
```
or meanwhile access to the box is denied (""bricked"") ?
Also please post some hardware and specific configuration data (RAM, processor, mainboard, harddrive type, lvm/cryptsetup/etc. etc.)
thanks",0,0,msr
159,"Server is on the `EC2`, currently 1 core, 2GB RAM (`t2.small`), HDD - the magnetic one, I believe it is documented [here](https://aws.amazon.com/ebs/previous-generation/).
HDD that I'm copying from is LVM based, consisting of 3 PVs. HDD are not encrypted.
Also FYI, currently `rsync` is calculating deltas and should start moving data again very soon, if that adds something to the stats.
```
# cat /proc/spl/kstat/zfs/arcstats
6 1 0x01 91 4368 35041278654 31678952268315
name type data
hits 4 42347862
misses 4 10285010
demand_data_hits 4 0
demand_data_misses 4 68
demand_metadata_hits 4 34887103
demand_metadata_misses 4 6345542
prefetch_data_hits 4 0
prefetch_data_misses 4 0
prefetch_metadata_hits 4 7460759
prefetch_metadata_misses 4 3939400
mru_hits 4 20830447
mru_ghost_hits 4 2991105
mfu_hits 4 14056656
mfu_ghost_hits 4 1782606
deleted 4 2167955
mutex_miss 4 90369
evict_skip 4 1781974161
evict_not_enough 4 15390660
evict_l2_cached 4 0
evict_l2_eligible 4 35359622656
evict_l2_ineligible 4 55754657792
evict_l2_skip 4 0
hash_elements 4 5452
hash_elements_max 4 48255
hash_collisions 4 57676
hash_chains 4 69
hash_chain_max 4 4
p 4 7007439
c 4 34933248
c_min 4 33554432
c_max 4 1053282304
size 4 105425184
hdr_size 4 1969640
data_size 4 0
metadata_size 4 30842880
other_size 4 72612664
anon_size 4 753664
anon_evictable_data 4 0
anon_evictable_metadata 4 0
mru_size 4 27006976
mru_evictable_data 4 0
mru_evictable_metadata 4 3653632
mru_ghost_size 4 6850560
mru_ghost_evictable_data 4 0
mru_ghost_evictable_metadata 4 6850560
mfu_size 4 3082240
mfu_evictable_data 4 0
mfu_evictable_metadata 4 32768
mfu_ghost_size 4 25972224
mfu_ghost_evictable_data 4 0
mfu_ghost_evictable_metadata 4 25972224
l2_hits 4 0
l2_misses 4 0
l2_feeds 4 0
l2_rw_clash 4 0
l2_read_bytes 4 0
l2_write_bytes 4 0
l2_writes_sent 4 0
l2_writes_done 4 0
l2_writes_error 4 0
l2_writes_lock_retry 4 0
l2_evict_lock_retry 4 0
l2_evict_reading 4 0
l2_evict_l1cached 4 0
l2_free_on_write 4 0
l2_abort_lowmem 4 0
l2_cksum_bad 4 0
l2_io_error 4 0
l2_size 4 0
l2_asize 4 0
l2_hdr_size 4 0
l2_compress_successes 4 0
l2_compress_zeros 4 0
l2_compress_failures 4 0
memory_throttle_count 4 0
duplicate_buffers 4 0
duplicate_buffers_size 4 0
duplicate_reads 4 0
memory_direct_count 4 947
memory_indirect_count 4 259028
arc_no_grow 4 0
arc_tempreserve 4 0
arc_loaned_bytes 4 0
arc_prune 4 3960
arc_meta_used 4 105425184
arc_meta_limit 4 789961728
arc_meta_max 4 816526072
arc_meta_min 4 16777216
arc_need_free 4 0
arc_sys_free 4 32911360
```
```
# cat /proc/cpuinfo processor : 0
vendor_id : GenuineIntel
cpu family : 6
model : 63
model name : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
stepping : 2
microcode : 0x25
cpu MHz : 2394.530
cache size : 30720 KB
physical id : 0
siblings : 1
core id : 0
cpu cores : 1
apicid : 0
initial apicid : 0
fpu : yes
fpu_exception : yes
cpuid level : 13
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm xsaveopt fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips : 4789.06
clflush size : 64
cache_alignment : 64
address sizes : 46 bits physical, 48 bits virtual
power management:
```
```
# dmidecode --type memory
# dmidecode 3.0
Scanning /dev/mem for entry point.
SMBIOS 2.4 present.
Handle 0x1000, DMI type 16, 15 bytes
Physical Memory Array
Location: Other
Use: System Memory
Error Correction Type: Multi-bit ECC
Maximum Capacity: 2 GB
Error Information Handle: Not Provided
Number Of Devices: 1
Handle 0x1100, DMI type 17, 21 bytes
Memory Device
Array Handle: 0x1000
Error Information Handle: 0x0000
Total Width: 64 bits
Data Width: 64 bits
Size: 2048 MB
Form Factor: DIMM
Set: None
Locator: DIMM 0
Bank Locator: Not Specified
Type: RAM
Type Detail: None
```
Thanks",0,0,msr
160,"I haven't seen the problem for quite a while now (seemed to have gone since 0.6.5.6), but this morning I had the very same hung task with 0.6.5.7 (single occurrance) come up again. Sigh...",0,1,msr
161,"I can confirm that I have not seen this issue happen for ages on the same hardware. I have not had it reoccur, probably within the last 3-6 months.",0,0,msr
162,"Still happening with heavy RSync backups.
```
[88234.807775] INFO: task rsync:18699 blocked for more than 120 seconds.
[88234.807929] Tainted: P O 4.8.13-1-ARCH #1
[88234.808026] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[88234.808159] rsync D ffff880124bcb9d0 0 18699 18698 0x00000000
[88234.808171] ffff880124bcb9d0 00ffffff81003016 ffff88021cd00d00 ffff880127fa6800
[88234.808179] ffff88022fc17ff0 ffff880124bcc000 ffff88021d8eaa10 ffff88021d8eab58
[88234.808185] ffff88021d8eaa38 0000000000000000 ffff880124bcb9e8 ffffffff815f40ec
[88234.808190] Call Trace:
[88234.808203] [<ffffffff815f40ec>] schedule+0x3c/0x90
[88234.808217] [<ffffffffa0171e1f>] cv_wait_common+0x10f/0x130 [spl]
[88234.808225] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[88234.808235] [<ffffffffa0171e55>] __cv_wait+0x15/0x20 [spl]
[88234.808294] [<ffffffffa03095f8>] txg_wait_open+0xa8/0xe0 [zfs]
[88234.808338] [<ffffffffa02c73db>] dmu_tx_wait+0x32b/0x340 [zfs]
[88234.808380] [<ffffffffa02c747b>] dmu_tx_assign+0x8b/0x490 [zfs]
[88234.808422] [<ffffffffa0342d09>] zfs_write+0x3f9/0xc80 [zfs]
[88234.808461] [<ffffffffa034f567>] ? zio_destroy+0xb7/0xc0 [zfs]
[88234.808500] [<ffffffffa0352c78>] ? zio_wait+0x138/0x1d0 [zfs]
[88234.808507] [<ffffffffa016ab9a>] ? spl_kmem_free+0x2a/0x40 [spl]
[88234.808551] [<ffffffffa0337510>] ? zfs_range_unlock+0x1a0/0x2c0 [zfs]
[88234.808590] [<ffffffffa035826c>] zpl_write_common_iovec+0x8c/0xe0 [zfs]
[88234.808627] [<ffffffffa0358497>] zpl_write+0x87/0xc0 [zfs]
[88234.808635] [<ffffffff81208797>] __vfs_write+0x37/0x140
[88234.808642] [<ffffffff810c7be7>] ? percpu_down_read+0x17/0x50
[88234.808648] [<ffffffff81209566>] vfs_write+0xb6/0x1a0
[88234.808652] [<ffffffff8120a9e5>] SyS_write+0x55/0xc0
[88234.808658] [<ffffffff815f8032>] entry_SYSCALL_64_fastpath+0x1a/0xa4
[88234.808664] INFO: task imap:19376 blocked for more than 120 seconds.
[88234.808803] Tainted: P O 4.8.13-1-ARCH #1
[88234.808898] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[88234.809029] imap D ffff8801b875fad8 0 19376 1488 0x00000000
[88234.809036] ffff8801b875fad8 00ff88021d8eab00 ffffffff81a0d500 ffff8801a9f95b00
[88234.809042] ffff8801b875fab8 ffff8801b8760000 ffff88021d8eaa10 ffff88021d8eab58
[88234.809048] ffff88021d8eaa38 0000000000000000 ffff8801b875faf0 ffffffff815f40ec
[88234.809053] Call Trace:
[88234.809059] [<ffffffff815f40ec>] schedule+0x3c/0x90
[88234.809067] [<ffffffffa0171e1f>] cv_wait_common+0x10f/0x130 [spl]
[88234.809073] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[88234.809081] [<ffffffffa0171e55>] __cv_wait+0x15/0x20 [spl]
[88234.809131] [<ffffffffa03095f8>] txg_wait_open+0xa8/0xe0 [zfs]
[88234.809175] [<ffffffffa02c73db>] dmu_tx_wait+0x32b/0x340 [zfs]
[88234.809217] [<ffffffffa02c747b>] dmu_tx_assign+0x8b/0x490 [zfs]
[88234.809259] [<ffffffffa0342299>] zfs_dirty_inode+0xe9/0x300 [zfs]
[88234.809265] [<ffffffff8122a3b4>] ? mntput+0x24/0x40
[88234.809272] [<ffffffff81189adb>] ? release_pages+0x2cb/0x380
[88234.809278] [<ffffffff811c763e>] ? free_pages_and_swap_cache+0x8e/0xa0
[88234.809315] [<ffffffffa035a2cc>] zpl_dirty_inode+0x2c/0x40 [zfs]
[88234.809322] [<ffffffff81237785>] __mark_inode_dirty+0x45/0x400
[88234.809362] [<ffffffffa0346fe8>] zfs_mark_inode_dirty+0x48/0x50 [zfs]
[88234.809399] [<ffffffffa0358826>] zpl_release+0x46/0x90 [zfs]
[88234.809404] [<ffffffff8120b42f>] __fput+0x9f/0x1e0
[88234.809408] [<ffffffff8120b5ae>] ____fput+0xe/0x10
[88234.809414] [<ffffffff8109a0d0>] task_work_run+0x80/0xa0
[88234.809420] [<ffffffff8100366a>] exit_to_usermode_loop+0xba/0xc0
[88234.809425] [<ffffffff81003b2e>] syscall_return_slowpath+0x4e/0x60
[88234.809430] [<ffffffff815f80ba>] entry_SYSCALL_64_fastpath+0xa2/0xa4 ```
Most of the time performance is fine.",0,0,msr
163,"So, I replaced the drive which might have been causing issues, with a new Samsung 840 PRO SSD, partitioned 50GB OS, 4GB Swap and the rest available as L2ARC.
I've set the arc max size to 4GB on a system with only 8GB memory.
Tonight, the same issue occurred, rsync and then everything ground to a halt. So, it seems unlikely to be a disk issue. Nominal read speeds are 100MB/s+ so things are humming along quite nicely.",0,0,msr
164,I can reproduce this issue fairly easily so just let me know what information you'd like me to collect and I'll try to do it.,0,0,msr
165,"```
[78889.098553] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[78889.098667] Tainted: P O 4.8.13-1-ARCH #1
[78889.098719] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[78889.098790] txg_sync D ffff88021c893ab8 0 1372 2 0x00000000
[78889.098798] ffff88021c893ab8 00ffffff810a5d2f ffff8802212b5b00 ffff88021d3ece00
[78889.098802] 0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[78889.098806] ffff880105123b60 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[78889.098809] Call Trace:
[78889.098818] [<ffffffff815f40ec>] schedule+0x3c/0x90
[78889.098821] [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[78889.098825] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[78889.098828] [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[78889.098832] [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[78889.098835] [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[78889.098843] [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[78889.098846] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[78889.098851] [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[78889.098886] [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[78889.098912] [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[78889.098939] [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[78889.098942] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[78889.098969] [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[78889.098997] [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[78889.099002] [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[78889.099006] [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[78889.099010] [<ffffffff8109be38>] kthread+0xd8/0xf0
[78889.099013] [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[78889.099016] [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[78889.099019] [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[79380.619090] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[79380.619245] Tainted: P O 4.8.13-1-ARCH #1
[79380.619342] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[79380.619475] txg_sync D ffff88021c893ab8 0 1372 2 0x00000000
[79380.619487] ffff88021c893ab8 00ffffff810a5d2f ffffffff81a0d500 ffff88021d3ece00
[79380.619494] 0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[79380.619500] ffff8801e9c0c850 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[79380.619506] Call Trace:
[79380.619519] [<ffffffff815f40ec>] schedule+0x3c/0x90
[79380.619525] [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[79380.619531] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[79380.619537] [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[79380.619543] [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[79380.619547] [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[79380.619560] [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[79380.619565] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[79380.619574] [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[79380.619623] [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[79380.619672] [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[79380.619724] [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[79380.619729] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[79380.619780] [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[79380.619830] [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[79380.619839] [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[79380.619847] [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[79380.619854] [<ffffffff8109be38>] kthread+0xd8/0xf0
[79380.619860] [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[79380.619866] [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[79380.619872] [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[81223.826929] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[81223.827091] Tainted: P O 4.8.13-1-ARCH #1
[81223.827226] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[81223.827381] txg_sync D ffff88021c893ab8 0 1372 2 0x00000000
[81223.827393] ffff88021c893ab8 00ffffff810a5d2f ffff8802157c3400 ffff88021d3ece00
[81223.827400] 0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[81223.827407] ffff88015f37d0f0 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[81223.827413] Call Trace:
[81223.827426] [<ffffffff815f40ec>] schedule+0x3c/0x90
[81223.827432] [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[81223.827439] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[81223.827444] [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[81223.827451] [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[81223.827455] [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[81223.827468] [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[81223.827473] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[81223.827482] [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[81223.827531] [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[81223.827580] [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[81223.827632] [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[81223.827636] [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[81223.827687] [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[81223.827738] [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[81223.827747] [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[81223.827755] [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[81223.827762] [<ffffffff8109be38>] kthread+0xd8/0xf0
[81223.827767] [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[81223.827773] [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[81223.827779] [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[94923.528386] CE: hpet increased min_delta_ns to 20115 nsec
[102727.918958] INFO: task rsync:5577 blocked for more than 120 seconds.
[102727.919092] Tainted: P O 4.8.13-1-ARCH #1
[102727.919170] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[102727.919277] rsync D ffff8801dfe9ba10 0 5577 5576 0x00000000
[102727.919287] ffff8801dfe9ba10 00000000ffffffff ffffffff81a0d500 ffff8802157c3400
[102727.919293] ffffffffa0340567 ffff8801dfe9c000 ffff88021ed41148 ffff88021ed41180
[102727.919298] ffff88021ed41170 0000000000000000 ffff8801dfe9ba28 ffffffff815f40ec
[102727.919303] Call Trace:
[102727.919352] [<ffffffffa0340567>] ? zio_destroy+0xb7/0xc0 [zfs]
[102727.919358] [<ffffffff815f40ec>] schedule+0x3c/0x90
[102727.919367] [<ffffffffa0203e1f>] cv_wait_common+0x10f/0x130 [spl]
[102727.919373] [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[102727.919380] [<ffffffffa0203e55>] __cv_wait+0x15/0x20 [spl]
[102727.919414] [<ffffffffa02b814b>] dmu_tx_wait+0x9b/0x340 [zfs]
[102727.919448] [<ffffffffa02b847b>] dmu_tx_assign+0x8b/0x490 [zfs]
[102727.919482] [<ffffffffa0333d09>] zfs_write+0x3f9/0xc80 [zfs]
[102727.919514] [<ffffffffa0340567>] ? zio_destroy+0xb7/0xc0 [zfs]
[102727.919544] [<ffffffffa0343c78>] ? zio_wait+0x138/0x1d0 [zfs]
[102727.919579] [<ffffffffa0328510>] ? zfs_range_unlock+0x1a0/0x2c0 [zfs]
[102727.919610] [<ffffffffa034926c>] zpl_write_common_iovec+0x8c/0xe0 [zfs]
[102727.919640] [<ffffffffa0349497>] zpl_write+0x87/0xc0 [zfs]
[102727.919646] [<ffffffff81208797>] __vfs_write+0x37/0x140
[102727.919652] [<ffffffff810c7be7>] ? percpu_down_read+0x17/0x50
[102727.919656] [<ffffffff81209566>] vfs_write+0xb6/0x1a0
[102727.919660] [<ffffffff8120a9e5>] SyS_write+0x55/0xc0
[102727.919665] [<ffffffff815f8032>] entry_SYSCALL_64_fastpath+0x1a/0xa4
```
Happened last night during backups.",0,0,msr
166,"Okay, so I've replaced the drive which had the high await time, and also increased the memory to 16GB, still having issues:
```
[12785.566973] CE: hpet increased min_delta_ns to 20115 nsec
[25560.317294] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25560.317451] Tainted: P O 4.9.6-1-ARCH #1
[25560.317542] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25560.317668] txg_sync D 0 1392 2 0x00000000
[25560.317678] ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25560.317686] ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25560.317692] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25560.317698] Call Trace:
[25560.317713] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25560.317719] [<ffffffff816061cd>] schedule+0x3d/0x90
[25560.317726] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25560.317781] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25560.317821] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25560.317859] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25560.317867] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25560.317873] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25560.317884] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25560.317891] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25560.317900] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25560.317938] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25560.317984] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25560.318035] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25560.318041] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25560.318091] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25560.318096] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318145] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25560.318154] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25560.318161] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25560.318167] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25560.318171] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318176] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318180] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318184] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[25683.204571] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25683.204722] Tainted: P O 4.9.6-1-ARCH #1
[25683.204814] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25683.204940] txg_sync D 0 1392 2 0x00000000
[25683.204950] ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25683.204958] ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25683.204965] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25683.204970] Call Trace:
[25683.204985] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25683.204992] [<ffffffff816061cd>] schedule+0x3d/0x90
[25683.204999] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25683.205055] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25683.205095] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25683.205134] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25683.205142] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25683.205148] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25683.205159] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25683.205166] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25683.205175] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25683.205213] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25683.205259] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25683.205310] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25683.205315] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25683.205365] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25683.205371] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205419] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25683.205428] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25683.205436] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25683.205441] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25683.205445] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205450] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205454] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205458] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[26051.866268] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[26051.866424] Tainted: P O 4.9.6-1-ARCH #1
[26051.866516] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[26051.866642] txg_sync D 0 1392 2 0x00000000
[26051.866652] ffff8803e6f2c000 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[26051.866660] ffff88041b50b400 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[26051.866667] 00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[26051.866673] Call Trace:
[26051.866687] [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[26051.866694] [<ffffffff816061cd>] schedule+0x3d/0x90
[26051.866700] [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[26051.866756] [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[26051.866796] [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[26051.866834] [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[26051.866842] [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[26051.866847] [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[26051.866859] [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[26051.866866] [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[26051.866874] [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[26051.866913] [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[26051.866959] [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[26051.867009] [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[26051.867015] [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[26051.867065] [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[26051.867070] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867119] [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[26051.867128] [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[26051.867135] [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[26051.867140] [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[26051.867145] [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867149] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867153] [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867157] [<ffffffff8160a995>] ret_from_fork+0x25/0x30
```",0,0,msr
167,"Running
```
% pacman -Q | egrep ""zfs|spl""
spl-linux 0.6.5.9_4.9.6_1-2
spl-utils-linux 0.6.5.9_4.9.6_1-2
zfs-linux 0.6.5.9_4.9.6_1-2
zfs-utils-linux 0.6.5.9_4.9.6_1-2
```",0,0,msr
168,"Memory available is okay:
```
% free -h
total used free shared buff/cache available
Mem: 15G 12G 2.7G 692K 630M 3.0G
Swap: 4.0G 0B 4.0G
```
iostat seem okay (sde is samsung 840 pro)
```
% iostat -mx 10
Linux 4.9.6-1-ARCH (hinoki) 06/02/17 _x86_64_	(2 CPU)
avg-cpu: %user %nice %system %iowait %steal %idle
0.69 0.00 2.26 3.18 0.00 93.88
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 4.39 13.96 0.19 0.23 47.25 0.06 3.16 11.42 0.57 2.28 4.18
sdb 0.00 0.00 4.39 13.69 0.19 0.22 46.63 0.06 3.13 11.19 0.55 2.26 4.09
sdc 0.00 0.00 4.29 13.58 0.19 0.23 47.97 0.06 3.61 12.91 0.66 2.58 4.62
sdd 0.00 0.00 4.32 13.80 0.18 0.23 46.40 0.06 3.18 11.60 0.55 2.29 4.15
sde 0.00 0.30 1.46 11.25 0.02 0.36 61.82 0.01 0.41 0.78 0.36 0.22 0.28
avg-cpu: %user %nice %system %iowait %steal %idle
10.43 0.00 47.68 28.80 0.00 13.08
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 54.20 0.80 3.27 0.01 122.01 0.31 5.70 5.77 0.88 3.10 17.06
sdb 0.00 0.00 54.60 0.80 3.09 0.01 114.60 0.43 7.70 7.80 0.75 3.62 20.07
sdc 0.00 0.00 54.20 1.00 3.15 0.01 117.16 0.39 6.98 7.11 0.00 3.80 21.00
sdd 0.00 0.00 56.30 1.00 3.08 0.01 110.27 0.43 7.57 7.70 0.30 3.87 22.17
sde 0.00 5.60 27.30 159.50 0.08 7.38 81.70 0.06 0.34 0.21 0.36 0.22 4.20
avg-cpu: %user %nice %system %iowait %steal %idle
5.69 0.00 43.65 30.28 0.00 20.38
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 43.30 0.90 2.45 0.01 113.81 0.35 7.86 8.03 0.00 4.38 19.34
sdb 0.00 0.00 40.30 0.90 2.31 0.01 114.99 0.36 8.68 8.87 0.44 4.47 18.40
sdc 0.00 0.00 41.80 0.80 2.39 0.01 115.08 0.34 8.08 8.22 0.38 4.68 19.93
sdd 0.00 0.00 41.20 0.80 2.30 0.01 112.34 0.44 10.39 10.58 0.88 4.56 19.16
sde 0.00 3.10 25.80 136.70 0.07 5.80 74.00 0.05 0.28 0.48 0.25 0.21 3.47
avg-cpu: %user %nice %system %iowait %steal %idle
3.42 0.00 38.83 28.72 0.00 29.02
Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util
sda 0.00 0.00 26.70 0.90 1.58 0.01 117.86 0.26 9.37 9.63 1.78 5.51 15.20
sdb 0.00 0.00 28.50 0.90 1.53 0.01 107.54 0.25 8.59 8.86 0.00 5.04 14.83
sdc 0.00 0.00 27.40 0.90 1.41 0.01 103.12 0.27 9.34 9.65 0.00 5.49 15.53
sdd 0.00 0.00 28.60 0.90 1.43 0.01 100.18 0.24 8.21 8.47 0.00 4.84 14.27
sde 0.00 0.80 27.30 84.50 0.08 2.67 50.38 0.02 0.21 0.27 0.19 0.19 2.07
```",0,0,msr
169,"Just wanted to give a short update to my post in this issue from last year: in the mean time I have upgraded to ZFS 0.6.5.8 from Debian's backports, still using Debian 8. Unfortunately I still get the exact same timeout in the kernel logs.",0,0,msr
170,"@kpande if you're going to assert that, can you please describe what's big enough?
I have resources 4X the average described in this thread and I also see the same issues, in this case with the combination of zfs send and nfsd load.",0,0,msr
171,"> severely undersized hardware
You may be right.
I have 4x 4TB drives, and one 256 SSD as OS/Cache. 16GB memory, 2 core CPU (1.5Ghz Atom). This system is almost exclusively used for RSync backups. I do feel that this is pretty reasonable for my needs.
The ARC cache is 12GB, the OS has 4GB left over. The L2 cache, if enabled , is about 170GB.
I'll have to check how many files it is, but I'm not sure if it's multi-millions or not.",0,0,msr
172,"@kpande you are right I forgot to give any relevant infos about my underlying hardware. Sorry about that, here are hopefully all the relevant infos:
My server is a virtualization Server running Xen 4.4 with currently 6 virtual machines which all have their logical volumes (LVM) stored on a RAIDZ1 volume with 3x 2TB Seagate SATA enterprise disks (ST32000645NS). The Debian 8 OS is independent and located on two internal SATA-SSD disks of both 16 GB in RAID1 using Linux MD for mirroring. The CPU is an Intel E5-2620 v3 @ 2.40GHz with 6 cores/12 threads. Out of these 6 cores 4 vCPUs have been pinned to the host/hypervisor/dom0 using the `dom0_max_vcpus=4 dom0_vcpus_pin` Linux kernel options. The server has 64 GB of TruDDR4 ECC memory and out of this 64 GB of memory 6 GB has been reserved to the host/hypervisor/dom0 using the respective Linux kernel options `dom0_mem=6G,max:6G`. Finally I have reserved 2 GB RAM of these 6 GB for the ARC using the following `zfs_arc_max=2147483648` zfs module option. I have also disabled ZFS prefetch if that is of any relevance (`zfs_prefetch_disable=1`).
Below is the output of an actual ARC summary (server has bee rebooted 5 days ago):
```
ZFS Subsystem Report	Tue Feb 07 15:11:32 2017
ARC Summary: (HEALTHY)
Memory Throttle Count:	0
ARC Misc:
Deleted:	3.05m
Mutex Misses:	7
Evict Skips:	7
ARC Size:	77.69%	1.55	GiB
Target Size: (Adaptive)	100.00%	2.00	GiB
Min Size (Hard Limit):	1.56%	32.00	MiB
Max Size (High Water):	64:1	2.00	GiB
ARC Size Breakdown:
Recently Used Cache Size:	93.75%	1.88	GiB
Frequently Used Cache Size:	6.25%	128.00	MiB
ARC Hash Breakdown:
Elements Max:	171.83k
Elements Current:	97.97%	168.34k
Collisions:	68.60m
Chain Max:	6
Chains:	12.12k
ARC Total accesses:	102.96m
Cache Hit Ratio:	48.88%	50.32m
Cache Miss Ratio:	51.12%	52.63m
Actual Hit Ratio:	48.88%	50.32m
Data Demand Efficiency:	0.00%	49.34m
CACHE HITS BY CACHE LIST:
Most Recently Used:	63.44%	31.92m
Most Frequently Used:	36.56%	18.40m
Most Recently Used Ghost:	0.08%	42.37k
Most Frequently Used Ghost:	0.00%	0
CACHE HITS BY DATA TYPE:
Demand Data:	0.00%	167
Prefetch Data:	0.00%	0
Demand Metadata:	100.00%	50.32m
Prefetch Metadata:	0.00%	0
CACHE MISSES BY DATA TYPE:
Demand Data:	93.74%	49.34m
Prefetch Data:	0.00%	0
Demand Metadata:	6.26%	3.29m
Prefetch Metadata:	0.00%	145
ZFS Tunable:
metaslab_debug_load 0
zfs_arc_min_prefetch_lifespan 0
zfetch_max_streams 8
zfs_nopwrite_enabled 1
zfetch_min_sec_reap 2
zfs_dbgmsg_enable 0
zfs_dirty_data_max_max_percent 25
zfs_arc_p_aggressive_disable 1
spa_load_verify_data 1
zfs_zevent_cols 80
zfs_dirty_data_max_percent 10
zfs_sync_pass_dont_compress 5
l2arc_write_max 8388608
zfs_vdev_scrub_max_active 2
zfs_vdev_sync_write_min_active 10
zvol_prefetch_bytes 131072
metaslab_aliquot 524288
zfs_no_scrub_prefetch 0
zfs_arc_shrink_shift 0
zfetch_block_cap 256
zfs_txg_history 0
zfs_delay_scale 500000
zfs_vdev_async_write_active_min_dirty_percent 30
metaslab_debug_unload 0
zfs_read_history 0
zvol_max_discard_blocks 16384
zfs_recover 0
l2arc_headroom 2
zfs_deadman_synctime_ms 1000000
zfs_scan_idle 50
zfs_free_min_time_ms 1000
zfs_dirty_data_max 624856268
zfs_vdev_async_read_min_active 1
zfs_mg_noalloc_threshold 0
zfs_dedup_prefetch 0
zfs_vdev_max_active 1000
l2arc_write_boost 8388608
zfs_resilver_min_time_ms 3000
zfs_vdev_async_write_max_active 10
zil_slog_limit 1048576
zfs_prefetch_disable 1
zfs_resilver_delay 2
metaslab_lba_weighting_enabled 1
zfs_mg_fragmentation_threshold 85
l2arc_feed_again 1
zfs_zevent_console 0
zfs_immediate_write_sz 32768
zfs_dbgmsg_maxsize 4194304
zfs_free_leak_on_eio 0
zfs_deadman_enabled 1
metaslab_bias_enabled 1
zfs_arc_p_dampener_disable 1
zfs_object_mutex_size 64
zfs_metaslab_fragmentation_threshold 70
zfs_no_scrub_io 0
metaslabs_per_vdev 200
zfs_dbuf_state_index 0
zfs_vdev_sync_read_min_active 10
metaslab_fragmentation_factor_enabled 1
zvol_inhibit_dev 0
zfs_vdev_async_write_active_max_dirty_percent 60
zfs_vdev_cache_size 0
zfs_vdev_mirror_switch_us 10000
zfs_dirty_data_sync 67108864
spa_config_path /etc/zfs/zpool.cache
zfs_dirty_data_max_max 1562140672
zfs_arc_lotsfree_percent 10
zfs_zevent_len_max 64
zfs_scan_min_time_ms 1000
zfs_arc_sys_free 0
zfs_arc_meta_strategy 1
zfs_vdev_cache_bshift 16
zfs_arc_meta_adjust_restarts 4096
zfs_max_recordsize 1048576
zfs_vdev_scrub_min_active 1
zfs_vdev_read_gap_limit 32768
zfs_arc_meta_limit 0
zfs_vdev_sync_write_max_active 10
l2arc_norw 0
zfs_arc_meta_prune 10000
metaslab_preload_enabled 1
l2arc_nocompress 0
zvol_major 230
zfs_vdev_aggregation_limit 131072
zfs_flags 0
spa_asize_inflation 24
zfs_admin_snapshot 0
l2arc_feed_secs 1
zio_taskq_batch_pct 75
zfs_sync_pass_deferred_free 2
zfs_disable_dup_eviction 0
zfs_arc_grow_retry 0
zfs_read_history_hits 0
zfs_vdev_async_write_min_active 1
zfs_vdev_async_read_max_active 3
zfs_scrub_delay 4
zfs_delay_min_dirty_percent 60
zfs_free_max_blocks 100000
zfs_vdev_cache_max 16384
zio_delay_max 30000
zfs_top_maxinflight 32
ignore_hole_birth 1
spa_slop_shift 5
zfs_vdev_write_gap_limit 4096
spa_load_verify_metadata 1
spa_load_verify_maxinflight 10000
l2arc_noprefetch 1
zfs_vdev_scheduler noop
zfs_expire_snapshot 300
zfs_sync_pass_rewrite 2
zil_replay_disable 0
zfs_nocacheflush 0
zfs_arc_max 2147483648
zfs_arc_min 0
zfs_read_chunk_size 1048576
zfs_txg_timeout 5
zfs_pd_bytes_max 52428800
l2arc_headroom_boost 200
zfs_send_corrupt_data 0
l2arc_feed_min_ms 200
zfs_arc_meta_min 0
zfs_arc_average_blocksize 8192
zfetch_array_rd_sz 1048576
zfs_autoimport_disable 1
zfs_arc_p_min_shift 0
zio_requeue_io_start_cut_in_line 1
zfs_vdev_sync_read_max_active 10
zfs_mdcomp_disable 0
zfs_arc_num_sublists_per_state 4
```
Do you need any more information? and what do you think about this setup? is my hardware undersized?",0,0,msr
173,I checked and the majority of my backups are < 200k files and < 20GBytes.,0,0,msr
174,"I hope it helps:
> user@server ~ $ cat /var/log/syslog | grep zfs
> Mar 21 04:01:11 server kernel: [204939.534145] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534234] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534339] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534439] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534539] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534648] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534752] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371676] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371764] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371869] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371969] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372069] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372177] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372282] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209034] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209123] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209228] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209329] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209429] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209537] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209641] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:07:13 server kernel: [205302.045999] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046088] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046192] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046293] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046392] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046501] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046605] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:09:14 server kernel: [205422.882950] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883039] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883144] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883244] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883343] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883452] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883556] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:11:15 server kernel: [205543.719843] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:11:15 server kernel: [205543.719933] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720037] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720137] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720237] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720345] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720449] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556609] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556698] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556802] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556903] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557003] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557112] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557216] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393394] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393483] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393587] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393687] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393787] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393896] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:15:17 server kernel: [205785.394000] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:17:18 server kernel: [205906.229973] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230062] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230166] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230289] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230389] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230498] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230602] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066643] dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066732] dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066837] zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066937] ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067037] ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067146] zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067250] zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 07:54:33 server systemd[1]: zfs-import-cache.service: Main process exited, code=exited, status=1/FAILURE
> Mar 21 07:54:33 server systemd[1]: zfs-import-cache.service: Failed with result 'exit-code'.
> user@server ~ $ cat /etc/issue
> Ubuntu Bionic Beaver (development branch) \n \l
> > user@server ~ $ uname -a
> Linux server 4.15.0-12-generic #13-Ubuntu SMP Thu Mar 8 06:24:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
> user@server ~ $ dpkg -l zfs*
> Desired=Unknown/Install/Remove/Purge/Hold
> | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
> |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
> ||/ Name Version Architecture Description
> +++-==============-============-============-=================================
> un zfs <none> <none> (no description available)
> un zfs-dkms <none> <none> (no description available)
> un zfs-dracut <none> <none> (no description available)
> un zfs-fuse <none> <none> (no description available)
> un zfs-initramfs <none> <none> (no description available)
> un zfs-modules <none> <none> (no description available)
> ii zfs-zed 0.7.5-1ubunt amd64 OpenZFS Event Daemon
> un zfsutils <none> <none> (no description available)
> ii zfsutils-linux 0.7.5-1ubunt amd64 command-line tools to manage Open
> user@server ~ $
> user@server ~ $ df -h /data
> Filesystem Size Used Avail Use% Mounted on
> data 14T 11T 3.1T 78% /data
it was mv process from one zfs to another in the same pool (actually it behaves as cp, not faster). the process hangs forever (at least few hours) and not allow to terminate with a signal. zpool reports errors-free. Now I've started `zpool scrub`, let's see what it will report.
UPDATE:
> user@server ~ $ zpool status
> pool: data
> state: ONLINE
> scan: scrub repaired 0B in 18h22m with 0 errors on Thu Mar 22 02:32:52 2018
> config:
> > NAME STATE READ WRITE CKSUM
> data ONLINE 0 0 0
> raidz2-0 ONLINE 0 0 0
> sdf ONLINE 0 0 0
> sdg ONLINE 0 0 0
> sdh ONLINE 0 0 0
> sdi ONLINE 0 0 0
> sdj ONLINE 0 0 0
> sdk ONLINE 0 0 0
> sdl ONLINE 0 0 0
> sdm ONLINE 0 0 0
> > errors: No known data errors
Let me know if you need other infos.
With best regards, Ag",0,0,msr
175,"I have two associated models.
``` ruby
class Girl < ActiveRecord::Base
belongs_to :boy
end
class Boy < ActiveRecord::Base
has_one :girl
end
```
And I define factory girl for them:
``` ruby
factory :girl do
boy
name 'saria'
end
factory :boy do
name do
puts create boy'
'leo'
end
end
```
Then i create a girl with an exsisting boy:
``` ruby
boy = create :boy
girl = create :girl, boy: boy
```
I will get following result:
``` ruby
>>>create boy
>>>create boy
```
I am wondering why the block of name of boy runs twice.
And, if i put code that can only runs once, troubles come.
Version of my factory_girl:
- factory_girl (4.4.0)
- factory_girl_rails (4.4.1)",0,0,msr
176,"I have the same issue or question.
In my case this leads to a huge amount of time spent invoking the factory twice.
Only half of the objects are actually used, because one of the two created objects are discarded anyway.
Is there any way to prevent this from happening?
It would save a lot of time in my test suite.",0,0,msr
177,"+1, same question as this makes difficult for me to test some things and this is not documented",1,0,msr
178,"Ran into a similar problem where we had two associations attempting to create unique parent instances when we wanted them to belong to the same parent. ``` ruby
Class Term < ActiveRecord::Base
has_many :candidates
has_many :periods
end
Class Period < ActiveRecord::Base
belongs_to :term
end
Class Candidate < ActiveRecord::Base
belongs_to :term
end
```
In order to avoid needing to make multiple factory calls from the spec to reach the same term we ended up defining the factories like this:
``` ruby
factory :period do
term { Term.first || association(:term) }
end
factory :candidate do
term { Term.first || association(:term) }
end
```
This way the pre-existing Term can be referred to instead of creating two instances.",0,0,msr
179,"Calling `Thing.first` introduces non-determinism in the test suite, since records can be created at various layers without the developer even being aware. I'd recommend creating a record and assigning it directly if the intent is to share the value.",0,0,msr
180,@joshuaclayton is there any chance that you could give some practical example to how to do that?,0,0,msr
181,"@scaryguy sure thing!
``` ruby
term = create(:term)
period = create(:period, term: term)
candidate = create(:candidate, term: term)
```",0,0,msr
182,Thank you @joshuaclayton !,0,0,msr
183,"**From OP:**
```ruby
class Girl < ActiveRecord::Base
belongs_to :boy
end
class Boy < ActiveRecord::Base
has_one :girl
end
```
---
For real? Girl belongs to boy didn't feel a little gross to anyone?
Let's write examples without reducing women to objects, please, and call this stuff out when we see it.",0,0,msr
184,"@Euraldius, both `Boy` and `Girl` are ""objects"", as in Ruby objects. Let's not make such a big deal out of nothing, really, it's just an example. Nobody questioned or undermined human rights or the equality of genders.",0,0,msr
185,"Bumping this, OPs question still isn't answered. In the original post, OP is already using the suggestion [provided here](https://github.com/thoughtbot/factory_girl/issues/683#issuecomment-248977879).
The only solution I can come up with is to say (in the context of OP example)
```
factory :girl do
boy { |weird_thing| somehow_inspect_weird_thing_to_check_if_boy_was_already_provided(weird_thing) }
name 'saria'
end
```
But this doesnt seem very idiomatic.
Also, at least where I live, there is a a constant, subtle, barrage of messaging coming from the TV and other sources telling us that women are the property of men. This is insidious, and FWIW, I think we should try to be conscious not to take part. cc @linkyndy.",0,0,msr
186,"just use `.where(name: 'James').first_or_create!(foo: bar)` in the block
so you could have something like
```
Class Term < ActiveRecord::Base
has_many :candidates
has_many :periods
end
Class Period < ActiveRecord::Base
belongs_to :term
end
Class Candidate < ActiveRecord::Base
belongs_to :term
end
```
Then do something like
```
factory :period do
term { Term.where(name: 'Term 1').first_or_create! }
end
```",0,0,msr
187,"@linkyndy Do you seriously think @Euraldius did not understand that those labels referred to Ruby objects? Let me slowly explain to you the point that was being made:
When engineers use labels such as ""boy""/""girl"" in ways that reinforce toxic gender stereotypes, such engineers show their ignorance and (perhaps unconscious, perhaps latent) misogyny. So, stop. Think about how you use your language. And be considerate.
Another related example: the widespread use of repulsive ""master""/""slave"" labels in software systems.
Just fucking think about it for a second... bro.",0,1,msr
188,"https://github.com/thoughtbot/factory_bot/pull/943
https://github.com/thoughtbot/factory_bot/pull/1051
Bad example or not, the issue has been handled on this and many other levels.
We still haven't really found a great solution to the original problem, but if the only submissions here are going to be in regards to the social issues involved, then the should be locked. @Euraldius @joshuaclayton",0,0,msr
189,"I solved this problem by using `:inverse_of` on the association in both models.
```
class Belonging < ActiveRecord::Base
belongs_to :having, inverse_of: :belonging
end
```
```
class Having < ActiveRecord::Base
has_one :belonging, inverse_of: :having
end
```
And then @Manifold0 I used an after-create callback on the factory for the model without the foreign key, so they look like this:
```
factory :belonging do
having
end
```
```
factory :having do
after(:create) do |having, evaluator|
having.belonging || create(:belonging, having: having)
end
end
```
In the callback, `inverse_of` allows for the associated Belonging to be returned from memory even if it isn't saved yet, as is the case when you `create :belonging`. You can do some dynamic checking, while not relying on something like `Belonging.first`.
@Kriechi this will help your test performance by not creating extra throwaway records, and also affect your actual production performance by making fewer trips to the db. Which is the actual purpose of `inverse_of`.",0,0,msr
190,"What do you think about:
```ruby
factory :singleton do
initialize_with do
Singleton.where(
name: 'unique_name'
).first_or_initialize
end
end
```
this moves the logic to the definition of your singleton factory.",0,0,msr
191,"If you're really working with a Singleton, I'm not sure you need FactoryBot at all. If it's not really a singleton, than first_or_initialize has the non-determinism problem mentioned above. Am I missing something about what you're trying to do?",0,0,msr
192,"I feel hurt by the word Singleton because it reminds me of who I think I am.
This message is not trollish intent to hurt anyone, rather a part of a philosophical debate. (cc @skatenerd)
I wholeheartedly agree that we should fight any discrimination.",0,1,msr
193,"@mib32 sry i got a bit defensive, seeing how the original namechange discussion ... turned out",1,0,msr
194,"@drewcimino The requirement in my case was that there is only one entry with a specific name in the db table. Every reference to a `:singleton` with the same name should be the same instance. `first_or_initialize` is not atomic and could lead to race conditions. My tests run single threaded so there shouldn't be any race condition. Anyway the unique condition would be enforced on database level with a unique index, but a race condition would break the test ...",0,0,msr
195,"i can see how the girl-belongs-to-boy rubs the wrong way, linkyndy. imagine a similar example, only swap the Girl model to Black and Boy model to White.",0,0,msr
196,"@adler99 So it seems your Singleton class isn't really a singleton, but does have relatively few saved records, like some sort of reference data. If the objects of this class are _that_ standardized - that is, a record with a given name always has the same associated data - I would create them at the beginning of the test suite with a seeds-file-like script, and building some class methods for accessing them.
Something like:
```
Singleton.drewcimino
=> #<Singleton id: 1, name: ""drewcimino"">
```
as opposed to:
```
Singleton.where(name: ""drewcimino"").first_or_initialize
=> #<Singleton id: 1, name: ""drewcimino""> OR #<Singleton id: nil, name: ""drewcimino"">
```",0,0,msr
197,"Using the `term`, `period` and `candidate` example, if I add another layer, say:
```
Class Bear < ActiveRecord::Base
belongs_to :candidate
belongs_to :period
end
```
Is it possible to create the `term` inside of the `bear` FactoryBot beforehand and then get candidate and period to use the `bear` to create internally?
Probably a wrong example here but is it possible to do it inside (sometimes an object belongs to an object that belongs to an object, and if we were to manually create each object from the top and reference the next one outside of the factorybot file then it would be a mess everywhere), something like this:
```
# this is wrong but you get the idea
FactoryBot.define do
factory :bear do
# Run this first so everytime a bear gets created, a term for this bear gets created first to be used later
before(:create) { create(:term }
# Using the term created before this
association :period, factory: :period, term: term
association :candidate, factory: :candidate, term: term
end
end
```",0,0,msr
198,"@CyberMew If you want to complete the period and candidate with Bear#term after Bear/Term are created, I think an `after` block is the prescription.
```
FactoryBot.define do
factory :bear do
association :term
association :period
association :candidate
after(:build) do |bear|
bear.period.term ||= bear.term
bear.candidate.term ||= bear.term
end
end
end
```
I use `after(:build)` instead of `after(:create)` and AR methods instead of passing ids around, so this setup will work for both build(:bear) and create(:bear). The associations follow the build strategy (build/create) of the parent, so if you create they'll assign first and then automatically populate all the ids on save.
You can force build strategies if you want to, but I don't recommend it as you can easily end up with some funky saved/not-saved ""associated"" records. But there's a bit on that here: https://github.com/thoughtbot/factory_bot/blob/master/GETTING_STARTED.md#associations",0,0,msr
199,"I came here for code and got distracted by social issues. I don't have a problem with discussing our society. In fact, I'd like to know, who decides which social issues are worth discussing while working? Is it as simple as me labeling my issue an injustice - then I proceed to make my case? Currently, I have access to this comment box, so I suppose I have an equal right to say - but can someone point me to the rules?
I opted for `before(:create)`
```
FactoryBot.define do
factory :album do
name { Faker::Music::album }
year { rand(1887..Time.new.year).to_i }
before(:create) do |album|
album.artist = create(:artist)
album.genre = create(:genre)
end
end
end
```",0,0,msr
200,I believe we need `many-to-many` association between `Boy` and `Girl` classes.,0,0,msr
201,"Came here because of a frustrating engineering problem, disappointed to see people are discussing irrelevant social issues.
* Yes the social issues are important
* No github issues is probably not the right place to discuss/debate them
* Call the classes `Yob` and `Lrig`, I don't care, I need a solution to the inter-dependent factory objects",0,1,msr
202,"As a reminder, anybody participating here agrees to follow our [Code of Conduct](https://thoughtbot.com/open-source-code-of-conduct). A code example where a girl belongs to a boy does not follow our Code of Conduct and I appreciate the folks who called that out.
I'm going to lock this issue. If somebody would be willing to open a new issue with a better example and a summary of the problem and potential solutions offered so far I would be most grateful.",0,0,msr
203,"Hi all,
I am very new in scikit-learn. My questions is: how to download my own dataset (csv file).
I will be highly appreciated any answers.
Thanks.
Martin",0,0,msr
204,The documentation of sklearn is really very useful and should answer your question: http://scikit-learn.org (basically you have to put your data in numpy arrays),0,0,msr
205,"This is something that could have a bit more documentation than is in there
currently. You might find Pandas useful.
On 29 October 2014 09:45, Alexander Fabisch notifications@github.com
wrote:
> The documentation of sklearn is really very useful and should answer
> your question: http://scikit-learn.org (basically you have to put your
> data in numpy arrays)
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60845063
> .",0,0,msr
206,"@jnothman Should we reopen this issue and add a new section in the documentation? For example in this section: http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading your own data"").",0,0,msr
207,"see #2801
On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
wrote:
> @jnothman https://github.com/jnothman Should we reopen this issue and
> add a new section in the documentation? For example in this section:
> http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> your own data"").
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069
> .",0,0,msr
208,"My own dataset means the dataset that I have collected by my self, not the
standard dataset that all machine learning have in their depositories (e.g.
iris or diabetes).
I have a simple csv file and I on my desktop and I want to load it inside
scikit-learn. That will allow me to use scikit-learn properly and introduce
it to my colleges to serve our community.
I need a very simple and easy way to do so.
I will be highly appreciated any useful advice.
On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> see #2801
> > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> wrote:
> > > @jnothman https://github.com/jnothman Should we reopen this issue and
> > add a new section in the documentation? For example in this section:
> > http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> > your own data"").
> > > > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > .
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212
> .",0,0,msr
209,"See http://pandas.pydata.org/pandas-docs/stable/io.html
On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> My own dataset means the dataset that I have collected by my self, not
> the
> standard dataset that all machine learning have in their depositories
> (e.g.
> iris or diabetes).
> > I have a simple csv file and I on my desktop and I want to load it inside
> scikit-learn. That will allow me to use scikit-learn properly and
> introduce
> it to my colleges to serve our community.
> > I need a very simple and easy way to do so.
> > I will be highly appreciated any useful advice.
> > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > see #2801
> > > > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> > wrote:
> > > > > @jnothman https://github.com/jnothman Should we reopen this issue
> > > and
> > > add a new section in the documentation? For example in this section:
> > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> > > your own data"").
> > > > > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > > .
> > > > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > > > .
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671
> .",0,0,msr
210,"Thanks for the link. I checked it out, but the process looks complicated.
Perhaps if there is a short youtube video explains the process much easier,
otherwise I do not know what to do to solve this matter.
On 29 October 2014 19:12, jnothman notifications@github.com wrote:
> See http://pandas.pydata.org/pandas-docs/stable/io.html
> > On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> > > My own dataset means the dataset that I have collected by my self, not
> > the
> > standard dataset that all machine learning have in their depositories
> > (e.g.
> > iris or diabetes).
> > > > I have a simple csv file and I on my desktop and I want to load it
> > inside
> > scikit-learn. That will allow me to use scikit-learn properly and
> > introduce
> > it to my colleges to serve our community.
> > > > I need a very simple and easy way to do so.
> > > > I will be highly appreciated any useful advice.
> > > > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > > > see #2801
> > > > > > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> > > > > > wrote:
> > > > > > > @jnothman https://github.com/jnothman Should we reopen this issue
> > > > and
> > > > add a new section in the documentation? For example in this section:
> > > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html
> > > > (""Loading
> > > > your own data"").
> > > > > > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > > > .
> > > > > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > > > > .
> > > > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671>
> > > > .
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60906075
> .",0,0,msr
211,"It probably looks something like:
import pandas as pd
data = pd.read_csv(open('myfile.csv'))
target = data[target_column_name]
del data[target_column_name]
# Then fit a scikit-learn estimator
SVC().fit(data, target)
On 29 October 2014 23:19, MartinLion notifications@github.com wrote:
> Thanks for the link. I checked it out, but the process looks complicated.
> Perhaps if there is a short youtube video explains the process much
> easier,
> otherwise I do not know what to do to solve this matter.
> > On 29 October 2014 19:12, jnothman notifications@github.com wrote:
> > > See http://pandas.pydata.org/pandas-docs/stable/io.html
> > > > On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> > > > > My own dataset means the dataset that I have collected by my self, not
> > > the
> > > standard dataset that all machine learning have in their depositories
> > > (e.g.
> > > iris or diabetes).
> > > > > > I have a simple csv file and I on my desktop and I want to load it
> > > inside
> > > scikit-learn. That will allow me to use scikit-learn properly and
> > > introduce
> > > it to my colleges to serve our community.
> > > > > > I need a very simple and easy way to do so.
> > > > > > I will be highly appreciated any useful advice.
> > > > > > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > > > > > see #2801
> > > > > > > > On 29 October 2014 18:07, Alexander Fabisch <
> > > > notifications@github.com>
> > > > > > > > wrote:
> > > > > > > > > @jnothman https://github.com/jnothman Should we reopen this
> > > > > issue
> > > > > and
> > > > > add a new section in the documentation? For example in this
> > > > > section:
> > > > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html
> > > > > (""Loading
> > > > > your own data"").
> > > > > > > > > > —
> > > > > Reply to this email directly or view it on GitHub
> > > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > > > > > > .
> > > > > > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > > > > > .
> > > > > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > > > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671>
> > > > > .
> > > > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60906075>
> > > > .
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60913843
> .",0,0,msr
213,"Hi jnothman, Thank you so much for your help, I really appreciate your cooperation.
I tried applying your code. Thus, once I interned (import pandas as pd). Directly I had the following message in red color:
import pandas as pd
No module named 'dateutil'
Traceback (most recent call last):
File ""<pyshell#0>"", line 1, in <module>
import pandas as pd
File ""C:\Python34\lib\site-packages\pandas__init__.py"", line 7, in <module>
from . import hashtable, tslib, lib
File ""pandas\tslib.pyx"", line 37, in init pandas.tslib (pandas\tslib.c:76813)
ImportError: No module named 'dateutil'
What should I do? Thanks a lot",0,0,msr
214,"It just means you do not have the dateutil module installed. You can install it by doing ```
sudo apt-get install python-dateutil
```
hth",0,0,msr
215,"You can have a look at this for more details, http://stackoverflow.com/questions/20853474/importerror-no-module-named-dateutil-parser",0,0,msr
216,"Thanks MechCoder for your contribution. I tried ""sudo apt-get install python-dateutil"", but it is not clear to me at what stage should indicate this code?
Do you think that there is an easy way to load my (excel or csv) file suing any simple ways such as open folder (regular way). There is another matter also which how to determine the class label that I want to predict form my dataset using scikit-learn. But anyway this step supposed to be after loading the file itself. Not easy process at all.
Is there any youtube tutorial about loading dataset (not iris which is everywhere or other famous. stuff). Video is easy than links",0,0,msr
217,"HI all,
I wrote the following code:
import pandas as pd
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
and i got this error:
File ""<ipython-input-10-dd0ba70fe93f>"", line 2
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
^
SyntaxError: invalid syntax
could you plz guide me.",0,0,msr
218,"We could tell you what the problem is but I think in this case you will learn more from it if you find it on your own. You should read the error message carefully. It is a Python syntax error.
```
File """", line 2
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
^
SyntaxError: invalid syntax
```",0,0,msr
219,"On 22 Aug 2015 08:33, ""samira afzal"" notifications@github.com wrote:
> HI all,
> I wrote the following code:
> > import pandas as pd
> data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > and i got this error:
> File """", line 2
> data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> ^
> SyntaxError: invalid syntax
> > could you plz guide me.
I recommend you finding another tool where you can work with easily without
headache yourself with this particular one. This is what I did myself.
Good luck
Martin
> —
> Reply to this email directly or view it on GitHub.",0,0,msr
220,"To be clear, these previous posters are saying that being somewhat
comfortable with the Python language is a prerequisite to using
scikit-learn. You have missed some quotes around a string. This shows great
unfamiliarity with Python (and a characteristic of most programming
languages), and scikit-learn is probably not the best place to start.
On 22 August 2015 at 18:04, MartinLion notifications@github.com wrote:
> On 22 Aug 2015 08:33, ""samira afzal"" notifications@github.com wrote:
> > > HI all,
> > I wrote the following code:
> > > > import pandas as pd
> > data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > > > and i got this error:
> > File """", line 2
> > data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > ^
> > SyntaxError: invalid syntax
> > > > could you plz guide me.
> > I recommend you finding another tool where you can work with easily without
> headache yourself with this particular one. This is what I did myself.
> > Good luck
> Martin
> > > —
> > Reply to this email directly or view it on GitHub.
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-133650026
> .",0,0,msr
221,"Just want to support @MartinLion --- I am a scikit-learn newbie and have just have spent a frustrating time going thought the docs, and I can't find anywhere how to read my own data (and not a prepared toy dataset), and what the python format of data is.",0,0,msr
222,"Kindly refer - - [How do I load my data to work with scikit-learn?](http://stackoverflow.com/q/21492726/3109769)
- [How to load data from CSV file?](http://stackoverflow.com/q/11023411/3109769)",0,0,msr
223,"> • How do I load my data to work with scikit-learn?
> • How to load data from CSV file?
We should add these in the FAQ.",1,0,msr
224,"should we instead add as a section in the tutorial below/above [""Loading an example dataset""](http://scikit-learn.org/stable/tutorial/basic/tutorial.html#loading-an-example-dataset)?",0,0,msr
225,"Also could you tag this ""Question"", ""Documentation"" and reopen it?",0,0,msr
226,"> should we instead add as a section in the tutorial here?
We should reference it. But I don't see this as tutorial material because
it is outside the scope of scikit-learn. We can only give pointers
That's an answer that the users really don't want to hear, because there
point of view is that they have a lump of data and they want it inside
scikit-learn. The answer is: this is not a problem that scikit-learn
solves, go see pandas if you have CSV, scikit-image if you have images,
database connectors (SQLAlchemy?) if you work on databases...
I guess that we should have a sentence like this in the tutorial, where
you reference, with pointers.
As a side note, the kind of errors hit by the users on the thread of this
issue (lack of basic knowledge of Python for instance) tells me that we
cannot solve their problem. They need to go to entry-level tutorials on
Python, and get a bigger picture. Maybe we should make sure that we give
pointers to these in the right spots, eg early on in the tutorial.",0,0,msr
227,"> > should we instead add as a section in the tutorial here?
> > We should reference it. But I don't see this as tutorial material because
> it is outside the scope of scikit-learn. We can only give pointers
> > That's an answer that the users really don't want to hear, because there
> point of view is that they have a lump of data and they want it inside
> scikit-learn. The answer is: this is not a problem that scikit-learn
> solves, go see pandas if you have CSV, scikit-image if you have images,
> database connectors (SQLAlchemy?) if you work on databases...
> > I guess that we should have a sentence like this in the tutorial, where
> you reference, with pointers.
> > As a side note, the kind of errors hit by the users on the thread of this
> issue (lack of basic knowledge of Python for instance) tells me that we
> cannot solve their problem. They need to go to entry-level tutorials on
> Python, and get a bigger picture. Maybe we should make sure that we give
> pointers to these in the right spots, eg early on in the tutorial.
Well, take it easy!!!
I don't know whether you are one of scikit-learn staff or not, but I need
to say that your way of talking is harming both scikit-learn staff and
users (us), due to the two reasons:
First reason, criticizing people (like what you did) and assuming that they
are novice in Python so they don't know how to work with scikit-learn,
means you or the staff are trying to blind their eyes to the truth that
scikit-learn staff are not able to create a clear tutorial to allow loading
the real data, at least. In addition, pretending that the tutorial of
scikit-learn is perfect in spite all the questions regarding loading the
real data (not the toy data as it is too easy to be imported comparing to
the real data) is something needs to be reconsidered, and this means that
scikit-learn staff don't care about the name of scikit-learn at all.
Second, we can understand from your unsuitable way of talking that you
already forgot that scikit-learn is a product, and we as users are
customers, so either you or the staff of scikit-learn should respect all of
us and thank us for any comment or bug fixing. This is the professional way
of behavior. So I recommend you to think of your words before saying them. If
you are knowing the way of loading the real data and you'd like to help,
don't only say go see pandas, better you answer people's question nicely
rather than hurting them with your words, but if you're simply not able to
do that, so keep quite.
On the other hand, regarding the question ""should we instead add as a
section in the tutorial here?"", I would like to say ""_YES_"", you or
scikit-learn staff should add a section in the official tutorial about how
to load your own data either CSV, or ARFF or text or whatever, as users are
interested to load their own data, this is very critical issue should be
considered in the tutorial (not to be ignored). *If you rely on the user,
then what is your work? *
Nevertheless, for those who are still struggling with scikit-learn, I would
like to say, this is not the end of the world, and as I mentioned
previously, find another to tool make your life much easier. For this
reason, and in order to save your time, I would like to recommend some
tools to assist you in data mining procedures. For instance, Waikato
Environment for Knowledge Analysis (WEKA),
http://www.cs.waikato.ac.nz/ml/weka/, last version is WEKA 3-7-13, is a
collection of machine learning algorithms for data mining tasks. WEKA
allows you to use its schemes either from GUI or writing Java code, so its
very easy for non-programmers. Additional to WEKA, R is also an excellent
tool for data mining stuff, you can also perform tasks of R from WEKA or
vice versa. However, if you have a patience to design a prediction process
manually (drag/drop), RapidMiner is a great tool for this propose where you
can design a very nice flow to achieve your target.
Thanks David van Leeuwen for your support.
Good luck in your analysis.
Cheers,
Martin
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-160153930
> .",0,1,msr
228,"Hey Martin,
Kindly don't be offended.
He did not criticize :) He, being one of the top contributors to scikit-learn has to make tough decisions as to what will go into our codebase and what will not, as a more verbose documentation or tutorial might not be preferable for a lot of people. Gael has in fact contributed a lot of user guides himself to scikit learn to help users.
The reason why he was opposing that addition to the tutorial was that there are multitude of ways in which users have their data stored and such a user guide on how to get the input data from all of them (a text file/csv file/database/zipped archive), is indeed out of scope for scikit learn, which is a machine learning library.
The most important thing to note here is that **it is very clearly explained by documentations of libraries which handle data, like numpy or pandas.**
It is expected from the user that he or she knows this! Since it seems to not be very clear, he suggests that we add a FAQ, pointing the user to such userguides, which are more elaborate than we could possibly get :)
It may appear that our tutorial could be a bit more elaborate on how the inputs are obtained. But the thing, in general, with userguides is that, it could _always_ be a little bit more elaborate, which makes us set a hard limit on how detailed our userguides can get, to help contain the userguide in a maintainable format :) If you think from that perspective, you yourself would understand our situation.
As this issue is open someone will indeed send a PR soon adding a nice FAQ entry and an example, maybe, which could help clarify your (or any other new user's) doubts on input formats.
Cheers!",0,0,msr
229,"Hello @MartinLion ,
we understand your eagerness to solve your problem, and your frustration when it is not solved.
However, you seem quite misinformed about what is scikit-learn, how it works, and how the project is developed. Therefore, I would like to make some points clear for you. As you can see from
http://scikit-learn.org/stable/about.html, scikit-learn is a community effort that is developed by a team of volunteers, mostly on their free time. Gaël is one of the creators of the project and its current leader:
scikit-learn would certainly not be the same without his contribution (the same for other volunteers), and he certainly did not deserve your dismissive words.
What I would like to emphasize is that there is no such thing as a scikit-learn ""product"", or scikit-learn ""staff"" (only a handful of people have worked full time on the project). You mention ""we as users are
customers"", but how much are you paying for using scikit-learn? Despite the important development cost, users get scikit-learn for free (and of course that's how it's intended to be). In fact, the development of the project relies on a fragile alchemy: users' needs being a top priority for developers, and users reporting bugs and concerns in the most positive way. The kind of ""ranting"" that you wrote can be very discouraging for developers, who contribute their free time and their expertise just because they believe that scikit-learn is a useful tool for the community. Some prominent developers stopped contributing to open-source software precisely because of such ""customer-like attitude"" of a few people underlining only shortcomings, and dismissing the huge development efforts. Please try to see the bright side as well: you received advice and comments from a lot of people, I'm sure that there was something for you to learn out of it, even if it did not solve your problem. Also, although users' needs are indeed a top priority of scikit-learn (it has an amazing documentation, of which most scientific Python packages can be jealous!), each software addresses a well-targeted niche of users, and it is just normal that scikit-learn cannot fit all users. For example, it is preferable to use scikit-learn with already a good knowledge of Scientific Python. So, I'm really glad that you found a
package that suited your needs better, but please also acknowledge the time and good will that people gave away when answering you.
So, folks, let's all show some good will and keep a constructive dialog.
That's how the project we love will keep on rocking!",0,0,msr
230,"> For this reason, and in order to save your time, I would like to recommend some tools to assist you in data mining procedures. For instance, Waikato Environment for Knowledge Analysis (WEKA), http://www.cs.waikato.ac.nz/ml/weka/, last version is WEKA 3-7-13, is a collection of machine learning algorithms for data mining tasks. WEKA allows you to use its schemes either from GUI or writing Java code, so its very easy for non-programmers. Additional to WEKA, R is also an excellent tool for data mining stuff, you can also perform tasks of R from WEKA or vice versa. However, if you have a patience to design a prediction process manually (drag/drop), RapidMiner is a great tool for this propose where you can design a very nice flow to achieve your target.
Maybe we should make clear that scikit-learn is a Python **library**. It does not have the same scope as WEKA or RapidMiner. It fits perfectly into the [scientific Python ecosystem](http://www.scipy-lectures.org/) but you should be willing to write code if you want to use it.",0,0,msr
231,"Perhaps I should elaborate on my original frustration, to give you some context. I've been programming in Python almost exclusively for a year now (I am a late convert), and am fairly familiar with the ecosystem---I've done lot's of webservice related things, but also manipulation of resources related to automatic speech recognition. I do my scientific work in [Julia](http://julialang.org/) since a couple of years, and before that, in R, octave, c++/c (some 30 years in total). The Julia ecosystem is quite dynamic, and it is all very exciting, but Python just has this very large ecosystem and very clean coding, which makes it very attractive to use for little side experiments. This time I had to do some topic classification of (single sentence) text documents. Now there is an abundant choice of language technology tools in Python, and I believe that via [lda](https://pythonhosted.org/lda/index.html) I got to scikit-learn. Great tutorials, lovely datasets and all, but I found it very difficult to find out how to organize my own data so that I could load this in. Just now, I browsed through the user guide again to find the docs for ""load_files"", but I could't find an entry. So a google search for ""sklearn.datasets.load_files"" got me there just now, and I happened to remember the particular module path from more painstaking searches yesterday (it is mentioned somewhere in a tutorial). For me, the essential information would have been: ""Organize your data one document per file, one directory per class""---more or less what's under the documentation for [load_files](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html). This all makes perfect sense, but I come from a community where usual formats are ""one datapoint per line"", often with the class label on that line. But having said all this, I am pretty impressed how the Python (text) community has standardized data representation, from what I've seen so far. But perhaps because of the widely used standard data representation, this aspect has naturally less attention in documentation. As a final note, whenever I try to teach students how to use some scientific tool set or another, I have to spend quite some time on ""how to import your data"". Nobody likes to do it, it can be a lot of effort for what you potentially use only once, and is therefore always a difficult threshold.",1,0,msr
232,"@davidavdav I agree that loading data is a difficult and important thing. However, it is a domain-specific problem. You have a particular type of data. I have another. My data is medical images of brain activity. I can tell you how I organize my data and load them. I can even tell you that we have written a whole package about this, with its own documentation. But that will probably not help you.
What you want is something that tells you how to organize and load _your_ data. Now, it may be that your data is something fairly classic, that many people have; for instance tabular data most often stored in CSV files. In which case there is a need for a package doing this data loading. I don't believe that it should be in scikit-learn. It needs to be in a package that is specialized for this data. For instance, we are not going to put image processing in scikit-learn. For tabular data, the dedicated package is pandas; as I mentioned in my reply we need to point to it. We, the scikit-learn team, want to make plugin pandas into scikit-learn easier. But it is not as easy as it may seem and it takes time (one of our core devs is prototyping something).
I realize rereading your post that your data is most likely text documents. So my two examples of data (medical images and tabular data) were both wrong :$. Maybe the documentation on processing text with scikit-learn could indeed be improved and touch a bit on data organization. I don't know, I very seldom process text. But if you want to do add a few words on this, you are most welcomed to do a pull request. Anyhow, this illustrate my point about the diversity of the data: this whole thread is mostly about loading CSV files, as can be seen from earlier comments (before the thread exploded into a rant). The important thing is not the ""CSV"", which is the container, but the data model that underlies a CSV file. This data model is that of columns of data with different nature. It's a very different data model than processing text documents.
And finally, you are unhappy that teaching people ""how to import your data"" is time consuming. I don't think that there is an easy fix for this, even in a specific domain. The reason being that data meaning (ie data semantics) is still very much an open area. It's intrinsically hard to describe what the data means and how it's organized. You can try a simple experience: grab a dataset from someone you don't know, about an experiment you don't know, and try understanding it. Not even loading it, just understanding it. I am sure that it will take time. What takes a human time tends to be very difficult for a computer.",0,0,msr
233,Hm I don't think we added pointers to the FAQ yet. It's certainly a FAQ.,0,0,msr
234,I wrote a short tutorial on how to get the dataset from a text format to a pandas DataFrame for use by sklearn here http://cis399-he.tumblr.com/post/151024047044/load-your-data-to-scikit-learn,0,0,msr
235,fixed in #7516,0,0,msr
236,"Well you could add some information about common errors that happen when loading own data. For example I have bumped on `Unknown label type: 'continuous-multioutput'` and not a hint, anywhere, what that could mean. I've tried passing:
- a numpy array created from csv reader
- a numpy array created from pandas
- regular array parsed line by line
- pandas dataframe
- a numpy array created from csv reader converted to regular array
- etc...
I'd recommend adding section summarizing assumptions about data, for example it was sheer luck that I found information that data sets may not contain NaN's Nulls etc. Also, what should be the parameters of a numpy array and how should pandas dataframe look like.",0,0,msr
237,"Perhaps that error message could be clearer, but I think passing regression targets to a classifier (as in your #7801) is a usage error nothing to do with loading your own dataset.",0,0,msr
238,"@MartinLion @MartinLion Thank you to brought up this issue.
I am facing your problem now. I want to upload my IRIS like dataset. Which is 500 rows and 16 columns.
and
Thank you @chenhe95 ,
I have seen your tutorial titled ""Can I go to the bathroom"" , that is a real work , real help to users like us.
And Thank you all developers,
You worked free (or almost free) to deliver this library to people. I appreciate that. @MartinLion has spirit of learning, I praise that.
Actually stubbornness is a virtue in academic.",0,0,msr
239,"@tursunwali,
Thanks for the kind words.
Please let me know if you still have any issues with your data or learning
process.
Regards,
Martin
On 20 November 2016 at 12:47, tursunwali notifications@github.com wrote:
> @MartinLion https://github.com/MartinLion @MartinLion
> https://github.com/MartinLion
> > Thank you to brought up this issue.
> I am facing your problem now. I want to upload my IRIS like dataset. Which
> is 500 rows and 16 columns.
> and
> Thank you @chenhe95 https://github.com/chenhe95 ,
> I have seen your tutorial titled ""Can I go to the bathroom"" , that is a
> real work , real help to users like us.
> > And Thank you all developers,
> You worked free (or almost free) to deliver this library to people. I
> appreciate that.
> > @MartinLion https://github.com/MartinLion has spirit of learning, I
> praise that.
> Actually stubbornness is a virtue in academic.
> > —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-261758848,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AI_3_7E0UU8Os42HXUC5n2eLVYW6hqe6ks5q_9DngaJpZM4Cz90f
> .",0,0,msr
240,"When relying on vagrant to download a box I frequently see connection speeds like this:
```
default: Downloading: http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
default: Progress: 20% (Rate: 179k/s, Estimated time remaining: 0:41:37)
```
(Rate: **179k/s**)
Yet when I use wget to the same URL:
```
wget http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
--2015-02-10 09:52:12-- http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
Resolving boxes.example.com... 10.1.0.17
Connecting to boxes.example.com|10.1.0.17|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 830674320 (792M) [text/plain]
Saving to: 'packer_c6_2.5.2_virtualbox.box'
packer_c6_2.5.2_virtualbox.bo 0%[ ] 7.12M 696KB/s eta 19m 50s
```
(Rate: **696KB/s**) or often higher.
This particular example was pulled when on Wifi and connected to an IPSEC VPN.",1,0,msr
241,"Hi @spkane Some boxes are hosted on Atlas and sometimes Atlas is just acting as a proxy to a user-hosted box. If you give more information on the specific box(es) you're downloading, we can do some research.",0,0,msr
242,@sethvargo This box is actually a box I built using packer and it is hosted on a remote server. I'm trying to understand why the download in significantly slower using vagrant then using wget to the exact same URL.,0,0,msr
243,"@spkane sorry - I misread your original issue.
I would suspect (and maybe @mitchellh could elaborate more) a few things:
1. Ruby is slow and somehow throttling the subprocess
2. Wget is faster than curl (which is what Vagrant is using)
3. Vagrant is also allocating time to unpack the box
4. Wget is allowing for some type of compressed download
It would be helpful if you could benchmark this with curl for reference.",0,0,msr
244,"I really can't explain this. Vagrant doesn't do anything during the subprocess Ruby-wise: it subprocesses to `curl`. It doesn't even do the download in Ruby. Perhaps wget is using multiple connections to download multiple parts? I really don't know, but unless we get more information I have to assume that Vagrant is fine here. Is `curl` just as slow? Vagrant is just subprocessing to curl until it completes.",0,0,msr
245,"I'm experience the same slow experience. Anyone can try aria - http://aria2.sourceforge.net/ and http://stackoverflow.com/questions/3430810/wget-download-with-multiple-simultaneous-connections
It's seems a little bit faster, but, man, you can set this up using default vagrant download mechanism and take a walk or make yourself a sandwich. Get way from screen for a little bit.",0,0,msr
246,"Having the same problem here:
1. Upload a box manually to atlas
2. Create a new Vagrantfile with just `vm_cfg.vm.box_url = <user>/box-name`
3. `vagrant up` - box downloads slowly
4. wget box url from atlas (see `vagrant up` output) - box downloads lightening fast",0,0,msr
247,"I wish there was just a +1 for this. Me too. Same connection for all 3 attempts. VPN turned off.
- `vagrant up` took 25+ minutes.
- `wget` took 3 minutes.
- `curl` took 4 minutes.",0,0,msr
248,"Ubuntu vivid64 is downloading at ~56kbps. I'm on a 100mbit symmetric connection.
edit: it timed out before it could finish.
edit2: I can confirm that https://atlas.hashicorp.com/ubuntu/boxes/vivid64/versions/20160128.0.0/providers/virtualbox.box downloads dramatically faster over wget than via ""vagrant up"".",0,0,msr
249,"I'm trying to download the scotch/box and current download speeds using vagrant are less than 10kbps.
default: Progress: 0% (Rate: 2603/s, Estimated time remaining: 33:17:38)
However just as bad using wget.",0,0,msr
250,ditto; some popular boxes are very slow to download - i'm updating ubuntu/trusty64 as we speak and it's dropping below 1Kb/s. Been seeing this for a couple wks now.,0,0,msr
251,+1 -- exact same as last comment,0,0,msr
252,"Same here:
```
$ vagrant box add lazygray/heroku-cedar-14
==> box: Loading metadata for box 'lazygray/heroku-cedar-14'
box: URL: https://atlas.hashicorp.com/lazygray/heroku-cedar-14
==> box: Adding box 'lazygray/heroku-cedar-14' (v1.0.6) for provider: virtualbox
box: Downloading: https://atlas.hashicorp.com/lazygray/boxes/heroku-cedar-14/versions/1.0.6/providers/virtualbox.box
==> box: Box download is resuming from prior download progress
box: Progress: 3% (Rate: 281k/s, ...
```",0,0,msr
253,"same here
```
vagrant box update
==> default: Checking for updates to 'laravel/homestead'
default: Latest installed version: 0.4.1
default: Version constraints: >= 0
default: Provider: vmware_desktop
==> default: Updating 'laravel/homestead' with provider 'vmware_desktop' from version
==> default: '0.4.1' to '0.4.2'...
==> default: Loading metadata for box 'https://atlas.hashicorp.com/laravel/homestead'
==> default: Adding box 'laravel/homestead' (v0.4.2) for provider: vmware_desktop
default: Downloading: https://atlas.hashicorp.com/laravel/boxes/homestead/versions/0.4.2/providers/vmware_desktop.box
default: Progress: 0% (Rate: 42210/s, Estimated time remaining: 6:10:54))
```",0,0,msr
254,Is there any way to use something like axel to stream downloads in quicker?,0,0,msr
255,"I guess there's nothing preventing people from sharing boxes via torrent. For example, below is a magnet link for the heroku-cedar-14 box:
> magnet:?xt=urn:btih:5bb1480d5316f229bb71be55b56b06278de41a67&dn=heroku-cedar-14.box&tr=http%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=http%3A%2F%2Fannounce.torrentsmd.com%3A6969%2Fannounce&tr=http%3A%2F%2Fbt.careland.com.cn%3A6969%2Fannounce&tr=http%3A%2F%2Fexplodie.org%3A6969%2Fannounce&tr=http%3A%2F%2Fmgtracker.org%3A2710%2Fannounce&tr=http%3A%2F%2Ftracker.tfile.me%2Fannounce&tr=http%3A%2F%2Ftracker.torrenty.org%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.trackerfix.com%2Fannounce&tr=http%3A%2F%2Fwww.mvgroup.org%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.me%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2710%2Fannounce&tr=udp%3A%2F%2Fcoppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Fexodus.desync.com%3A6969%2Fannounce&tr=udp%3A%2F%2Fglotorrents.pw%3A6969%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.glotorrents.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker4.piratux.com%3A6969%2Fannounce
Anyone know a good website where one can search for torrents of vagrant boxes?",1,0,msr
256,"@wkretzsch - I personally don't know at the moment about any torrent sites - but for me I wouldn't want to trust torrent links as the source for my infrastructure testing. It's a possible option but security is also important. For me official vagrant boxes from folks like puppetlabs hosted on Atlas are so slow to download at times that I wish this issue could be resolved. For internal vagrant boxes that I build for my company we have the option to host on S3 or Artifactory or private Atlas org.
@mitchellh - yes - curl is just as slow (for me). I don't think it is a Vagrant issue - but a backed server hosting issue. Granted - not a Vagrant issue per se.",0,0,msr
257,"![screenshot from 2016-03-17 14-04-58](https://cloud.githubusercontent.com/assets/7142025/13838783/486408ba-ec49-11e5-9903-19cc1e031395.png)
Yes, this is because curl can only use one of my 3 connections at the same time. No, that's not the connection's rated speed. The rated speed is 45mbps. Yes, bittorrent does perform better. Just sayin-- your rationale for not supporting bittorrent is kinda thin here.",0,0,msr
258,@tehmaspc surely there must be a way for a website to publish the hash of their box along with a torrent link?,0,0,msr
259,"I wish in general, there was a way to have incremental images, like docker images, with vagrant boxes. For the provisioners, which bootstrap (cfengine, chef, salt, puppet, docker, etc) by downloading their platform, I wish there was a way to download a packaged up installer, so that other fresh images that use that provisioner, e.g. ubuntu + docker, would not need to download the goods again. Box updates and provisioner downloads were already painful, but recently, have been beyond notoriously slow.",0,0,msr
260,"Just went to update my box for the first time (trusty64 - noticed the warning on my vagrant up command output), and it's going to take my 1.5 hours on a 150MBps connection - pathetic. It's 2016 - I don't know the specifics of what's going on here, but surely we can fix this, like, by the end of next week? The tech that goes into modern technologies like vagrant is amazing, something this basic should be overcome in mere hours.",1,0,msr
261,"Amen, Matt, Amen. This is about UX.
There should be a recognition that line speed != line speed and practical
steps can be taken to overcome the daunting issue of line speed != line
speed.
Jacob Gadikian
E-mail: faddat@gmail.com
SKYPE: faddat
Phone/SMS: +84 167 789 6421
On Sun, Apr 10, 2016 at 6:32 AM, Matt Porter notifications@github.com
wrote:
> Just went to update my box for the first time (noticed the warning on my
> vagrant up command output), and it's going to take my 1.5 hours on a
> 150MBps connection - pathetic. It's 2016 - I don't know the specifics of
> what's going on here, but surely we can fix this, like, by the end of next
> week? The tech that goes into modern technologies like vagrant is amazing,
> something this basic should be overcome in mere hours.
> > —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/mitchellh/vagrant/issues/5319#issuecomment-207881297",0,0,msr
262,"I just tried asking Vagrant to download ubuntu/trusty64, and was getting speeds of <= 5 KiB/sec. I killed it and tried again using the exact same command, and got 29 MiB/sec. I think @mitchellh is correct in that this doesn't really seem like a Vagrant issue. If anything, it seems more like an Atlas issue (so possibly the ELB and/or whatever's sitting behind it). I highly doubt it has anything to do with the routes or hops between end-users and the ELB VIPs -- you wouldn't typically see such a polarizing set of speeds in that case, especially considering both VIPs terminate in us-east-1.
If for no other reason, it'd be highly desirable to see these made available through a CDN rather than a centrally-located ELB. Then again, I'm just one guy (who isn't paying for this service), so take that for what it's worth. Pretty thankful it's there either way.",0,0,msr
263,"It's not just an Atlas issue. I have boxes and metadata.json on S3, with a Fastly CDN in front and regularly have the exact same issue: sometimes vagrant downloads at 100kbps and sometimes it downloads at > 5mbps. You can cancel a slow download and half the time a retry gets you the faster speeds.",0,0,msr
264,"I contacted support about this around the same time I chimed in here initially. Their response is that Vagrant uses curl to download things so they don't see this as a Vagrant problem. IMO that's an unprofessional cop-out because they chose to use curl, know that there are problems and aren't considering swapping out with an alternative to eliminate the problem for their users.",0,1,msr
265,"I can confirm that this is still an issue. All my peers also report times of >1h, while the connection here for other connections is around 200MB/s.
```
vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'ubuntu/trusty32' could not be found. Attempting to find and install...
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'ubuntu/trusty32'
default: URL: https://atlas.hashicorp.com/ubuntu/trusty32
==> default: Adding box 'ubuntu/trusty32' (v20160406.0.0) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty32/versions/20160406.0.0/providers/virtualbox.box
default: Progress: 11% (Rate: 43801/s, Estimated time remaining: 1:36:50)
```",0,0,msr
266,"While I am unsure of the origin of the problem, I really do wish that Hashicorp would get back to its unrelenting focus on user experience with this one. **Muli-hour downloads (that should take 1-10 minutes)==bad ux.**",0,1,msr
267,"Currently downloading an image for the 5th time (@13Xk/s, even with `wget`). Keep disconnecting me while around 50-90%. But it ALWAYS downloads at full speed either early morning / late night EST. Assuming it is a traffic issue, but regardless very bad UX.
```
box: Progress: 47% (Rate: 106k/s, Estimated time remaining: 0:14:50)
```",0,0,msr
268,I have been trying for 2 day's now and still can not get it to download... its a shame.. it is really not impressing new comers to laravel .. i can only get 34ks speed.........,1,1,msr
269,"Speeds ok from the UK:
```
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'bento/centos-7.2' could not be found. Attempting to find and install...
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'bento/centos-7.2'
default: URL: https://atlas.hashicorp.com/bento/centos-7.2
==> default: Adding box 'bento/centos-7.2' (v2.2.6) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/bento/boxes/centos-7.2/versions/2.2.6/providers/virtualbox.box
default: Progress: 11% (Rate: 7728k/s, Estimated time remaining: 0:01:19)
```
What is your location?",0,0,msr
270,"Also, https://atlas.hashicorp.com/ URL's are delivered from Amazon Web Services (atlas-frontend-atlas-230110478.us-east-1.elb.amazonaws.com) so I doubt they are tight for bandwidth ;-)
Are the slow downloads being made from locations a long distance away from the AWS us-east-1 DC, perhaps thats the root cause of the issue?
Maybe the AWS CDN could be used to cache files around the world?",0,0,msr
271,"I'm located in Vermont, which is pretty us-east-1 last I checked :dart:",0,0,msr
272,I am in Brazil.... got it to download.... 10m connection here took 4.6 hours!!!!! my wife just gave birth to our 8th little girl.. It only took her 40 minutes !!!!! lol There is a big problem with there download!!!!!,1,0,msr
273,"Same here, I have a 50Mbps connection...
`
default: Progress: 44% (Rate: 102k/s, Estimated time remaining: 0:15:01))
`",0,0,msr
274,"Sign me upp here, 100mb symmetric connection (Fiber) sloooow as shit, doing 150kb/s",1,1,msr
275,"after looking at the years of complaints of slow download with no effort of resolving the issue,,, i think its time to start emailing Laravel to stop endorsing homestead until the issue is resolved..... maybe that will get their attention!!!! this is a real problem... 15 retries and then 4.6 hours to download a file is irresponsible on their part........",1,1,msr
276,"I bet it gets closed, but if you don't ask you don't get:
https://github.com/mitchellh/vagrant/issues/7307",1,0,msr
277,Just snagged a box at 85mbit. You all fix something recently? Much better than it used to be.,0,0,msr
278,"This is painful to do anything on any more - On 100mbps synchronous connection and getting 168kb, either overloaded servers or throttling",1,1,msr
279,"In looking to debug curl being slow -- I found a [stackoverflow post ](http://stackoverflow.com/questions/30984641/debugging-slow-download-with-curl) that suggests that --trace-ascii /dev/null makes your curl go at the speed you'd expect. For me, I'm trying to download [CentosOS 7](http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box) and here are my results:
NO --trace-ascii option:
```
$ curl http://cloud.centos.org/centos/nt/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
0 483M 0 489k 0 0 77724 0 1:48:44 0:00:06 1:48:38 69721
```
With trace-ascii the first time:
```
$ curl http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box --trace-ascii /dev/null
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
5 483M 5 24.5M 0 0 4259k 0 0:01:56 0:00:05 0:01:51 4599k
```
Does anyone else see the same behavior?",0,0,msr
280,"The download is extremely slow on my end too. I'm trying vagrant for the very first time. Might ditch this software and go back to my native apache2 instead.
<img width=""913"" alt=""screen shot 2016-08-17 at 12 31 40 pm"" src=""https://cloud.githubusercontent.com/assets/4960876/17724673/926a8510-6476-11e6-82ab-fb7276c448c4.png"">",1,0,msr
281,Help. I have same problem. I can't wait 3 hours! Very slow! Stupid!,1,1,msr
282,i gave up a year ago..download to slow.. problems after down load.. have to download for 3 hours again.... Vagrant will not fix the problem that has been there for several years now.. you would think that after 3 or 4 years of this problem they would address the issue.....,1,1,msr
283,8 hours to download! I hate you all!,1,1,msr
284,"Lol, I hate you too @daryn-k :)",1,1,msr
285,Guys why is this issue closed? This is still an outstanding issue and needs to be addressed ASAP. I am experiencing the same issue.,0,1,msr
286,Wow! Downloading boxes is painful please fix this. PLEASE?,1,1,msr
287,"I think it really has to do with time of day, traffic, alignment of the planets, etc. I haven't had slow speeds in a while. It seems very hit or miss. In fact, if it is slow you and start and stop it with the chance of getting a better connection. I'm not sure this is really the fault of the vagrant framework as much as it is the nature of large bottlenecked downloads.",0,0,msr
288,"@mitchellh These errant speed symptoms from hashicorp's servers could be indicative of bumping up against AWS's IOPS credits for GP2 filesystems. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#IOcredit
We had some testing infrastructure on drupal.org that would run fine for a long time, then suddenly drop to a crawl because we had ""spent"" all of our IO credits. It could be possible that hashicorps' servers are bumping up against the same limit. `sar -b` could give some insight as to whether or not this explains the random performance drops.",0,0,msr
289,It could be an idea for Hashicorp to move the images for download into S3 and use that for downloads... that would save on running instances specifically for downloads.,0,0,msr
290,"Downloading boxes used to be quick, now it's so slow it makes vagrant a no-go for quick and simple developer environments.",1,1,msr
291,"Trying to download ubuntu/xenial64. Download speed maxes out at 150 KB/s on a 1 Gbps symmetrical fiber connection. WTF. Remaining time 1 hour? I could probably download the ISO, read the guide on how to set up my own box, and finish earlier.
EDIT: Interestingly, speed went up by factor 10 when I tried to download the same box in the browser simultaneously.",1,1,msr
292,">use latest devops tools to speed things up
>spend days watching max 420k/s download speeds",1,0,msr
293,"Same here, I have a 30Mbps connection
> default: Adding box 'ubuntu/trusty64' (v20170313.0.5) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty64/versions/20170313.0.5/providers/virtualbox.box
default: Box download is resuming from prior download progress
default: Progress: 0% (Rate: 80568/s, Estimated time remaining: 1:26:22)",0,0,msr
294,"@DeadlySystem We have the same experience, when I download the same box (url) using `curl` from the commandline (during the `vagrant up`)",1,0,msr
295,"Any update on this, fetching box from Hashicorp is painfully slow.
![screenshot_2017-04-02_21-08-15](https://cloud.githubusercontent.com/assets/1684989/24587412/8f0329d8-17e8-11e7-9743-f793458b7daf.png)",0,0,msr
296,"Hey, quick thought:
If this uses curl (not libcurl) through some sort of ruby-controlled, bash-mediated process, why not just remove curl for one of:
* ipfs
* aria2
Both would do the job better than curl.",0,0,msr
297,"It honestly looks like they dont give a shit, rules this out as an option
for me!
On 3 Apr 2017 8:15 AM, ""Jacob Gadikian"" <notifications@github.com> wrote:
> Hey, quick thought:
>
> If this uses curl (not libcurl) through some sort of ruby-controlled,
> bash-mediated process, why not just remove curl for one of:
>
> - ipfs
> - aria2
>
> Both would do the job better than curl.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mitchellh/vagrant/issues/5319#issuecomment-291019093>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABBAihR9ng4t2Jq1XTmAjMyMCnlEFtxRks5rsB4WgaJpZM4Deq5d>
> .
>",1,1,msr
298,"I'm on a 150Mbps line.
vagrant up = HOURS
vagrant box add = HOURS
browser download /wget = HOURS
May not be a vagrant issue per se, BUT IT IS. If your infrastructure can't handle it then your product is broken.
BAD UX",1,1,msr
299,"I opened a ticket about customizing the download tool, but it got rejected as too complicated to impliment ;-(
That said, my recent download speeds have been ok from the UK for a while. This example downloaded just now:
```
default: Box Provider: virtualbox
default: Box Version: >= 0
==> default: Loading metadata for box 'bento/ubuntu-16.04'
default: URL: https://atlas.hashicorp.com/bento/ubuntu-16.04
==> default: Adding box 'bento/ubuntu-16.04' (v2.3.4) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/bento/boxes/ubuntu-16.04/versions/2.3.4/providers/virtualbox.box
default: Progress: 12% (Rate: 8940k/s, Estimated time remaining: 0:01:06)
```
Maybe you guys are just too far from their AWS instances for a good download speed?
All their download servers look to be in New York with no CDN to distribute content.
```
$ dig atlas.hashicorp.com
; <<>> DiG 9.10.3-P4-Ubuntu <<>> atlas.hashicorp.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 44169
;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1
;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4000
;; QUESTION SECTION:
;atlas.hashicorp.com. IN A
;; ANSWER SECTION:
atlas.hashicorp.com. 120 IN CNAME atlas.hashi.co.
atlas.hashi.co. 60 IN A 52.206.86.0
atlas.hashi.co. 60 IN A 52.200.255.5
atlas.hashi.co. 60 IN A 52.55.203.197
;; Query time: 42 msec
;; SERVER: 10.1.1.14#53(10.1.1.14)
;; WHEN: Mon Apr 24 09:20:24 DST 2017
;; MSG SIZE rcvd: 124
```
http://geoiplookup.net/ip/52.55.203.197
http://geoiplookup.net/ip/52.206.86.0
http://geoiplookup.net/ip/52.200.255.5
Maybe its worth the people getting slow downloads detailing what ISP they are using? Maybe you are all on an ISP with a high contention ratio?",1,0,msr
300,https://github.com/mitchellh/vagrant/issues/8434#issuecomment-291979521,1,0,msr
301,"> Maybe its worth the people getting slow downloads detailing what ISP they are using?
Any ISP, in any state I've traveled to in the last couple years.
This problem is squarely on whatever Hashicorp is doing for hosting, and that's where it needs to be fixed. If they're unable or unwilling to fix it, a torrent solution would certainly help without taking their resources aside from development for adding a torrent client to the tool. If all this stuff is on S3 anyway, AWS provides torrent seeding out of the box.",1,0,msr
302,"I was getting painfully slow speeds and so I decided to see if upgrading Vagrant would change anything. Before I updated I was getting speeds of around 50-200kbps. After the update I was using the full 70mbps of my connection.
So for those of you that have slow speeds, try updating to v1.9.4 if you aren't already running it.",0,0,msr
303,"Hi, I'm on the latest Vagrant 1.9.4 and the download speed is so slow that keeps disconnect. No way to download latest laravel homestead 2.1.0 box...
When I downloaded the 2.0 with Vagrant 1.9.3 (or previous, can't remember) it was flawless",0,0,msr
304,"same issue here, very slow download rates
Rate: 35033/s (keeps jumping between 20000 and 60000...)
Estimated time remaining: 9:26:13 :(
(on vagrant 1.9.5)",1,0,msr
305,Now it's fast again... really depends on time of day and luck (as stated above...).,0,0,msr
306,"I'm having this same problem when trying to download Bento boxes. I'm using Vagrant 1.9.5 with 5.1.22. I've tried several times, during various days. My normal download speed is 1.7mb~ but when dowloading boxes I can't get pass 40kb.
If I try downloading with wget I get the same low speed.",0,0,msr
307,"Maybe the move will help:
https://www.vagrantup.com/docs/vagrant-cloud/vagrant-cloud-migration.html",1,0,msr
308,"Gonna try again tonight to see if the migration helped. Anyway I noted that with a different ISP the download rates were fine, so I guess it's not a problem with Vagrant itself (like many users already reported).",0,0,msr
309,"Well, nothing has changed, it still downloads at a snail's pace given that i am on a 125 Mbps connection!",1,1,msr
310,Sooooooooooooooo slowly！！！！:(,1,1,msr
311,"create a new issue, this has been closed for ages",1,0,msr
312,"i just downloaded latest scotch-box vagrant box, spent ~1hour for that, and then i found an issue which could be fixed only by downgrading to previous version of box, so i'm here again downloading second box for next hour of my working time. So.. it's good since i got per-time payment, but on the other hand probably i'll have some problems explaining why i spen last day downloading stuff from the internet",1,0,msr
313,I'm also facing the slow download issue.,0,0,msr
314,"Try downloading from a wired connection, it's exponentially faster!
On 4 September 2017 at 17:02, Omar Tariq <notifications@github.com> wrote:
> I'm also facing the slow download issue.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mitchellh/vagrant/issues/5319#issuecomment-326939886>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABmbHxWHtAnCnEw34FB4oXX8lTQrOsNRks5se9-_gaJpZM4Deq5d>
> .
>
-- Best regards,
Shivaprabhu Wali.",0,0,msr
315,"Typical issue, been around for ages, everybody's moaning about it, and nothing is being done.",1,1,msr
316,"Came here via Google and this might help some people:
When I run `vagrant init [box] && vagrant up`, it loads super slow.
But when I run `vagrant box add [box]`, it's faster.
Going from 120 kb/s to >1000 kb/s.",1,0,msr
317,@PascalAOMS Thanks for the tip. At least it worked for me.,0,0,msr
318,"Today I've tried [aria2](https://aria2.github.io/) with 16 simultaneous connections to download laravel/homestead managed to get up to 9.6MiB
```
Download Results:
gid |stat|avg speed |path/URI
======+====+===========+=======================================================
80fa38|ERR | 7.8MiB/s|C:/development/homestead/47dce273-9892-4691-a746-c4f351ae44a5
```",1,0,msr
319,From last two days i'm trying to download vigrant box but every time i try to download it gives me error and downloading spedd is very slow. I also tried to install it manually but it also didn't worked.For newbie to Laravel it pretty frustrating and i'm still not able to download it.Don't know what should i do now.,1,0,msr
320,"I'm seeing the same problem. Running latest Vagrant 2.0.0. When downloading a box (configured using external URL on Vagrant Cloud, so it's actually redirecting to Rackspace Cloud Files) it's painfully slow. If I use cURL or Chrome to download the same URL (https://vagrantcloud.com/joomlatools/boxes/box/versions/1.5.0/providers/virtualbox.box) from the same machine it only takes a couple of seconds to complete the download. What could be the issue here?",0,0,msr
321,"Using terminal to add a box is extremely slow, more than 1 hour with 100kb/s. If I use a browser to directly download the box it is much faster, just a few minutes with 600kb/s. What's the reason for having such a huge difference? Seems as adding the box through terminal won't use all the available bandwidth.",1,0,msr
322,"I am trying to download boxes more quickly, and following the advice of some of the above comments, I used [aria2](https://aria2.github.io/) through [homebrew](http://brewformulas.org/Aria2) to download much faster in parallel. Sample command (url was from original box add attempt from Vagrant):
$ aria2c -x16 https://vagrantcloud.com/ubuntu/boxes/xenial64/versions/20171118.0.0/providers/virtualbox.box
My speed was around 100k/s with Vagrant's download, up to 1 MB/s with aria2.
Then when you finish downloading that, you [can add the box using](https://stackoverflow.com/a/26655618/532524):
$ vagrant box add ubuntu/xenial64 ../xenial64.box
I think you can remove the downloaded box after this since it will be copied to [the standard box storage path](https://stackoverflow.com/a/10226134/532524) after adding it. Hope this helps save someone else some time.",0,0,msr
323,"@panozzaj's comment above doesn't quite work if a `Vagrantfile` expects a specific version of a box (that command will add a box without a version). Instead, you can do the below.
# Steps to get a specific box at a specific version without using `vagrant` to perform the download
I'm using laravel/homestead in this example because it's what I was trying to get.
1. Download the box using a better download client (e.g. your browser, curl, wget, whatever).
2. Create a new json file (anywhere), add this to it (note, tweak the `name`, `version` and `url` keys to match the box you want, don't worry about the `checksum` key yet. For `url`, use the path to the file you just downloaded, example below):
```
{
""name"": ""laravel/homestead"",
""description"": ""Whatever you want"",
""versions"": [{
""version"": ""4.0.0"",
""providers"": [{
""name"": ""virtualbox"",
""url"": ""file://c:/users/madmatt/Downloads/47dce273-9892-4691-a746-c4f351ae44a5"",
""checksum_type"": ""sha1"",
""checksum"": ""abc123""
}]
}]
}
```
3. In your `Vagrantfile`, add the following lines `vm.box` and `vm.box_url` keys):
```
Vagrant.configure(""2"") do |config|
config.vm.box = ""laravel/homestead""
config.vm.box_url = ""file://c:/users/madmatt/path/to/the/json-file-you-created-in-step-2.json""
end
```
4. Run `vagrant up` as normal.
5. `vagrant` will complain that the sha hash doesn't match (`The checksum of the downloaded box did not match...`). Take the string that appears next to 'Expected', copy and paste that into the `checksum` key of the json file you created in step 2.
6. Run `vagrant up` again, this time it should load from the local file, store it as the correct name and version, and successfully run it.
Hopefully someone finds these steps useful... funnily enough I have done all this research (having never used vagrant before), have tested it a bunch of times, and the original `vagrant box add laravel/homestead` command that I started running 3 hours ago is still only 8% complete, even though in that time I've downloaded the box file 8 times outside of vagrant.
The rest of this is just my experience, no need to read further ;)
# My experience (aka. why is vagrant so slow to download boxes)
Was getting 420b/sec (yes, that's bytes per second) on a gigabit connection downloading https://vagrantcloud.com/laravel/boxes/homestead/versions/4.0.0/providers/virtualbox.box.
I downloaded the same file via browser, curl and wget with speeds varying between 12 and 27MB/sec. I then tried doing both at the same time - I was able to download via Chrome, Firefox, curl and wget before ```vagrant box add laravel/homestead``` had downloaded 1%.
The URL I got in the browser was ```https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/47dce273-9892-4691-a746-c4f351ae44a5?X-Amz-Algorithm=<snip>&X-Amz-Credential=<snip>&X-Amz-Date=<snip>&X-Amz-Expires=<snip>&X-Amz-SignedHeaders=<snip>&X-Amz-Signature=<snip>```
I don't know what the problem here is, but I can think of a couple:
* Whatever UA vagrant uses is bad, and AWS severely limits it
* Whatever vagrant uses to download isn't following redirects, or somehow never ends up downloading from AWS infrastructure, instead using some other terribly overloaded server/proxy",0,0,msr
324,how is this closed? still a major issue,1,1,msr
325,"Perhaps it's time to fork. This project is core to a lot of development environments, leaving us all subject to the whims of HashiCorp... and on this issue we seemingly cannot even get a reasonable official response.
Vagrant is MIT-licensed. Boxes could easily be distributed via torrent, or we can even just specify URLs in our Vagrantfile. We don't need the Vagrant Cloud dependency if some basic enhancements around box handling are made. Are there any maintained forks already in existence?
Any thoughts from others?",1,0,msr
326,Definitely still an issue!,1,0,msr
327,"Are there any trusted torrent links available? I am trying to download bentoo/ubuntu16.04 and i am getting 12 kbps and upto 15kbps when i try to do wget, i am sure that my connection is fine!",1,0,msr
328,"Still an issue here, have to update my box at ~150k/s, not able to do a lot of useful work in the meanwhile..",1,0,msr
329,Still an issue.,1,0,msr
330,@bradisbell The real solution is probably a docker-based development environment. Just waiting for that ecosystem to settle down .,1,0,msr
331,🍺 🍺 🍺 🍺 🍺 🍺 I'm just going to leave these here for any poor sod waiting for this download.,1,1,msr
332,"I found a root cause and a solution for my situation. I increased my download speed 100x:
Root cause: Slow speed of `ubuntu.com` cloud box servers
Solution: Switch to another trusted source for Ubuntu boxes, Puppet Labs.
I've been using `ubuntu/xenial64` for developing apps that must deploy to Ubuntu 16. (Heroku) And download speeds have become ridiculously slow, or failed entirely. I used wget and watched it follow the redirects and saw that it's actually hitting `ubuntu.com`. I tried several ways to download from Ubuntu, but they all resulted in speeds around 50 KB/s. So I went back to the Vagrant box listing to see who else might have a Xenial 64 box, and if I'd trust them. Turns out Puppet does, and I trust them as much as (or even more than) Ubuntu. My `Vagrantfile` now has the config line:
```
config.vm.box = ""puppetlabs/ubuntu-16.04-64-nocm""
```
And it downloaded in just a few seconds, as opposed to possibly an hour or never.",0,0,msr
333,@dogweather Thanks for this! My download time estimate went from 3 hours to 3 minutes when I switched to a box from the list at https://app.vagrantup.com/puppetlabs/,0,0,msr
334,"@tristanmason You're welcome! Ironically: In the past week I switched to Docker Compose, and it's **AMAZING**. I can't believe I've waited this long to use it. I used this [Quickstart for Rails](https://docs.docker.com/compose/rails/) doc.
* Configuration is 1/10 the size and complexity of Vagrant for my typical Rails app
* Build and Boot times are much faster
* On my Linux desktop in particular, there is a 10x speed increase vs. running on Mac.
* There's a huge reduction in RAM required. * I'm going to save a ton of money on development computers. I.e., 8GB RAM is plenty for a Docker-based dev machine. (Not so good for Vagrant+VirtualBox though.) And a top-tier i7 isn't necessary anymore either to get good speeds.
I was able to setup a Docker-based dev environment on four computers in a fraction of the time it takes me to create a single Vagrant setup from scratch.
I'm going to write up a blog with detailed metrics - I'll post it here.",0,0,msr
335,"@dogweather Vagrant and Docker seems similar but they are different things. Also, you can use a Docker provider within Vagrant.",1,0,msr
336,"@oncet Yep, very different! But for my use case, they're equivalent and commodified: Ways I can create and launch an isolated dev environment with just one or two commands. Docker Compose works great and is far simpler than Vagrant (note, this not just ""Docker"" per se, which only launches containers one by one) and I'd need to hear about a compelling reason to try a Vagrant+Docker solution, which sounds pretty damn complex. ;-)",1,1,msr
337,Pain. This is a pain. I'll never be able to download the 5.0.1 I guess,1,1,msr
338,"On a 100mbps connection in Manila, with Vagrant 2.0.1, I was trying to download:
https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/20180110.0.0/providers/virtualbox.box
...and I was getting speeds of anywhere from 1k-150k via `vagrant up` or even `wget`. Total download time: **12 hours!**
Then I did one thing: **I VPN'd to California.**
Suddenly, my download took only 3 mins. So that's something worth trying possibly.",1,0,msr
339,"Same here, I thought this is a personal problem lol.
Even update box still get really poor download speed, hashicorp please fix :(
```
==> default: Updating 'ubuntu/xenial64' with provider 'virtualbox' from version
==> default: '20171221.0.0' to '20180122.0.0'...
==> default: Loading metadata for box 'https://vagrantcloud.com/ubuntu/xenial64'
==> default: Adding box 'ubuntu/xenial64' (v20180122.0.0) for provider: virtualbox
default: Downloading: https://vagrantcloud.com/ubuntu/boxes/xenial64/versions/20180122.0.0/providers/virtualbox.box
==> default: Box download is resuming from prior download progress
default: Progress: 13% (Rate: 52999/s, Estimated time remaining: 0:31:46)
```",0,0,msr
340,"Ignoring such persistent issue means something on Hashicorp's end, I believe.",1,0,msr
341,"@dqlopez from the previous comments, this is a solved issue: it's purely the speed of ubuntu's servers. The solution is to use some other org's images.",1,0,msr
342,"Confirming that when I used `bento/ubuntu-16.04` instead of `ubuntu/xenial64` ([why?](https://github.com/hashicorp/vagrant/issues/6616#issuecomment-227776489)), I got pretty good download speeds. The box downloaded in less than a minute, on a 300 Mbps fibre optic connection. (Unfortunately I didn't think to copy and paste the log.)",1,0,msr
343,"I promised I'd report back on my transition to Docker, so here goes. I spent a ton of time creating a config that delivers the one-liner `vagrant up` experience. It's `docker-compose up`. And there's only one dependency, Docker. Not two (VM _plus_ Vagrant).
My post about performance of a Docker dev environment on Mac and Linux: [medium.com](https://medium.com/@dogweather/dev-environment-performance-tests-docker-vs-native-mac-vs-linux-old-vs-new-883399d05182)
Repos with my configuration, tailored for [Ruby on Rails](https://github.com/dogweather/rails-docker-compose) and [Phoenix](https://github.com/dogweather/phoenix-docker-compose) development. Very very similar. Can be tailored for any language, I'd imagine.
cc: @fredngo",1,0,msr
344,"@dogweather Frankly, I don't see how your commentary on docker has anything to do with the issue at hand. Please stop taking this thread off-topic.",1,1,msr
345,"Docker is not an alternative to Vagrant. They work completely differently and are intended for different environments. While I'm all for a discussion on the merits of when each should be used, this thread is not really the place for it.
Edit: I love both and use them both in development and production environments.",1,0,msr
346,"Haven't read the full thread, but it was a significant difference between using PowerShell and Git Bash.",1,0,msr
347,"Well, I cant download a single box tonite. I'm in the UK.
```
The box 'puppetlabs/ubuntu-16.04-64-puppet' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: [""https://atlas.hashicorp.com/puppetlabs/ubuntu-16.04-64-puppet""]
Error: The requested URL returned error: 404 Not Found
```
> config.vm.box = ""puppetlabs/ubuntu-16.04-64-nocm""
> config.vm.box = ""bento/ubuntu-16.04""
> config.vm.box = ""ubuntu/xenial64""
> none of those work.
BUT if you download the box manually via wget, all works fine...
```
nick@TX200-S5:~/workspaces/elk-vagrant$ wget https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
--2018-03-04 00:30:45-- https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 50.17.237.77, 54.221.226.80, 54.243.175.62, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|50.17.237.77|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box [following]
--2018-03-04 00:30:46-- https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box
Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.32.82
Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.32.82|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 666915336 (636M) [application/vnd.previewsystems.box]
Saving to: ‘virtualbox.box’
100%[==========================================================================================>] 666,915,336 2.23MB/s in 4m 48s 2018-03-04 00:35:35 (2.21 MB/s) - ‘virtualbox.box’ saved [666915336/666915336]
nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add bento/ubuntu-16.04
The box 'bento/ubuntu-16.04' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: [""https://atlas.hashicorp.com/bento/ubuntu-16.04""]
Error: The requested URL returned error: 404 Not Found
nick@TX200-S5:~/workspaces/elk-vagrant$ wget -O bento-ubuntu-16.04 https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
--2018-03-04 00:37:41-- https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 54.243.252.123, 50.19.252.69, 54.243.137.45, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|54.243.252.123|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA [following]
--2018-03-04 00:37:41-- https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA
Resolving archivist.vagrantup.com (archivist.vagrantup.com)... 50.16.237.173, 23.21.92.233, 54.221.212.171, ...
Connecting to archivist.vagrantup.com (archivist.vagrantup.com)|50.16.237.173|:443... connected.
HTTP request sent, awaiting response... 307 Temporary Redirect
Location: https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7 [following]
--2018-03-04 00:37:42-- https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7
Resolving vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)... 52.216.164.43
Connecting to vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)|52.216.164.43|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 428203828 (408M) [binary/octet-stream]
Saving to: ‘bento-ubuntu-16.04’
100%[==========================================================================================>] 428,203,828 2.09MB/s in 3m 6s 2018-03-04 00:40:48 (2.19 MB/s) - ‘bento-ubuntu-16.04’ saved [428203828/428203828]
```
A direct box add works too
```
nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add ""bento/ubuntu-16.04"" https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
==> box: Box file was not detected as metadata. Adding it directly...
==> box: Adding box 'bento/ubuntu-16.04' (v0) for provider: box: Downloading: https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
box: Progress: 7% (Rate: 2247k/s, Estimated time remaining: 0:03:13)
```",0,0,msr
348,"I would like to know how can I manually add the .box file in an offline manner. It is not clear in the documents on how to do that. I think the correct place for modification is Vagrantfile. However, I can not figure out which parameter should I change.",0,0,msr
349,@mahmoodn Your question is totally unrelated to this thread. The [box documentation](https://www.vagrantup.com/docs/boxes.html) should help you with your question.,0,0,msr
350,"```
https://vagrantcloud.com/laravel/boxes/homestead/versions/5.2.0/providers/virtualbox.box
homestead-7: Progress: 36% (Rate: 95492/s, Estimated time remaining: 0:54:16)
```
99% of the time, this is what I'm dealing with. On a stable fibre connection. No WiFi. Every now and again I get a short burst of speed that pushes progress up around 5%, but overall, I'm left waiting several hours. This is insane. People have proposed work arounds and mitigations but those aren't solutions. When this issue was first closed, it was implied that `curl` might be to blame. Perhaps this is true, and if it is, then it should be switched out for a different solution.",0,0,msr
351,"I suffer the same problem also My connection on speedtest.net 6.6Mbps while the download takes only 0.6 Mbps of my bandwidth ![image](https://user-images.githubusercontent.com/33742499/38153904-5e74252a-346f-11e8-9623-e891b6bc3466.png)
![image](https://user-images.githubusercontent.com/33742499/38153877-41067678-346f-11e8-9a15-3ef259286fa9.png)",0,0,msr
352,"Hi everyone,
This issue overall is very complicated in that it is not easily reproducible and is very dependent on location, network health, and the actual download location of the target box. In many cases the box being downloaded is located on a third party system. One feature that was introduced in Vagrant a couple releases ago was the notification of redirect to show when boxes are being downloaded from a third party server.
There have also been claims that the embedded curl is downloading files slower than the system curl. I have not been successful in validating this claim and do not see a difference in download speed between my system curl and the embedded curl. However, as of Vagrant 2.0.4, the [`VAGRANT_PREFER_SYSTEM_BINS`](https://www.vagrantup.com/docs/other/environmental-variables.html#vagrant_prefer_system_bin) environment variable is available for all platforms. When enabled, Vagrant will use local system binaries if available instead of the embedded binaries. If you observe faster download speeds with your local curl binary, this provides a switch to easily enable Vagrant to use the local binary.
My apologies for the frustration this issue has caused. It is something I have investigated multiple times, but the number of variables involved makes this extremely difficult to get a valid reproduction. I have updated Vagrant's functionality to make the underlying cause more easily understandable where available (redirect notifications) or easier to work around. I am continuing to investigate our options for box downloads from Vagrant Cloud in different regions of the world (which have occasionally presented with issues). However, with box downloads from third party locations, there is little I am able to do to control their available bandwidth or any kind of throttling they may impose.
Cheers!",0,0,msr
353,"Code:
```
$ cat typetest.rs struct Foo<Elem, List:AsSlice<Elem>> {
elems: List,
}
fn main() {
}
```
Compile:
```
$ rustc typetest.rs typetest.rs:1:12: 1:16 error: parameter `Elem` is never used
typetest.rs:1:16: 1:16 help: consider removing `Elem` or using a marker such as `core::marker::PhantomData`
error: aborting due to previous error
```
I'm pretty new to rust, so forgive me if I've missed something. But I can't see a way to say ""my struct contains a type T2, which implements `SomeGenericTrait<T>`"" without my struct also being parameterized over T, as I'm doing here. But rust thinks that I'm not using T.
I tried adding:
```
dummy: Elem,
```
to my struct def, which makes it compile. So it doesn't look like this is just one error message obscuring another, rust does really think this is the only thing wrong with my code.
---
```
$ rustc --version --verbose
rustc 1.0.0-dev (2fc8b1e7c 2015-03-07) (built 2015-03-08)
binary: rustc
commit-hash: 2fc8b1e7c4ca741e59b144c331d69bf189759452
commit-date: 2015-03-07
build-date: 2015-03-08
host: x86_64-unknown-linux-gnu
release: 1.0.0-dev
```",0,0,msr
354,"This is actually intended behavior due to a [recently merged RFC](https://github.com/rust-lang/rfcs/blob/master/text/0738-variance.md). The idea here is that the compiler needs to know what constraints it can put on the type parameter `Elem`, and usage of the [`PhantomData`](http://doc.rust-lang.org/std/marker/struct.PhantomData.html) type will instruct the compiler how it can do so.
You can check out the [examples](http://doc.rust-lang.org/std/marker/struct.PhantomData.html#examples) for `PhantomData` to see some usage, and in this case you'll probably want to do something along the lines of:
1. Refactor the struct so unused type parameters aren't needed. This can often be done with associated types, something along the lines of:
``` rust
trait A { type B; }
struct C<T: A> {
inner: A::B,
}
```
2. Use `PhantomData<Elem>` to indicate that the compiler should just consider the type parameter used, considering `Foo<A, B>` as containing something of type `A` as well as `B`.
You probably don't want to store `dummy: Elem` as it will require you to store an extra element and could have other memory implications as well.",0,0,msr
355,"Thanks for the helpful explanation, @alexcrichton. You also conveniently educated me on associated types, which I was wondering about but didn't know what to search for (I was wondering ""why is `Iterator` not generic""). It turns out this is just what I needed.",0,0,msr
356,"I just stumbled upon this limitation in a context where `PhantomData` is of no help: generic enums. Consider the following piece of code:
```
enum Foo<A, B>
where A: Bar<B> + Sized
{
X(A),
Y,
Z
}
trait Bar<B> {
fn bar(&self, &B);
}
```
Edit: I'm aware of several workarounds. These include adding `PhantomData` to `X`, but this pollutes the enum (in the sense that construction of a value by the user becomes iffy).",0,0,msr
357,"Hi, I had a similar problem regarding to unused type parameter.
Here is the simplified code, I need to control the visibility for quicksort's sort function. ```rust
struct Quicksort<T>; // <---- unused parameter warning
trait Sorter<T> {
fn sort(&self,ar<T>){...}
fn join(...);
fn split(...);
}
impl<T> Quicksort<T> {
pub fn sort(&self,ar<T>){
Sorter::sort(self, ar<T>) ;
}
}
impl<T> Sorter<T> for Quicksort<T>{
fn join(...) {...}
fn split(...) {...}
}
```
Any help to resolve the problem? because struct Quicksort does not have any fields. The type parameter is solely for the sort function from trait.
Otherwise if I removed `impl<T> Quicksort<T>`, then I don't need the T for Quicksort at all.",0,0,msr
358,"For those who will look here for answers:
""In most cases you do not need additional type parameters at all"".
That's it. Just remove them.
Taking @Piping example for illustration.
Let's get rid of type parameter for `Quicksort`.
```rust
struct Quicksort;
```
Now `rustc` obviously won't complain about unused parameters since there are none.
But at some point we actually need that type-parameter.
Continuing. Type parameter can be introduced at function level.
```rust
impl Quicksort {
pub fn sort<T>(&self,ar<T>){
Sorter::sort(self, ar<T>) ;
}
}
```
Now when it comes to implementing a trait things goes smoothly again since type parameter is already exists in `Sorter` trait.
```rust
impl<T> Sorter<T> for Quicksort {
fn join(...) {...}
fn split(...) {...}
}
```
There are few places where you actually need artificial type parameter.
1. When type conceptually contains some generic type.
2. Type must implement a trait with associated type.
```rust
trait Trait {
type X;
}
```
Now either the type in question will be able to implement the `Trait` with some predefined `X`.
```rust
struct Foo;
impl Trait for Foo {
type X = u32;
}
```
Or it has to have this artificial type parameter after all.
```rust
struct Foo<T>(PhantomData<T>);
impl<T> Trait for Foo<T> {
type X = T;
}
```",0,0,msr
359,"@neithernut In case you're still interested, I had a similar problem and I solved it by creating an additional variant that stores the `PhantomData` and a never type `Empty`:
```rust
pub enum Empty {}
pub enum Foo<A, B> {
X(A),
// ...
Void(Empty, PhantomData<B>),
}
```
[playground example](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=c297fb0f024a70630ce2ffa1e6810b3e)
The good news is that it's zero-cost and does not pollute the variants you do use.
The bad news is that, until [never type exhaustive matching](http://smallcultfollowing.com/babysteps/blog/2018/08/13/never-patterns-exhaustive-matching-and-uninhabited-types-oh-my/) comes to Rust, this approach will pollute the code when you match on your enum (although you could macro your way around it):
```rust
match foo {
Foo::A(_) => (),
// ...
Foo::Void(empty, _) => match empty {},
// ^
// phantom data
}
```",0,0,msr
360,@alexcrichton I also had some issues with this and generic enums. It would be very useful if the compiler could recognize that the type parameter is only used in a trait bound for another type parameter. I don't understand the reason why this would not work now.. Can't the compiler derive the constraints from the trait the type parameter is passed to?,0,0,msr
361,"@mankinskin I agree. PhantomData in this instance just pollutes the code. It is an implementation detail that should never be exposed in a public API (which is what happens with both @AdrienChampion 's and @neithernut 's code. It's also exceedingly irritiating, adding code complexity, complicating construction (so making it harder to just pass generic enum members as closures w/o writing extra code, etc. It's a code smell in Rust itself.",0,0,msr
362,"Looking back to @alexcrichton's response and the RFC, I think the question is not so much why unused type parameters cause an error, but why the compiler considers type parameters unused in examples like the OP's.
To me, it seems like `Elem` has one usage (in `List: AsSlice<Elem>`), albeit not in the body of the struct. It seems like that should constitute a usage for the purpose of variance inference, since it implies a variance bound: `Elem` must be invariant since `AsSlice`'s parameter is invariant.
In Scala for example, the equivalent would be `class Foo[E, S <: Seq[E]](val seq: S)`. Scala doesn't infer variance, but it ""knows"" that `E` must be covariant or invariant. If we try to make it contravariant (with `-E`), we get `error: contravariant type E occurs in covariant position`.
If getting the compiler to recognize these variance bounds would be complicated, perhaps lifetime and non-lifetime parameters could be handled differently? As the RFC notes, only lifetimes have subtype relationships, so it seems like non-lifetime parameters might as well be ""inherently"" invariant, and not subject to inference. Code like `struct Thing<'a, 'b: 'a>(&'b u32);` would continue to be rejected, but I don't think there's any ""legitimate"" reason to write lifetime parameters like that.",0,0,msr
363,"I think perhaps we should reopen this one. The associated RFC doesn't seem to justify a compiler error in code like:
```rust
pub struct Foo<A> {
a: A,
}
pub struct Bar<A, F>
where
F: AsRef<Foo<A>>,
{
foo: F,
}
```",0,0,msr
364,"Changes of this magnitude would require an RFC these days, and I would encourage folks to either open an RFC or a thread on users or internals rather than continuing to comment on an issue that was closed almost six years ago.",0,0,msr
365,"But can't this issue be reopened? This is kind of the first place you end up at when googling about this, and the discussion is basically already here. I don't think there is a lot of designing left to do, it just needs to be implemented in the right way.
I can try to write up an RFC some time, but it is not high on my priority list right now and I am completely unexperienced with rustc. Opening up this issue for now would at least raise attention to it and helps resolving this faster.",0,0,msr
366,"Large scale changes to the language, of which this is one, require an RFC. I am not on the lang team, but this isn't actually a bug: this is a request for a change to how the language works. A bug being open for it is not the procedure by which this would be changed.",0,0,msr
367,"@mankinskin I agree completely, and I feel your frustration; I share it, and it's got worse as rust has become more widely used. Rust's core team also have a very narrow view of what a bug is - one that with my software craftsmanship hat on I couldn't disagree with more strongly.
Rust's development has been captured by the bureaucratically minded; the same mindset that has infested wikipedia, and parodied beautifully in [Brazil](https://www.youtube.com/watch?v=KZ-SdU53MnY).
Personally, I find it rude; it shows disinterest in the views of those outside of the main development, and the idea that anyone can have a good idea.",0,1,msr
368,"@raphaelcohn I know what you mean and we should always be wary of overdoing that, it might stagnate innovation, but I also think there is value in organization and strict review. Rust's promise is to be a secure language, that means it must takes steps carefully, which makes it more difficult to innovate. I agree that there eventually needs to be an RFC for this but I don't really see the point of restricting discussions about feature changes in these github issues. I suppose they use this as a bug tracker, and new features are not supposed to be tracked here.
In general, I think there are a lot of improvements that could be made to the entire open source workflow nowadays. The tools don't really integrate well, basically just using hyperlinks. For open source work there ought to be issue dependencies, resource budgets, integrated documentations and tests, voting on forks, funding, user permissions, all of that. That would make open source a lot more ""open"" and transparent. It has to come eventually, but there still needs some work to be done.",0,0,msr
369,"Moderation note: what @steveklabnik said is correct. The kind of unconstructive broad criticism that has taken place in the past few comments is not appropriate here. Since this discussion doesn't seem to be headed anywhere good, and the next steps if one were to make progress have been clarified, I'm going to lock this thread.",0,0,msr
370,very good feature :+1:,0,1,msr
371,+1 for this feature : ),0,0,msr
372,1,0,0,msr
373,1,0,0,msr
374,1,0,0,msr
375,1,0,0,msr
376,1,0,0,msr
377,:+1: would definitely be useful.,0,1,msr
378,Is there and ETA for this?,0,0,msr
379,Not at the moment,0,0,msr
380,1,0,0,msr
381,1,0,0,msr
382,1,0,0,msr
383,1,0,0,msr
384,+1 extreamly important,0,0,msr
385,1,0,0,msr
386,1,0,0,msr
387,1,0,0,msr
388,1,0,0,msr
389,1,0,0,msr
390,1,0,0,msr
391,1,0,0,msr
392,1,0,0,msr
393,1,0,0,msr
394,when to achieve?,0,0,msr
395,@vkarpov15 : any progress on this?,0,0,msr
396,"Since there's still no support for this, I solved this by keeping a base model and exporting another schema to a separate file and then adding properties to it as needed.
Hope that helps:
```js
/** BaseModel.js **/
const mongoose = require('mongoose');
const myModel = new mongoose.Schema({
name: {type: String, required: true},
});
module.exports = mongoose.model('Model', myModel);
/** schema.js **/
const mongoose = require('mongoose');
module.exports = function(props) {
const params = {
description: {type: String}
};
if(props) Object.assign(params, props);
return new mongoose.Schema(params);
};
/** SubModel.js **/
const mongoose = require('mongoose');
const BaseModel = require('./BaseModel');
const BaseSchema = require('./schema');
const subModel = BaseSchema({
myProp: {type: Number}
});
module.exports = BaseModel.discriminator('SubModel', subModel);
```
You create more sub models by creating more sub-schema factories.",0,0,msr
397,1,0,0,msr
398,1,0,0,msr
399,1,0,0,msr
400,1,0,0,msr
401,1,0,0,msr
402,1,0,0,msr
403,1,0,0,msr
404,1,0,0,msr
405,"I recently took the time to open source a solution to this issue, it's call [mongo-schematic-class](https://github.com/Omninox/SchematicClass). It allows you to use native ES6 classes (or function classes, if you prefer) and normal inheritance to define your schemas.",0,0,msr
406,Any updates?,0,1,msr
407,"Example list: http://www.imdb.com/list/export?list_id=ls076157636&author_id=ur58869533p (CSV)
Or RSS: http://rss.imdb.com/list/ls076157636/
We need to provide a way to set the defaults when adding a series via this mechanism, likely on a per feed basis. Trakt is also a candidate for a feature such as this, so we could automatically import a series from a trakt list.
A new series will need a root folder, a profile, monitored state, a type (standard, daily or anime) and whether it should use season folders. When the add series page allows adding tags when adding a series we should provide the same here.",0,0,msr
408,"Radarr already has this feature, so it should be easy to copy?",0,0,msr
409,"I just discovered this feature in Radarr, and would love to see it in Sonarr as well.",0,0,msr
410,"Lists works great in Radarr, and would be an amazing addition to Sonarr",0,0,msr
411,This would be an amazing addition to Sonarr. Thanks!,0,0,msr
412,"I'd love to help out building this, but I got 0 experience with this project, and can't seem to make logic of radarr's version.",0,0,msr
413,"Same here, as Radarr can import Trakt lists I'd love to see this on Sonarr :)",0,0,msr
414,I'd love to see this too!,0,0,msr
415,1,0,0,msr
416,"Works great on Radarr, would be awesome to get it at Sonarr. That would mean one place to select, and let Sonarr/Radarr read list, search, tag, download, done.",0,0,msr
417,Maybe through a Trakt integration using list feature? Analogue how it is done in Radarr.,0,0,msr
418,I started with Radarr and assumed Sonarr could parse your IMDB list for shows. I was a bit surprised to find out it couldn't so someone pointed me here to chime in and toss my hat into the vote. Though I see it has been 3 years so I am guessing it is not as easy to do. Still tossing in my vote :),0,0,msr
419,I could really do with this feature to allow new releases to sync to sonarr automatically...,0,0,msr
420,It hard to believe that this was opened three years ago.... I assumed it's not that complicated since I started with radarr. It's the only thing missing in sonarr to match radarr. Hope it will come soon.,0,1,msr
421,that would be nice,0,0,msr
422,putting my vote in for what feature i most want from sonarr that it doesn't have yet. i have discovered a lot of great movies pulling in top 50 stuff on radarr would like the same type of thing for series.,0,0,msr
423,"d love to see the feature in Sonarr, too",0,0,msr
424,For now i used [sonarr-trakt-tv]( https://github.com/preram48/sonarr-trakt-tv) to sync TV Shows to sonarr as a temporary solution. It's works great but missing some options. (For example there is a quality profile that i don't see in sonarr-trakt-tv) But it get's the job done.,0,0,msr
425,"I desperately need this feature. I have setup of Radarr and Radarr4K. Radarr would import lists from different places and the 4K versions imports them from the normal Radarr.
I'd love to be able to the same with Sonarr",0,0,msr
426,Add another person to the list,0,0,msr
427,If you don't have anything meaningful to add and you want to add your support add a 👍 to the original post and subscribe to the issue. Everyone that has commented doesn't need to receive an alert that you're also interested in the feature.,0,0,msr
428,"Please, could you implement namespacing for this library ? If anyone has to include in a project is bad practise load single file to use the library.",0,0,msr
429,"@diegomariani if you're using HHVM or any type of opcode cache, you shouldn't worry about it being a single file. at any rate, the library is over 12,000 lines long, so if you want to fork it and namespace it out its going to be a fairly massive undertaking, and even if you did the performance gain would be neglible at best.",0,0,msr
430,"Actually you don't need to use the singe-file distribution. You could clone the repository and use it directly. ReadBean is developed ""the typical way"" with one file per class, namespaces matching the directories and so on. If you use composer, you can add RedBean and get the namespaced, multi-file version. If you're confident using dev-master: https://packagist.org/packages/gabordemooij/redbean, or you could use my fork where I only added the composer.json : https://packagist.org/packages/simirimia/redbean, https://github.com/simirimia/redbean. There you get some stable versions as well.",0,0,msr
431,"Ah, update: since 4.2 is released you get stable version here as well: https://packagist.org/packages/gabordemooij/redbean -> so no more need for my fork",0,0,msr
432,@simirimia If installing via composer can we still use everything as `R` alias ??? Installing redbean via composer would be advantageous to me but i was unable to get it to work previously,0,0,msr
433,"@r3wt If you look at the [composer.json](https://github.com/gabordemooij/redbean/blob/ea3f70175178d5385d45c83a32efa51495f80b0c/composer.json#L17-L21), you will see that the namespace `RedBeanPHP\` is autoloaded. Then, if you go in this `RedBeanPHP/` folder, you will see a [R.php file](https://github.com/gabordemooij/redbean/blob/d8f9f4f5ab01b285a587dd54f5175d013c316ade/RedBeanPHP/R.php#L6-L10), which means that the `R` facade is just in the namespace `RedBeanPHP\`.",0,0,msr
434,"Personally I use the one-file solution for all my projects.
It might be bad practice but it's damn easy, and I have never heard anyone complain, nor did it cause any bugs or conflicts.",0,0,msr
435,Do you not use Composer in your project? It really is powerful to manage dependencies.,0,0,msr
436,"@tleb so how would i alias RedBeanPHP to R ?
say i have a class like
Class DB {}
and in it i reference redbean lik `\R` how can i set that alias in my class? `\RedbeanPHP\R` becomes `\R`
FYI the reason i write my code like this is so i can isolate database code from the rest of the code. this way, if needing to change the database library or even database type, i only have one class to edit. so its like an api wrapper with methods like `isEmailInuUse($email)` etc.",0,0,msr
437,"``` php
// You could call R directly
\RedbeanPHP\R::freeze(true);
// Or use `use`
use RedbeanPHP\R;
R::freeze(true);
// Or use an alias
use RedbeanPHP\R as Redbean;
Redbean::freeze(true);
// Or create a facade of the R facade
class R extends \RedbeanPHP\R {}
\R::freeze(true);
```
Also, to make sure every database library you use use the same method names, you should create [an interface](http://php.net/interface).",0,0,msr
438,"i just used `class_alias('\RedBean\R','\R');` and it worked fine.",0,0,msr
439,"I never use composer. I never even use apt-get, yum, npm or any other kind of dependency resolution. Good software has no dependencies other than an OS / VM.",0,0,msr
440,"I understand where you are coming from Gabor, but our jobs depend on is to be agile-- that is to say, we must get alot done I short periods of time. Now I could spend time wget'ing, untaring,and requiring red bean manually, but it makes more sense to just type composer install.",0,0,msr
441,"@gabordemooij Do you know that what you are saying goes against the Unix philosophy:
```
Write programs that do one thing and do it well. Write programs to work together.
```
Tools such as `composer`, or `apt-get`, or any dependency manager, try to make it simpler to link programs together, so that programs can focus on what they must do, and so that they can do it well. If we created the same thing over and over again, every single project would fail, because they would need to do everything from scratch, even though people already created those required programs. Dependency managers are tools to build upon, they are made to create good software based on other software which achieve the task they are asked to almost perfectly.",0,0,msr
442,"@gabordemooij
You should be killed. Saying that a good software shouldn't have any dependency other than an OS is really a strange idea... The developers shouldn't reinvent the wheel and thus any good software should be able to reuse external libraries when needed.
BTW: An OS is nothing more than a bunch of libraries and executable that were compiled to form a system fully usable. Does an OS shouldn't have any external dependencies?",0,1,msr
443,"@tleb composer does not link programs, it links libraries. And, as such it creates huge programs that do everything - which is basically the opposite of the UNIX philosophy. Apt-get also rarely installs programs, it installs mostly lib-this, lib-that, lib-different-version etc. Programs relying on a ton of external libs are definitely not 'Unixy'.
@nuxwin I did not say developers should re-invent the wheel. Neither is this an inevitable consequence of having no dependencies. I simply believe a well designed base system would be preferable - but that's harder to build. @nuxwin Please, let's refrain from statements like 'should be killed' and keep the discussion civilized. Open source forums are already famous for their anti-social tone. We're just people and everyone is entitled to his/her own opinion. If someone disagrees there is either an opportunity to improve knowledge or exchange new ideas.
Also - I believe I accidentally sidetracked this thread with my comment about dependencies in general, sorry for that.",0,0,msr
444,"@gabordemooij
Don't get bad ;) This was just an expression.",0,0,msr
445,"@nuxwin ok, no problem",0,0,msr
446,"@gabordemooij, well, I'm curious, and not sure I got the point. You believe all the libraries belong to the base system? And hence good software is impossible with bad base system?",0,0,msr
447,"I -too- would like to continue this debate but I do not think a Github issue is the best place for this. If anyone could recommend a more appropriate place for this (an IRC channel, a website ?).",0,0,msr
448,@tleb maybe there --> http://www.php-fig.org/irc/,0,0,msr
449,"Well, it's not that I was going to debate on this. I rather wanted to clarify @gabordemooij's point of view. For now it seems self-contradictory to me.",0,0,msr
450,"@x-yuri I acknowledge sometimes you can't do without dependencies. But I believe people have given up to early on the problem. Often when you install dependencies you end up with all kinds of stuff you don't want and you lose control to a certain extend. I believe that we, as a community should invest more in solid base systems. I admire Linux distros like Slackware that have a rich package repo with very few dependencies. I think we should try harder to get our 'bases right' instead of trying to fix fundamental problems with tooling.
Also, I hate the fact, I can't just grab a lib from the web anymore because some of them seem to be tied pretty closely to the whole composer workflow, they have no standalone tarballs, they need autoloader this and that... But I have to admit I am very conservative and stubborn in some matters. There's lots of other issues as well, composer uses git/github as repositories, interfering with my use of it as a version control system (preferred layouts) etc etc..
Please note that when I say things like 'Good software should this or that...' it's just my opinion, I do not expect everyone to agree. However I would like to discuss the usefulness of composer, unfortunately, as soon as you 'ask questions' or 'criticize' a popular technique you're considered to be 'a moron'. These discussions seem to end up in complete flame wars and honestly as long as that culture persists I don't feel compelled to explain every decision I make (luckily the RedBeanPHP forum is quite civilized though).",0,0,msr
451,"I've ended up using composer simply because I find it a challenge to keep my dependencies up-to-date. I don't have a lot of them, but when you have lots of projects, it starts to quickly get out of hand.",0,0,msr
452,"@sbrl don't use composer. Just don't. If you cannot manage your dependencies you have a very big problem and composer hides it. Updating bad code in zillions of libs will not magically improve the reliability and security of your code.
- get rid of all libraries that are not crucial
- remove all libs with dependecies, they are by definition created by idiots
- keep things simple, if you need that much libraries, you failed at keeping things simple",0,1,msr
453,"I don't do this usually but I had to screenshot this whole discussion. Guy doesn't use namespaces, uses single file for his projects and he bashes the practice of tracking your dependencies with - listen to this - **programs**. If anyone stumbles upon this page via google, please take time to read what @gabordemooij wrote and take note - NEVER justify your bad decisions by making stuff up.",0,1,msr
454,"> don't use composer. Just don't. If you cannot manage your dependencies you have a very big problem and composer hides it. Updating bad code in zillions of libs will not magically improve the reliability and security of your code.
As a security engineer who works primarily in PHP and contributes to open source cryptography libraries for the PHP community, I [strongly disagree and say that using Composer is a ***damn good* idea**](https://paragonie.com/blog/2017/12/2018-guide-building-secure-php-software#dependency-management).
> Updating bad code in zillions of libs will not magically improve the reliability and security of your code.
Uh, yes, it will!
Outdated third-party libraries with known (i.e. not zero-day) vulnerabilities are one of the most reliable RCE vectors when testing a system. (Aside: Thank you, projects that ship their own PHPMailer bundles, for all the easy footholds into otherwise challenging networks to penetrate.)
This isn't intuitive for most people, so I've laid out [the case for automatic security updates](https://paragonie.com/blog/2016/10/guide-automatic-security-updates-for-php-developers#why-automatic-updates) before.
Composer makes it easier for projects to keep dependencies up-to-date. This creates a measurable improvement to the security and stability of real world systems.",0,0,msr
455,"Updates often introduce new bugs and new vulnerabilities. So your just replacing old bugs with new bugs. Only right after an update 'you consider your software safe', but as time progresses it turns out the initial update was not safe at all but by then most developers pretend the software 'is now suddenly unsafe because it is old' - that's not true, it was already compromised right after the update. Besides that, dependency management systems make it worse, because you don't even know what you install anymore. There may be even some toxic packages among the dependencies:
https://news.ycombinator.com/item?id=15272394
https://news.ycombinator.com/item?id=11340510
https://sensorstechforum.com/arch-linux-aur-repository-found-contain-malware/
While I don't pretend that bug-free software can be written, a better solution might be to focus on simplicity. Dependency management systems compromise simplicity because they make it easy to solve the natural issues regarding dependencies. In a sense, they hide complexity - and paradoxically therefore they increase the risk of importing new bugs and vulnerabilities.
So, I reverted the commit that removed the Composer file because, even though I am against it, I also don't really care, I just get a bit angry when Composer related problems (like what version string to use) creep into the RedBeanPHP repository. And yes, you make screenshots all you want and laugh about RedBeanPHP or me. I don't give a shit. Have fun.
BTW, remember that this is an open source project, you can fork and nobody is forcing you to use RedBeanPHP so, if you don't like it:
https://www.doctrine-project.org/
bye bye!",0,1,msr
456,"Literally everything you just said can apply to any dependency manager including Git, Github, apt, whatever.
How are redbean releases more secure than another PHP library? They aren't. You aren't signing your releases and there's no verification system in place. The fact is humans make up the ""PHP community"" and it's up to us to verify security. And as far as human verification goes I'll trust the millions of users of a given popular package on packigist versus having to manually check the Git of a project like yours. Honestly though just looking at your source tree your ORM is straight out of PHP 4.x times. Your project has a custom unit testing schema because apparently PHPUnit isn't secure enough? You have no `spl_autoload_register` anywhere in your code even though it makes sense here to use it. You're still publishing redbean on packigist with a composer.json. So I guess ""trust me"" but no one else is enough for you. Like honestly wtf even is [this](https://github.com/gabordemooij/redbean/blob/master/p533patch.php)? You can't even generate code coverage reports because of your home baked unit testing system which I'm sure doesn't support clover.xml output for code coverage. Which is why you don't have a code coverage badge.
Yea yea packigist needs signed releases to give it some ""security"" but honestly the fact is the market has moved on.
In most PHP shops these days it's about speed & quantity of code / features over ""security"" concerns.
Why would I implement AWS myself when they give me a library to use? Why would I implement the Fitbit API manually when [someone already wrote one](https://github.com/djchen/oauth2-fitbit)? Especially when it comes with unit tests and lots of users who will call stuff out if it breaks or is compromised.
We're all idiots? Really dude? The company I work for and all the jobs under it could literally not exist without the good work put in by hundreds of developers I've never met. Because it's literally impossible for me to sit down and write an API implementation for every service we interact with. And yet we are currently running a company based on tying all these things together into a coherent service.
I think you seriously need to re-evaluate your philosophy and take a look at a lot of the [good](https://thephpleague.com/) work being done currently in the PHP space by dedicated people.
And finally. If you care sooooo much about security maybe it's time to upgrade to PHP 7.1 because security support for 5.6.x ends in 3 months.",0,0,msr
457,"> Updates often introduce new bugs and new vulnerabilities. So your just replacing old bugs with new bugs. Updates also fix bugs and old vulnerabilities. Using a dependency manager lets you lock down to a specific version of a library. I can fork a library, perform changes, tag it, point my `composer.json` to it. I don't have to do anything manually. If I want to know which *version* I use, I don't have to dig through files.
I could literally go through every single word you wrote and try to argue with you but it's pointless. You suffer from NIH syndrome, your practices are bad and as you wrote - you don't give a shit.
You are spreading dangerous blatant lies in order to justify something you wish were true. That's the problem. The second problem, which is your personal problem, is your attitude. I know there's absolutely no way we can have a discussion, your opinion is set and I can tell you'll do anything to defend your POV. Yes, this is your project, you are entitled to opinion but in your world 1 + 1 yield 75, not 2. Ego is a dangerous thing. I won't insult you, there's no merit to that, we should be civilized after all. I'll just kindly ask you, just ASSUME you might be wrong. Just for abstract thinking's sake. I really believe there's no malicious intent behind what you wrote, but could there be a possibility you don't have all the information at hand to make statements such as ""dependencies suck"" or ""dependencies in libraries suck""? What would be the point of a library then? If I deal with SAML or OAuth2, I require a library which in turn requires encryption primitives. Should I rewrite those primitives and libraries to deal with SAML and OAuth2 so I'm ""safe"" and not an idiot?",0,1,msr
458,"> Literally everything you just said can apply to any dependency manager including Git, Github, apt, whatever.
Git is a VCS, Github is a code hosting platform, so these are not DMs. And yes, it applies to apt. That's why I recommend to use Slackware, Crux or *BSD instead.
> How are redbean releases more secure than another PHP library?
1. 180000+ tests
2. 10 years of maturity
3. Careful code review process
4. Focus on eliminating code, simplicity (minimalism)
5. 0 dependencies
Updating RedBeanPHP is also easy: just copy-paste the rb.php file and done!
> You aren't signing your releases
I provide sha256 checksums on the download page:
https://redbeanphp.com/index.php?p=/download
So you can check whether the downloaded file has not been corrupted or modified - improvements can be made here.
> I'll trust the millions of users
You use Windows I suppose (millions use it!) ? Seriously. How on earth does quality improve just by USING something?!
> your ORM is straight out of PHP 4.x times
Therefore it must be bad. Because new is always better? > Why would I implement AWS myself when they give me a library to use?
AWS is a platform, not a library. You can't implement AWS with just a library.
> We're all idiots? Really dude?
I don't know about the rest in this thread but you seem to suffer from the Dunning–Kruger effect:
https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect
> You suffer from NIH syndrome
To a certain extend agreed, and how do you think I that happened after 20 years of coding?
> I'll just kindly ask you, just ASSUME you might be wrong
That's all I ever do. I know I might be wrong. But to be sure I need good arguments.
> Should I rewrite those primitives and libraries to deal with SAML and OAuth2 so I'm ""safe"" Never use OAuth:
https://hueniverse.com/oauth-2-0-and-the-road-to-hell-8eec45921529
Never roll your own crypto. Use a libreSSL-based crypto solution:
https://www.libressl.org/",0,0,msr
459,"Sha256 isn't a digital signature.
https://paragonie.com/blog/2015/08/you-wouldnt-base64-a-password-cryptography-decoded
On Mon, Sep 10, 2018, 7:59 AM Gabor de Mooij <notifications@github.com>
wrote:
> Literally everything you just said can apply to any dependency manager
> including Git, Github, apt, whatever.
>
> Git is a VCS, Github is a code hosting platform, so these are not DMs. And
> yes, it applies to apt. That's why I recommend to use Slackware, Crux or
> *BSD instead.
>
> How are redbean releases more secure than another PHP library?
>
>
> 1. 180000+ tests
> 2. 10 years of maturity
> 3. Careful code review process
> 4. Focus on eliminating code, simplicity (minimalism)
> 5. 0 dependencies
>
> Updating RedBeanPHP is also easy: just copy-paste the rb.php file and done!
>
> You aren't signing your releases
> I provide sha256 checksums on the download page:
> https://redbeanphp.com/index.php?p=/download
> So you can check whether the downloaded file has not been corrupted or
> modified - improvements can be made here.
>
> I'll trust the millions of users
> You use Windows I suppose (millions use it!) ? Seriously. How on earth
> does quality improve just by USING something?!
>
> your ORM is straight out of PHP 4.x times
> Therefore it must be bad. Because new is always better?
>
> Why would I implement AWS myself when they give me a library to use?
> AWS is a platform, not a library. You can't implement AWS with just a
> library.
>
> We're all idiots? Really dude?
> I don't know about the rest in this thread but you seem to suffer from the
> Dunning–Kruger effect:
> https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect
>
> You suffer from NIH syndrome
> To a certain extend agreed, and how do you think I that happened after 20
> years of coding?
>
> I'll just kindly ask you, just ASSUME you might be wrong
> That's all I ever do. I know I might be wrong. But to be sure I need good
> arguments.
>
> Should I rewrite those primitives and libraries to deal with SAML and
> OAuth2 so I'm ""safe""
>
> Never use OAuth:
> https://hueniverse.com/oauth-2-0-and-the-road-to-hell-8eec45921529
>
> Never roll your own crypto.
> Use a libreSSL-based crypto solution:
> https://www.libressl.org/
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/gabordemooij/redbean/issues/450#issuecomment-419887784>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ALDfXqfBpQ30trHJ2WyimuRMOIGEXff-ks5uZlQvgaJpZM4EsR4H>
> .
>",0,0,msr
460,"```
How are redbean releases more secure than another PHP library?
> 1. 180000+ tests
> 2. 10 years of maturity
> 3. Careful code review process
> 4. Focus on eliminating code, simplicity (minimalism)
> 5. 0 dependencies
```
This is seriously your answer? You demonstrate a lack of understanding of what actually makes a package manager have ""secure"" releases. A secure release is signed with a private key that only you would have. At the moment anyone can get into your Github user and commit anything they want to your project. Sure you can revert it once you get your account back but until then damage can be done.
Your hashes mean nothing. Anyone can edit your website and edit those hashes anytime they want. None of what you mentioned gives increased security.
I also have a 10+ year old ORM with hundreds of projects and more then a decade of maturity. But you know what? Nobody cares.
If you have no code coverage report your tests mean nothing.
Never use oauth? lmao. The company I work for. Literally our entire business model depends on APIs that use oauth. I have no choice. It's not like oh lemme just make my own Fitness tracker real quick.
This entire thread can be summed up as ""I made up my mind 10 years ago and I don't feel like learning anything new so leave me alone"".",0,0,msr
461,"This issue was closed 3 years ago. Why resurrect something just because someone on the internet does things differently? Apparently [someone posted](https://www.reddit.com/r/PHP/comments/9edtkx/if_you_cannot_manage_your_dependencies_you_have_a/) this issue on /r/php, what good could come out of this?
I'm a composer user but I don't go out of my way trying to hammer into people's mind that they should do things differently. There are already forks with composer support anyway. Just move on people. This is borderline harassment at this point.
Thank you for writing RedBean @gabordemooij ❤",0,0,msr
462,"There's a person here, who can obviously write code, read what others say, have a conversation and who is obviously NOT stupid. It's also a person who created an open source project that others use. Now, that same person states certain things that one of my younger colleagues caught because he reads reddit and PHP page (I'm not a reddit user). He uses RedBean in his website and asks me how come we're doing opposite of what is stated here. Why do we use a package manager? I wondered where he got that from, and I stumbled upon this issue. I'm not trying to change people's opinions, but being able to affect someone you don't know through written word means there's a responsibility you bear - and that responsibility means you have to provide FACTUALLY correct information.
The information in this small internet discussion between all parties is sadly, not factually true. I agree that picking everything apart and doing the classic ""no, you're wrong"" type of communication is basically harrassment so I'll stop.
I don't agree with what's written here, and the only reason I'm typing this is to inform future visitors not to blindly trust everything they read, I'll even accept that I'm an idiot for using package managers, OAuth2, SAML etc. - it doesn't matter. The right tool for the job should be used, regardless of what one **thinks** might be ""correct"".
Have a nice day everyone, sorry for intruding your project Gabor and I wish you all the best - I had no ill intent, I'm sure you didn't either and I hope you're not offended, world will still continue revolving regardless of this little discussion :)",0,0,msr
463,"> So, I reverted the commit that removed the Composer file because, even though I am against it, I also don't really care,
Okay, thanks for the historical context.
> I just get a bit angry when Composer related problems (like what version string to use) creep into the RedBeanPHP repository.
Oh, I can commiserate with you on this point all day long. They're frustrating, especially when you're the person who causes them and you're puzzled about how to fix them.
I don't have a horse in either the ""Does Gabor use Composer?"" or ""Does RedBean use Composer?"" races. I do care about making PHP more secure at the ecosystem level, and that obligates me to seek out and fight against misconceptions about software security.
To be clear:
* Automatic updates *may* introduce new bugs, but they'll fix **the bugs script kiddies are actively exploiting**. Simplicity isn't a panacea here, but it is helpful. (I write my code to be easily audited by third parties, and the incidence of bugs is much lower as a result.)
* Digital signatures are a little more involved than hashes.",0,0,msr
464,"Anyway, as I already admitted you're right concerning the checksums, it only protects against accidental corruption. I will use signify() to sign the releases in the future. https://man.openbsd.org/signify",0,0,msr
465,"Okay. There's little left to discuss then, it may be a good idea to lock this thread to only collaborators.",0,0,msr
466,That was a good idea. Hopefully we can now continue the discussion in a more fruitful way.,1,0,msr
467,"I just discovered signify() is OpenBSD-only, so maybe I'll use GnuPG signatures instead. The signatures will be uploaded to github and the google groups forum. That way, to fake the sigs you must hack the RedBeanPHP website, Github and Google at the same time. Do I miss something here?",0,0,msr
468,Files now contain Signify signatures. GnuPG may follow later.,1,0,msr
469,"would be great to make filter for auto skip marked extention types from download.
ex: i need only music without covers, etc. it annoying to deselect it all time (imagine 50 albums, 4Gb covers and 1 seed). so here can help filter to .jpg, .png, etc.
if need to download photos - turn off filter checkbox in file list.
you can do sets of rules, but one is enough.
<bountysource-plugin>
---
Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/23896213-file-extension-filter?utm_campaign=plugin&utm_content=tracker%2F298524&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F298524&utm_medium=issues&utm_source=github).
</bountysource-plugin>",0,0,msr
470,Sounds useful :thumbsup:,1,0,msr
471,"2015? :-( This will very likely never added then, that sucks.",1,1,msr
472,"This feature could help security:
If a user downloads only known files (e.g. audio/video), it is wise to block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS, (PY?), etc.
Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi <> Matrix.avi.exe
Also, some video torrents are bundeled with trojans, e.g. ""codec.zip"" or ""driver.exe"" containing malware. Today I spotted the attached file, an LNK file, an extension that is hidden on Windows.
None of my users download software via torrent, so I'd like to block it for them, or set blocked extensions; potentially, i'd like to block the whole torrent altogether, if a potential software extension is found in one of its files.
(My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY, CPL, DLL, LNK, SCR)
In the screenshots below: Windows hides LNK extension, a trojan disguised as AVI video:
![lnk-virus](https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png)
![virus2](https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png)",0,0,msr
473,"Id like to throw my support behind this feature request. Many torrents come with useless .txt files that do nothing but clutter up a directory. Also, a popular site I use has started to include an .exe file that I now have to deselect every time I download something. It would be great to be able to have these files automatically excluded. Being able to blacklist certain file names would also be a great addition to this feature.",0,0,msr
474,Throwing my hat in..,1,0,msr
475,"I suggest a feature for a simple list of file names ```do_not_download.exe``` and extensions ```*.exe``` that get marked as **Priority** -> **Do Not Download** automatically for all torrents. List may be accessed in **Options** -> **Downloads**.
Searching for references:
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L684
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L2068
https://www.libtorrent.org/reference-Core.html find ```file_priorities``` under ```add_torrent_params``` header.
https://github.com/qbittorrent/qBittorrent/blob/2d7b833ae6cb2145465cc7e47df398628ac95651/src/base/bittorrent/session.cpp#L1949",0,0,msr
476,"Glad i found this post, really like this feature to, are we sure (i couldn't find it) that there is no such option already?",0,0,msr
477,Definitely have my vote. It will also save (not much but) some space and unnecessary Data download for Countries that charge per Mb on top of per Speed.,0,0,msr
478,"Love it. You would need to be able to override it on a per torrent bassis,
but for people who download primarily just a couple of different file types
(cough).
On Sat, Oct 6, 2018, 4:35 PM shula <notifications@github.com> wrote:
> This feature could help security:
>
> If a user downloads only known files (e.g. audio/video), it is wise to
> block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS,
> (PY?), etc.
>
> Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi <>
> Matrix.avi.exe
>
> Some video torrents are bundeled with fake ""codec.zip"" ""driver.exe""
> containing malware. Today I spotted the attached file, an LNK file, an
> extension that is hidden on Windows.
>
> None of my users are downloading software via torrent, so I'd like to
> block it for them, or set blocked extensions; potentially, i'd like to
> block the whole torrent altogether, if a potential software is found in it.
>
> My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY,
> CPL, DLL, LNK, SCR.
>
> In the screenshots below:
> How windows hides LNK extension, which is a sure malware when only
> downloading media:
>
> [image: lnk-virus]
> <https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png>
> [image: virus2]
> <https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-427604310>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGbY-5e2u-o8-6NhlR-Z9UlUBj9q6YGgks5uiRPzgaJpZM4FT95h>
> .
>",0,0,msr
479,"Please add this feature since lot's of trackers now put lots of ""junk files"". Yes we can use other clients but qb has a lot to offer and you can always disable this feature if it bothers you, so it's a win-win.",0,0,msr
480,"I'd love this, too.",1,0,msr
481,This please! We need this!,1,0,msr
482,Is it possible now to completely fail specific torrent if it contains not appropriate file name?,1,0,msr
483,"+1 on the feature request.
In the meantime, I wrote a short cmd script which can be referenced in the _Tools->Options->Downloads->Run external program on torrent completion_ which renames files with a suspicious extension (.exe \ .scr \ .cmd \ .bat) to prevent them from running when double clicked.
```
rem Find suspicious files in directory and rename them
rem Usage: fsus.cmd <dirname>
@echo off
SETLOCAL
set extensions=""\.lnk \.exe \.cmd \.scr \.bat""
echo looking in %1 for %extensions%
for /f %%F in ('dir %1 /s /b') do (
(echo %%F | findstr /r %extensions% > NUL) && move %%F bad_%%F.BAD && echo Renamed %%F
)
ENDLOCAL
```",0,0,msr
484,"Plugin in python3 for this:
https://gist.github.com/oltodosel/566e051191f3a58b905db2cc6980656f",0,0,msr
485,Has there been any updates if/when this feature would be added?,1,0,msr
486,Since 2015 and this still isn’t added yet? Come on. This would be such a useful feature.,0,1,msr
487,"I would love to have this feature implemented. There could be a global file extension filter, or have the filter setup by category (so categories can have different filters).",0,0,msr
488,2015-2020 is not create this function. qBit - Shit!,0,1,msr
489,"Why isn't this issue considered critical?
Distracted users shouldn't run viruses so easily.",1,0,msr
490,"> Why isn't this issue considered critical?
To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3"", and not ""distracted users clicking files with hidden extensions"" - that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK. Alternatively, just use better sites and download better torrents.
This is why this isn't ""critical"".",0,0,msr
491,"> To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3""
That's definitely my motivation for wanting this feature. I use the RSS downloader and it would be nice to automatically exclude unnecessary files.",0,0,msr
492,"> > Why isn't this issue considered critical?
> > that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK.
As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.
Also, it is arguably because torrent clients aren't smart that these types of malicious torrents are still going around, and that's why I think qBittorrent should provide this feature.
IMHO certain extensions should also be skipped by default, for the same reason. It would be a great security and usability improvement.
This also allows you to skip certain extensions because you prefer so, but the security issue should be considered prominently.",0,0,msr
493,"> As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.
If you are downloading torrents with malicious .lnk files, you need a solution for a more urgent problem: don't download such torrents, use better sites/sources, or pay more attention. After all, it is the user's responsibility to not fall for phishing emails as well. Inspect URLs/files you click.
Alternatively, you could try to enable showing `.lnk` extensions: https://www.tenforums.com/customization/111886-how-show-lnk-extension.html
But because Windows is Windows, this might lead to undesirable presentation elsewhere (such as the start menu).",0,0,msr
494,"Your solution is for power users, has usability drawbacks, and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.
I think the qBittorrent team should step up and fix this. There is almost never a good reason to download certain file extensions, and users should actively check those files for download.",0,0,msr
495,"You can't expect people that execute random files to know how to use power features. That something can (and should) be done another way isn't a reason to not include a security feature.
These are the same people that download from the first torrent site that shows up in Google search. So the ""use a better site"" isn't a valid argument either.
Obviously all this is only true if your target is the mass and not just tech-savvy people.",0,0,msr
496,"> Your solution is for power users, has usability drawbacks, Fighting phishing emails is something everyone has to learn to do, no matter the occupation. I think it is reasonable to demand a certain level of proficiency and common sense.
> and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.
""Malicious users are taking advantage of distracted/careless users"" would be a more accurate statement. Do you think the possibility of receiving phishing emails is a flaw of E-mail? If so, is the possibility of hearing the voice of a scammer in real life, believing what they say, and giving them money, a flaw of your ears? Should your auditory system should autoblock certain words/sentences on its own? Perhaps it should be the brain acting on the information instead.
Furthermore, the greater issue of downloading these kinds of torrents should not be underestimated. You have to go out of your way, even when searching for illegal content, to find these kinds of torrents. And no, the `.exe` in RARBG torrents does not count as an example of this practice in a popular site; it is actually just a harmless text file with the `.exe` extension designed to prevent mirroring by software that, ironically, relies on ""file extensions"" to make assumptions about their content.
Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.",0,1,msr
497,">You can't expect people that execute random files to know how to use power features. That something can (and should) be done another way isn't a reason to not include a security feature.
These are the same people that download from the first torrent site that shows up in Google search. So the ""use a better site"" isn't a valid argument either.
>
>Obviously all this is only true if your target is the mass and not just tech-savvy people.
First of all, it is indeed a shame that the Windows default is wrong, and that the way to change it requires some knowledge to do so. But that is a Windows problem. One can post an issue on the relevant forum/issue tracker about that, not here.
Secondly, regardless of such setting, I'm not really keen on catering to this kind of ignorance/stupidity of ""I'm carelessly clicking on stuff and ignoring warnings and expecting it to work"". If someone doesn't care to learn how to properly use a saw, should their complaints to the manufacturer be taken seriously when they cut themselves?
We should strive to make things easier to use. But not to the point of bending over backwards to a level of stupidity/ignorance/carelessness that shouldn't be endorsed or excused, at the expense of time, effort, and other important things.",0,0,msr
498,"> Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.
You don't need to click through anything. Once you've clicked the .lnk file an .exe will download in the background without any warning in less than a second, and it will run again unnoticed at the next restart. There are virtually endless possibilities to the harm that can be done by these attacks.
Furthermore, they're not even detected by most antivirus software.
If the qBittorrent team needs an example, I can provide it.",0,0,msr
499,"> You don't need to click through anything. Once you've clicked the .lnk file an .exe will download in the background without any warning in less than a second, and it will run again unnoticed at the next restart. There are virtually endless possibilities to the harm that can be done by these attacks.
> Furthermore, they're not even detected by most antivirus software.
> > If the qBittorrent team needs an example, I can provide it.
There are always exceptions to the rule. I am sure there are some examples of software bypassing UAC, or just being dangerous enough without needing to do so in the first place. But this is a secondary point anyway.
I should add to https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-652597093:
Again, I'm not saying it would be bad to have this feature. It would be good for automation purposes, for example. But I don't think it is fair to consider it ""critical due to user security considerations."". Of course just by being there it could serve as an additional safety net. But that's not the main purpose and it's by no means critical for that purpose.",0,0,msr
500,"I'm sorry Francisco but I really think you are underestimating the problem.
If we followed your logic, antivirus software and antispam software should be banned, because humans should be infallible and never miss a single malicious file.
This type of attack with torrents is too common, at a certain point client software (qBittorrent) becomes complicit in this kind of exploit, which has been reported as far back as 2018 here above.
If the same arguments were used by browsers, e.g. Firefox, sandboxing bugs shouldn't be fixed because the user should only visit ""trusted"" websites.
I hope the rest of the team doesn't treat security the same way as you do. This is an overdue feature, requested since 2015.
The team has been asked before for directions on where to look in order to implement a pull request, but no attention has been given to the matter.
If not even this convinces anybody who possesses the knowledge and expertise to propose a fix, I really have nothing more to add. I have seen the developers here dedicate a lot of time and effort to ""betterment"" projects such as the new webUI and API, therefore I really am surprised such a seemingly small-to-implement but security-critical issue has not been given space in 5 years.",0,0,msr
501,"@simo1994 > If we followed your logic, antivirus software and antispam software should be banned, because humans should be infallible and never miss a single malicious file.
This is one hell of a strawman fallacy, but ok. Not to mention that you seem to be advocating in favor of antivirus/anti-spyware. In a sane computing environment, a black-box proprietary software made by a for-profit corporation running with administrative privileges (aka """"""antivirus"""""") is not considered a layer of security. The true solutions for the problems these programs claim to solve lie somewhere else in the stack - don't use Windows, use Free (as in Freedom) software, package managers with cryptographic signing, etc...
> This type of attack with torrents is too common,
These kinds of claims are worthless if not substantiated with data. You can't just claim something and ask others to disprove it. Start by defining what is ""too common"". 30% of torrents? 40%? 50%? In which sites? The burden of proof lies with you. You're welcome to open a new issue investigating this, with some pretty graphs.
> at a certain point client software (qBittorrent) becomes complicit in this kind of exploit, which has been reported as far back as 2018 here above.
- This is not an ""exploit""
- ""complicit"" is a strong word. Are email clients/servers ""complicit"" for people falling for the Nigerian prince scam?
> If the same arguments were used by browsers, e.g. Firefox, sandboxing bugs shouldn't be fixed because the user should only visit ""trusted"" websites.
Again, ridiculous strawman. Browsers need sandboxing for any kind of user due to the unfortunate way the Web has evolved. In fact, mainstream browsers don't even come secure enough out of the box IMO.
> I hope the rest of the team doesn't treat security the same way as you do.
I treat security as seriously as anyone else who really understands it. In reality, you'd be surprised how little of it _you_ actually understand, to be able to make the arguments you make and then accuse others of not treating security seriously. Hopefully one day you'll be able to look back at this and chuckle, it will mean you have evolved and learned.
>The team has been asked before for directions on where to look in order to implement a pull request, but no attention has been given to the matter.
If not even this convinces anybody who possesses the knowledge and expertise to propose a fix, I really have nothing more to add.
Anyone is welcome to submit PRs to implement the feature. There isn't any sense of urgency, because likely all of those with the ability to implement it also know that this isn't ""security critical"".
> I have seen the developers here dedicate a lot of time and effort to ""betterment"" projects such as the new webUI and API, therefore I really am surprised such a seemingly small-to-implement but security-critical issue has not been given space in 5 years.
Again, it's not ""security critical"". Stop with the fear-mongering and FUD.
> I really have nothing more to add.
Me neither. I agree that everything relevant to this feature request has been said. Now it's up to whoever wants to implement it.",0,0,msr
502,"TL;DR:
- Anyone is welcome to implement this, and I'm also convinced it won't be rejected if implemented. As mentioned above, it may be useful for certain automation scenarios.
- The default should be to not filter anything, IMO. Otherwise, it would be unexpected behavior - people would wonder why certain files don't download by default. If qBittorrent ever gets some sort of ""onboarding"" UX, this could be one of the tunables (e.g. do you want to filter ""potentially malicious files"" by default?).
- No, this is not ""security critical"" (see discussion above).",0,0,msr
503,"### Expected Behavior
All directories created by `npm install` should have the npm config setting `umask` applied to their file mode/permissions. The `umask` setting is [clearly described as](https://docs.npmjs.com/misc/config#umask) the ""value to use when setting the file creation mode on files and folders.""
### Observed Behavior
During `npm install`, if directories are created by the `tar` module as a side-effect of extracting a file, the umask setting is ignored. For example, when tar extracts `foo/bar/baz.js`, it creates the `foo` and `foo/bar` directories (if they don't exist) before writing `baz.js`. These directories do not respect the umask setting.
Instead, npm's umask setting is ignored and [`process.umask()` is used](https://github.com/npm/fstream/blob/master/lib/writer.js#L15). However, there is a further bug/unexpected behavior where the `tar` module will actually [do the equivalent of `chmod a+x`](https://github.com/npm/fstream/blob/master/lib/writer.js#L352) on directories created as a side-effects of file extraction, meaning the even `process.umask()` isn't strictly observed.
### Details
- OS: Mac OS X 10.9.5
- `npm --version`: `2.13.5`
- `node --version`: `v0.10.40`
- `npm config get umask`: `0077`
- `umask`: `0077`
An example is seen if you install `grunt-lib-phantomjs`. The directory `node_modules/phantomjs/lib` (as an example) should have a mode of `0700`; instead, it has a mode of `0711`.",0,0,msr
504,"Since I see this has been tagged as a feature request and support request, I want to be clear that this is definitely a bug in npm.
The umask setting in npm is being completely ignored for a seemingly arbitrary subset of operations during `npm install`. This is almost certainly not the intended behavior. It also explicitly disagrees with the behavior described in the documentation, and is inconsistent even within the observed behavior of npm (sometimes it respects umask, other times it doesn't, for no good reason).
The underlying cause (cited in the issue) is that the tar module isn't aware of npm's `umask` setting, and just does its own thing (using the `process.umask()` value, then overriding the execute bit on it.)
But in summary, this is 99.9% likely to be a real bug, not a new feature or user usage problem.",0,0,msr
505,"This issue still exists in npm version 3.4.0 and is most certainly a bug and not a new feature or user usage problem.
Please see https://github.com/npm/npm/issues/4197 which is correctly tagged as a bug.
#### Details
- OS: Debian GNU/Linux 7.8 (wheezy)
- `npm --version`: `3.4.0`
- `node --version`: `v4.2.2`
- `npm config get umask`: `0022`
- `umask`: `0077`",0,0,msr
506,"This is marked as a feature request in part because the current behavior that npm has is underspecified. The first step to making npm's behavior here clearer is to nail down what the current behavior is, and why. Tagging this with `footgun` gets it onto npm's road map, and the next step for the CLI team is to unearth and document the historical reasoning for how the various pieces of npm (including `node-tar`) handle permissions.",0,0,msr
507,"After discussing this as a team, we think the right thing for npm to do is to ignore whatever permissions or UIDs are set in the package tarball, and explicitly squash everything to be written with the current user's user ID and umask in all cases _except_ when npm is being run as root without `--unsafe-perm` being overwritten (you should never have files owned by nobody on your filesystem).
This is a mildly tricky bit of work because it requires good tests, and also because we need to make sure whatever API calls the CLI uses don't cause problems on Windows, but this is something we plan to address within the medium-term. If somebody else wants to treat the first paragraph as a rough spec and start working on a patch, that would be very welcome!",0,0,msr
508,"Did this issue get lost (honest question, no sarcasm)?
In my opinion it needs lot more love as the possible security implications could be quite catastrophic on a multi-user system and this issue is reported for over a year now.
The worst case scenario is that such directory results in the possibility to replace its content by a non-root user with evil code that may me executed by another user (including possibly root).
What I observed (even with newest npm 3.10.9) that it **sometimes** creates node_modules directories with permission 777. when doing the `npm install` multiple times the results vary, most of the time it results in 755 but sometimes in 777). This seems to have nothing to do with the source tarballs content (retrieved from registry.npmjs.org) but a more general issue. As mentioned, its not deterministic and the tarballs definitively don't contain any files/directories with such permissions.
This problem was observed while creating packages for a Linux distribution and boiled down to finding this ticket.",0,0,msr
509,"We're closing this issue as it has gone thirty days without activity. In our experience if an issue has gone thirty days without any activity then it's unlikely to be addressed. In the case of bug reports, often the underlying issue will be addressed but finding related issues is quite difficult and often incomplete.
If this was a bug report and it is still relevant then we encourage you to open it again as a new issue. If this was a feature request then you should feel free to open it again, or even better open a PR.
For more information about our new issue aging policies and why we've instituted them please see our [blog post](http://blog.npmjs.org/post/161832149430/npm-the-npm-github-issue-tracker-and-you).",0,0,msr
510,"@othiym23 @isaacs @iarna
can you please reopen and revisit this issue, npm 5.6.0 still randomly ends up creating node_modules directories with 777 that contain code.
PS: This npm-robot that auto-closes a security issue because nobody replied is quite damaging",0,0,msr
511,"Aug 20, 2015
O I am laffin.",0,0,msr
512,"Issue not fixed in 30 days?
Must be gone!",0,1,msr
513,"Not treating security seriously, are we?",1,0,msr
514,"Yeah, this seems to have gotten swept up -- the bot shouldn't have just closed the issue like that.
There was a step forward and a step back on this over the past year, and we haven't touched the issue since. The `patch-welcome` tag continues to apply, so if you think writing code is a more worthwhile endeavor than snarking on foss issue trackers, we super welcome your contributions!",0,0,msr
515,"The current default is to display end caps on error bars.
![errorbar_demo_features](https://cloud.githubusercontent.com/assets/2631586/9798244/bde5a546-57cd-11e5-89c8-e37e225a301c.png)
I argue that end caps are not useful as visual cues for most plots. They are seriously harmful in ""busy"" plots with many data points, where error bars start to overlap and the end caps start to cross other error bars.
Tufte teaches us that we should maximize the data-to-ink ratio. End caps do not provide any additional information over the error bar, so they should not be there by default. End caps are useful to indicate a secondary interval, like the systematic uncertainty of a point, a beautiful example is Fig. 13 in this paper:
http://arxiv.org/pdf/1409.4809v3.pdf
I recently introduced a patch to make the default length of these end caps part of the configuration, so that they can be turned off by default (by setting errorbar.capsize to 0 in matplotlibrc). Please make this the new default. :)",0,0,msr
516,"As a user (not yet a contributor), I'm not a fan of this change. I'd guess that >90% of the plots I see in astronomy have end caps, so that seems like the natural default choice.",0,0,msr
517,"fergalm's argument in favour of end caps is not a very good. Here are some counter-arguments:
1) I'd argue that the new defaults should be based on proven design principles, not on what's common. If we chose based on common use, then we should also keep jet as the default colormap, because ""everybody"" is using jet.
2) matplotlib is popular in the astronomy community. I wouldn't be surprised if endcaps are common, exactly because they are the default in matplotlib. So unless we change the default to something that is better, fergalm's argument will remain a tautology.
3) No information is lost when end-caps are removed, but visual clarity is gained in all plots, and especially in busy plots with many points and nearly overlapping error bars.",0,0,msr
518,"I would also be against removing caps as well because it isn't actually
""extra ink"". If you think about it, the lines are really the extra ink.
Those lines don't communicate anything useful except to connect the end
caps to the mean, which could easily be accomplished by having grid lines
(which is another proposal). Endcaps are also useful in projector
situations because long, thin, vertical lines aren't always very visible in
conference rooms.
On Thu, Sep 17, 2015 at 2:25 PM, Fergal Mullally notifications@github.com
wrote:
> As a user (not yet a contributor), I'm not a fan of this change. I'd guess
> that >90% of the plots I see in astronomy have end caps, so that seems like
> the natural default choice.
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/matplotlib/matplotlib/issues/5047#issuecomment-141176072
> .",0,0,msr
519,"> 1) I'd argue that the new defaults should be based on proven design principles, not on what's > common. If we chose based on common use, then we should also keep jet as the default > colormap, because ""everybody"" is using jet.
""What's common"" is a great design principle. It's also known as the principle of least surprise. I suspect most people will be surprised if they try to plot an error bar and don't get end caps by default
> 2) matplotlib is popular in the astronomy community. I wouldn't be surprised if endcaps are common, > exactly because they are the default in matplotlib. So unless we change the default to something > that is better, fergalm's argument will remain a tautology.
The current matplotlib style of drawing error bars mimics a style that pre-dates matplotlib by many years. For example, pgplot draws end caps, and it was created in 1983.
> 3) No information is lost when end-caps are removed, but visual clarity is gained in all plots, and > especially in busy plots with many points and nearly overlapping error bars.
In sparse plots the end caps draw attention to the error bars, so I think visual clarity is lost by removing them.
There's a lot of ""I think"" and ""I like"", because ultimately this is an aesthetic choice. But my vote is against.",0,0,msr
520,"Always remember the style module so if you don't like the defaults it is very easy to override the ones you don't like.
I think a 'tufte' style sheet would be a worthwhile thing to have even if it is not everyone's cup of tea.",0,1,msr
521,"> ""What's common"" is a great design principle. It's also known as the principle of least surprise. I suspect most people will be surprised if they try to plot an error bar and don't get end caps by default
The principle of least surprise is a good one, I agree, and the change I am requesting is not very surprising.
On the other hand, ""What's common"" is a terrible design principle. All progress is about change, not about doing the same thing all over. Do you have an iPhone? Think about why they are so popular. Before we got them, nobody could imagine that phones and software could be that intuitive and playful. Now we cannot imagine how we could live with the clunky interfaces we had before. iPhones were different in a better way, and that's why they took over. I don't want to be dragged into an argument about commonness and the ""majority of users"", because it is not the point.
> I would also be against removing caps as well because it isn't actually ""extra ink"". If you think about it, the lines are really the extra ink.
The command is called ""errorbar"". It draws error _bars_, you can hardly call that the ""extra ink"". If the lines are too thin, they can be made bigger. This is again not the point. I am also not against end caps in principle, as shown in the paper cited initially. The end caps alone (without a bar) are very useful to indicate a secondary uncertainty, typically a systematic uncertainty, in addition to a statistical uncertainty indicated by the error bar.
The default changes are a great chance for matplotlib to go forward. I offered my arguments, and I can continue to defend them, but I respect the decision of the higher-ups whatever they decide. As a friend and promoter of matplotlib, I am just trying to contribute in a positive way.",0,0,msr
522,"Here a direct comparison with thicker lines instead of caps:
![bla](https://cloud.githubusercontent.com/assets/189880/9970616/6a98ea80-5e56-11e5-97e3-ce2fbf53c96f.png)",0,0,msr
523,"For what it's worth (and I agree it's not worth a whole lot), I did an unscientific poll of the astronomers in my office. I asked ""If you were starting to use a new plotting package, would you prefer if the plot error bar function plotted endcaps by default or not"". There were 6 votes in favour of end caps by default, and 2 vote against. One of those two voting against changed their minds after discussing it with me, but it's probably fairer to record their initial vote.",0,0,msr
524,"In addition to the nice comparison from Tillsten, here is an example of a more busy plot. It visually compares two Gaussian distributions. I used the standard settings for the plot on the left and alpha=0.5. For the plot on the right, I used capsize=0, linewidth=2, alpha=0.5.
Plots with a high density of data points like this are typical in my area of research, high energy physics and astroparticle physics. With endcaps, the plot looks more messy and are harder to read. Our brain merges all these thin lines into a kind of blur. Look at the points near the maximum, for example. Without the end caps, the points stand out more, and the two distributions are easier to separate by eye.
![figure_1](https://cloud.githubusercontent.com/assets/2631586/10051780/4d8ec330-61f1-11e5-9986-b6aa416ae1e8.png)",0,0,msr
525,"I'm convinced. Losing the endcaps would make for a more functional as well as aesthetically pleasing default. By emphasizing the precise ends, the endcaps are visually misleading; errorbars are _rough_ estimates of a range, typically based on arbitrary cutoffs in an assumed statistical distribution. It would make more sense for them to fade out at their ends than to have them capped by lines; but that's getting too fancy. A simple line is a reasonable compromise.",0,0,msr
526,"We should probably pick a default line thickness and marker size that would
result in properly centered lines. This has actually been a long-standing
pet peeve of mine for errorbars (you can see the non-centered-ness in the
examples posted here).
On Wed, Sep 23, 2015 at 2:05 PM, Eric Firing notifications@github.com
wrote:
> I'm convinced. Losing the endcaps would make for a more functional as well
> as aesthetically pleasing default. By emphasizing the precise ends, the
> endcaps are visually misleading; errorbars are _rough_ estimates of a
> range, typically based on arbitrary cutoffs in an assumed statistical
> distribution. It would make more sense for them to fade out at their ends
> than to have them capped by lines; but that's getting too fancy. A simple
> line is a reasonable compromise.
> > —
> Reply to this email directly or view it on GitHub
> https://github.com/matplotlib/matplotlib/issues/5047#issuecomment-142682505
> .",0,0,msr
527,"Yes, I noticed that too. I presume it is caused by snapping to pixel boundaries. Unfortunately, I don't see how that can be solved via the defaults. Sizes are in points, and the translation to pixels depends on dpi, which can be changed at any time. We would need a way of ensuring that anything subject to snapping gets expanded or contracted to a width that is an odd number of pixels. Or something like that. attn: @mdboom",0,0,msr
528,One think i don't like about thicker lines is that this leads to more visual weight on the points with the biggest errors. Probably the best would increase the alpha with the error size :),0,0,msr
529,"what purpose do error bars have?
- for categorical x axes there are violin plots that better show distributions. another alternative would be to use vertical gradients instead of hard ends (the intensity of the color would be the probability)
- for continuous x axes, you should use a line plot instead of individual dots and correspondingly a shaded error region instead of the error bars.",0,0,msr
530,"flying-sheep: this is starting to get somewhat off-topic, but violin plots are not the ultimate answer to everything. I would argue again with Tufte: why use more ink when the same information can be represented with less?
Simple error bars are good indicators if:
- the represented probability distribution is normal and everybody knows it; then a central value (point) and a simple +/- 1 sigma interval (error bar) summarizes the information perfectly
- the represented probability distribution is not known, e.g. for systematic errors that are only estimated as a kind of upper limit
A histogram with Poisson uncertainties on the individual bins is a standard case that is well represented by errorbar.",0,0,msr
531,"I have to say that I find this change simply horrible. It removes the information where error bars end and thus renders them unusable without the caps. Consider the following example:
![without_caps](https://user-images.githubusercontent.com/22542812/45039484-04e2f100-b064-11e8-9be0-704e66d698a9.png)
![with_caps2](https://user-images.githubusercontent.com/22542812/45040016-3f995900-b065-11e8-8390-102fdadfb454.png)
The first plot indicates that the errors of line1 are too small to be seen (hidden behind the marker) and thus negligible. The plot with caps however reveals that this is certainly not true. The vertical lines are arguable only visual clutter however they are a guide to the eye to find the actual errors.
Obviously I can just change the settings (after finally finding out these things are called caps). I find this change however very dangerous, as in a less obvious case I am tricked into reading the plot wrong. This should never happen! Thus, I would argue to make the caps bigger than the actual marker by default.",0,1,msr
532,"@DerWeh I'm sorry you had this issue, however it is unlikely that we will revert this change. The use cases for Matplotlib are very broad and a stylistic choice that makes sense in one field (or with one set of data) does not make sense with another. This issue appears to happen when 1) you have discrete x values shared across all of your datasets 2) the y values are almost identical 3) the error range of the first set is strictly less than the second at all points. If any of those things were not true, we would not be having this conversation ;). You might also consider setting the alpha of each of the artists to 0.5 so you can see when they overlap. I would argue the real bug here is the interlaced z-order layering of the markers, errorbars, and caps...",0,0,msr
533,"@DerWeh The standard solution to your plotting problem is to add slight offsets for two sets of points. Shift the orange points a pixel to the left and the blue points a pixel to the right. Then they are very nicely comparable. Furthermore, if you insist on caps, although they are intentionally absent in most high-quality scientific papers in my field, you can still override matplotlibrc. Just set the capsize to 3.",0,0,msr
534,This is a kind of solution: https://stackoverflow.com/a/49838455/2803344,0,0,msr
535,"I lost this argument a number of years ago, but I will say I have yet to meet anyone who thinks the default is the better choice.",1,0,msr
536,"the overlay problem is just a symptom of another problem with that plot: directly overlaying errorbars is not something you should ever do. moving them slightly to the side is how that plot should have looked in the first place.
there’s quite a few people who think the new default is better in this very thread.
but to be fair, matplotlib doesn’t make it easy to do the right thing. `hist` automatically staggers the bars, but you can’t do the same for other kinds of plots.",0,0,msr
537,"> there’s quite a few people who think the new default is better in this very thread.
But none of them I've ever met. The people I work with who've expressed an opinion all dislike it. But I'm not going to relitigate it.",0,0,msr
538,"well, physicians and physicists also preferred the jet colormap, because it was ubiquitous in their respective fields. studies show that it however decreases their performance in interpreting plots. what people prefer is never a good argument. it’s at most a hint at what *might* have proven itself through experience",0,0,msr
539,"I agree with @flying-sheep. You need objective rational arguments for your position. The new defaults were chosen so the average plot looks better. There are principles and rational arguments behind this decision, especially ""increase the data-to-ink ratio"", https://infovis-wiki.net/wiki/Data-Ink_Ratio.
You can still change the default to whatever you prefer, on a plot-by-plot basis and even permanently, by changing your `matplotlibrc` file.",0,0,msr
540,"I don't want to reopen the discussion here, I will just accept that matplotlib makes horrible choices and will always keep the overhead in my code, to always remember it.
@HDembinski The data-ink ratio is actually an argument against this choice. The cap contains the complete information, namely how big the uncertainty is. The error line itself contains no information whatsoever. It is solely a guide to the eye, to find the end more easily. What I really want to use this answer for, is to strongly emphasis that the defaults should not focus on making nice looking plots but unambiguous ones. Aesthetics is to a certain point always up to the beholder, and he is encouraged to change the design in a way he finds pleasing. However, wrong information should never be conveyed. That is why it was so important to change the default color map. This belongs to me in the same category.
The plot I showed was exactly such an example (and it is a real-life example). I tried different algorithms and suddenly my plot showed me that algorithm 1 is orders of magnitude better, as there was no more visible error in comparison to plot 2. You can now say, just change the design how you like it. But that just makes it even more dangerous for me, as I might forget about the terrible choice matplotlib does. I go to a different machine and might do the same error again and might lose a workday if I am not careful.
Thus, I just plead, focus that the plots represent the information. What you do afterwards to make the plots look better doesn't bother me. If I want a good-looking plot, I will change it anyway. But never misguide me, when I just want a quick representation of my data.",0,0,msr
541,"W/o wading through the religious war above, I think this behaviour would be an obvious one to expose via an rcParam if it’s not already. If it is an rcParam already, then I’m not sure what the problem is.",0,1,msr
542,"Like I said, I will live with it. But I still consider it extremely harmful behavior. I don't care about aesthetics, first the representation has to be correct.
The real life example is shown [here](https://github.com/matplotlib/matplotlib/issues/5047#issuecomment-418405715). I compared to algorithms, took a quick look, and saw that algorithm 1 is orders of magnitude better, as the error is not visible with bare eye. Thus, I move on using algorithm 1. This is what I consider harmful default behavior, as the default plot gives me wrong information.
Of course, I can change the configuration. But this only increases the problem, I do some quick analysis on a different machine and forgot that I have to change the behavior. The result: I draw wrong conclusions and lose in the best case some hours till I realize my error. It is basically the same issue matplotlib previously had with the color map defaulting to jet.",0,0,msr
543,"This thread is getting contentious and needlessly personal.
It is the case that this behavior can be controlled both by an rcparam (for global control) and via a kwarg (for per-call control). It is also the case that we are not going to change the defaults.
Given those three things I am going to lock this thread as I do not think further discussion is going to be productive.",0,1,msr
544,"As a procedural note, was between ""resolved"" and ""too heated"" in the lock dialog. People with commit rights will still be able to comment and can unlock the thread.
If anyone has issues with me locking this and either can not (or does not want to) comment here, send me an email.",0,1,msr
545,"There's this thing with the attachment and emoticon buttons that they activate on hover (bringing up the little windows)... and to be quite honest I find that a really weird behaviour.
So I'd just like to ask the others here in the issue-land whether it's just me or they too are bothered by it.
It doesn't seem like a hard thing to change either way (I mean, to activate them on click like every other button/menuitem/whatever).",0,0,msr
546,"+1, it's VERY annoying.
Especially taking in account a wide-spread reading habit to point with a mouse onto a key areas. One tiny move over the last message timestamp and a huge popup blows in eyes.
**Add.**:
Regardless of any ""reading habits"" this ugly popup impertinently pops up during a very generic mouse operation:
1) right-click inside the input field and paste a short (not more then 1 line) text
2) move mouse to the right to click on the ""Send"" button",0,0,msr
547,"I agree with @ralesk and @VitRom . We worked for years with the point and click system, it's really annoying that hovers make the tooltip appears accidently. I was thinking maybe it's better to show the emoticons tooltip as a fixed sidebar, same as this:
![dfgfdg](https://cloud.githubusercontent.com/assets/15929497/11879979/cc1ff232-a4fe-11e5-965e-056c15f4dd5e.png)
This will prevent accident hover thing, and also increases speed in searching and choosing the sticker we want to send.",0,0,msr
548,"Still wishing for the hover popups to be turned into click-activated ones. Any feedback from developers on this, and other users?",0,0,msr
549,I still regularly bump into not wanting to open the emoji popup but managing to and ending up almost clicking an emoji.,1,0,msr
550,"After working a lot with Telegram, I get used to it and now I'm too lazy for clicking on emoji button for opening it! 😄",0,0,msr
551,"A two kind of jokes are here, one with a smile(s), and another one for those who understands that a joke's salt not in the smile(s).
Hover-activation is for the first only.
Er... :-)
PS.
Just curious HOW is this stupid hover should work on the completely click-driven systems like Android phones. AFAIK there's no any mouse nor any other source of ""hover"" events. So WTF how this ""brilliant smart design piece"" decision been placed into production at all?",0,1,msr
552,My biggest problem with all the UI bugs I reported is that nobody from Telegram bothers to respond to it.,1,1,msr
553,#1751 btw,1,0,msr
554,Thanks @stek29. Amazing response time there too.,0,0,msr
555,"The user's list from a group, with less than 100 members, just pops up when I move the mouse to the Back Button when I want to go back to main screen. That's pretty annoying, because I end up viewing a user's profile if I just insta-click.
![image](https://cloud.githubusercontent.com/assets/8980291/24885070/c1ce0fa8-1e4c-11e7-9489-1388c6fec3c2.png)",0,0,msr
556,"@gottesman That's a regression, this dropdown should appear below the top bar (which leads to group profile or back), I'll fix that in the next version, thanks.",0,0,msr
557,I wonder if there's even a point of that popup. I mean going to the group profile gives you a much more convenient user list anyway.,0,0,msr
558,"@john-preston It's way better now in the new Alpha :thumbsup:
![image](https://cloud.githubusercontent.com/assets/8980291/24935824/b2229352-1f24-11e7-9eee-a8f99a557632.png)",0,0,msr
559,"@john-preston but what about this original issue with emoji toolbar? If you afraid to change UX, simply add option to switch between mouse move and click on emoji button.",0,0,msr
560,@VBelozyorov Now you can just use three column mode and not have emoji pan appear on hover event.,1,0,msr
561,"Excuse me, what is > three column mode
I've tried to search, but no success.",1,0,msr
562,@VBelozyorov You can click on the emoji icon near the send button and switch to a three column mode.,1,0,msr
563,"Hmm…
I have v1.0.29 on Linux (openSUSE Leap 42.2)
But I have no send button:
![Version and input field](https://cloud.githubusercontent.com/assets/860208/25097142/ed017dae-23ab-11e7-8d9f-3a5dcf55c85e.png)
Meanwhile emoji toolbar looks like this:
![Emoji toolbar](https://cloud.githubusercontent.com/assets/860208/25097238/62c0876a-23ac-11e7-8dd6-31b48c8ba994.png)
It's quite three-columned for me, but it's still appears on mouse hover on emoji button",0,0,msr
564,"@VBelozyorov I mean click on the emoji icon (near to send button when send button is displayed, when it is not — near to the record audio message button, in the bottom right corner of the screen).",0,0,msr
565,"Sorry, I do not understand - what should happen when I click on it?
What happens now: _hover_ - _toolbar appear_ - _click_ - _toolbar disappear_
And all goes the same way on next _hover_, behavior is not changes.
![vokoscreen-2017-04-17_21-10-37](https://cloud.githubusercontent.com/assets/860208/25098745/6c930ffa-23b2-11e7-8ae7-f5fc5d4ba8c7.gif)",0,1,msr
566,@VBelozyorov Looks like your screen is too small for the third column then :( Sorry. It was supposed to become like this:,0,0,msr
567,![image](https://cloud.githubusercontent.com/assets/17900494/25098912/f3e46472-23b2-11e7-83a2-8f1fc09f65ac.png),0,0,msr
568,"1920×1080 isn't big enough?
Tried with maximized window, of course. (GIF with that resolution too big to upload here)",0,1,msr
569,"@VBelozyorov Ah, I forgot! It is available only in the alpha version right now, you can try it from https://desktop.telegram.org/changelog#alpha-version or wait for the next stable version update.",0,0,msr
570,"It's interesting, but not solution (for me). All those emoji overload window and make message perception more difficult.
Thanks for make this panel optional.",0,0,msr
571,"I stand with @VBelozyorov :) Glad that the emoji toolbar three panel mode is (will be) optional; it certainly doesn't solve the issue for those who use the collapsed (narrow window, single panel) mode. The hover on the bottom icons (or really, everywhere) gets in the way too much.",0,0,msr
572,P.S. Thanks for finally responding to this issue.,1,0,msr
573,"Just wanted to chime in to keep this thread active because this is actually pretty important. A setting to turn off emoji panel open-on-hover is the proper solution in my opinion. I love telegram and will continue to use it, but this happens to me dozens of times per day and gets really frustrating.",0,0,msr
574,"The smileyface is too close to the ""scroll to bottom"" thing and the hover thing always opens and instead of scroll to bottom, your cursor now points to a heart icon which is where the scroll to bottom buttom was before the hover thing appeared over it. Annoying.",0,1,msr
575,I can't count the amount of times I have to try and retry and retry clicking something because the god damned hover smileys popped up.,0,1,msr
576,"I'm here because of the stupid emoji menu that annoys the living daylight out of me. I NEVER use emoji or stickers or gifs, so the only purpose the emoji popup has for me is to drive me insane.",0,1,msr
577,"It is just unbelievable...
The only thing many people here need is a configurable hover timeout for damned emoji button. No bloody UI filosophy or UX ideology. Just one configurable timeout value. What the heck?",0,1,msr
578,How has it taken nearly 3 years to just add a simple 250ms delay on that damn emoji button?,1,1,msr
579,"@S-U-M-1 You're saying ""has taken"" as if it was done.",0,1,msr
580,@YanDoroshenko Exactly.,1,0,msr
581,@S-U-M-1 Erm... So... It's done???,0,0,msr
582,Not that I can see...,0,0,msr
583,"@Kirrrr No, I'm saying ""exactly"" as in it wasn't even done, let alone ""has taken"" time to do.",0,0,msr
584,Showing on click was added to Settings > Advanced > Experimental.,0,0,msr
585,"I use RoR 4.2.5
in migration:
```
t.decimal :price, precision: 12, scale: 2
```
Convenient to use `@value.price?` to check for the value of any. Check for is not nil and not zero.
```
@value.price = nil
@value.price? #=> false
@value.price = 0
@value.price? #=> false
```
All Ok!
But!
```
@value.price = 0.01
@value.price? #=> false
@value.price = 0.9
@value.price? #=> false
@value.price = -0.9
@value.price? #=> false
@value.price = 1.0
@value.price? #=> true
@value.price = -1.0
@value.price? #=> true
```
WHY?",0,0,msr
586,"@Sega100500 Can you print what value it assigns to price when you assign ""0.9""? i.e. ``` ruby
@value.price = 0.9
p @value.price
```
and paste the output here.
I tried above code and its working. I have doubt if it assigns 0 (floors the number) when you try to assign 0.9 (decimal number).",0,0,msr
587,"@anujaware ```
0.9
```
What are you expecting something else?",1,0,msr
588,Then it should return true. I was thinking if its assigning 0 in your case. I am not able to reproduce this error.,1,0,msr
589,"@anujaware Web console:
```
>> p @product.price
=> 0.9
>> p @product.price?
=> false
```",0,0,msr
590,"@anujaware Exactly!
""Then it should return true""
But it return false",1,0,msr
591,@Sega100500 You can debug this in lib/active_record/attribute_methods/query.rb#query_attribute(attr_name) in activerecord gem.,0,0,msr
592,"@anujaware Web console:
```
>> @product.price = 0
=> 0
>> p @product.price?
=> false
>> @product.price = 1.0
=> 1.0
>> p @product.price?
=> true
>> @product.price = 0.9
=> 0.9
>> p @product.price?
=> false
```",0,0,msr
593,"@anujaware ruby -v
ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-linux]
RoR 4.2.5
May be it help ?",0,0,msr
594,"@anujaware BUG!!! in `query_attribute(attr_name)`
```
if column.nil?
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
!value.blank?
end
elsif column.number?
!value.zero?
else
!value.blank?
end
```
Steps:
```
>> @product.price = 0.9
=> 0.9
>> @product.price.zero?
=> false
>> @product.class.columns_hash['price']
=> nil
>> @product.class.columns_hash[:price]
=> nil
>> p @product[:price]
=> 0.9
>> 0.9 === Numeric
=> false
>> Numeric === 0.9 || 0.9 !~ /[^0-9]/
=> true
>> 0.9.to_i
=> 0
>> 0.9.to_i.zero?
=> true
```",0,0,msr
595,"I think this line `Numeric === 0.9 || 0.9 !~ /[^0-9]/` creating the issue. As `===` checks the case equality so when we compare `Numeric === 0.9` it returns `true` as `Numeric` is an ancestor for `Float` numbers. I think the fix would be something like this:
```
if column.nil?
if Numeric === value || value !~ /[^0-9]/
return !value.to_f.zero? if Float === value
!value.to_i.zero?
else
return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
!value.blank?
end
elsif column.number?
!value.zero?
else
!value.blank?
end
```",0,0,msr
596,"@sivagollapalli Not the most elegant solution. But even so.
PLEASE FIX it",1,1,msr
597,"@sivagollapalli ""As === checks the case equality so when we compare Numeric === 0.9 it returns true as Numeric is an ancestor for Float numbers.""
FALSE!
```
>> 0.9 === Numeric
=> false
>> 0.9 !~ /[^0-9]/
=> true
```",0,0,msr
598,Still I am not able to reproduce this issue with ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-linux] and rails 4.2.5. Before having solution. I am not getting why you are not getting that column in column_hash. And why we have Numericality check. Which database are you using?,0,0,msr
599,"@Sega100500 I am not able to reproduce this issue.
I have tried both Ruby 2.2.2 and 2.2.3. Also I double checked with Sqlite3 and Postgresql on both Ruby versions.
```
$ ruby -v
ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-darwin14]
$ rails -v
Rails 4.2.5
```
``` ruby
2.2.3 :003 > pr
=> #<Product id: nil, price: nil, created_at: nil, updated_at: nil>
2.2.3 :004 > pr.price
=> nil
2.2.3 :005 > pr.price?
=> false
2.2.3 :006 > pr.price = 0
=> 0
2.2.3 :007 > pr.price?
=> false
2.2.3 :008 > pr.price = 0.01
=> 0.01
2.2.3 :009 > pr.price?
=> true
2.2.3 :010 > pr.price = 0.09
=> 0.09
2.2.3 :011 > pr.price?
=> true
2.2.3 :012 > pr.price = -0.09
=> -0.09
2.2.3 :013 > pr.price?
=> true
```
Can you create a new Github repo to reproduce this error?",0,0,msr
600,"@anujaware ""Which database are you using?""
MySQL",1,0,msr
601,"@anujaware ""I am not getting why you are not getting that column in column_hash""
Because this SQL:
```
SELECT products.*, contents.*, products.id AS product_id FROM `contents` INNER JOIN products ON products.content_id = contents.id WHERE (contents.visible != 0) AND `contents`.`visible` = 1 AND `contents`.`furl` = 'apple-iphone-5s-16gb' AND `products`.`id` = 191 AND `products`.`category_id` = 312 GROUP BY contents.id
```
to get with related content-page
```
>> @product.class.columns_hash.keys
=> [""id"", ""parent_id"", ""level"", ""position"", ""node_type"", ""user_id"", ""group_id"", ""restrictive_group_id"", ""visible"", ""system"", ""list"", ""no_children"", ""no_images"", ""no_files"", ""key_lock"", ""key"", ""url"", ""furl"", ""alias"", ""name"", ""brief"", ""text"", ""title"", ""seo"", ""keywords"", ""description"", ""special"", ""content_parameter"", ""content_date"", ""created_at"", ""updated_at""]
```
```
>> @product.price = 0.9
=> 0.9
>> @product[:price]
=> 0.9
```
It is VALID COE! But you have not considered this possibility.
columns_hash NOT contain joined columns but access to they - it possible!",0,0,msr
602,@adityashedge Too simple example for ALL possible usages.,1,0,msr
603,"@anujaware Anyway
the code
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
It is BUG! Not correct to check Float numbers.",0,0,msr
604,"@Sega100500 Looks like there is no issue. Just now I have checked with `mysql` and `sqlite3` with `4.2.5` and `ruby-2.2.3`. All my tests are passing. If you still face issue please update below template and submit so that we can check.
Please ignore my previous comments. ```
begin
require 'bundler/inline'
rescue LoadError => e
$stderr.puts 'Bundler version 1.10 or later is required. Please update your Bundler'
raise e
end
gemfile(true) do
source 'https://rubygems.org'
gem 'rails', '4.2.5' #gem 'sqlite3'
gem 'mysql'
end
require 'active_record'
require 'minitest/autorun'
require 'logger'
ActiveRecord::Base.establish_connection(adapter: 'mysql', database: 'bug_test')
ActiveRecord::Base.logger = Logger.new(STDOUT)
ActiveRecord::Schema.define do
create_table :products do |t| t.decimal :price, precision: 12, scale: 2
end end
class Product < ActiveRecord::Base
end
class BugTest < Minitest::Test
def test_price
p = Product.new
p.price = 0.9
assert_equal true, p.price?
p.price = 0.01
assert_equal true, p.price?
p.price = -0.9
assert_equal true, p.price?
p.price = 1.0
assert_equal true, p.price?
p.price = -1.0
assert_equal true, p.price?
end
end
```",0,0,msr
605,"@sivagollapalli See above
1. joined tables
2. bug code for check Float is:
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
Also any questions?",0,0,msr
606,"@sivagollapalli Are you crazy?
```
create_table :products do |t| t.string :price
end ```
At BEGIN
```
t.decimal :price, precision: 12, scale: 2
```",0,1,msr
607,The code in `lib/active_record/attribute_methods/query.rb#query_attribute(attr_name)` is incorrect to check Float value! Why are you trying to prove the opposite in every way?,0,1,msr
608,"@Sega100500 oops, that's my bad. Now I modified column to `decimal` but it didn't change my result. Still my tests are passing. Could you please paste exact code snippets of your project so that it easy to debug?",0,0,msr
609,"@sivagollapalli
Oooooohhhh!
SEE ABOVE!!!
1. joined table
```
SELECT products.*, contents.*, products.id AS product_id FROM `contents` INNER JOIN products ON products.content_id = contents.id WHERE (contents.visible != 0) AND `contents`.`visible` = 1 AND `contents`.`furl` = 'apple-iphone-5s-16gb' AND `products`.`id` = 191 AND `products`.`category_id` = 312 GROUP BY contents.id
```
1. Then `columns_hash` not contains `price` column
2. Then code:
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
Is BUG!
This can be understood?
See the sources, Luk!
`lib/active_record/attribute_methods/query.rb#query_attribute(attr_name)`",0,0,msr
610,"@Sega100500 You can use this repo to reproduce the exact error. It does not occur for a single table(model). What I understand from your comments is that it occurs when you join multiple tables.
https://github.com/adityashedge/test-2.2.3-22424.git",0,0,msr
611,"@anujaware ""when you join multiple tables""
Not only that!
In case when
```
if column.nil?
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
!value.blank?
end
```
if `column.nil?` - is true
The code:
```
if Numeric === value || value !~ /[^0-9]/
!value.to_i.zero?
else
```
is BUG to check Float number
`(0.9).to_i.zero?`
What do you think will be the result?",0,0,msr
612,"@Sega100500 less yelling please.
http://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#create-an-executable-test-case",0,0,msr
613,"@anujaware Pure Ruby code:
```
p (0.9 === Numeric)
p (0.9 !~ /[^0-9]/)
p (0.9).to_i.zero?
```
result:
```
false
true
true
```
And now turn on the brain!",0,1,msr
614,"@matthewd No yelling.
I just point to bug.
Why do I still have to prove anything? Why do I have to write some other tests?
It seems pretty clear, I explained the situation.",0,1,msr
615,@Sega100500 https://github.com/rails/rails/issues/22424#issuecomment-160188274 is the end of this conversation.,0,0,msr
616,"@matthewd
Your point is - only you decide RoR will work correctly or incorrectly.",0,0,msr
617,"Well apart from the unrelated things, is this really an issue? Need some solid confirmation.",0,0,msr
618,"> Your point is - only you decide RoR will work correctly or incorrectly.
@Sega100500 pretty much yes. He is part of the core team and he is more than able to decide how Ruby on Rails will work.
But he didn't closed the issue because you are wrong or not. Let me explain why.
[By our licence](https://github.com/rails/rails/blob/master/railties/MIT-LICENSE):
> THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND
which means that nobody, again nobody, has obligation to fix your issues, no matter how wrong they are. We do because we want to, and you removed all the our desire to help you with comments like:
> PLEASE FIX it
> Are you crazy?
> And now turn on the brain!
At this point I may point you to our code of conduct http://rubyonrails.org/conduct. We believe you are not respecting it and we will not accept you behavior here.
@adityashedge I believe it is an issue but I don't care if it will fixed anymore. If you want to fix it please open a PR, otherwise we will fix when we think it is time.",0,0,msr
619,"@Sega100500 your report which had virtually no context and then you take people to task for not being able to reproduce the problem - even the variable name you use `@product` would tend to make people think that you were using a `Product` model. It's only 12 messages in where you provide the SQL which you have to read closely to infer that you're actually using the `Content` model and then `select`-ing the product columns onto that model. In fact you never even mention `select` anywhere in this thread. When you fail to provide the single piece of information that would allow people to reproduce the issue is it any wonder that the issue would be closed? When you were politely asked to provide exact code examples you just dismissively directed @sivagollapalli back to previous messages - at no point did you ever provide a model definition.
It's only when you provide a bug test example in #22434 that it's immediately obvious what the problem is - this is why we always suggest doing one as it helps both you and us by focussing on the core problem and removing extraneous factors like gems, etc.
On, one other thing - the code is `Numeric === value` not `value === Numeric` - you should check whether something is commutative before abusing them, e.g:
``` irb
>> Numeric === 0.9
=> true ```",0,1,msr
620,"Jira issue originally created by user johnconnor:
It seems that passing the limit to a subquery is not working
```
$subquery = $em->createQueryBuilder()->from('...')->where('...')->setMaxResults(5);
$query = $em->createQueryBuilder()->from('...')->where(
$qb->expr()->in('p.id', $subquery->getDQL())
);
$query->getQuery()->getResult();
```
The query works but the is no specified limit in the resulting SQL.
I am aware that DQL does not support the limits and offsets, so i guess there should be another way to get this working?",0,0,msr
621,"Comment created by @deeky666:
Can you please tell which database platform you are using? Because limiting results heavily depends on the platform used.",0,0,msr
622,"Comment created by johnconnor:
MySql",1,0,msr
623,"Comment created by @deeky666:
Hmmm I am not quite sure if the limit/offset is invoked for subqueries but I don't see why it shouldn't. Also I think this is not a DBAL issue because the limit/offset support for MySQL is the easiest we have on all platform. See: https://github.com/doctrine/dbal/blob/master/lib/Doctrine/DBAL/Platforms/MySqlPlatform.php#L51-L63
The query doesn't have to be modified but instead only the limit clause is appended to the query. Can you maybe provide the generated SQL for that query?",0,0,msr
624,"Comment created by johnconnor:
I think if you try to build any query with QueryBuilder, set a limit to it with setMaxResults then call getDQL method, you should see that the output contains no info about limit.
So if you look at my code example , at $qb->expr()->in('p.id', $subquery->getDQL()), then you will see that the getDQL passes to the IN expression a query which already DOES NOT have limit. So this is the place where any info about limits and offsets gets lost.
So I fail to see what it has to do with any specific db engine,however I can provide the mysql resulting query if you want,though it looked perfectly normal to me,just lacks the LIMIT part.",0,0,msr
625,You any news on this ? please maybe a hack ?,0,0,msr
626,Just to note - it's still impossible to use subqueries with limits.,0,0,msr
627,"New note 3 years later, it is still impossible to use subqueries with limits. Any workaround ?",1,0,msr
628,"@adrienpayen @vladimmi if I had a contribution graph like yours, I would refrain from complaining about a lack of contributions. You've done nothing to fix an issue that directly impacts you, so why would you expect people that are not impacted to do the work for you? Please stop the spamming and start being constructive.
You could provide a failing test case, or use a debugger to pinpoint where the issue is.",0,1,msr
629,"@greg0ire If I was such a tender snowflake, I would refrain from using public services having comments - especially bug trackers. And especially I would at least read ticket before posting anything there - because ""failing case"" was provided 7 years ago in the very first message and ""pinpointed where the issue is"" in the same day several messages later.",0,1,msr
630,"Well then you can move on to the next step: fix it!
If I had to read every thread when people ask for news, I wouldn't be able to make any progress. That being said, the first comment is not really what I meant by failing _test_ case. You can still work on that before attempting to fixing the bug. Make a PR with the code snippet above in a PHPUnit test.
> tender snowflake
Hmmmm… I think the discussion is too heated, don't you agree?",0,1,msr
631,"Jira issue originally created by user pcnc:
Good afternoon,
When hydrating an Embeddable with nullable attributes the result is an instance of the Embeddable class , this is obviously correct and expected behavior. If all the attributes are null the hydrator will still return an instance of the class with all of its properties null , even if I persist and flush my Entity with the Embeddable being set as null . For clarification :
```
class MyEntity
{
protected $myEmbeddable;
public function setMyEmbeddable(MyEmbeddable $myEmbeddable = null)
{
$this->myEmbeddable = $myEmbeddable;
}
[...]
}
$newEntity = new MyEntity();
$newEntity->setMyEmbeddable(null);
$em->persist($newEntity);
$em->flsuh($newEntity);
```
Calling $newEntity->getMyEmbeddable() will return an instance of the MyEmbeddable object with all of it's attributes set to null .
I expected $newEntity->getMyEmbeddable() to be NULL . Can someone clarify is this is expected behaviour ? In case it is , how can I achieve what I'm looking for ? Best regards",0,0,msr
632,"Comment created by eugene-d:
See https://github.com/doctrine/doctrine2/pull/1275",0,0,msr
633,This is actually really confusing definitely when you combine it with the [docs](http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables),0,0,msr
634,I must say I find this rather a bug than an improvement... agree with @boekkooi regarding the docs...,1,0,msr
635,Can anyone confirm if this is a dupe of #4670 and #4568?,1,0,msr
636,"well this _is_ #4568; and #4670 is rather a duplicate of #1275, both actually ""features"" or ""improvements"".
This here however is a Bug because embeddables whose all properties are nullable and being null hydrate to an empty object when being stored as null. This is misleading (saving != retrieving) and not according to the docs.",0,0,msr
637,"@afoeder the order of the tickets is scrambled due to the fact that they were imported from Jira in December, heh",1,0,msr
638,"Hi @Ocramius Has there been any further discussion on this topic? We've just hit into the same problem as described here on our first Doctrine project. Let me know if there's anything we can do to help — provide usage examples, code samples, discussions, etc.",0,0,msr
639,"@Harrisonbro as it currently stands, doctrine will not support nullable embeddables. That functionality may be implemented later, by implementing embeddables as hidden one-to-one records.",0,0,msr
640,OK. Is there any way for us to implement this on a case-by-case basis (eg. by hooking into the hydration process of an embeddable somehow) so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? Obviously we're not keen to allow value objects to be instantiated in an invalid state and then check an `isValid` method.,0,0,msr
641,"> so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? Then just use the lifecycle system to (de-)hydrate VOs on your own, no?",0,0,msr
642,"> Then just use the lifecycle system to (de-)hydrate VOs on your own, > no?
Do you mean lifecycle callbacks — maybe `postLoad` — as shown in http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/reference/events.html#lifecycle-callbacks? Looks like those only work on entities, not value objects, as far as I can see? Eg. if I used `postLoad` an embeddable will already have been hydrated with invalid data (if the data in the database is all `null`, for example). Alternatively, if I move the VO properties onto the entity directly I’ve lost the nice encapsulation that embeddable so usefully provides (eg. if I had a `Product` entity with a `SalePrice` with 2 properties, `value` and `currency` I’d have to move those 2 properties onto the entity. Whilst I could then have those properties be private and do an `is_null` check for those 2 properties before instantiating and returning the VO from `getSalePrice() : SalePrice { … }` it does rather compromise my entity.
I’m almost certainly missing something here, sorry. Rather new to Doctrine so still learning!",0,0,msr
643,"@Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.",0,0,msr
644,"> @Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.
OK, gotcha.
So in the example I gave — a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` — would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?
If so, that seems workable and means the entity is responsible for checking if the VO should be instantiated (rather than having the VO able to be ‘invalid’ and have to implement an `isValid()` method).
Example code of what I mean:
``` php
class Product
{
private $sale_price_amount;
private $sale_price_currency;
public getSalePrice() : SalePrice
{
if (
is_null($this->sale_price_currency) || is_null($this->sale_price_amount)
) {
return null;
}
return new SalePrice(
$this->sale_price_currency, $this->sale_price_amount
);
}
}
```
Is that something like what you’re suggesting instead of nullable embeddables?",0,0,msr
645,"> So in the example I gave — a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` — would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?
Correct.
Basically, since this is a scenario that Doctrine can't cover right now (because of how RDBMS DDL works), you can just implement it in userland for the few times where it pops up.",0,0,msr
646,"OK great, thanks a lot for the help.",1,0,msr
647,"Thanks, I silently kept up reading your conversation :)
At the moment, my workaround is the following:
```
class Site
{
/**
* @var DomainName
* @ORM\Embedded(class=""DomainName"", columnPrefix=""domain_"")
*/
private $domainName;
public function domainName()
{
return ((string)$this->domainName === '' ? null : $this->domainName);
}
}
/**
* @ORM\Embeddable
*/
class DomainName
{
/**
* Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
*
* @var string
* @ORM\Column(nullable=true)
* @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
* @see https://github.com/doctrine/doctrine2/pull/1275 [2]
*/
private $name;
/**
* Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
*
* @var string
* @ORM\Column(name=""escaped_name"", nullable=true)
* @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
* @see https://github.com/doctrine/doctrine2/pull/1275 [2]
*/
private $escapedName;
public function __construct($name)
{
Assertion::notEmpty($name, 'The domain name must be provided.');
Assertion::regex($name, '/^(?!www\.)([\pL\pN\pS-]+\.)+[\pL]+$/u', 'The domain name ""%s"" must be a valid domain name without the www. subdomain, but might have others.');
$this->name = $name;
$this->escapedName = static::escapeDomainName($name);
}
public function containsSubdomain()
{
return substr_count($this->name, '.') >= 2;
}
public static function escapeDomainName($name)
{
return preg_replace('/\./', '-', $name);
}
public function __toString()
{
return (string)$this->name;
}
}
```",0,0,msr
648,"I like that approach, @afoeder — you still do an `is_null` check in the entity's getter method (`Site::domainName()` in your case) but you can still use an embeddable rather than having to hydrate your VOs yourself.
I suppose the major downside of your approach is that you do still have a VO in an inconsistent state, whereas if you don't let Doctrine hydrate the VO as an embeddable you avoid this; a bit more boilerplate & checking code, but you never have a VO in an invalid state.
Really it's just a trade-off between the 2 options. Others reading this should just be aware of the 2 options and their various merits.",0,0,msr
649,"I would have preferred to comment on #1275, but the present issue has the benefit to be still open.
My 2 cents on the sensitive subject of nullable embedded properties:
- when the Embeddable has at least one non-nullable `@Column`, and this field is null in the database, **there should be no ambiguity** and **`null` should be assigned to the embedded property**. Otherwise (currently!) you get an empty, invalid value object that has non-nullable properties set to `null`. IMHO, the current implementation is broken here.
- when all `@Column` in the Embeddable are nullable, there should be a boolean setting in the `@Embedded` annotation that controls whether or not you want an empty value object or a `null` value when all fields are `null` in the database. **Your choice**.
Finally, you only have a real problem when you have a fully nullable embeddable, **and** want to make the distinction between a `null` property and an empty object. People have suggested to add an extra column in the table, which would work, but would add a ton of complexity for what I think is an edge case. To clarify, **I think this edge case should not be supported by Doctrine**.
The previous two bullet points, however, **I would strongly suggest working on them ASAP**. I'll be happy to help, provided that lead developers are happy with the concept.",0,0,msr
650,"Thanks for that clear explanation, @benmorel. I quite agree with your suggested specification of how Doctrine _should_ behave, and that it should be worked on. This issue is the top priority I'd like to see addressed in doctrine. I too would be happy to help out with the development and testing of this, if the Doctrine team agree.",0,0,msr
651,"We came across the same issue, in our domain a `StreetAddress` is optional, but if given, it has to contain all fields. All fields on the `Embeddable` are `nullable: true`, so the DB is working. The domain is ensuring valid state. So I created that listener to make Doctrine load the objects the way the domain contains objects prior persisting: https://gist.github.com/havvg/602055f1488271f68e5bc82f9a828b4d
Well, it only requires knowledge on the embeddable itself, but easy workaround for now. I hope this helps other developers until the issue will be resolved by Doctrine.",0,0,msr
652,"@havvg How to use this workaround?
For example I have User entity with embeddable class Gender. Something like that:
https://gist.github.com/szepczynski/d3028eb9f92fd7aadd08a578c7a92ad3
I know that I need the User entity should have registered postLoad listener NullableEmbeddableListener but I have no idea how to register it. Can you provide any example? I guess that I need somewhere call addMapping?",0,0,msr
653,"@BenMorel did you by any chance work on a PR for this? I'd really hope to have this ""bug"" resolved, but I don't understand doctrine well enough to be able to write a pretty PR for this issue.",0,0,msr
654,"@Evertt Not yet, and I won't until I get the green light from lead developers. I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!",0,1,msr
655,@BenMorel that's too bad and I totally understand! It's sad that huge projects like these at some point seem to slow down to a point that it just seems frozen.,0,0,msr
656,"That's the dark side of open source: projects rely solely on the free time developers can invest in them, and at some point they're just too busy on other businesses and/or family life to carry on with developments.
I, too, feel like Doctrine is slowing down; it's just unfortunate that there aren't enough (available) lead developers to keep up the pace with pull requests: many developers are there to offer their help, but without enough consideration from project leaders, it's just wasted brain processing time.",0,0,msr
657,"@BenMorel the pace did slow down a bit last year, but the last months have been quite active :) Also see #6211",0,0,msr
658,doctrine 2.x is frozen because doctrine 3 is actively developing (I read somewhere post by @Ocramius),1,0,msr
659,@szepczynski except that there's no ETA for doctrine 3 so that could take years for all we know. If it really takes that long it would be nice for improvements to still be added to doctrine 2.,0,0,msr
660,"> If it really takes that long it would be nice for improvements to still be added to doctrine 2.
It would make it a mess to migrate these additions to something completely redesigned. From what I can see in the last dozen releases, doctrine functionality already abundantly covers the 90% of use-case scenarios, so we could even call it ""feature complete"", if it wasn't for some rough edges that you encounter when you explore more shady features.",0,0,msr
661,"@Ocramius I wouldn't call this issue right here a ""shady feature"". I think this is a very essential part of the embeddables system.",0,0,msr
662,"Right, and embeddables have barely been added in `2.5`, and are already removed in `develop` (`3.x`), as their fundamental internal working mechanisms need to be rewritten",0,0,msr
663,"@Ocramius Sorry to pollute this thread, but would the [transaction object](https://github.com/doctrine/dbal/pull/634) and [default lock mode](https://github.com/doctrine/doctrine2/pull/949) fit in 3.0?",0,0,msr
664,"@BenMorel most likely, yes",1,0,msr
665,"> Right, and embeddables have barely been added in 2.5, and are already removed in develop (3.x), as their fundamental internal working mechanisms need to be rewritten
@Ocramius I'm not sure I understand you right. Do you mean they will come back in 3.x after their internal working mechanisms have been rewritten?",0,0,msr
666,"@Evertt yes, but likely as completely rewritten/redesigned.",1,0,msr
667,"@Ocramius Are there ways we can help the development of Doctrine, either v2 or v3? It's a tool we all use so would love to,support development if a can. _(Sorry to pollute this thread but not sure where else to write.)_",0,0,msr
668,"@Harrisonbro https://github.com/doctrine/doctrine2/milestones/3.0
Let's please stop going further OT. If you have a question, make a new issue.",0,1,msr
669,FTR: there is a small 3rd party library that provides a listener for setting embedded entities that are all null to null (the gist that was discussed above): https://github.com/tarifhaus/doctrine-nullable-embeddable,0,0,msr
670,"@BenMorel With all due respect, I have to go r/quityourbullshit on you here:
> I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!
For Doctrine 2, [there are 26 pull requests from you](https://github.com/doctrine/doctrine2/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 2 Open, 2 Closed without merge (one was fixed differently, one would introduce a lot of pain with future pull requests), and 22 merged.
For DBAL, [there are 15 pull requests from you](https://github.com/doctrine/dbal/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 1 open, 2 Closed without merge, and 12 merged. A quick peek into other repositories (common, annotations, bundle, etc.) shows merged pull requests only.
Feel free to point out pull requests that you are waiting to get merged, but please don't say stuff like that without backing it up when other people sacrifice lots of free time to get you free software. Thank you.",0,1,msr
671,"@alcaeus Thanks for investing your time investigating my contributions to this repository.
You have already wonderfully pointed out my unmerged pull requests, you just forgot to mention their opening date:
- doctrine/dbal#634 : opened 3 years ago
- doctrine/doctrine2#949 : opened 3 years and 6 months ago
I did invest quite a lot of time on these two pull requests, and since 2 years I am getting next to no feedback, despite multiple attempts to draw attention from the team.
Yes, I had PRs merged as well (did I ever say I didn't?), but the lack of feedback on these last two blocked my motivation to contribute further to this project for now.
> [...] please don't say stuff like that without backing it up when other people sacrifice lots of free time to get you free software.
I hope I have backed it up to your taste, and rest assured that I know [quite well](https://github.com/BenMorel) what it's like to sacrifice some time on free software.
Cheers.",0,1,msr
672,"@BenMorel the way you made it sound was that people completely disregarded your work, which they don't. That's why I took the time to look through your contributions to see what's going on.
As for the pull request you mentioned, (please keep in mind that I'm not an ORM guy) it looks like it's a fairly large pull request that touches transaction logic in the DBAL, affecting most of what it (and thus ORM) does. Pull requests like that take time to review and evaluate. I'm not trying to make excuses for other people here, I'm just hoping you can bear with the people maintaining it. That said, there is currently a development push going for 3.0, so maybe there's an opportunity to get those pull requests merged.
Open source projects need contributors to move forward and evolve, especially when maintainers have little or in some cases no time for the project. However, writing pull requests is just a small part of that work - most of it is looking at issues, figuring out what's going on, evaluating pull requests and making sure your user base is not going to burn you at the stake because you messed up. That can be very time consuming and tiring, so unfortunately, large pull requests are often the first to stay open simply because of the effort it takes to review and merge them.",0,1,msr
673,"@BenMorel I agree with you. The Doctrine mantra I keep hearing seems to be along the line of ""Well, it kind of works in most cases and changing things is hard so let's not do it"".
They say a picture says a thousand words and I think this picture sums up my feelings about Doctrine right now: ![](https://i.imgur.com/Rl0VmKc.jpg)
Disclaimer: Owners, don't take offense - it's meant in a lighthearted way and is just my personal opinion. I know you work hard on this, and for that I thank you. I just disagree a bit with the general negativity that I personally see towards any major changes. I know you're working on 3.0 but Symfony and others are leaving you in the dust. If there's too much work, consider giving other contributors more rights or bumping the major version more often so that there can be BC breaks.",0,1,msr
674,"@ryall I think that's the nail on the coffin then.
Here's the exit:
![selection_176](https://user-images.githubusercontent.com/154256/35387238-2c99dbda-01cf-11e8-8599-5740938bbc9d.jpg)
Closing and locking.",0,1,msr
675,"i don't know how to make it work; i'm trying to install a project that requires a composer installation. when i try to install the composer i'm having the is error
```
Some settings on your machine make Composer unable to work properly.
Make sure that you fix the issues listed below and run this script again:
The json extension is missing.
Install it or recompile php without --disable-json
The phar extension is missing.
Install it or recompile php without --disable-phar
The filter extension is missing.
Install it or recompile php without --disable-filter
The hash extension is missing.
Install it or recompile php without --disable-hash
The openssl extension is missing, which means that secure HTTPS transfers are impossible.
If possible you should enable it or recompile php with --with-openssl
```
To solve the openssl extension missing problem; i've installed openssl 1.0.1q and link ssl/bin/openssl to usr/bin/openssl; meanwhile openssl is in my usr/local/src/php-5.6.13/ext.
The other missing i enabled them via the ./configure command option with, --enable-filter, --enable-hash, --enable-json, --enable-phar. To be precise you will find the configuration
./configure \
--prefix='/usr/local/src/php-5.6.13' \
--disable-cgi \
--with-libdir='/lib/x86_64-linux-gnu/' \
--with-config-file-path='/etc/php5' \
--with-zlib \
--with-pcre-regex \
--with-mysql \
--with-pdo-mysql \
--with-mysqli \
--enable-simplexml \
--enable-xml \
--enable-mysqlnd \
--enable-pcntl \
--enable-debug \
--enable-maintainer-zts \
--enable-mbstring \
--enable-bcmath \
--enable-exif \
--enable-ftp \
--enable-soap \
--enable-sockets \
--enable-opcache \
--enable-zip \
--enable-embedded-mysqli \
--with-fpm-user=www-data \
--with-fpm-group=www-data \
--enable-fpm \
--enable-session \
--enable-sysvmsg \
--enable-sysvsem \
--enable-sysvshm \
--enable-xmlreader \
--enable-xmlwriter \
--enable-phar \
--enable-json \
--enable-hash \
--enable-libxml \
--enable-filter \
--with-openssl \
--with-curl
I've done the following below before running the ./configure and when i run make && make install i'm having this underneath some long running text that stops then this message appears. the message is this ""No rule to make target `pharcmd', needed by`all'. Stop.""
rm ./configure
rm -rf autom4te.cache/
rm -rf aclocal.m4
./buildconf --force
make clean.
Please how can i solve my problem.",0,0,msr
676,You also need to load the extensions in your php.ini,0,0,msr
677,"Composer has nothing to do with how you compile PHP. It simply points out what it needs/expects. If your compiled version does not offer that, then either recompile, or use any of the common default binaries provided for Linux/OSX/Windows which do contain everything that is necessary.
This issue is more about installing and configuring PHP though, so I'm closing it.",0,1,msr
678,"Composer is garbage this kind of mistake should not happen, error unacceptable.",0,1,msr
679,"Why HTML::Document.parse(""one < two"") returns: ``` html
<html><body><p>one </p></body></html>' ```
?
This method cuts off the part of a text that goes after '<' sign. Because of this, the Ruby on Rails's #simple_format method works the same way: simple_format(""text_before < text_after"") => ""text_before """,0,0,msr
680,"Rails 4.2.4
Nokogiri 1.6.7",0,0,msr
681,"FYI, this works
``` irb
>> Nokogiri::HTML::Document.parse('one > two').text
=> ""one > two"" ```
One thing to bear in mind is that this may well be an issue in libxml2 itself",0,0,msr
682,"Hi,
Thanks for asking this question. Nokogiri uses XML parsing libraries under the hood (libxml2 for MRI, xerces/nekohtml for JRuby) and so we have very little control over how those libraries decide to ""fix"" broken markup.",0,0,msr
683,"Are you kidding me ? Your code does not work on a few different servers with different operating systems and you telling me ""We don't care, the library is to blame"". The ""less than"" sign is a common case, which can occur in any text.",0,1,msr
684,"The ""less than"" sign by itself is not valid HTML or XML markup. There's no spec on how to fix broken markup.
So yes, I'm saying that the underlying libraries, written in C or Java, which are responsible for tokenizing and parsing the documents passed into Nokogiri, and most especially for choosing how to ""fix"" that invalid markup, are beyond the control of Nokogiri.
Lastly, I just want to ask that you keep your tone constructive. I'll point you at our Code of Conduct that's in our [CONTRIBUTING.md](https://github.com/sparklemotion/nokogiri/blob/master/CONTRIBUTING.md) document. It's easy to misinterpret your previous comment as entitled, rude, and judgmental. As a result I've locked this issue down to ensure nobody else will respond to you in kind.
I'm sorry we weren't able to help you in this instance, good luck on future endeavors.",0,0,msr
685,"Final thought, explaining the behavior, is that ""one < two"" is interpreted by both parsers as the string ""one "" followed by an incomplete tag ""<two"".
If you want ""one < two"" to be treated as ""one &lt; two"" (that is, a text string) then I'd recommend escaping it first. Perhaps the Rails team, based on the context of `simple_format`, can selectively apply escaping? You'll have to take it up with them.",0,0,msr
686,"It's just not funny anymore. ![https://twitter.com/_sciwoman/status/659519813564424192](https://cloud.githubusercontent.com/assets/1756909/16565049/1dafe0a4-420b-11e6-8899-0af645ee6781.png)
![https://twitter.com/rjw1/status/2741916767](https://cloud.githubusercontent.com/assets/1756909/16565059/30da5254-420b-11e6-8c9a-585bfc90a30c.png)
![https://twitter.com/krausefx/status/659611440911773696](https://cloud.githubusercontent.com/assets/1756909/16565071/3d0c7228-420b-11e6-851e-aab8583ea8aa.png)
## The sane way - NOPE
```
$ sudo gem install nokogiri
Building native extensions. This could take a while...
ERROR: Error installing nokogiri:
ERROR: Failed to build gem native extension.
current directory: /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ext/nokogiri
/System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby -r ./siteconf20160704-86627-119yyod.rb extconf.rb
Using pkg-config version 1.1.7
checking if the C compiler accepts ... yes
checking if the C compiler accepts -Wno-error=unused-command-line-argument-hard-error-in-future... no
Building nokogiri using packaged libraries.
Using mini_portile version 2.1.0
checking for iconv.h... yes
checking for gzdopen() in -lz... yes
checking for iconv... yes
************************************************************************
IMPORTANT NOTICE:
Building Nokogiri with a packaged version of libxml2-2.9.4.
Team Nokogiri will keep on doing their best to provide security
updates in a timely manner, but if this is a concern for you and want
to use the system library instead; abort this installation process and
reinstall nokogiri as follows:
gem install nokogiri -- --use-system-libraries
[--with-xml2-config=/path/to/xml2-config]
[--with-xslt-config=/path/to/xslt-config]
If you are using Bundler, tell it to use the option:
bundle config build.nokogiri --use-system-libraries
bundle install
Note, however, that nokogiri is not fully compatible with arbitrary
versions of libxml2 provided by OS/package vendors.
************************************************************************
Extracting libxml2-2.9.4.tar.gz into tmp/x86_64-apple-darwin15/ports/libxml2/2.9.4... OK
Running 'configure' for libxml2 2.9.4... OK
Running 'compile' for libxml2 2.9.4... OK
Running 'install' for libxml2 2.9.4... OK
Activating libxml2 2.9.4 (from /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ports/x86_64-apple-darwin15/libxml2/2.9.4)...
************************************************************************
IMPORTANT NOTICE:
Building Nokogiri with a packaged version of libxslt-1.1.29.
Team Nokogiri will keep on doing their best to provide security
updates in a timely manner, but if this is a concern for you and want
to use the system library instead; abort this installation process and
reinstall nokogiri as follows:
gem install nokogiri -- --use-system-libraries
[--with-xml2-config=/path/to/xml2-config]
[--with-xslt-config=/path/to/xslt-config]
If you are using Bundler, tell it to use the option:
bundle config build.nokogiri --use-system-libraries
bundle install
************************************************************************
Extracting libxslt-1.1.29.tar.gz into tmp/x86_64-apple-darwin15/ports/libxslt/1.1.29... OK
Running 'configure' for libxslt 1.1.29... OK
Running 'compile' for libxslt 1.1.29... OK
Running 'install' for libxslt 1.1.29... OK
Activating libxslt 1.1.29 (from /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8/ports/x86_64-apple-darwin15/libxslt/1.1.29)...
checking for main() in -llzma... yes
checking for xmlParseDoc() in libxml/parser.h... no
checking for xmlParseDoc() in -lxml2... no
checking for xmlParseDoc() in -llibxml2... no
-----
libxml2 is missing. Please locate mkmf.log to investigate how it is failing.
-----
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of necessary
libraries and/or headers. Check the mkmf.log file for more details. You may
need configuration options.
Provided configuration options:
--with-opt-dir
--without-opt-dir
--with-opt-include
--without-opt-include=${opt-dir}/include
--with-opt-lib
--without-opt-lib=${opt-dir}/lib
--with-make-prog
--without-make-prog
--srcdir=.
--curdir
--ruby=/System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby
--help
--clean
--use-system-libraries
--enable-static
--disable-static
--with-zlib-dir
--without-zlib-dir
--with-zlib-include
--without-zlib-include=${zlib-dir}/include
--with-zlib-lib
--without-zlib-lib=${zlib-dir}/lib
--enable-cross-build
--disable-cross-build
--with-xml2lib
--without-xml2lib
--with-libxml2lib
--without-libxml2lib
To see why this extension failed to compile, please check the mkmf.log which can be found here:
/Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.8/mkmf.log
extconf failed, exit code 1
Gem files will remain installed in /Library/Ruby/Gems/2.0.0/gems/nokogiri-1.6.8 for inspection.
Results logged to /Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.8/gem_make.out
```
## The use-system-libraries way – NOPE
```
$ sudo gem install -n /usr/local/bin nokogiri -- --with-xml2-include=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/libxml2 --use-system-libraries
$ bundle config build.nokogiri ""--use-system-libraries --with-xml2-include=/usr/local/opt/libxml2/include/libxml2""
$ bundle install
Fetching gem metadata from https://rubygems.org/
Fetching version metadata from https://rubygems.org/
Fetching dependency metadata from https://rubygems.org/
Using RedCloth 4.2.9
Using i18n 0.7.0
Using json 1.8.3
Using minitest 5.8.4
Using thread_safe 0.3.5
Using addressable 2.4.0
Using coffee-script-source 1.10.0
Using execjs 2.6.0
Using colorator 0.1
Using ffi 1.9.10
Using multipart-post 2.0.0
Using gemoji 2.1.0
Using net-dns 0.8.0
Using public_suffix 1.5.3
Using sass 3.4.21
Using rb-fsevent 0.9.7
Using kramdown 1.10.0
Using liquid 3.0.6
Using mercenary 0.3.5
Using rouge 1.10.1
Using safe_yaml 1.0.4
Using jekyll-feed 0.4.0
Using mini_portile2 2.0.0
Using jekyll-paginate 1.1.0
Using jekyll-sitemap 0.10.0
Using rdiscount 2.1.8
Using redcarpet 3.3.3
Using terminal-table 1.5.2
Using bundler 1.12.5
Using jekyll-textile-converter 0.1.0
Using tzinfo 1.2.2
Using coffee-script 2.4.1
Using ethon 0.8.1
Using rb-inotify 0.9.7
Using faraday 0.9.2
Using jekyll-sass-converter 1.3.0
Installing nokogiri 1.6.7.2 with native extensions
Errno::EACCES: Permission denied - /Library/Ruby/Gems/2.0.0/extensions/universal-darwin-15/2.0.0/nokogiri-1.6.7.2/gem_make.out
Using activesupport 4.2.6
Using jekyll-coffeescript 1.0.1
Using typhoeus 0.8.0
Using listen 3.0.6
Using sawyer 0.7.0
An error occurred while installing nokogiri (1.6.7.2), and Bundler cannot continue.
Make sure that `gem install nokogiri -v '1.6.7.2'` succeeds before bundling.
```
## The use-system-libraries way but put it in /usr/lib – NOPE
```
$ sudo gem install nokogiri -v '1.6.7' -- --use-system-libraries --with-xml2-include=/usr/include/libxml2 --with-xml2-lib=/usr/lib
Building native extensions with: '--use-system-libraries --with-xml2-include=/usr/include/libxml2 --with-xml2-lib=/usr/lib'
This could take a while...
ERROR: While executing gem ... (Errno::EPERM)
Operation not permitted - /usr/bin/nokogiri
```",0,0,msr
687,I am facing the same issue. Does anybody have an idea how to solve it?,0,0,msr
688,"@arielelkin I agree that Nokogiri should ""just work"", and in fact it generally DOES just work on Linux and Windows. OSX is the outlier, and it's because of the combinatorial explosions of configuration options, along with the fact that Apple has decided to deliver xcode (and xcode update) configurations that are hostile to developers maintaining non-trivial C extensions.
I'm not sure what outcome you're hoping to achieve with an approach like the above -- rude, disrespectful, entitled -- but watch and hopefully learn as I treat you with more respect than you've treated Nokogiri maintainers.
We believe at this point that the installation tutorial:
```
http://www.nokogiri.org/tutorials/installing_nokogiri.html
```
works for nearly all known configurations of OSX. Did you go through the tutorial and follow all the steps for OSX? If so, that would be useful information to note when opening a support request. If not, help me understand how we can better point people like you at that tutorial? In particular, the xcode bit is really required for Mac users at this point.
If the tutorial doesn't address the problem you're having, then please make sure you provide both the output from `gem install` and also the `mkmf.log` file.
I'll further note that above, you've tried to install three different versions of Nokogiri. I'll also point out that you've got permissions errors even when you sudo, which makes me think you've either got a really weirdly configured system, or else you're trolling.
---
I'd like to note that the Nokogiri Code of Conduct
```
https://github.com/sparklemotion/nokogiri/blob/master/CONTRIBUTING.md
```
asks everyone to be nice and reminds you that Nokogiri maintainers are volunteers, who don't get paid to work on it, and have personal lives and families. Your approach in this issue is offensive and rude, and in all honesty represents most of what I dislike about being an open source maintainer. You expect something for nothing, and heap abuse on volunteers who have spent hundreds of thankless hours working to get Nokogiri to work on OSX.
If you've surmounted your personal communication and anger-management challenges and read this far, I'll ask that if you go through the installation tutorial and it doesn't address the problem you're having, that you open a new issue and respectfully provide enough information to help a maintainer diagnose the issue on your particular system.
I'll be locking this issue both because your request violates the CoC, but also just to preserve Yet Another Disrespectful Request for posterity, and so that when I finally get fed up and ragequit open source, I'll have plenty of interesting Github Issues to point people at.
Finally -- everyone in the Ruby community who saw this issue come in and didn't do anything to police it -- you're part of the problem, too. For every open-source maintainer who quits, y'all wring your hands and are full of regret; but these are the opportunities you miss to prevent that from happening. A community that doesn't police rudeness and disrepect is not a community I'm happy to be a part of.
---
@SUzB - sorry you got caught in the crossfire on this, but you've also made a very unconstructive post wherein the OP claims a problem and shows three very different logs for three different versions of Nokogiri, and you say, ""I am facing the same issue."" No, no you're not.
Please feel free to open a new issue providing enough information for us to diagnose your issue. At the very least, info about your OS, and if you're OSX then we'll need to know more about xcode and will need to see your logs.",0,1,msr
689,"### How to use GitHub
* Please use the 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are interested into the same feature.
* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.
* Subscribe to receive notifications on status change and new comments. **Is your feature request related to a problem? Please describe.**
I think it would be great if Nextcloud could notify the Admin via Email if there are pending upgrades. This would make it easier to deal with upgrades also on instances one is not actively using but rather just maintaining.",0,0,msr
690,"Since Nextcloud 10 we have update notifications delivered by the notifications API. When you have a sync client set up and connected, it will bring the notifications to your desktop.
However I just discussed with one of my colleges today, that it would also be cool to send a reminder email after some days to the admin when the instance is still not updated.",0,0,msr
691,why not just make it auto-update able?,0,0,msr
692,"To me this is very important since as a sysadmin I manage some Nextcloud instances that I don't use myself.
Using the desktop app to know a new upgrade is pending is thus not a solution.",0,0,msr
693,"Alternatively, how about Nextcloud instances that you do use, but not through the web interface or a Nextcloud app? Then you don't see any notifications at all.
i.e. I use Nextcloud primarily for syncing contacts/calendar. This is done through the online accounts in GNOME on my laptop and syncevolution on my phone. Neither of these tell me anything about upgrades.",0,0,msr
694,"an email on pending updates is really important. having such a widespread app, you should add such a important feature to make the web a little bit more secure.
> that it would also be cool to send a reminder email after some days
i am clearly against a delay. the email should be sent instantly, not with some days delay. if there is a security hole, the admins want to be notified (instantly). i assume that updates are only shown to the admin user, so keep in mind, that you require me to work as admin with my sync app to get such important informations.
workaround i found (german text but the code is english): https://spielwiese.la-evento.com/xelasblog/archives/75-Mailbenachrichtigung-bei-verfuegbaren-Nextcloud-Updates.html",0,0,msr
695,"Well, the thing is: if you don't use the admin account for syncing, you might also not have set an email address for it. Also this issue here is just meant to be an additional reminder.
> i assume that updates are only shown to the admin user, so keep in mind, that you require me to work as admin with my sync app to get such important informations.
You are assuming wrong. In the admin section you can actually select which groups get update notifications. By default it's admin only, yes. But if you don't sync with the admin user (like I do), you can also change it to any other group.",0,0,msr
696,"Additionally those update notifications are displayed are displayed as push notifications on android and iOS clients, and also the desktop client shows the notifications.",0,0,msr
697,"@nickvergessen I tend to disagree here. As an admin, I may only use ~~ownCloud~~ Nextcloud using the web UI or just for syncing calendars and contacts. Or I may administer an instance for other people so I don't use it myself at all. Running a client all time just to be informed about updates is a not very comfortable.
--
Edited: Oops. As a defense: This only shows how seamless the migration to Nextcloud was 😉",0,0,msr
698,"Well if you are using ownCloud we can't help you anyway. [/joke]
But well, I do agree that this should be fixed. I'm just saying it's not as bad as people are drawing it.",0,0,msr
699,"It's as bad as we're saying, for the group of people such as myself, who currently receive no notifications, because we don't use the sync app or similar.
Maybe it's only a small number of users, but for those users, it is a moderately serious security issue, as there is no obvious way to get notified promptly about upgrades. Currently, I typically find out about security updates weeks or sometimes even months after they are released, when I just happen to log in to the web interface.",0,0,msr
700,I added https://github.com/nextcloud/server/releases.atom to my rss reader. Edit: I just found https://coderelease.io (send you an email when new release on github),0,0,msr
701,"I wouldn't mind if the fix was exposing a parameter in `occ` that told you if your instance is up to date or not.
I could write a cron job or an icinga2 test against that and get notified.",0,0,msr
702,@baldurmen https://github.com/nextcloud/server/pull/10836 (unfortunately too late for nextcloud 14),0,0,msr
703,"@danielkesselberg of course these links help, but still they show all releases. normally a admin does not care for beta releases or release candidates. the admin wants to know is there a security update for my current installation. we are all flooded with messages from multiple services: emails, newsletter, github, rss feeds, social networks,... so nextcloud should add this feature to help admins to net get drowned in too many unneeded emails and only send an email if the admin should act.",0,0,msr
704,"A perspective that may not have been considered just yet: For the past year I've been wondering what's wrong with my instance that it's not sending me an email when new versions are released. It only occurred to me as I was typing up a question on the forum that I may have misunderstood the function, and to read the notification option carefully and search for it (which brought me here). Others may be letting their instance go unupdated not realising that there's numerous updates available.
The reason I haven't seen the notifications is because I rarely use the web app, like others here. The sync is done on my server, and I access the synced directory over the network.",0,0,msr
705,This would also be useful for update notifications of apps. Sure I also have the sync client and see notifications there. But the workflow of a sysadmin for these kind of things are usually via email notifications.,0,0,msr
706,"I was using the old `owncloud` client and I used to get notified of updates this way, but now that I switched to the `nextcloud` client I don't get those notifications anymore...
I've hacked some icinga2 tests to check for app updates and core updates for now, but this is getting really painful :(",0,0,msr
707,That should actually work quite fine,0,0,msr
708,"> This would also be useful for update notifications of apps.
I want to emphasize this statement, because it seems to be much more important to me. I am currently managing more than 20 Nextcloud instances which I am not using by my own. So I rarely login there. I actually do follow the Nextcloud releases stream, so I know when its time to update my instances. Some of them are even managed instances and therefor updated by another company. But for all of these instances there are different apps active, for some instances there are even other users enabling/disabling apps. So I would need to follow all these release channels and then know which apps are active in which instances. Then login manually to the concerned instances and press the update button. And thats even the case for the managed instances because normally app updates are not included in the management. That is more than inconvenient and its feeling very much unprofessional. A Mail notification with a direct link would help here, because then only the concerned instances would notify. An autoupdater for apps **as an option** (and activity notification) would make this even more handy for a lot of environments.",0,0,msr
709,"I've just seen this and I have to agree, email notification would be really useful.",0,0,msr
710,Since the addition of `occ update:check` you can run a cronjob to alert you by email when there are updates available or use the ouput in a proper monitoring system.,0,0,msr
711,that wont work if you don't have shell access. This is true for managed nextcloud setups.,0,0,msr
712,Why not simply add a checkbox-based choice of whether the specified groups (to be informed about updates) should also be informed by email?,0,0,msr
713,Agree with @ilippert,0,0,msr
714,a checkbox would be perfect!,1,0,msr
715,@fosple @joostvkempen please use [GitHub reactions](https://github.blog/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/). If you comment everyone subscribed to the issue get's a notification.,0,0,msr
716,Any progress on this issue ? Really it could be so usefull to receive these mails,0,0,msr
717,"or at least allow us to config a webhook which is triggered if an update is available, then we can send the emails by ourselfs",0,0,msr
718,I agree that an email notification is very important,0,0,msr
719,"I would really like to see this feature, too. This is just missing IMHO.",0,0,msr
720,Should be solved via https://github.com/nextcloud/notifications/issues/314 now,0,0,msr
721,"I needed a global install of NVM because I have some node based cron jobs and a few legacy applications that are rather picky about which version of node they are able to work on. For a time I worked around this by sourcing the nvm script everywhere but that seems to be a somewhat unmaintainable solution. The requirements I have:
- NVM should just work in non-interactive sessions
- Every user should be able to select a installed version of node (or use ```$ nvm exec```)
- Some users should be able to install newer versions of node
This is the solution I came up with: (shout out to @icecoldphp, for the initial version)
0. I've done this on a debian 8 machine, as the root user
1. Create a group called ""nvm"", ```# groupadd nvm```
2. Add root to the nvm group ```# usermod -aG nvm root```
3. Goto the ```/opt``` directory and create a directory called nvm - Make sure the groupd owner is nvm ```# chown :nvm ./nvm```
- Set the permissions so that the group is allowed to write in there and all file will inherit the group ```# chmod g+ws ./nvm```
4. Follow the [git install](https://github.com/creationix/nvm#git-install) steps using ```/opt/nvm``` as the directory - To make sure the group can also write aliases, cache downloads and install global packages make sure the directories exist and have the correct permissions:
```
# mkdir /opt/nvm/.cache
# mkdir /opt/nvm/versions
# mkdir /opt/nvm/alias # chmod -R g+ws /opt/nvm/.cache
# chmod -R g+ws /opt/nvm/versions
# chmod -R g+ws /opt/nvm/alias
```
5. Using the following snippet create ```/etc/profile.d/nvm.sh```:
```#/etc/profile.d/nvm.sh
#!/bin/bash
export NVM_DIR=""/opt/nvm""
[ -s ""$NVM_DIR/nvm.sh"" ] && . ""$NVM_DIR/nvm.sh""
```
6. Ensure that the script is executable ```# chmod +x /etc/profile.d/nvm.sh```
7. If you want to use nvm in non-interactive sessions as well make sure to source the nvm file in ```/etc/bash.bashrc``` before the line saying ```# If not running interactively, don't do anything``` by adding ```. /etc/profile.d/nvm.sh```.
8. For bash completion (which is inherently interactive ;) add ```[ -s ""$NVM_DIR/bash_completion"" ] && \. ""$NVM_DIR/bash_completion""``` after the section about bash completion.
Every user can select a version of node (as the permissions for public are ```r-x```) and users in the nvm group can install and remove versions of node (permissions for the group are ```rwx```).
My questions are:
- As a developer I know next to nothing about linux, could this be improved, is it bad style, etc? Any feedback is welcome.
- Should this be documented in the NVM README.md?",0,0,msr
722,"nvm is not intended to be global or system-wide - it's per-user, per-shell-session.
Thus, each user account must have its own `$NVM_DIR`. They can certainly share an `nvm.sh`, but I'd recommend they all have their own one of those too.
I would not want to document anything in the readme that encourages people to use nvm across user accounts - there's other tools for that.",0,0,msr
723,"There is of course [n](https://github.com/tj/n) (with [n-install](https://github.com/mklement0/n-install)) which with a coaching could do the same. I'll give that a shot and create a gist of the process, what are the possible other tools, other than say apt-get, brew or some system level package manager which you are usable to manager the version of node in your shell?",0,0,msr
724,"Yes, `n` is the sole system-wide node manager I'd recommend.
However, I'd suggest just installing `nvm` in the cronjob user, and invoking the cronjobs such that `nvm.sh` is sourced.",0,0,msr
725,Yes it is always good to go with what nvm is intended for. per user. Go for per user installation. I have a similar kind of a situation (I used to install node without nvm previously) and did the same.,0,0,msr
726,"> nvm is not intended to be global or system-wide - it's per-user, per-shell-session.
You know, for an util that should eliminate version discrepancies and staff it sure does increase it a lot... I mean I have server that deploys web projects on git pushes via hooks (i.e user = git). But sometimes I need to log in and redeploy the same things manually by invoking the git hooks manually (user = me). And sometimes my colleagues have to do the same (user = foo)... And then there is a process manager (PM2) that should be central for everyone, but it relies on node as well...
And everybody have it's own node&npm. Except root, so trying to sudo yields even less results, i.e ``node: command not found``. Ok, your util is the wrong one for these kind of things, but NPM in it's [official docs explicitely says to use nvm](https://docs.npmjs.com/getting-started/installing-node) to avoid permission problems that unavoidably accompany multiuser usage.",0,1,msr
727,"Yes - the hazard is ""multiuser usage"". In your use case, everyone should be running `node` as the same user.",0,0,msr
728,"I'm writing a PHP wrapper for a NodeJS binary and I have this exact same problem. PHP is running as ""www"" so it **has no home directory**.
PHP can't ""see"" the NVM environment.
I had this exact same problem with getting node running in crontab but at least there I could control the user to be anything.
---
Thank you @AndreSteenveld This is yet another example of open-source project maintainer arrogance. Your use case doesn't matter so jump through these hoops.",0,1,msr
729,"@hparadiz that's pretty hostile. I'm not being arrogant here, nor am I saying the use case doesn't matter - I'm saying that this project explicitly does not support this use case. ""Arrogance"" would be assuming that free labor on an open source project you don't pay for is obligated to support your use case.",0,1,msr
730,"Literally didn't even call anyone out by name other than the one person who helped and I THANKED them. This issue is still a problem. Hasn't been closed. @Spown makes a good point too.
Yes, it *is* a project maintainer's hubris to dismiss people's use cases as invalid. Heck, I do it all the time. Just cause something is free doesn't excuse you from doing things people don't like. So it's free? I'm not allowed to complain about a vital feature being missing? I guess that makes me hostile. But remember for everyone 1 of me who bothered to write something here there's 99 others that find this page, get super annoyed, and move on with their day deciding on one solution or another.
People want to use node, nvm, and npm **system-wide** in a **global** context. This isn't rocket science. Why do people have to install it manually for every user on a system?
Like would it really kill you make a link in /usr/sbin and check the user that ran the command?
Congrats your tool is now an instrumental part of NodeJS development. It's time to level up and make it a system-wide command.",0,1,msr
731,"@hparadiz I would prefer people with that desire and use case to use a different tool, since that's not what nvm is designed for. Just because you want to cut down a tree doesn't mean a pocket knife is the right tool for the job.",0,1,msr
732,"Hey guys,
A bit of heresy from me over here that relates to this thread.
(but let's not launch the Spanish Inquisition (tm) just yet)
We needed properly managed and stable node version on our bespoke in-team-dev machines as well as on some specific live use-cases, and for that have forged an Ansible role to do just that. You may call it a hack, or you may not, it's your free will, but we're using NVM as the drop in replacement for the distro's `node` package, accessible globally. NVM turned out to be heavens apart in terms of stability than any `apt + n` combination we tried. I appreciate the tool (kudos and big thanks to you @ljharb) so it's the solution we incorporated.
For anyone interested (hope you don't mind sharing here bro):
https://github.com/grzegorznowak/ansible-nvm-node
For anyone else, please use the proper user-scoped approach. Whatever floats your servers chaps.",0,1,msr
733,"This problem exists when running node via cron as well since cron doesn't source any bash scripts and therefore won't see node installed via nvm. The solution for now tends to be to source the nvm bootstrap before every cron command:
`* * * * * source .nvmrc; node /my/script.js`
See: https://unix.stackexchange.com/questions/67940/cron-ignores-variables-defined-in-bashrc-and-bash-profile
The _real_ solution is still to install node globally but what do I know? I'm just a silly user.",0,1,msr
734,"@hparadiz there's no need for the passive-aggressive comments. If you want to install node globally, go for it - but nvm isn't the right tool for that job.",0,1,msr
735,"@hparadiz , cron is notorious for that. Be it node or not. You would want to put in full paths to whatever binary you get to use inside a cron.
Thanks to your question, I just realized cron doesn't even see /usr/local/bin, so will update my role to use the most common of paths: `/usr/bin/node`, and if you did similarly yourself then you could use the absolute path to your gulps etc, like:
`/var/lib/nvm/versions/node/v8.11.3/bin/gulp`
and the hack would be completed, I think.",0,1,msr
736,"Your assertion that nvm is the wrong tool for the job is a slap in the face of the paradigm created by BSD systems for the past three decades. If I install a binary on a machine it's expected that the binary is available to the entire system. Not just a single user. If I install ffmpeg on a machine I expect that I can run ffmpeg from my terminal but also from any running process. Expecting a process to source a bash script for every binary is madness. If everyone followed your paradigm the *nix eco system would be way worse off.
Here's [documentation](https://www.freebsd.org/doc/handbook/dirstructure.html) on BSD directory structure. Here's a [pretty good discussion](https://unix.stackexchange.com/questions/11544/what-is-the-difference-between-opt-and-usr-local) on where user level packages should be installed. Sadly nvm follows none of these well documented installation paradigms. If you did we wouldn't be having this discussion.
Furthermore if there are actually two users on a machine that need to use nvm you are duplicating binaries on-disk for each user for absolutely no reason. You could easily store them globally and link them to the user's home directory should you choose to continue to use your existing directory structure. My nvm folder on my Macbook Pro is already 479M with only two versions of node. If I told every developer to install node via nvm for every user I'd be looking at gigabytes of files on-disk for absolutely no reason. I tend to like efficiency and this is not it.
The only reason people use nvm is because node's release schedule is very rapid and apt, yum, brew, and other built in system package managers choose to not update fast enough. For this reason you have people using nvm even though they need node installed globally on a machine.
Which brings me to my next point: node is a run-time binary for running Javascript code. A run-time _should_ be installed globally. This is how literally every single run-time works.
By default when someone claims to have a piece of software that is a ""version manager"" I would expect it to do that on my system, not just within my bash terminal.
You might as well change the description of this project to:
> Node Version Manager - Simple bash script to manage multiple active node.js versions (for the current terminal user in bash compatible shells only).
You have multiple people in this thread asking for this and you claim that nvm isn't designed for this. Which is odd since nvm is obviously downloading node to a directory on the user's machine and managing directory links and aliases for them. Having it be global is a tiny additional feature on top of what you already have. You could simply soft link the directory where node is installed to /usr/local/ and be good to go (if root).
Finally for security reasons Linux [explicitly supports users](https://www.tecmint.com/add-users-in-linux/) (section 6) without a home directory. A common one in Ubuntu is `www-data` for web servers. In this thread you are explicitly saying you won't support this completely valid use case.
I'm not trying to be mean or passive aggressive. What you're sensing is disappointment.",0,1,msr
737,"@grzegorznowak
Yes, cron is notorious but that's only because most people have no idea that the cron running environment is completely different from their terminal bash environment.
Make it tell you what's going on:
```bash
* * * * * (source ~/.bash_profile; node -v; date) >> ~/crontest.log
```
```
$ tail -f ~/crontest.log v8.4.0
Wed Sep 26 20:11:01 EDT 2018
```
You actually do *not* want full paths to the binaries in your crontab. Use [`env`](https://linux.die.net/man/1/env).
You can actually make your JS files themselves executable!
```bash
printf '#!/usr/bin/env node\nconsole.log(process);' > myscript.js
chmod +x myscript.js
./myscript.js
```
And yea... it's pretty dope.
<img width=""780"" alt=""image"" src=""https://user-images.githubusercontent.com/195216/46116560-bfc95f00-c1ca-11e8-9871-bcd1b9d7f375.png"">
tldr; make your crons executables themselves and use env",0,1,msr
738,"@hparadiz that's not actually how BSD systems work. Installing a binary to a location provides the binary *only if it's available in the PATH*, and the PATH can be set per-user, or per shell session, so in fact contextual binary usage *is* the predominant paradigm - assuming that a binary is global everywhere is a slap in the face to how BSD systems work, and shows your ignorance of the same.
That nvm may not follow some conventions is irrelevant; they're *conventions*, not requirements.
I don't agree that a runtime should be installed globally, nor that it is. Many binaries are installed per-user.
That multiple people are asking for (free labor) doesn't mean they're entitled to it; I continue to claim that if you want this feature, YOU SHOULD NOT USE NVM FOR IT. That you're unhappy with that isn't really my concern.
If you're disappointed, I'm sorry for that - but there's no need to take that out on me, or this thread.",0,1,msr
739,"Please see https://github.com/creationix/nvm/issues/1533#issuecomment-303203372 if you have any questions - `nvm` is not designed for, or intended for, multiple users, and will never support the same.
This issue remains open because there's clearly some documentation change that could be made in the readme to make this more clear. A PR to do so is welcome.",0,0,msr
740,"#### I'm opening this issue because:
- [ ] npm is crashing.
- [ ] npm is producing an incorrect install.
- [x] npm is doing something I don't understand.
- [ ] Other (_see below for feature requests_):
#### What's going wrong?
package.json
```
{
""dependencies"":{""some-module"":""~1.2.3""}
}
```
currently available version 1.3.0 & 1.2.4
when perform `npm update some-module`
npm override version in package.json to `^1.3.0`
### supporting information:
- `npm -v` prints: 5.0.0
- `node -v` prints: 8.0.0",0,0,msr
741,"Hm. So, npm4 doesn't do this either afaik (nor does any older version). Usually, the way to do this is to set `save-prefix='~'` in your local project's `.npmrc` (or in `~/.npmrc`).
Having npm autodetect this and preserve this sounds like a nice thing to have. I think it's reasonable to take a PR for it 👍 I'll double-check with @iarna who might have more context about the implications, though.",0,0,msr
742,I understand it. But when doing update it should not change range in package.json,1,0,msr
743,"Had the same issue. (if I understand the original issue correct)
I did ```diff
- ""hapi"": ""16.2.x"",
+ ""hapi"": ""16.3.x"",
```
Run `npm update` and it did
```diff
- ""hapi"": ""16.3.x"",
+ ""hapi"": ""^16.3.0"",
```
npm 4 did not do this and this behaviour is not wanted so it is a bug to me!
npm -v: 5.0.0
node -v: 8.0.0",0,0,msr
744,"Fair enough! Thanks for pointing that out, @AdriVanHoudt",0,0,msr
745,"np, point me in the right direction and I might even try to PR :P",0,0,msr
746,"@AdriVanHoudt ...I should probably take off that tag huh. The more I think about it, the more I realize I've *no idea* how npm4 even did this. @iarna might know but I've no clue :s",0,1,msr
747,"I didn't even saw the label haha
I what do you mean by no idea how? npm 4 just did not edit package.json on update (don't know the internals ofc)",0,0,msr
748,"oh I thought you meant that `npm up --save pkg` would magically preserve this.
Ok yeah, this is back to feature request.
This is actually something someone might put up a PR for, here's the summary: add some logic to [the bit in `deps.js` that calculates the version spec to use](https://github.com/npm/npm/blob/latest/lib/install/deps.js#L273-L280) to default to the specific prefix used by that version, _if there was one_. Forget about the `save-exact` case, since it's expected that people _must_ use `save-exact` to get npm to save as a non-range and I don't think we're gonna change this (and if we do, it should be a separate thing with a separate discussion).
Give that a whirl, and if the patch works, put up a PR? 👍",0,0,msr
749,"Just to clarify my issue (let me know if you prefer a separate one)
I have a package.json with
```
""hapi"": 16.2.x
```
I run `npm i`
It installed 16.2.0
I update my package.json to ```
""hapi"": 16.3.x
```
I run `npm update`
it installs 16.3.0 (as it should)
it updates my package.json to ```
""hapi"": ^16.3.0
```
Why would it edit the package.json? npm 4 does all the sames things except that :O",0,0,msr
750,@AdriVanHoudt try `npm update --save` in `npm@4`. That's the difference.,0,0,msr
751,"@zkat :mindblown: ok that makes sense. 🙄 I would still prefer if it kept the .x notation, is that possible?",0,0,msr
752,"@AdriVanHoudt as per twitter, if you can figure out a good, clean way to do this and you have well-defined semantics for it, we might take a patch (I haven't checked with @iarna but I defer to her if she'd rather not have this)",0,0,msr
753,"@zkat I think I will switch to just using the default notation, it is more correct and works better with the lockfile imo",0,0,msr
754,"@zkat Why does `npm update` default to `--save` at all?
I don't really see the point, since I don't need to change my semver ranges except for breaking changes (e.g. `^1.0.0` => `^2.0.0`) and shouldn't those be manual anyways? 😕 I find the commands in `yarn` to make a lot more sense in comparison:
- `yarn upgrade` will upgrade each package to the latest version specified by `package.json`, without modifing `package.json`
- `yarn upgrade package` will upgrade `package` to `latest`, and save the new version with the default range in `package.json`
- `yarn upgrade package@^2.0.0` will upgrade `package` according to the given semver range, and if needed save the new range in `package.json`
### Example of npm breaking my ranges
I have a package `a` that depends on package `b`.
`b` has three published versions: `1.0.0`, `1.0.1` and `1.1.0`.
For some reason, I do not want to use `b@1.1.0`, so I set my dependencies to `""b"": ""~1.0.0""`
I run `npm update`, thinking that I will get `b@1.0.1`, which I do. But npm will change my dependencies to `""b"": ""^1.0.1`.
The next time I run `npm update`, I will get `b@1.1.0`, which is *not* in the original range!
---
It feels like I will have to use `npm update --no-save` all the time, and also instruct everyone on my team to do the same. 😕 Or is there a better solution?",0,0,msr
755,"@Maistho If you want to stick with `~`, I recommend you set it in your _project_ config with `echo 'save-prefix=""~""' >> npmrc` (in your project dir). That'll set that to default.
As for everything else? Well, that's the result of setting `--save` as the default across the CLI. This is the behavior as it's always been if you'd set `--save`.
`update` is a command we've been intending to overhaul, and we know it's something where the warts start getting more obvious as soon as you start saving. I wish we'd had time to actually redesign it by the time npm5 came out, but we really just didn't have the bandwidth for it, and it seemed better to just get something out there asap and then do a proper redesign. The same goes for `npm outdated`, which is right now tied up directly with update.
The stuff you're saying is super useful and sounds like a really good direction to take the command in. It's good to hear about the different expectations people have of this so we can make sure to write the thing people want this to be.
If you care enough for it, it'd be great to get a more complete RFC filed that describes the `update` that you wish we had and that outlines your needs from it. If that sounds like too much, just hang tight -- we'll be getting to it soon. 👍 Thanks y'all for your commentary, and for your understanding!",0,1,msr
756,"@zkat Thanks for the detailed response :smile: Setting the save-prefix will instead change all `^` prefixes into `~`, which will just replace my problem with a different one (since I want to mix caret and tilde ranges).
I'd the more than happy to write an RFC concerning `update`, but I'm not sure how I would go about it. Do you have any example/guide on how it should be structured?",0,0,msr
757,"Landed here after noticing npm changing my package.json in ways similar to the above. In my case, `npm update some_module` changed the package declaration from `some_module: ^0.1.6` to `some_module: 0.1.8` (dropping the caret specifier). As it turns out this was because I'd added `save-prefix=` to `~/.npmrc`.
This behavior feels pretty broken to me. I added the caret to this particular module as a way of saying, ""take minor updates"". In removing it, NPM effectively reversed that decision, which is categorically wrong.
I believe the problem here is that the`save-prefix` option is given precedence over what is in `package.json`. Instead, `save-prefix` should only apply when installing a new module. For existing modules, the range specifier should not be altered.
Also, given that `package-lock.json` is now a thing, I can see a case for saying `npm update` shouldn't touch `package.json` at all. (But I'm still trying to understand how package.json and package-lock.json interact, so not gonna get on a soap box about that.)",0,0,msr
758,"touching package.json only makes sense on update if you go from `x: ^0.1.6` to `x: ^0.1.8` since then you say `0.1.8` is the new lowest version. In all other cases it feels weird and counter intuitive that update touches package.json. Since you want it to get your modules up to date that is all.
I think an upgrade command which does this (and preserves the notation in package.json) makes more sense.",0,0,msr
759,"Usually my dependency versions are carefully crafted, so I don't really see a use case for `npm update` or `npm update --save` ever, except if you don't care about versions at all.
Is there a shorter alias of `npm update --no-save` or any way I can reset the previous update behaviour?
Btw, the [update docs](https://docs.npmjs.com/cli/update) currently don't mention the npm@5 behavior.",0,0,msr
760,"I think that the changed behavior of NPM is really very dangerous. We very carefully monitor which module is being changed and we always use exact versioning for all modules. The fact that now `npm update` is changing `package.json` is totally a breaking change. It is absolutely unclear why this change was done. No docs, no arguments, ... nothing. 😟",0,1,msr
761,"The use of Tilde is even in the docs. https://docs.npmjs.com/cli/update#tilde-dependencies
IMO this is a bug, not a feature-request...",0,0,msr
762,"> it'd be great to get a more complete RFC filed that describes the update that you wish we had and that outlines your needs from it.
(from https://github.com/npm/npm/issues/16813#issuecomment-306572698)
This is the important bit preventing this from moving forward. This is one of those things where a number of good behaviors are mixing in unexpected ways.",0,0,msr
763,"This is still a relatively major issue, but it seems the trail has gone cold for quite a few months. We want to keep very close tabs on our dependencies, where we mostly use tildes but not for everything. Every time we update, NPM is adding an unnecessary chance for human error should the developer forget to correct the change made to `package.json` before committing the changes. Sometimes it adds carats, sometimes it removes the prefix entirely. Why hasn't this gotten more attention?
`npm -v 5.7.1`",0,1,msr
764,"This was opened 1 year ago tomorrow. How can we get some traction on this?
First, I think the ""feature-request"" label needs to be removed and replaced with a ""bug"" label. This is not a feature request. This is npm update behaving in a way that produces unexpected and dangerous results.",0,0,msr
765,@zkat It looks like you removed the Bug label originally. Can you add it back?,0,0,msr
766,"This is a serious bug. `npm update` must preserve the semantics of the ranges when it updates `package.json`.
It is ok to update:
* `~1.2.3` -> `~1.2.8` * `^1.2.3` -> `^1.4.0`
But it is **not ok** to replace `~` by `^`.",0,0,msr
767,Or maybe update should have strong/weak options. Strong option would update all dependencies to latest and save them with carets in package.json. Weak option would only update to the latest on the existing ranges (and preserve existing tildes).,0,0,msr
768,Just discovered `npx npm-check -u` and `npm-merge-driver`. Looks this is not such a major issue because there are better ways to update.,0,0,msr
769,"> But it is not ok to replace ~ by ^.
Ouch, saw the same in a few of our projects. This is really bad and a regression.",0,0,msr
770,This is why we will move to yarn and pnpm.,1,0,msr
771,This is not a regression. This is the behavior npm has always had when it comes to this. The change is that it's more notable due to auto save.,0,0,msr
772,"I've locked this thread to make it clear that this is no longer the place to have feature request discussions. This is not a bug.
If anyone cares enough to see this behavior changed, please [file a formal RFC](https://github.com/npm/rfcs) with more details around expected behavior. We no longer have the time or bandwidth to discuss more informal, off-the-cuff ideas.
Furthermore, if what you want to do is discuss ideas more openly without a formal proposal, with the understanding that the npm team won't really engage with you about it, you can also post in the [ideas category in npm.community](https://npm.community/c/ideas), which is a great place to brainstorm before filing an RFC.",0,1,msr
773,"Hi,
Leaflet is a very useful component but I'm facing currently a behavior which seems very weird and I could not find a workaround for it.
I'm using Leaflet 1.0.3.
I'm using Chrome Version 58.0.3029.110 (64-bit). The same happens on mobile devices my app is ported to (android/iOS).
I'm running Windows 10 Pro 64 bit
1. I'm adding a marker to the map and I want to affect its ordering by zIndex means:
`var marker = new L.Marker(location,
{
icon: new L.Icon(
{
iconUrl: iconUrl,
iconSize: [size.width, size.height],
iconAnchor: [anchor.leftOffset, anchor.topOffset]
}),
draggable: draggable,
clickable: true,
zIndexOffset: zIndexOffset
});`
In my example I'm using zIndexOffset of 202.
2. After adding the marker to the map (marker.addTo(map)) immediately I see that the marker object changes and get a zIndex of 619. Actually each application run with the same initial zIndexOffset I get a different zIndex on marker object after adding it to the map.
3. I have an event listener on 'mousedown' event and when I inspect the marker object received by the listener zIndex is again different (477). Meaning it has changed even from the arbitrary value set in setp 2.
4. In case I zoom in and out the map the zIndex will change again.
All of this doesn't allow me to set different markers on top of others since step 2 change ruins any logic I may use in my code.
Here's the example of what happens in the [leaflet playground](http://playground-leaflet.rhcloud.com/gexa/edit?html,console,output):
How could this be fixed or maybe any workaround?
Thanks in advance",0,0,msr
774,"This is not a bug, this is behaviour as expected.
You have to keep in mind that the option name is zIndex**Offset**, and **not** `zIndex`. The `zIndex` of each marker depends on the *vertical coordinate of the marker*. Look at this screenshot from https://github.com/Leaflet/Leaflet.Icon.Glyph:
![](https://camo.githubusercontent.com/09ba35cbec1a15aaf2a599be333a85d701f69ead/68747470733a2f2f6c6561666c65742e6769746875622e696f2f4c6561666c65742e49636f6e2e476c7970682f64656d6f2e706e67)
Do you see how you can only see the tip of the bottom-most markers? That's because the `zIndex` of the markers in the bottom row is greater than in the second row, which have a `zIndex` greater than those in the third row, and so on. The `zIndex` is not arbitrary. It grows by 1 for every pixel down. It also gets recalculated on zooming and some view resets.
Thus, the zIndex**Offset** option is an **offset** which applies to the seemingly arbitrary `zIndex`. Thus, if I wanted the center marker to be on top of the rest, I'd set a `zIndexOffset` of at least the height of a marker, and it would look like:
![image](https://user-images.githubusercontent.com/1125786/27128107-a2a6bbb4-50fd-11e7-8244-12efd37cc3d3.png)
> All of this doesn't allow me to set different markers on top of others since step 2 change ruins any logic I may use in my code.
> Here's the example of what happens in the leaflet playground:
If the problem is that you can't set which marker goes on top, *show me an example of how that fails*, because that playground only logs a value, it does not show me markers failing to display on top of other markers. You have felt into the [XY problem](https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem).
If you still need fine control over things on top of things, consider [using map panes](http://leafletjs.com/examples/map-panes/), or post a more obvious example.",0,0,msr
775,"Hi,
Thanks for your quick response.
I've found this behavior right now and from my perspective it's definitely wrong...
Suppose you have only 2 markers on the map, A is above the B in pixels perspective. Then A will always appear below the B in zIndex means. But if I'll drag the A to be positioned below B in pixels perspective and leave it, after this action A will appear on top of B in zIndex meaning. Each marker dragging recalculates its zIndex. This results in incorrect behavior in our app.
What we need is to allow drawing several segments of polylines. Each segment is just several polylines connected while each tip of the segment and each connection of 2 polylines are marked with markers.
Then suppose I have 2 such segments while their markers appear wherever you like on the map in a mixed mode. But what I need is the ability to have one of the segments active and each of its markers shown on top of markers of inactive segment. But in case any marker of the active segment is very close to a marker of inactive segment and the marker of active segment appears below the marker of inactive segment in pixels perspective, it will appear under the marker of inactive segment which is wrong and I have no tool to make it work in a correct way with the logic you provide.
What can be done about it?",0,0,msr
776,"> Each marker dragging recalculates its zIndex.
...and it should definitely reapply the `zIndexOffset`, does it not?
> But what I need is the ability to have one of the segments active and each of its markers shown on top of markers of inactive segment.
> What can be done about it?
As I've already said: [map panes](http://leafletjs.com/examples/map-panes/). You can have the markers for the """"active"""" segment drawn on a different pane, which has a static `zIndex` relative to the other panes. Note that `zIndexOffset` applies to the `zIndex` of the DOM elements *inside* a pane, but you can control the `zIndex` of the parent DOM element, which is the pane.",0,0,msr
777,"Okay, I'll check the panes. It may help me solve the segments problem but then appears a problem with markers of the same segment.
In case 2 markers are close to each other and the one that should be active is below the other that is inactive the same problem will reappear..
So what is your suggestion? Use pane for each marker?",0,0,msr
778,"I don't know. Show me an example of the problematic case (playground or screenshots or anything) and we might be able to have ideas.
But please remind that this is a bug tracker, for bugs in Leaflet. Don't expect us to offer consultancy on how to build X using leaflet, that's out of the scope.",0,0,msr
779,"perhaps use some `marker.on('mouseover', function(l) { l.bringToFront() })` and `marker.on('mouseoout', function(l) { l.bringToBack() })`. I'm not 100% certain names of events are correct, you can maybe check them out.",0,0,msr
780,@themre Unfortunately this solution is not perfect also. Suppose there are 2 markers exactly at the same location from map's perspective but different coordinates. There always be only one marker on top and mouseover event won't bring the marker that's below on top of the one that's on top.,0,0,msr
781,"@IvanSanchez Look at the problem on the picture. The marker with the black arrow is the active marker.
![1](https://user-images.githubusercontent.com/14091163/27130503-297b3b54-510f-11e7-8c39-ccea05ce53d8.jpg)",0,0,msr
782,@lentyaishe And what are the `zIndexOffset`s and the computed `zIndex`es of both?,0,0,msr
783,"@IvanSanchez The active marker: zIndexOffset = 5, zIndex = 239
Inactive marker: zIndexOffset = 10, zIndex = 567
Do you want me to try artificially update the zIndexOffset of the active marker on click and check whether it will reappear on top of inactive one? I really doubt it'll happen.",0,0,msr
784,"@lentyaishe No, I want you to set the `zIndexOffset` of the ""active"" marker to a much higher value than the inactive one (which I'd suggest letting at zero). Try 50 or 100, or 500.",0,0,msr
785,"@IvanSanchez Actually it did.. I've used marker.setZIndexOffset(100) on the clicked active marker (A) that was below and it appeared on top of inactive one (B) but then the click on B made it on top and clicking A again has left it behind. This means that I'll need to recalculate all markers zIndexOffsets on the map to make it work as I need which is too much. Also in our setup we may have dozens of markers on the map. This way we'll reach enormous values for zIndex which also seems not good...
That's the point that in case I could have managed the actual zIndex of markers on the map it would solve everything since I could have just decided that all active objects may have zIndex above 500 and all others will have below that. Then I should have just change values of last active objects zIndexes below 500 and the newly activated one above etc. And zIndexes of markers within a specific segment also could have similar logic, i.e below 550 inactive, above 550 - active. Anyway then it's my problem to make zIndexes work for me as expected. And currently since the map component decides for me the actual zIndex value, it messes all my logic up.",0,0,msr
786,"> Actually it did.. I've used marker.setZIndexOffset(100) on the clicked active marker (A) that was below and it appeared on top of inactive one (B) but then the click on B made it on top and clicking A again has left it behind. This means that I'll need to recalculate all markers zIndexOffsets on the map to make it work as I need which is too much.
No. It means that you have to reset the `zIndexOffset` of a marker whenever it changes state from inactive to active, or from inactive to active.
> This way we'll reach enormous values for zIndex which also seems not good...
A `zIndex` is an integer which can go up to the millions, and we *never* had any issue with high values. We do know that some browsers have a limit on integer CSS properties of 2^23 (see https://github.com/Leaflet/Leaflet/commit/c5172f3088b820325e726f57f0b09b16b1ec498d, https://github.com/Leaflet/Leaflet/commit/17c180e1eff97932c3450447d5d17fda199f7c2c), but `zIndex`es have never been a big problem, because Leaflet recalculates them once in a while on every `viewreset`, so the values for both the CSS transforms and the `zIndex`es are within a manageable range.
In my experience, it's better to learn about the [stacking context](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/Stacking_without_z-index) than to be afraid of misusing `z-index`.
> That's the point that in case I could have managed the actual zIndex of markers on the map it would solve everything [...]
No you wouldn't, because if you're not resetting the `zIndex`es of the markers which transition from active to inactive, you would end up with the same problem.
I mean, if you want to *really* try if your logic would work without Leaflet's management of `zIndex`/`zIndexOffset`, you can manually disable it with something like
```js
L.Marker.prototype._updateZIndex = function(offset) {
// this._icon.style.zIndex = this._zIndex + offset;
}
```
So far, you've been subtly suggesting that Leaflet is the culprit and that your logic is infallible. [Programmers like myself don't react well to this kind of bug reporting](https://www.chiark.greenend.org.uk/~sgtatham/bugs.html#symptoms). I bet you a beer that the problem is in the logic in your side, and that it won't matter if you manage raw `zIndex`es or `zIndexOffset`s.
If you can *isolate* a case where Leaflet fails to raise a marker's `zIndex` when raising its `zIndexOffset`, of fails to lower it, please do share a playground or similar.",0,0,msr
787,"> you can manually disable it
Thanks for the suggestion. I'll try it out. It also gave me an insight on how you implement the layouting.
> your logic is infallible
Not at all. :) I just think that the API should allow disabling the recalculation of `zIndex`es and allow the API customer break his own head on how to implement what he needs.
> If you can _isolate_ a case where Leaflet fails...
The scenario is extremely simple. Use `zIndexOffset` for 2 markers with difference of 5. Since the zIndex that each marker gets from your algorithm is hundreds you may come up with the following:
`zIndexOffset(A) > zIndexOffset(B)` set by the code
but
`zIndex(A) < zIndex(B)` calculated by leaflet
And since `zIndex` is recalculated on each map operation: drag, move, zoom etc. there's no way to get the actual difference in `zIndexOffset` for markers to make it work correctly for `zIndex`es that will be calculated.",0,0,msr
788,"Well, finally I came up with a solution that works fine for me:
Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
Anyway, **thanks a bunch for your rapid help**!",0,0,msr
789,"@lentyaishe I was used this gist an run like a charm. Maybe this can be a Feature Request?
https://gist.github.com/up209d/4c68f2391f2302e510eb81aa8bcd4514",0,0,msr
790,"> Well, finally I came up with a solution that works fine for me:
> Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
> > Anyway, **thanks a bunch for your rapid help**!
I have the same problem as you. Marker's Z-index changes when scaling, but I don't want to change z-index. I want to know your final solution.",0,0,msr
791,"> > Well, finally I came up with a solution that works fine for me:
> > Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
> > Anyway, **thanks a bunch for your rapid help**!
> > I have the same problem as you. Marker's Z-index changes when scaling, but I don't want to change z-index. I want to know your final solution.
This is my final solution.",0,0,msr
792,"The documentation says:
```
By default, marker images zIndex is set automatically based on its latitude. Use this option if you want to put the marker on top of all others (or below), specifying a high value like 1000 (or high negative value, respectively).
```
It should mention that the ""automatic"" setting based on the latitude shall not be greater than N, instead of ""specifying a high value like 1000"".
Regardless, you need to separate them by thousands for each ""level"" or ""step"" if you want to have them show as you want.
Consider their problem, they want those with a lower latitude to show below the others, but you want them to be displayed in levels not considering the latitude.
They cannot really store all the possible offsets you set, unless you pass a list first. **Basically they should say in the documentation that we use a range of 0-N and you should set each level of z-index you wish to display regardless of location as (N+1 * level_desired). You might as well considering N to be 1000, it seems to work but obviously could change.**",0,0,msr
793,"It's not explained correctly in this issue, here's a more understandable explanation.
**zIndexOffset difference of 40000 does not place marker visibly above/on top of another marker that is 10 degrees lower vertically on the cartesian map.**
@lentyaishe use the example I provided in your opening comment of this issue so that people understand the problem.
https://plnkr.co/edit/hyWro1sGgmjbeDPk
**The Google logo should be visually above (per z-index) the Yandex logo.**",0,0,msr
794,"@andrewhodel use the `zIndexOffset` in the marker options not in the icon options:
```
var n = L.marker([30.505, 0.57], {icon: myIcon, zIndexOffset: 80000}).addTo(map);
```",0,0,msr
795,"@Falke-Design does not work.
<img width=""638"" alt=""Screen Shot 2022-02-07 at 4 22 30 PM"" src=""https://user-images.githubusercontent.com/741705/152796083-55234af0-474a-4d07-97ad-e894016f3662.png"">",0,0,msr
796,"It does work, look into the updated sampel: https://plnkr.co/edit/GcnZAG6c5vjPaXIl",0,0,msr
797,"@Falke-Design I think it's simply a sad reality of the layers and how things don't understand beyond them.
I've already confirmed it. It does not work with zIndexOffset with svg icons as shown in the screenshot.
It's all being set with marker_opts{}.zIndexOffset",0,0,msr
798,"@Falke-Design it's very simple to understand that when a DOM element has a z-index of 4231 and shows beneath a DOM element with a z-index of 4220 that something is awry.
The marker layer maximum is 600 and set to never change in leaflet.
It's really easy to read that in the code with `grep`.",0,0,msr
799,Please provide a sample that doesn't work. I already showed you in the current sample from you that it is possible.,0,0,msr
800,"There's nothing but the DOM here, really.
There's no need to explain anything else, if 600 is the containing element limit set statically in the library it doesn't matter what you set them at and that is based on how the layers are applied.
@Falke-Design",0,0,msr
801,"The fact that you would argue beyond the DOM goes to show that you aren't trying to help, only to entrap.
It's simple.
@Falke-Design",0,1,msr
802,It is not always that simple to understand what somebody else tries to explain. I hope for you that you find someone who wants to help you because I will not and I don't think that anyone is willing to help you if you keep that attitude.,0,1,msr
803,"Will you accept inline type hints for the peewee like
```
# type: () -> None
```
as PR?",0,0,msr
804,"No, type hints are, in my opinion, an abomination that has no business being part of the Python language :)",0,1,msr
805,May I at least add peewee's .pyi stubs to python/typeshed repository?,0,0,msr
806,"Anything you would like to do is totally fine by me, as the license for the software is permissive. I just do not want to merge anything into mainline that concerns type hinting.",0,0,msr
807,"@coleifer I'm expecting you will make a deal with static typing fans more and more. I'm partially agree you can do nothing in orm itself and should not, but you should try to understand `static dynamic` fans. Its not about limiting dynamism or to make new java from python, people just want `user.(name/id/group)` in IDE, they are tired to go from view to model definition, to 'google' any functions signature, to debug typo mistakes in runtime. Go has no objects and generics, and any data structure should upcast value to `interface{}`, C is just too low-level. You should check out [typeorm](http://typeorm.io/) and [diesel](http://diesel.rs/), Dart. Static typing was stigmatized long time as a way to make compilable, fast languages like java, cpp. It has different purpose in dynamic languages now.",0,0,msr
808,"> I'm expecting you will make a deal with static typing fans more and more.
No way. Python is a powerful dynamic language. If I want static typing or a strong type system I will use C or go...or hell, Cython. I enjoy coding in those languages, too, but if I'm using Python I'm going to play to it's strengths.
> IDE
Peewee's design goals are composability and consistency -- learn once, apply everywhere -- precisely so that peewee will ""do what you expect"" without needing to check the docs.",0,1,msr
809,"@coleifer I'm not a strong proponent of Static Typing, but the type annotations are extremely useful when developing to help catch bugs. I debug enough Python at runtime, the more I can do to reduce that, the better.
Mypy is optional, you can choose to put annotations on things, for clarity. If the inclusion of these things only ever produces net gain, why not apply them. It's not an either-or camp. The degree to which you add Static Types is varying.
Surely you can see the point of view of why Type annotations may be extremely useful? And in no ways detracts from the power and flexibility of Python by its inclusion.",0,0,msr
810,"Sorry to necro this thread but in my experience I've also noticed that type hints can also add a significant performance boost (sometimes >20%) since Cython can now use them for type inference. Granted, performance benefits aren't _guaranteed_ but they are likely. I see you have Cython support set up for PeeWee so it's entirely possible its performance could benefit from it.",0,0,msr
811,"Hi @coleifer,
You've done a great job with this lib. With or without type hints, it's nice to use.
But if someday you change your mind about type hints, I believe it won't hurt.",0,0,msr
812,"> No, type hints are, in my opinion, an abomination that has no business being part of the Python language :)
As a maintainer of typeshed, a big repository that is all about typing in Python, I think this is a perfectly valid opinion! I felt the same when I first learned about typing in Python. It shouldn't be something that library authors have to do, and it's optional by design.",1,0,msr
813,"This issue was opened nearly 5 years ago. In that time I've had plenty of opportunities to see python's type hinting in the wild. I still believe that it misses the mark, offerning none of the iron-clad guarantees you get from a statically-typed language, while being injurious to Python's inherent dynamism, readability, and simplicity. Since this issue tends to attract drive-by comments of little value, I'm going to close discussion for now.
When I want static typing I reach for a statically-typed language. When I want expressiveness, simplicity and flexibility, I reach for Python.",0,1,msr
814,"##### ISSUE TYPE
- Feature Idea
##### COMPONENT NAME
ansible-vault
##### ANSIBLE VERSION
```
ansible 2.3.1.0
config file = configured module search path = Default w/o overrides
python version = 2.7.13 (default, Apr 23 2017, 16:50:35) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]
```
##### CONFIGURATION
n/a
##### OS / ENVIRONMENT
n/a
##### SUMMARY
ansible-vault decrypt allows the decryption of completely encrypted yaml files, but it will not decrypt vaulted variables in an unencrypted yaml file with encrypted variables.
It would be nice, for CLI purposes, to have decrypt take a partially encrypted file, and give us the decrypted text.
##### STEPS TO REPRODUCE
* create `test.yml` file with single encrypted variable encrypted by `~/.vault_pass.txt`
* ansible-vault decrypt file
```
ansible-vault decrypt test.yml --vault-password-file ~/.vault_pass.txt
```
##### EXPECTED RESULTS
* Expected plain text output with encrypted variable decrypted.
##### ACTUAL RESULTS
```
ERROR! input is not vault encrypted data for test.yml
```",0,0,msr
815,@alikins I believe you look after vault,0,0,msr
816,Would be also good if 'ansible-vault view' worked for such files.,0,0,msr
817,"This might be something that will get covered in https://github.com/ansible/ansible/blob/devel/docs/docsite/rst/roadmap/ROADMAP_2_4.rst#id25
As a user, what would you expect the decrypted file to look like?
First thought is just to replace the !vault yaml scalar with the decrypted text. That probably makes the most sense for 'view'.
For 'decrypt' and especially 'edit', I'm not sure that will be sufficient. For 'edit', the re-encrypt phrase is going to need to be able to figure out which variable values originally came from a vaulted value. Especially if the file is edited significantly (reordering lines for example, or changing the variable name). So the file presented for editing would need to include some markers indicating the text that was decrypted/should be re-encrypted. A couple of ways to do that:
1) Add comments to mark the text, and doing some text manipulation/regexes to replace it with encrypted text in place. Something like:
``` yaml
# START VAULT PLAINTEXT - my_var
my_var: my text goes here
# END VAULT PLAINTEXT - my_var
some_plain_var: blippy
```
2) Add a new yaml type indicating text to be encrypted. Something like:
``` yaml
my_var: !vault-plaintext |
my text goes here
some_plain_var: blippy
```
It would be best if we could yaml parse the input, decrypt the value, serialize the yaml to a file for editing, let user edit it, then yaml parse the results, encrypt the value, and serialize to yaml and save.
But... doing that with the available yaml parser would lose comments and ordering of maps.
So likely some in place string/text manipulations will be required.",0,0,msr
818,"Not going to happen for 2.4, so bumped to 2.5.",0,0,msr
819,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",0,0,msr
820,"Any news when this is planned to be implemented in ansible?
We have lots of passwords as vaulted variables, hence updating\viewing them is troublesome.
I did some script (based on solution, from alikins last post) to at least parse such yml and decrypt every variable to stdout\file to see a decrypted file at once, but this is just a script that is not a complete solution (and it is decrypting only).
UPD: I ended up going thru ansible code to understand how it works with encrypted variables and wrote some tiny script that I can use in my automation jobs with Jenkins. I hope it would be useful for anyone who is waiting for this issue to be fixed.
https://github.com/andrunah/ansible-vault-variable-updater
It would be nice to have this functionality in ansible out of the box.",0,0,msr
821,"I do see this in 2.5.
```
root@ubuntu-xenial:~# ansible --version
ansible 2.5.0rc1 (stable-2.5 36566e62a7) last updated 2018/03/05 13:46:00 (GMT +000)
config file = /etc/ansible/ansible.cfg
configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
ansible python module location = /root/git/ansible/lib/ansible
executable location = /root/git/ansible/bin/ansible
python version = 2.7.12 (default, Nov 20 2017, 18:23:56) [GCC 5.4.0 20160609]
root@ubuntu-xenial:~# cat vars.yaml
ansible_ssh_pass: !vault |
$ANSIBLE_VAULT;1.2;AES256;my_user
31313064366365626535323066613234626234336664333266663161366233396365633063303539
3066363333666236666335656631666663373037643338630a303763363031373337663733326134
38336566366535373561373830386638663635363438333633313536333731646331366138383961
3331346163623661340a663862323337313562376338386539326438323562383136383832376266
31306663393532323761353761353435373432633632626365633734303335633436
nonpass: pass
root@ubuntu-xenial:~# ansible-vault view vars.yaml
Vault password:
ERROR! input is not vault encrypted datavars.yaml is not a vault encrypted file for vars.yaml
```",0,0,msr
822,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",0,0,msr
823,"I poked at this a little yesterday and braindumped some thoughts in code comments at https://github.com/alikins/ansible/commit/603cac4a041a10ec8186617c95ef539a9ece787a
(copied/paraphrased here for discussion)
> Open a file, figure out if it is all vault or yaml with vault strings, edit.
> > if yaml with vault strings, parse the yaml with AnsibleYaml > and secret. Replace with '!vault-plaintext vault-id' and plaintext. > Save, open editor. > On save/reencrypt, reparse the file with AnsibleYaml, get the
> plaintext of the to be reencrypted vaulted string, encrypt it
> (!vault-plaintext -> !vault,
> AnsibleVaultUnencryptedUnicode -> AnsibleVaultEncryptedUnicode).
> > And then, things get complicated... we can't just AnsibleYaml.dumps()
> the data structure out:
> 1. Comments and comment placement is not preserved which > is kind of annoying
> 2. AnsibleYaml can loads things into data structures
> that it can not `dumps()` out.
> Ie, we can't serialize a bunch of stuff we can deserialize.
> > So just AnsibleYaml.dumps'ing the datastructure back
> to a file will usually either fail or do the wrong thing.
> > #2 above is unlikely to get fixed soon if ever.
> #1 is mostly a limitiation of the PyYaml yaml module ansible uses. > Other implementation like Rueyaml can do this, but it is unlikely for > ansible to change this any time soon.
> > So, since we can't just serialize to yaml, we likely need to do some > string manipulation to replace the '!vault ' blob.
> > We would need to know exactly what the before string looked like
> and where in the file it is, and what the new !vault will look like.
> But we don't really know what the new !vault-plaintext > string will look like.
> > For that matter, we don't know if it will be in the same place,
> or if it will exist at all, or if it will be at the same path in the > datastructure after the edit. > > We could limit edit to only try to work in cases where
> those aren't changed. We also have no idea what the
> plaintext will look like.
> > ideas:
> - !vault-plaintext is a compound yaml type, with fields for
> the vault id to use, and for the plaintext. Could also
> possibly include some identifying info for what the !vault
> it replaced looked like. An example:
> > some_var: !vault-plaintext:
> vault_id: 'dev'
> decrypted_from: |
> $ANSIBLE_VAULT;1.1;AES256
> 66393964663765613335633461643334393234346231666665306635323635333137306339356232
> plaintext: |
> The new plaintext to replace decrypted_from with
> > That would give vault-edit enough info to do a reliable job > of replacing the previous content.
> The downside to that approach is that it points out the limitations of the current !vault format. It may also be useful to extend !vault to support getting a data structure with info in it instead of just the plain text scalar. At the moment, I'm not sure if it could do both but it seems possible.
Or could just call the extended info version of !vault !vault-extended or similar. At that point it might be possible to make !vault-extended the default vault blob format for vaulted files as well. ie, instead of
a vaulted file being:
```
$ANSIBLE_VAULT;1.1;AES256
66393964663765613335633461643334393234346231666665306635323635333137306339356232
3533306631646431663239623762366365663137383435380a393139303161383561303336623962
35373663663036333863373666326634616532376335333133326163376136353636633763623739
3736343064326662390a306438356239386665306437646665323836393032393565666136643362
3663
```
It would be yaml something like
``` yaml
--- - !vault-extended:
vault_id: 'dev'
cipher: AES256
encrypted_on: 2018-03-14
ciphertext: |
$ANSIBLE_VAULT;1.2;AES256;dev
66393964663765613335633461643334393234346231666665306635323635333137306339356232
```
ie, more or less like https://github.com/voxpupuli/hiera-eyaml",0,0,msr
824,Would it be possible for the user to tell you which scalars to decrypt - not try to do the whole file?,0,0,msr
825,"Right now the usability of encrypted variables compared to whole encrypted files is rather poor unfortunately. Especially in cases where I quickly need to access an encrypted variable (e.g. a password) I really don't want to google for solutions like https://stackoverflow.com/questions/43467180/how-to-decrypt-string-with-ansible-vault-2-3-0.
It is also a problem for `git diff` use cases (https://stackoverflow.com/questions/29937195/how-to-diff-ansible-vault-changes). Is improving this state still on the roadmap? I didn't find it neither for 2.5 nor 2.6...",0,0,msr
826,"Hi, so I've figured out a way to do this for checking individual values, using a yaml parser, **yq** https://github.com/mikefarah/yq (there's more than one yq project, but I used this). This works with ansible 2.5.2
I have a vars file, with encrypted and unencrypted values, `all.yml`
```
unencrypted_value: 1234
encrypted_value: !vault |
$ANSIBLE_VAULT;1.1;AES256
37316535353565313063353530353539666634363834626664366263666538346131653332353932
3637363030613037316336306466656432353463383230370a396530323164353563363434663238
30336436396264656663663837346162323762333063376631326633356533376566633563386637
6531383261396366640a363339616164333630373730613564646434386364396534653063666238
6131
```
I have a password file, `vault-password`
```
password
```
Using `yq`, I'm able to decrypt the value pretty easily, by selecting the encrypted value and passing it to the decrypt function
```
$ yq read all.yml encrypted_value | ansible-vault --vault-id vault-password decrypt
Decryption successful
secretsecret
```
Hope this helps!",0,0,msr
827,"Thanks, this helps if it is possible to install additional software. I would argue that ansible-vault should also have this functionality built-in.",0,0,msr
828,Yeah definitely that'd be the best option :),0,0,msr
829,"Also related, let rekey work on all encrypted variables in a file. There doesn't seem to be a good way to rekey all the encrypted variables, which makes encrypted variables super cumbersome now that we have to rekey (will end up having script this). Even if it just spits it back out to stdout that'd be a huge help instead of modifying the variables in the file directly.",0,0,msr
830,"Why is this issue still assigned to the `2.5 milestone` when `ansible 2.5` is already release a long time ago ? See #44556 for outdated milestones.
Please reassign to a current milestone, this is a really missing feature imo (especially the lack of rekeying functionality).",0,1,msr
831,It would be nice if rekey worked this way as well. Updating only the encrypted values in a mixed variable file.,0,0,msr
832,"Just giving another thumbs up on this; something like the `yq` solution above works okay and can be scripted, but having the functionality be part of `ansible-vault` itself would make management and re-keys so much simpler, and require one fewer dependency.",0,0,msr
833,"A simple but effective solution would be to keep the existing symantecs of ansible vault encrypt, decrypt and view commands, to detect and encrypt and decrypt values of complete files.
For existing users of encrypted files, it would be trivial to convert to the enrypted values. It could even be considered best practice is to keep encrypted values in files named such as secrets.yml, to make it easier to spot accidently unencrypted secrets.
During the encrypt phase, it would convert any unencrypted values to encrypted values. This would allow users to very simply add new values just by editing the ""secrets.yml"", test as required, then run the encypt command. Users would be able to enforce or check encryption by git hooks or similar.",0,0,msr
834,"That solution would be simple, but likely not enough. For example every variable can be encrypted with a different secret/vault identifier. Also encrypted and unencrypted variables can be mixed.
I'd still like to have a way at least to decrypt all variables belonging to a vault ID transparently using `ansible-vault`. Seriously, this is a usability problem since Ansible 2.3! This makes it nearly impossible for me to use vaulted variables, since being able to run `git diff` on changes is important.",0,0,msr
835,This is still a problem with Ansible 2.8... A solution would be really appreciated!,0,0,msr
836,"For others looking for a quick solution I created this script: https://gist.github.com/steffann/240d4170e45aa3cf7cf0df5e9beaf0ba
It uses [ruamel.yaml](https://yaml.readthedocs.io/), which preserves ordering, comments etc in the YAML file. Great when depending on decent git diffs etc :)",0,0,msr
837,maybe a bit unrelated but I like how [sops](https://github.com/mozilla/sops) does it.,0,0,msr
838,"Running into this issue again and it sucks. Please guys, this issue has been open for almost 2 years now and for people who really use ansible-vault, this is a major pain the butt.",0,1,msr
839,"same issue here, we need to unencrypt all values and it is a nightmare, this must be common function",0,0,msr
840,+1 for this functionality.,1,0,msr
841,"+1, really need it!",1,0,msr
842,"I solved this using debug mode. E.g.
`ansible localhost -m debug -a var='myVariable' -e ""@myFile.yml"" --ask-vault-pass`",0,0,msr
843,+1 it would be very handy!!!,0,0,msr
844,"> I solved this using debug mode. E.g.
> > `ansible localhost -m debug -a var='myVariable' -e ""@myFile.yml"" --ask-vault-pass`
Works beautifully! No need for `--ask-vault-pass` if you have the password in a file identified by the `ANSIBLE_VAULT_PASSWORD_FILE` environment variable:
https://docs.ansible.com/ansible/latest/reference_appendices/config.html#envvar-ANSIBLE_VAULT_PASSWORD_FILE",0,0,msr
845,+1 would be super helpful especially as we transition away from fully encrypted ansible-vault files to just files with encrypted variables (for ease of use/readability/ability to modify etc),0,0,msr
846,"A few expansions to @whirlwin's clever workaround:
Instead of a specific var, you can have ansible dump all vars (encrypted and plain) by specifying `-a var=""vars""`. There will be some noise in the result with a few stock vars, but it's a nice way to see everything at once.
You can also specify multiple source files by passing multiple `-e ""@...""` args.
Also, I haven't tested this personally but you can apparently avoid the implicit `localhost` warnings by setting `ANSIBLE_LOCALHOST_WARNING=false` on Ansible 2.6+.
All together:
```
ANSIBLE_LOCALHOST_WARNING=false ansible localhost -m debug -a var=""vars"" \
-e ""@file1.yml"" -e ""@path/to/file2.yml"" -e ""@path/to/file3.yml""
```
plus `--ask-vault-pass` if needed.
Obviously a built-in solution would be much better, but this makes mixed plain/encrypted var files somewhat workable.",0,0,msr
847,+1 for this functionality.,1,0,msr
848,"If you just need decryption instead of rekeying, I've written https://github.com/theblazehen/ansible_vault_decrypt_strings",0,0,msr
849,+1 for this feature,1,0,msr
850,"I've locked this to contributors for now. Adding +1 comments is too noisy. For future reference, add a reaction to the issue body, and don't comment.",0,0,msr
851,"Take a look at the `vault` and `unvault` Jinja filters that were added recently to core, might get what you need...",0,0,msr
852,"Continuing discussion from https://github.com/pallets/meta/issues/10#issuecomment-209980352
The naming is inconsistent:
- Github repo is `jinja`
- Pypi package name is `jinja2`
- Pallets project calls it ""Jinja"": https://www.palletsprojects.com/p/jinja/
- RTD namespace is jinja2.readthedocs.io
- Pocoo docs (currently the official ones) are ""Jinja"": http://jinja.pocoo.org/docs/2.9/
- file extensions are sometimes `.jinja`, `.j2`, `.jinja2`... Ansible project currently uses `.j2`
We should pick either ""Jinja"" or ""Jinja2"" and use it everywhere for consistency. I am open to either, ""Jinja"" is simpler and shorter, but ""Jinja2"" has a more distinctive ring to it and less likely to get confused with any other projects.",0,0,msr
853,"The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)
I really want to drop the ""2"" from the name. Start adding v2 builds to the ""jinja"" PyPI page. Deprecate the ""jinja2"" import and go back to the ""jinja"" namespace.",0,0,msr
854,@ThiefMaster @mitsuhiko @untitaker do you guys have opinions?,0,0,msr
855,I think we can do that but I would personally propose to align the 3.0 release with that.,0,0,msr
856,":+1: on waiting for 3.0.
---
> The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)
I may be able to fix that.
Edit: Yes, I can
> **Rename preview**
> jinja2 will be removed from 3486 questions
> jinja will be added to 3486 questions
> 5 commitments to jinja2 Documentation proposal will be moved to the jinja proposal
> A tag synonym mapping jinja2 → jinja will be created.
> (these counts include deleted questions and exclude overlapping tags)",0,0,msr
857,"What is the timeline for 3.0 release?
The sooner we start giving folks a heads up the better, so what about adding a deprecation warning now on `jinja2` imports and a warning on `jinja` imports that we will soon be pushing v3 out to the `jinja` namespace?",0,0,msr
858,"@davidism are you able to move the RTD namespace over to `jinja`? Per my comment above, it's currently under `jinja2`, and IIRC, you were driving the cleanup/ownership migration of the RTD namespaces for other projects?",0,0,msr
859,In a way the last major release of Jinja2 was a massive change in the engine. Not even sure if there is more stuff we need to break :D,0,0,msr
860,"Saving breaking changes and name consolidation for a Jinja v3 sounds great to me. We might as well try to find what breaking changes we can slate for it.
I'd like to remind everyone of a potential one - [allowing included block overrides](https://github.com/pallets/jinja/issues/243). That issue doesn't have to mean a breaking change, but if that's the route you all want to go, remaking/opening that issue with a v3 milestone is how I'd do it. Sorry for the tangent. :) Perhaps we can make another ticket for discussing what to break / milestone for Jinja v3.",0,0,msr
861,"nudge @davidism - per my comment above, are you able to modify the RTD namespace from jinja2 to jinja?",0,0,msr
862,"In the 2.11 release, I'm thinking of renaming the package to `jinja`, with a placholder module for `jinja2` that forwards all imports and issues a deprecation warning.
I'll still have to work out the timing of this next step, but I'd also like to try moving back to the ""Jinja"" name on PyPI. I think what I'd try to do is have a **Jinja** 2.11 build that includes the `jinja2` placeholder, and make the **Jinja2** 2.11 build just depend on `jinja>=2.11`, or have a small shim that explains installing the other name without breaking any code. I'm am willing to take on the extra effort of keeping these builds in sync for a while while we manage a transition.",0,0,msr
863,@davidism this shouldn't happen in a point release. This would break pickle and a bunch of other things.,0,0,msr
864,"Since I gave my blessings before I want to actually qualify this somewhat. I have some stomach ulcers with this change. Ultimately I don't think it's particularly useful for users (it just drops one character), introduces some backwards incompatibility concerns and it undoes a learning I made back when Jinja2 was originally released.
The reason the package renamed with 2.0 was that there was no way (and there still is no way) to have parallel installations of Python libraries that are incompatible unlike node or rust can. Because of that I think we're going to be sooner or later again in a stupid situation where Jinja 4.0 would need to be named ""Jinja4"" on pypi.
So I think while this rename is somewhat okay I generally don't think anymore that it's a good idea. I think this change would be without concerns if the Python import system were to support imports with different versions which however I gave up hoping for.",0,1,msr
865,"@coleifer I really have no idea what you're suggesting other than ""let's just revert this"". We won't release this as a patch/bugfix release, so I guess you are not happy that this will land in 2.11. Are you expecting us to release Jinja 3 for this? That would cause even more problems in a dependency tree that has multiple package dependant on Jinja.
Honestly I find your behavior completely unacceptable and hope it will have consequences.",0,1,msr
866,~fwiw we could also release a new (point) version of `jinja2` that reexports all of `jinja` (ie it is the shim). That usually works in Rust when you have multiple dependencies that depend on another package. You'd just have to update `jinja2` to make packages that depend on `jinja2` implicitly use the types from `jinja`.~ discard this. This is exactly what the shim is doing. I have no idea what the concern is.,0,1,msr
867,"@untitaker Interested in the issues you refer to with making the rename happen in Jinja 3.0 instead. Based on discussion with @ThiefMaster, it seemed that doing it in 3.0 made more sense, as it does represent a major change. We also thought about a 2.12 release for just the rename.
Jinja2 3.0 would be the shim and pull in Jinja 3.0 as a dependency.",0,0,msr
868,That would probably be fine but it would prohibit using the new `jinja` name with packages that explicitly depend on `Jinja2==2.*`. Which limits the potential usefulness of the shim.,0,0,msr
869,"Yeah, that was one of my initial reasons for going with 2.11. I guess 2.12 vs 3.0 comes down to deciding on if the rename is a major change even though jinja2 would continue to work and issue deprecation warnings. 3.0 was originally only going to be a major release because it dropped Python 3.
---
After some more discussion internally, we're reverting this. See #1131.",0,0,msr
870,"This PR is created in response to https://github.com/jbialobr/gitextensions/pull/3
It addresses some issues that bother me in https://github.com/jbialobr/gitextensions/pull/3 These are:
1. WorkingPathProvider depends on Directory, AppSettings and GitModule static methods. These are widely used across the app. I would not like to pass them to various objects through a multiparam constructor.
2. Turning static methods into instance methods: https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376 3. Incompleteness of arranging the exterior. Unconfigured calls return default(T), which may cause false positive tests results. I would prefer to be notified that unit test referenced not configured call. It does not address the separation CommandLineArgs concern https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376
This is only a sketch - there is no proper class per file and file per project separation. I am interested in discussing about the general idea of this solution.
The general idea of this solution is:
1. Wrap static methods in Gateway classes.
2. Gateway classes are singletons, they provide default implementation through public virtual members.
3. In unit tests Gateway classes are substituted in Setup method.
4. After the arrange phase unconfigured calls on substituted Gateways are not allowed.",0,0,msr
871,@jbialobr Do you have a link to the original discussion describing the underlying problem that both you and @RussKie were trying to solve?,0,0,msr
872,"Sure, I added this PR https://github.com/gitextensions/gitextensions/pull/3881
@RussKie would like to have tests for it and made a sketch PR https://github.com/jbialobr/gitextensions/pull/3 showing how he would see this done.",0,0,msr
873,"To set the context.
The current codebase features heavy use of inheritance and static methods.
OOP in general and inheritance in particular were all the rage about 10-15-20 years ago. As a concept it does have benefits of reuse, but it also carries downsides of maintenance overhead, complicated abstraction layers, difficult testability, side effects and surprises in behavioural changes... These days stateless interface-based implementations have become quite popular (especially with raise of functional languages) where each class has a single responsibility (or as close to that as possible) and gets all the dependencies injected via a constructor. This allows to write much cleaner and maintainable code, easier to unit test (as opposed to inheritance based classes where you would more likely be writing integration tests) and easier to alter behavior by substituting dependencies.
Another benefit of using interface-based architecture that you could plug in IoC container (e.g. autofac).
Over the years I have experience both sides, I was a big proponent of OOP but I have learnt the hard way that it makes it much harder to maintain such code, especially in large projects.
As I observed, a lot of bugs stem from implementations with overloaded concerns and lack of unit tests. As a result the code is quite confusing, inflexible and hard maintain.
In my opinion, one of the ways we could start reducing the tech debt and improving the codebase - start extracting separate concerns into individual classes and compose them as required. Gradually we would be able migrate most of the codebase. Static methods have their use too, but as any tool they have to be used appropriately.",0,0,msr
874,"My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces, leading to the following problems:
1. Tests are no longer executing against reality. Sometimes it's close, but it means tests are prone to both false positives (failed due to incorrect mock) and false negatives (passed even though the situation intended to be tested will fail at runtime).
2. Mocks are effectively a very clumsy DSL where logic must be manually reimplemented. Every mock necessarily reduces the long-term maintainability of the project, as well as the ability of developers to make behavior-preserving changes.
3. Mocks are not a substitute for properly covered tests. You can test code without mocks, or you can test code both with and without mocks, but mocks never save you from writing tests you would otherwise have to write.
As long as there is a clear understanding that mocks are only to be considered in the total absence of the ability to write a test using another approach, then I don't have a particular problem with either using or not using DI.",0,0,msr
875,"My early testing recommendation would be to enable codecov.io (or similar) service as soon as possible. Code coverage tooling which is tightly integrated with both the code review process on GitHub as well as the supporting CI system enables developers to write mid-level tests (against API boundaries) without sacrificing overall implementation coverage. Tests written at this level, which I call ""units of behavior tests"", have these advantages:
1. They are typically easy to understand
2. They demonstrate the intended use of APIs at module boundaries (public and internal), including edge cases
3. Failure of the test is typically easy to relate to a sequence of steps a user could take to produce incorrect behavior at runtime
4. They are typically easier to design in relation to anticipated use of the application
5. When coupled with code coverage tooling, they help reveal edge cases in API usage scenarios
Note that I consider user input/interaction, file system and network activity, and operating system interaction as API boundaries that can be tested against. APIs internal to the application can be added to this set in any number of ways.",0,0,msr
876,"> My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
I am not sure if completely I understand your point.
If each class has a single responsibility (SR), then DI should not add any ambiguity - for each use case you set dependencies to imitate boundary conditions, but you **only** exercise and validate functionality of the class under test.
Any changes to dependencies should not have any profound effect because the class is built against a contract and not a particular implementation. Certainly a degree of coupling is possible, but it typically it will be lose coupling.
Also a SR class (usually) define a limited number of methods (accessible via an interface). As a result you test the public surface. Generally there is a limited need to test internal or private methods (due to their absence).
Another benefit of smaller classes with SR - you know exactly what each class depends on. For example right now `GitModule` spans across 3,400+ lines of code carrying hundreds of methods. And we inject this monstrosity to classes which may need to access only one method from the lot...
If a class has multiple responsibilities and/or initialises its own concrete dependencies then it is virtually impossible to perform unit testing - instead you will be writing integration tests. Any changes to dependencies with high degree of probability will necessitate changes to the class, because there is a tight coupling to concrete implementations.
Wrt: code coverage - whilst it is a very useful metric, it shouldn't be viewed as an absolute. 100% CC can be very misleading.
Personally when I write tests I usually strive for 100% coverage taking in consideration various edge cases. I use NCrunch, it helps me to visualise what cases I may have missed.",0,0,msr
877,">Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
Yes, that's true. Regardless of the tools we use there is always a way to use them inappropriately.
I like to be pragmatic. For me it means: achieve maximum benefits at reasonable cost with decent safety.
Weighing those factors we have to take into account the existing code base and resources we have.
Thank you for pointing to codecov.io - it should help to find which pre-condition we lack in the test set (though 100% codecov coverage does not imply 100% pre-conditions coverage). Implementing codecov into GitExt project will show us all the most obvious pre-conditions against which should we write ""units of behavior tests"".
Then we have to write these tests, to do it, we have to weigh the factors I mentioned and decide whether to mock or to configure the exterior. I think we can't make this decision at a general level. Besides, one approach does not have to fit all concerns.
Here we have 3 dependencies, how would you see ideally written [this test](https://github.com/gitextensions/gitextensions/pull/3898/files#diff-67c45fbdcf3a92b53649d84f9f401c3eR26)?
```csharp
public void ReturnsRecentDirectory_if_RecentDirectory_IsValidGitWorkingDir()
{
//arange
DirectoryGateway.Inst.CurrentDirectory.Returns(string.Empty); <-- mocked
_ext.StartWithRecentWorkingDir = true; <-- mocked
string unitTestRecentWorkingDir = ""unitTestRecentWorkingDir"";
_ext.RecentWorkingDir = unitTestRecentWorkingDir;
GitModuleGateway.Inst.IsValidGitWorkingDir(unitTestRecentWorkingDir).
Returns(true);<-- mocked
//act
string workingDir = _workingPathProvider.GetWorkingDir(new string[0]);
//assert
workingDir.Should().Be(unitTestRecentWorkingDir);
}
```",0,0,msr
878,"@russkie. Thoughts of mef vs aurofac?
https://stackoverflow.com/questions/15572302/difference-between-mef-and-ioc-containers-like-unity-autofac-smap-ninject-w
On Sun, Aug 13, 2017, 11:47 AM Janusz Białobrzewski <
notifications@github.com> wrote:
> *@jbialobr* commented on this pull request.
> ------------------------------
>
> In GitExtensions/StaticDependencyInjection.cs
> <https://github.com/gitextensions/gitextensions/pull/3898#discussion_r132845408>
> :
>
> > + }
> +
> + public interface IInstanceFactory
> + {
> + T CreateInstance<T>() where T : class, new();
> + }
> +
> + public class NewInstanceFactory : IInstanceFactory
> + {
> + public T CreateInstance<T>() where T : class, new()
> + {
> + return new T();
> + }
> + }
> +
> + public class StaticDI
>
> Thanks, it is always better to be more thorough.
> It seems that I false assumed that the 3 dependencies are
> static/stateless. The all three services depends on IFileSystem. Changing
> this assumption, my concerns change as follow:
>
> 1. The dependencies should be passed down the tree through a
> multiparam constructor starting from the Main proc.
> 2. We should create a separate service GitWorkingDirService to which
> FindGitWorkingDir and IsValidGitWorkingDir should be moved.
> 3. It should be handled at the Nsubstitute level.
>
> I would not introduce the ServiceLocator as it is in conflict with 1)
>
> I will update this PR to comply with the updated assumption.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#discussion_r132845408>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADdhsRmqOyDUF78xYjEAV-J2TwxGgYuxks5sXxqYgaJpZM4O1aRu>
> .
>",0,0,msr
879,">>My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces
>I am not sure if completely I understand your point.
One problem I see in using mocks/spies is it often tests the implementation instead of expected behavior.
A typical unit test arranges the before state (internal and external), performs an action and asserts that the after state is equal to the expected state. Because mocked methods do not modify any external state we can not verify the after state, only asserting the expected sequence of calls on the dependencies. Changing the implementation breaks such an unit test.",0,0,msr
880,"> Thoughts of mef vs aurofac?
I haven't worked with MEF. Until you mentioned it I am not sure I was even acutely aware of it (though I may have heard the name). On paper MEF has benefits but as well it has a number of trade offs. To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry, so taking a dependency on it carries an inherent risk coupling to a technology which may become instantly obsolete (like Silverlight).
This leads to another problem - MEF requires a lot of additional ceremony (decorations). In event we want to migrate from MEF we won't be able to do so easily. If we use IoC switching containers is relatively trivial task and doesn't require re-architecture the whole app. I've just migrated our big solution from Funq to Autofac within two days and the footprint of the change was minimal. Also from what I've read MEF seems to be opaque in how it handles types, where IoC may be more transparent.",0,0,msr
881,"> One problem I see in using mocks/spies is it often tests the implementation instead of expected behavior.
> A typical unit test arranges the before state (internal and external), performs an action and asserts that the after state is equal to the expected state. Because mocked methods do not modify any external state we can not verify the after state, only asserting the expected sequence of calls on the dependencies. Changing the implementation breaks such an unit test.
This is only a problem if your implementations are stateful.
I am a big proponent of stateless implementations (""pure"" methods as they call it in functional land). In stateless world you test behaviors which do not depend on state in any way.",0,0,msr
882,"> To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry
It's been the primary container for Visual Studio since 2010. Over the years it's received a lot of love to meet the performance demands of that application, though not all of those benefits have made their way to the normal distributions of it *yet*. I would classify it as having fewer features, but within the bounds of its capabilities excels in ease of distribution and being generally well-understood.",0,0,msr
883,"Yep. I'm developing a smartsheet to TFS MVC website. The website handles
the communication with smartsheet and it uses mef plugins to react to the
webhook calls from smartsheet.
On Sun, Aug 13, 2017, 9:27 PM Sam Harwell <notifications@github.com> wrote:
> To me, one of the trade offs is that it doesn't seem to be widely embraced
> by the industry
>
> It's been the primary container for Visual Studio since 2010. Over the
> years it's received a lot of love to meet the performance demands of that
> application, though not all of those benefits have made their way to the
> normal distributions of it *yet*. I would classify it as having fewer
> features, but within the bounds of its capabilities excels in ease of
> distribution and generally well-understood.
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322081292>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADdhsTGGLts65YkEkkC2d0bCYikelDQgks5sX6KCgaJpZM4O1aRu>
> .
>",0,0,msr
884,">This is only a problem if your implementations are stateful.
I am a big proponent of stateless implementations (""pure"" methods as they call it in functional land). In stateless world you test behaviors which do not depend on state in any way.
But it is a real problem:
https://github.com/gitextensions/gitextensions/pull/3865/files#diff-dce953c015443e09345315dc6cdf8452R59
https://github.com/gitextensions/gitextensions/blob/0b9a310019965c8183599e9afa3a722df03ae1f4/UnitTests/GitCommandsTests/Remote/GitRemoteControllerTests.cs#L239
https://github.com/jbialobr/gitextensions/pull/3/files#diff-7ca5aacd7a7769eef82fb4b8f4d3d9ddR46",0,0,msr
885,"I'm not sure what you perceive as a problem in attached links, I'm sorry.
The classes under tests are stateless - they do not have any settable fields or properties (outside the container) nor they access any objects besides those passed via the container. The external dependencies are interface-based, and as such they are stateless too. So there are no side effects and there are external context mutations - so I can assert the correctness of my implementation.
The unit tests exercise my class implementations and check the flow of execution by checking how far each use case progresses and what methods within the method under test were called. There is no magic here :)
Of course there is an arrangement phase, you can't not have it. But the arrangement only sets a possible return from the underlying dependency (boundary conditions). My duty as a class developer write unit tests to account for these boundary conditions and facilitate stable predictable response.
**EDIT:**
Just realised some classes may be accessing `AppSettings` which arguably belongs to the external context, but for the most part we can presume AppSettings to be immutable. They should be abstracted too.",0,0,msr
886,"> > To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry
>
> It's been the primary container for Visual Studio since 2010. Over the years it's received a lot of love to meet the performance demands of that application, though not all of those benefits have made their way to the normal distributions of it yet. I would classify it as having fewer features, but within the bounds of its capabilities excels in ease of distribution and being generally well-understood.
My point that it is still contained within Microsoft.
Given that we know how Microsoft (and other big players) drop support for their products overnight I would be very cautious investing into tech stack which is difficult to deprecate.
We can certainly consider MEF for dealing with plugins but I feel very uneasy committing to it for the core of GE.",0,0,msr
887,">I'm not sure what you perceive as a problem in attached links, I'm sorry.
""... it often tests the implementation instead of expected behavior.""
The following test tests implementation.
```csharp
public void SetRemoteState_should_call_ToggleRemoteState(string remoteName, bool remoteDisabled)
{
var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
_configFile.GetConfigSections().Returns(x => sections);
_controller.ToggleRemoteState(remoteName, remoteDisabled);
_configFile.Received(1).GetConfigSections();
_module.Received(remoteDisabled ? 1 : 0).RemoveRemote(remoteName);
_configFile.Received(remoteDisabled ? 0 : 1).RemoveConfigSection($""{GitRemoteController.DisabledSectionPrefix}{GitRemoteController.SectionRemote}.{remoteName}"");
_configFile.Received(1).AddConfigSection(sections[remoteDisabled ? 1 : 0]);
_configFile.Received(1).Save();
}
}
```
I would see the assert part along the lines of this:
```csharp
public void ToggleRemoteState_ChangesDisabledValue(string remoteName, bool remoteDisabled)
{
//arrange
var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
_configFile.GetConfigSections().Returns(x => sections);
//act
_controller.ToggleRemoteState(remoteName, remoteDisabled);
//assert
_controler.GetRemote(remoteName).Disabled.Should().Be(!remoteDisabled);
}
}
```
This way we don't care about the implementation details, we only deal with pre and post conditions.
If the implementation changes from `remove then add` to `rename`, the first test will fail while the second will not.
>The external dependencies are interface-based, and as such they are stateless too.
Apparently we have a different understanding of ""stateless"". The config file holds a state. Invoking its methods changes this state.",0,0,msr
888,"> The following test tests implementation.
Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.
As a class consumer you don't care about it - you expect by calling a method something happens and that something happens in expected predictable manner.
Any changes to my class behaviors should only really affect my class. And for a consumer of the class the changes should be transparent. For example, as a consumer by calling `_repo.GetRecord(id: 1)` I expect to get record with id=1 - whether the record is in a database, xml file or a remote service I don't care. If repo implementaion is migrated from a database to a remote service or an xml file - it is an implementation detail for the repository implementation and I shouldn't change my implementation.
On the other hand, as a repository developer in its implementation I would probably want to make sure that the database connection is established before I attempt to interact with the database. Naturally in the repository tests I would expect to see something like `_db.Received(1).Open()`.
. . .
> Apparently we have a different understanding of ""stateless"". The config file holds a state. Invoking its methods changes this state.
Indeed we are :smile: As a consumer I interact with an interface - I don't know nor expected to care about the implementation detail of the instance given to me at runtime. By virtue of this the dependency is treated as stateless.
. . .
Btw `GitRemoteController` isn't a stateless implementation, it has a bunch of side effects and it maintains the state in `Remotes` list. So it is probably not the best example to use.
To sum it up:
We probably won't be able to make everything stateless, not from the get go anyway:
- the codebase isn't ready for it yet, and
- UI layer is typically stateful.
If we go with interface-based implementations it is much easier to write smaller SR implementations which can be easily tested. It is much easier to make implementations stateless too. You get benefits of declarative composition - it is easy to see objects connections and dependencies. As well as DI and substitutions - i.e. you could have Windows- and Linux-specific implementations without random `if running on Windows do this else do that`.",0,0,msr
889,">Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.
I see it totally different. As a class developer you need to ensure the expected behavior. The order of execution and the content of execution is subject of change in contrast to the expected behavior.
When you want to optimize your implementation you change it and run unit test to see if everything is ok.
When unit tests test the implementation you get tests failure.",0,0,msr
890,"> Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.
This view makes sense. I would lean towards agreeing with it¹ on paper on both technical merits and practicality merits. However, reality of developing over time tells a very different story.
This is not only not correct, but every time I have seen it followed it's had a disastrous impact on the project. The possibility of someone with this mindset working on tests is the reason I raised my original concern above.
¹ I was impressed with the approach joining another project that leveraged it. I only changed views after the problems were proven beyond any shadow of a doubt; fixing the tests was a long, hard road. But at the end every metric we came up with for measuring test quality was improved.",0,0,msr
891,">As a class consumer you don't care about it - you expect by calling a method something happens and that something happens in expected predictable manner.
I don't buy it. You write a method that calls:
...
dep.Proc1; ... dep.Proc2;
...
dep.Proc3; Then you write a unit test that ensures that `dep` received these calls in the exact order. This way you verify that you implemented it the way you think it should be implemented. However if the order should be Proc3, Proc2, Proc1 your unit test does not fail. Another thing is that reading a such specified assert you don't know what is the expected state after the method execution - you only know how test writer imagine it should be implemented. I think that tests should document expected behavior not implementation.",0,0,msr
892,"Ok, let consider this method https://github.com/gitextensions/gitextensions/blob/master/GitCommands/Remote/GitRemoteController.cs#L190-L205
```
public string RemoveRemote(GitRemote remote)
{
if (remote == null)
{
throw new ArgumentNullException(nameof(remote));
}
if (!remote.Disabled)
{
return _module.RemoveRemote(remote.Name);
}
var sectionName = $""{DisabledSectionPrefix}{SectionRemote}.{remote.Name}"";
_module.LocalConfigFile.RemoveConfigSection(sectionName, true);
return string.Empty;
}
```
1. Suppose there is a refactor gone wrong and a developer accidentally deletes ""!"" on line 197.
To ensure the correct handling of disabled remotes I would write a test something like: ```
[Test]
public void Ensure_disabled_remotes_dont_get_removed() {
// ...
_module.DidNotReceive().RemoveRemote(Arg.Any<string>());
}
```
2. I want to ensure the correct section name is passed to the config manager.
In the test I can expect to make a call
```
[Test]
public void Ensure_removing_correct_section() {
// ...
_module.LocalConfigFile.Received(1).RemoveConfigSection(""...."", true|false);
}
```
How would you write tests to ensure the correct behavior?",0,0,msr
893,"I would add a disabled remote to a configuration file, and then remove one of the other remotes. At the end of the test I verify that the disabled remote is still in the file.
If a test written in this form fails, there is no question about what it was testing or why the failure indicates bad behavior. There are many reasons why a test using the expected call sequence could fail. For example, if someone deletes the file because they thought they were removing the last item, the test passes even though the specific failure it was written to detect certainly occurred.",0,0,msr
894,"> I would add a disabled remote to a configuration file, and then remove one of the other remotes. At the end of the test I verify that the disabled remote is still in the file.
This won't be a unit test but a integration test since the class now needs to know how to write or read from a file. Which breaks SR principle.",0,0,msr
895,"```csharp
[Test]
public void Ensure_disabled_remotes_dont_get_removed() {
//arragne
_module.AddRemote(name);
_module.DisableRemote(name);
//act
_controller.RemoveRemote(name);
//assert
assert(_module.GetRemote(name).Disabled == true);
}
```",0,0,msr
896,"If `_module` isn't an interface you are too writing integration tests...
testing this class as well how module class works
On 14/08/2017 10:26 PM, ""Janusz Białobrzewski"" <notifications@github.com>
wrote:
> [Test]
> public void Ensure_disabled_remotes_dont_get_removed()
> {
> //arragne
> _module.AddRemote(name);
> _module.DisableRemote(name);
> //act
> _module.RemoveRemote(name);
> //assert
> assert(_module.GetRemote(name).Disbaled == true);
> }
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322176701>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AEMyXtMUnIlC8RAW-FInPnC9jRS9rtDeks5sYDzrgaJpZM4O1aRu>
> .
>",0,0,msr
897,"> This won't be a unit test but a integration test since the class now needs to know how to write or read from a file. Which breaks SR principle.
I prefer to describe tests as ""fast""/""slow"", or as ""stable""/""unstable"". A test as I described would be characterized as ""fast"", ""stable"", and also particularly well-suited to detecting regressions related to a specific scenario. When you start describing tests as ""unit"" or ""integration"", it's easy to lose sight of the fundamental purpose of testing.
The fundamental purpose of testing is to detect cases where the observable behavior of an application deviates from the expected behavior for some user scenario. I find this condition is reasonably relaxed to detecting changes in behavior under a theoretically-reachable scenario.
""Unit"" tests are a sub-optimal approximation of this that resulted from several limitations:
1. It is difficult for developers to reason about the line-level behavior of high-level testing.
1. Code coverage tooling was incomplete and/or slow, such that it was not practical to adopt as part of a team code review process.
1. High-level testing tends to be slow, unreliable, and/or difficult to stand up (e.g. only runs under a specific non-standard system configuration, or requires specialized software installations).
Code coverage tooling now performs very well, and eliminates the problems typically associated with the first item. We found that by moving away from ""unit tests"" as a goal, we were able to write tests that better represented reality. The focus instead was on test speed, stability, and independence from system configuration. The result was something between what would normally be classified as ""unit"" or ""integration"" tests - we used fakes¹ to eliminate external dependencies, and tested primarily against well-defined module boundaries². When possible, we fed inputs to the application in the same way it would actually occur for a user:
1. When testing the behavior of a subsystem, we tested using the same API that other code would be using to drive that subsystem. Code which we *couldn't* cover with this approach often meant the code was dead code under real-world conditions.
1. When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
1. When testing file system, we created files and folders and tested against them. We used ACLs or chmod to force access errors and held handles open to force sharing violations.
¹ Typically stub executables or our own implementations of interfaces in code
² We applied a `[ServiceInterface]` attribute to the boundary interface, which carried special requirements. These interfaces were required to be threading agnostic, safe for concurrent use without regard for the caller (new unknown callers could appear at will), idempotent, and only ever obtained from the single service container instance. Mocking was *forbidden* for any object which was not marked with `[ServiceInterface]`. For cases where one or more of these conditions could not be met, we used `[ServiceInterface(Restricted = true)]`, and required that the restriction (often a limitation on who is allowed to call it) be documented - use of this form was seen as technical debt, but used occasionally because the correct solution was too costly at the time.",0,0,msr
898,"> The order of execution and the content of execution is subject of change in contrast to the expected behavior.
This is a contradictory statement - the _order of execution_ is one of the key factors for _expected behaviour_.
And the _content of execution_ should not matter at all.",0,0,msr
899,"Thank you @sharwell for taking time and sharing your experience. I find your approach very pragmatic.
I think that at this point enough has been said and we should stop to write down the fundamental goals the tests should serve and the criteria they should meet. Only after that we can discuss how to achieve these goals and meet these criteria. These are mine:
**Goals:**
1. Ensure the expected behavior of the app. By saying expected behavior I mean that having given pre conditions the tested unit transforms the existing state into a state described by the post conditions. To be clear we should assert on the expected state not on the steps that lead to this state.
2. Document the expected behavior. **Criteria**
1. Independent from the environment - a test should arrange the environment in the way that makes it invariant regardless of changes made to the environment outside of the test.
2. Stable - a test should always produce the same result for the same starting conditions.
3. Easy to understand the pre and post conditions.",0,0,msr
900,">When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
@sharwell Is there any publicly available project that uses this approach - I would love to see this in action.",0,0,msr
901,"@sharwell great write up, thanks.
It looks like we come from different schools of thought and practice. Like yours my position comes from practical experience solving real engineering challenges.
I'm part of a small team building RAD framework and reusable components where pretty much every component can be substituted by a consumer. We have a number of major and minor releases concurrently powering hundreds of apps in production, and we provide full runtime backwards compatibility.
We can assume very little about consumers, their habits and requirements. In turn we must provide a great deal of flexibility but ensure stability of the framework core and consumer applications (to a degree). This is specifically important in security-related areas.
All of this dictates how we architect and engineer the framework, how we deal with dependencies, how we deploy and how we test.
Until we re-engineered the codebase to consist of interface based stateless single responsibility classes we had a number of challenges meeting the requirements (including production incidents). We also couldn't reason about stability and predictability of provided behaviors.
Now we have 1,800+ .NET unit tests, hundreds of JS tests and probably another few thousand of BDD tests. Some tests, especially security-related, test the order of execution - without that we can't reason about predictability, nor we can release new versions or provide backwards compatibility.
Smaller interface-based single responsibility implementations allow us to achieve * isolation (all dependencies are injected), * testability (all dependencies are mocked), * stability (via predictability), * flexibility (consumers only concern themselves with replacing a specific implementation instead of worrying about inter-dependencies). And as a bonus - our APIs are easy to register in an arbitrary IoC container.",1,0,msr
902,"@RussKie Your last post really resonates with me. One of the goals I had when I was pushing so hard to switch away from mocks (on a former project) was based on the type of application being shipped to customers. As you are well aware, anything considered public API surface area has special requirements related to versioning. These requirements constrain the development team substantially, but are a great asset from the customers perspective. However, in an application where the final product is the application (as opposed to a publicly-consumable API), these same requirements *burden* the development team. I didn't like the fact that when we wanted to make a change to our implementation, we kept running into pain points (leading to delays) resembling those faced by developers trying to make changes to public APIs.
Another application (this time a library) I've contributed to over time is StringTemplate. While it has a few known extension points built in, it was never designed to be arbitrarily reconfigurable for end users. Over time it's led to a frustrating experience for a small subset of users trying to do arguably normal things, just things we didn't think of from the start. For example, by failing to abstract the file system (location and reading) away from the template loader, it's not possible to use *some* features of StringTemplate when the templates are stored e.g. in a database, or stored using a compression mechanism we didn't think of from the start. Better adherence to SRP would likely improve the long term durability and flexibility of this library.
It sounds to me like GitExtensions is unlikely to suffer from most of the problems related to reusable components, and more likely to benefit from a testing approach that focuses on application behavior.
> > When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
>
> @sharwell Is there any publicly available project that uses this approach - I would love to see this in action.
Not to the extent of the one I've been referring to. I've started to add tests for UI related bug reports to PerfView, but these are quite limited and not particularly elegant currently.
When I built a system like this in the past, I started with the following:
1. Identify all mutable static state in the application. We will need a way to reset these to known default values between each test, ensuring that the outcome of any given test cannot influence the behavior of another (with a corollary that a test failure uniquely and deterministically identifies the culprit, which can then be executed in isolation to reproduce the failure):
* Mutable `static` fields
* File read and written
* Registry keys/values read/written
2. Identify all asynchronous operations. We will need a way to ensure that a test is not considered complete while one of these is executing (or scheduled for later execution, in the case of timers or delays).
3. Identify external dependencies for which the concrete implementation is likely problematic for high-speed deterministic testing. We will need a way to substitute the behavior of these items during testing. The method of substitution must guarantee that during the execution of any given test, all executing code is using the same implementation (either the original or the substituted one, but never a mix and never multiple substitutes).
* One obvious case is invocations of *git.exe* for many types of tests
* Loading resources from a network
Once you address the above, most tests are still written against module interface boundaries. However, when actually testing UI behaviors it's not particularly challenging to write those tests against the controls. Most of the problems typically associated with testing UI behavior are really just failures to address one of the above underlying concerns.",0,0,msr
903,"I would go as far as proposing moving away from statics altogether.
There are merits for extensions, but like any tool they have to be used sparingly to be used correctly.",0,0,msr
904,"Erik Dietrich of NDepend writes insightful articles discussing different aspects of code architecture and design. This is one of them along the lines of this discussion, worth reading https://blog.ndepend.com/singleton-pattern-costs/",0,0,msr
905,"The article is somewhat related, but the single-instance services I would advocate for behave very differently from the classical singleton pattern. Most of the differences were created to directly address one or more costs listed in that article.",0,0,msr
906,"Indeed, but it raises points how statics introduce side effects and hinder testabiltiy.",1,0,msr
907,">If `_module` isn't an interface you are too writing integration tests...
testing this class as well how module class works
I am happy to do that. I don't need unit tests, I need the app to be thoroughly tested. I expressed my needs here https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322384024
I don't want to say that asserting on implementation is always wrong. It can be helpful, but it should not be overused. I think it should be strictly limited and explicitly described when it can be used. I see to many corner cases in my mind when unit tests asserting on implementation go green and the app is not working as expected. There are even more cases I see when changing the implementation turns such unit tests into red ones. These of many assertions in a single test are the most likely to fail and the hardest to correct (I am ignoring here the fact that we should not have to correct tests for which the pre and post conditions have not changed). I don't mind writing tests that ensure a developer well understand description of the used interfaces and its operations. For example when resetting unstaged changes, I prefer to have a set of tests like:
1. Ensure staged files are untouched
2. Ensure ignored files are untouched
3. etc.
to have a test like:
1. Ensure the `git reset --hard` is executed.
>>The order of execution and the content of execution is subject of change in contrast to the expected behavior.
>This is a contradictory statement - the order of execution is one of the key factors for expected behaviour. And the content of execution should not matter at all.
Perhaps my English skills are too poor to express it correctly. I prefer to verify that the chosen by a developer sequence of execution leads to the expected state than to verify that the chosen by a developer sequence of execution is the sequence he chose. The order is indeed one of the key factors but there may exist many orders that lead to the expected behavior.",0,0,msr
908,"Lets look at this test: https://github.com/gitextensions/gitextensions/commit/9fb22f58727408d73baaea03ecf6cb69a66f997a#diff-3a91b3e9edc1faa5190d69a2fec75400R81
It is a copy-paste from the implementation. It is really a [test double](https://en.wikipedia.org/wiki/Test_double). It doubles the implementation, it doubles the false assumption that the path separator is `\`. While unit tests are useful, they should not replace integration tests.
Nevertheless, the architecture should allow to write both kinds of tests.",0,0,msr
909,"Locking this discussion for archival purposes, since I use it as a reference for an in-depth discussion from opposing viewpoints in other contexts.",0,0,msr
910,"Sam, I'd be interested in learning gained insights and outcomes of those discussions.",0,0,msr
911,"The word ""Caddy"" in the context of software is under a pending trademark application. By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)",0,1,msr
912,"> The word ""Caddy"" in the context of software is under a pending trademark application
So it's not a registered trademark?
> By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)
See issue #1 - this is planned 😃",0,0,msr
913,"Technically, there is an implicit trademark because of the continued use and recognition of the brand over the years, and it is legally enforceable. But we have also submitted a formal trademark request as well, which is pending. Just FYI, we have no problem with anyone forking Caddy. We enjoy open source!
Can I ask what your use case for Caddy is?",0,0,msr
914,"Pretty disingenous for you to refer to yourself as a member of Caddy's ""we"" when so far as I can tell you're not involved: https://github.com/mholt/caddy/graphs/contributors",0,1,msr
915,"@SirCmpwn I'm part owner of Light Code Labs, the legal entity that filed for the trademark.",1,0,msr
916,"I see. Well, in any case, be patient, and maybe also try contributing to your own web server?",0,1,msr
917,You got it 👍,0,0,msr
918,"Thanks for clarifying your involvement, @yroc92. It's great to hear you value open source!
In the United Kingdom, trademark violation requires a mark to be used ""in the course of trade"". I have no plans to make commercial use of this project in any form whatsoever (including but not limited to adding sponsor headers to HTTP responses and then charging for them to be removed).
With that said, I will be removing references to Caddy throughout the codebase as you have requested, though the primary benefit here in my opinion is reducing user confusion and ensuring users do not go seeking support from the wrong place.",0,0,msr
919,"@lol768 As long as you're dolling out legal advice on the Internet, you might well quote the other relevant section of the Trade Marks Act of 1994.
> A person may also infringe a registered trade mark where the sign is similar and the goods or services are similar to those for which the mark is registered and there is a likelihood of confusion on the part of the public as a result
See: section 10(2)(b) http://euipo.europa.eu/pdf/mark/nl_uk_1_en.pdf
I am not a lawyer, but I am somewhat versed in trademark law. Your dependence on ""commercial use"" may be unsupportable, but talk to an attorney for legal advice.
LCL are actually doing the right thing to defend their mark by providing notice.",0,1,msr
920,"Thanks for contributing your expertise @gonzopancho :) I do apologise for giving myself legal advice earlier.
The start of section 10 (2) states:
>A person infringes a **registered trade mark** if he uses **in the course of trade** a sign where
because:
Your quoted section (""10(2)(b)"") is indented **underneath the above**. Emphasis is mine.
----------
This is moot however, because I have already committed to change the name.",0,1,msr
921,"Just wanted to comment here and say thank you for the respect and the professional tone of the discussion. I was in class most of this morning and so Cory and I haven't had a chance to coordinate. I stand by Cory's choice to bring up the issue and I think this discussion was a good one to have.
As you know, per the Apache license, forks are required to maintain copyright notices and give credit to the original project, etc, so the name Caddy *should* be used somewhere in a visible place, I think the main concern was with the logo being used prominently. But the README was quickly updated, which we appreciate.
We're glad there are people who value Caddy enough and feel strongly enough about it to fork it with their own changes. Hopefully this won't result in a splintered effort to make the Web better and safer, instead we look forward to mutually benefitting somehow.",0,0,msr
922,"Hey @mholt,
Great to see you weigh in here and thanks for the conversation on HackerNews. It was useful to discuss things, even if I do completely disagree with the direction in which you're going with this.
> As you know, per the Apache license, forks are required to maintain copyright notices and give credit to the original project, etc, so the name Caddy should be used somewhere in a visible place, I think the main concern was with the logo being used prominently. But the README was quickly updated, which we appreciate.
Apologies for that. GitHub's fork feature is great at duplicating everything, so the README was misleading for a while. I'm happy to mention that this is a fork of Caddy in the README as required by the Apache license and will look at making this more obvious if it's not already apparent.
Are you happy for me to mention that Caddy has a paid support offering and link https://caddyserver.com/pricing in the README for users seeking commercial support? If it wasn't already clear, my intentions are to remove the code I disagree with for people who want to use Caddy _personally_ (though I'm also of the opinion that the binaries I will be distributing here shouldn't be limited). I'm not here to try and sell a commercial support offering.
> We're glad there are people who value Caddy enough and feel strongly enough about it to fork it with their own changes. Hopefully this won't result in a splintered effort to make the Web better and safer, instead we look forward to mutually benefitting somehow.
As long as you're intending on a) leaving the trademark issue alone and b) keeping the upstream repository open source (and licensed under an appropriate free software license), I'm happy to contribute any relevant fixes/improvements upstream and include the link I mentioned above.
It is unfortunate that at present you don't seem open to reversing your decision to introduce advertising into Caddy. I still dislike what you're doing with the binaries, but I don't dislike it enough to motivate me to continue maintaining this repository - it's really the header I have a problem with.
Caddy is an excellent piece of software, I would really urge you to read through the HackerNews thread again at some point and reconsider if this is the approach you want to take. Neither of us want to see more fragmentation here, I think.
Cheers,
Adam",0,0,msr
923,"> Apologies for that. GitHub's fork feature is great at duplicating everything, so the README was misleading for a while. I'm happy to mention that this is a fork of Caddy in the README as required by the Apache license and will look at making this more obvious if it's not already apparent.
Yeah, no problem. We were very quick to jump in here after the fork; I realize from my own experience that this isn't enough time. It was a very busy, fast-moving morning. :)
> Are you happy for me to mention that Caddy has a paid support offering and link https://caddyserver.com/pricing in the README for users seeking commercial support? If it wasn't already clear, my intentions are to remove the code I disagree with for people who want to use Caddy personally (though I'm also of the opinion that the binaries I will be distributing here shouldn't be limited). I'm not here to try and sell a commercial support offering.
Sure, you may certainly do that.
> As long as you're intending on a) leaving the trademark issue alone and b) keeping the upstream repository open source (and licensed under an appropriate free software license), I'm happy to contribute any relevant fixes/improvements upstream and include the link I mentioned above.
Sounds good to me. The use of our logo at the top of your fork was quickly resolved anyway.
> It is unfortunate that at present you don't seem open to reversing your decision to introduce advertising into Caddy.
Well, it's only been 7 hours. :) Let's give it some time and see how things are when the dust settles. Believe me, it's not a decision I took lightly.
> Caddy is an excellent piece of software, I would really urge you to read through the HackerNews thread again at some point and reconsider if this is the approach you want to take. Neither of us want to see more fragmentation here, I think.
Thanks - yes, I agree. (Frankly I'm not too convinced by the HN arguments -- as usual -- except for recognizing the fact that our pricing can be out of reach for small, bootstrapping startups; we encourage them to contact us about special pricing in the meantime.)",0,0,msr
924,">Frankly I'm not too convinced by the HN arguments
My main concern, which you probably saw, is that your website is deceptive. Nothing's wrong with getting paid for your work, but your website is designed in bad faith.",0,1,msr
925,"@SirCmpwn > My main concern, which you probably saw, is that your website is deceptive. Nothing's wrong with getting paid for your work, but your website is designed in bad faith.
Come again&mdash;what's deceptive?",0,0,msr
926,I wrote about it [on HN](https://news.ycombinator.com/item?id=15238969).,0,0,msr
927,"I'm confused at what this fork offers that Caddy doesn't... Is it the code that shows the headers that you removed all that's stopping you from using Caddy? You do realize that you can compile Caddy yourself without those headers in it, without forking it and maintaining a whole new project, right?
From my point of view people are upset by 2 things. 1: They can't get pre-compiled binaries without the header and 2: if they can, they have to pay for it.
If anything, this fork adds MORE work on you than what Caddy says to do to use it free of charge without the headers.",0,1,msr
928,"this trademark claim appears to be a false claim. the original caddy is hosted on github, and per the [github faq](https://help.github.com/articles/licensing-a-repository/):
> Note: If you publish your source code in a public repository on GitHub, according to the Terms of Service, other GitHub users have the right to view and fork your repository within the GitHub site. If you have already created a public repository and no longer want users to have access to it, you can make your repository private. When you convert a public repository to a private repository, existing forks or local copies created by other users will still exist. For more information, see ""Making a public repository private.""
i realize that you've already complied with the request, but based on this, it appears that the caddy team are not acting in good faith",0,0,msr
929,"> not acting in good faith
A misunderstanding or miscommunication is a far cry from ""not in good faith"". I'm gonna stand firm here and re-assert that our actions are in good faith.
The misunderstandings about the use of the logo have been resolved.",0,0,msr
930,The bad faith part comes in because so far as I can tell you are *deliberately* misleading users.,1,1,msr
931,"> I'm confused at what this fork offers that Caddy doesn't... Is it the code that shows the headers that you removed all that's stopping you from using Caddy?
Removal of adware and binaries that can be freely distributed and are not subject to an EULA.
> You do realize that you can compile Caddy yourself without those headers in it, without forking it and maintaining a whole new project, right?
Yes, I didn't overlook that :)
> From my point of view people are upset by 2 things. 1: They can't get pre-compiled binaries without the header and 2: if they can, they have to pay for it.
> If anything, this fork adds MORE work on you than what Caddy says to do to use it free of charge without the headers.
From my point of view (and from reading HackerNews) people are upset about the ads embedded in the webserver and served to the visitors. They're also upset about how the binaries are licensed.
The fork is absolutely more work for me. The point is that I care enough about this that I'm willing to create the fork, remove the code I disagree with, cross-compile it and then make free (as in freedom) binaries available to everyone - including those who may not be technically adept enough to set up their own golang workspace and compile it themselves.",0,1,msr
932,I think forking on Github does not require removing any reference to the project you forked from.,0,0,msr
933,"> The fork is absolutely more work for me. The point is that I care enough about this that I'm willing to create the fork, remove the code I disagree with, cross-compile it and then make free (as in freedom) binaries available to everyone - including those who may not be technically adept enough to set up their own golang workspace and compile it themselves.
Until you run into the issues that Caddy had where it actually costs a lot of money to maintain and use the infra to do such things in a way that is helpful.
Then we're right back where we started, you doing something in a way to help make money to support that stuff, everyone getting upset, and forking a forked project that was forked from another fork that forked from a project that started such tactics to help.
The circle of open sores, I like to call it.",0,0,msr
934,"I find it highly unlikely that's going to happen. I have infrastructure, if you ever need somewhere to host builds or tooling like caddy offers, reach out and I'll share it for free.",0,0,msr
935,"5 years later, fair to say this fork was created out of spite with no intentions to sustain it 😂
Grandstanding sure is cheap.",0,1,msr
936,They stopped the bad faith behavior upstream so there's no longer any reason for this fork to exist. Imagine sticking up for a company that you have nothing to do with when they go around being a trademark bully. You sure showed that petty FOSS volunteer the error of their defiance towards the honorable corporate entity!,0,1,msr
937,"> They stopped the bad faith behavior upstream so there's no longer any reason for this fork to exist. Imagine sticking up for a company that you have nothing to do with when they go around being a trademark bully. You sure showed that petty FOSS volunteer the error of their defiance towards the honorable corporate entity!
Imagine thinking that a solo developer trying to make a living from his work is some trademark bully and evil corporate entity, just because they have an LLC and premium support plans.",0,1,msr
938,"Trying to make a living does not make it okay to be deceptive and misleading or to threaten others. There are plenty of ways to make a living without being deceptive, especially for a talented programmer. Get your moral compass re-calibrated.",0,1,msr
939,"> 5 years later, fair to say this fork was created out of spite with no intentions to sustain it 😂
Bit of a strange thing to do, resurrecting an issue 5 years on to post .. that.
However - as Drew has already pointed out, the fork became obsolete as soon as upstream 180'd and dropped the tacky, baked-in ads. I'm glad no significant fork maintenance / investment in a build system was required in the end, given Matt took the sensible decision to merge the PR to remove the header (which had been added without even consulting his sponsors, putting some of them in an incredibly awkward position amongst the negative reception).
Most of us have now moved on. I am glad Caddy has a more sustainable funding position - I see this debacle as an illustration of the power of open source (and forking) providing a ""checks and balances"" mechanism when it comes to undesirable moves from upstream.",0,0,msr
940,"Sometimes I'm doing a PR and I want to update a specific dependency but I don't want to deal with updates of all my dependencies (aiohttp, flake8, etc…). If any breaking change was introduced in those dependencies, I want to deal with it in another PR.
As far as I know, the only way to do that would be to pin all the dependencies that I don't want to update in the Pipfile. But I find it to defeat the purpose of Pipenv in the first place :) .
So my feature request would be to be able to do something like:
```shell
$ pipenv lock --only my-awesome-dep
```
That would generate a Pipfile.lock with updates for only `my-awesome-dep` and its dependencies.
I can probably make a PR for that, but I would like to get some feedback first.",0,0,msr
941,"That could also be useful for `pipenv install`, as sometimes I want to install a new dependency without updating others.",0,0,msr
942,"There's a little thing to take into account here: Changing a single dependency could change the overall set of requirements.
Ex: Updating `foo` from 1.0 to 2.0 could require to update `bar` to >=2.0 (while it was <2.0 before), and so on.
I know that in the context of `pip-tools` itself (from which `pipenv` takes its dependency resolution algorithm), running the dependency resolution will only ""update"" the required packages when ""re-locking"" if there's an existing lock file. It does so by checking if the existing pins in the lockfile are valid candidate first when selecting candidate in the resolving. `pipenv` could probably do the same.
I think its a reasonable idea. Otherwise, if you want to update absolutely only one dependency, `pipenv` would have to have a mode to block if changing a dependency causes other changes, or else you would loose the guarantee of a valid environment.
I hope this helps!",0,0,msr
943,"Indeed, that was what I meant by:
> That would generate a Pipfile.lock with updates for only my-awesome-dep and its dependencies.",0,0,msr
944,"Agree 100% - and I'll go a bit farther: this should be the default.
That is, `pipenv install foo` should never touch anything besides `foo` and its dependencies. And `pipenv lock` should certainly never upgrade anything - it should just lock what's already installed.
AFAICT, this is how `npm`, `yarn`, `gem`, etc. work; it makes no sense to have a lockfile that doesn't actually lock packages, but trusts package authors to not break things in patch releases, and therefore upgrades them without being asked. I can see the use of allowing upgrades, but that should be opt-in, since it's more surprising than not upgrading them.
I apologize if I'm hijacking this issue for something else, but since this is so closely related to an issue I was about to create, I thought I'd start the conversation here. Feel free to tell me I should make a new one.",0,0,msr
945,"Just found this related issue as well: https://github.com/kennethreitz/pipenv/issues/418
Being able to specify `pipenv install --upgrade-strategy=only-if-needed` seems like what I'm looking for, though of course as I mentioned I think that should be the default, as it's becoming in pip 10. But being able to specify it semi-permanently via env var would be something, anyway.
I would be surprised if that change breaks anyone's workflow ([famous last words](https://xkcd.com/1172/)), since it's more conservative than `--upgrade-strategy=eager`.",0,0,msr
946,"Tried to work around this by setting `export PIP_UPGRADE_STRATEGY=only-if-needed` in my shell config. This doesn't work, and `pipenv lock` exhibits these surprising behaviors:
1. It ""upgrades"" packages that don't need to be upgraded (but...)
1. It actually doesn't upgrade the installed versions! i.e. `pip freeze` and `Pipfile.lock` show different versions!
Guessing pipenv is delegating to pip for the install, and pip respects its environment variable settings, but `pipenv lock` doesn't.",0,0,msr
947,"@k4nar What happens right now that you are finding undesirable? Because if you upgrade a dependency that has cascading requirements obviously it will have consequences for other dependencies. Are you suggesting some kind of resolver logic to determine the most current version of a specific package _in the context of the current lockfile_? I am hesitant to encourage too many hacks to resolution logic, which is already complicated and difficult to debug.
@brettdh I think I can shed some light because you have most of the pieces. `pipenv lock` doesn't install anything, and it doesn't claim to. It only generates the lockfile given your host environment, python version, and a provided `Pipfile`. If you manipulate your environment in some other way or if you use pip directly/manipulate pip settings outside of pipenv / are not using `pipenv run` or using `pip freeze` inside a pipenv subshell, it is quite easy for a lockfile to be out of sync from `pip freeze`. The two aren't really related.
To be clear:
1. `Pipfile.lock` is a strictly-pinned dependency resolution using the pip-tools resolver based on the user's `Pipfile`
2. If you want to maintain strict pins of everything while upgrading only one package, I believe you can do this by strictly pinning everything in your `Pipfile` except for the one thing you want to upgrade (correct me if I'm wrong @vphilippon)
As for your lockfile and `pip freeze` disagreeing with one another, I'd have to know more information, but I believe we have an open issue regarding our lockfile resolver when using non-system versions of python to resolve.",0,0,msr
948,"@techalchemy : If I have a Pipfile.lock with A, B and C where B is a dependency of A, I would like to be able to update A and B without updating C, or C without updating A and B.
Again of course I can pin all my dependencies & their dependencies in my Pipfile in order to do that, but that would be a burden to maintain (like most `requirements.txt` are).",0,0,msr
949,"I concur with everything @k4nar wrote. Sure, I could even just pin
everything in requirements.txt and not use pipenv. The point of pipenv is
to have one tool that makes that (and the virtualenv stuff, of course)
simpler to manage; i.e. all packages are locked by default to a version
that’s known to work, but it should be straightforward to upgrade a select
few (without unexpectedly upgrading others).
On Thu, Oct 26, 2017 at 4:28 AM Yannick PÉROUX <notifications@github.com>
wrote:
> @techalchemy <https://github.com/techalchemy> : If I have a Pipfile.lock
> with A, B and C where B is a dependency of A, I would like to be able to
> update A and B without updating C, or C without updating A and B.
> Again of course I can pin all my dependencies & their dependencies in my
> Pipfile in order to do that, but that would be a burden to maintain (like
> most requirements.txt are).
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/kennethreitz/pipenv/issues/966#issuecomment-339591307>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAFlnqUOEKARiFD8kEk3GVczF3NXBdVOks5swEKcgaJpZM4QEf-->
> .
>",0,0,msr
950,"Hm I see what you guys are saying. The premise of passing a setting to pip is not what I’m worried about, it’s resolving with pip-tools that concerns me. What does this behavior look like right now?",0,0,msr
951,"@techalchemy I mentioned the `pip freeze` difference as a shorthand for ""the package versions that `pipenv install` installs differ from the package versions that `pipenv lock` saves to `Pipfile.lock`.""
True, this only happens when I've changed pip's default args via environment variable; I was just pointing out that it was surprising that pipenv delegated to pip for installation but not for version locking; i.e. rather than locking what's installed, it locks what it thinks *should* be installed, potentially with unrequested upgrades.
Could you clarify your question a bit? I think ""resolving with pip-tools"" is referring to what `pipenv lock` is doing, and the reason it's not affected when I set pip defaults? And could you be more specific about what you mean by ""this behavior""?",0,0,msr
952,"@brettdh The locking mechanism include a notion of ""dependency resolution"" that does not exist in `pip`. Its handled by `pip-tools` (or rather, a patched version of it, integrated in a special way by `pipenv` that bring a few differences with the original tool). In short, the locking mechanism reads the `Pipfile` and performs a full dependency resolution to select a full set of package that will meet *every* constraints defined by the required packages *and their dependencies*.
@techalchemy > [...] it’s resolving with pip-tools that concerns me.
I'm not sure how those `--upgrade-strategy` would affect `pip-tools`, because it works on some low-level internals of `pip`. I have the feeling this would not give the expected result, as these option take into account what's installed, and that's not what's being dealt with in that mechanism. But we have another approach to this in `pip-tools` that could be done here.
The ""original"" `pip-tools` behavior is that it only updates what's is needed in the lockfile (in its context, its the requirements.txt), but this was ""lost"" in the way the resolver was integrated in `pipenv`. Let me explain why.
Pointing back to my resume of how `pip-tools` works: https://github.com/kennethreitz/pipenv/issues/875#issuecomment-337717817
Remember the ""select a candidate"" part? That's done by querying the `Repository` object.
In `pipenv`, we directly configure a `PyPIRepository` for the `Resolver`, but `pip-tools` does something else, it uses a `LocalRequirementsRepository` object, which keeps the existing pins from the previously existing `requirements.txt` (if found), and ""fallbacks"" on `PyPIRepository`.
So in `pip-tools`, the following happens when selecting a candidate:
1. Query `LocalRequirementsRepository` for a candidate that match `foobar>=1.0,<2.0`.
1. Check if an existing pin meets that requirements:
- If yes, return that pin as the candidate.
- If not, query the `proxied_repository` (`PyPIRepository`) for the candidate.
1. Use the candidate returned
Effectively, it means that existing pins are given a ""priority"" as candidate to try first.
But in `pipenv`, currently, it simply:
1. Query `PyPIRepository` (directly) for a candidate that match `foobar>=1.0,<2.0`.
1. Use the candidate returned.
So, I think the same behavior for the locking in `pipenv` could be done by parsing the `Pipfile.lock` to get the existing pins and use a `LocalRequirementsRepository`, like `pip-tools` does in its `pip-compile` command.",0,0,msr
953,@vphilippon do you have a sense of how difficult implementation on that would be?,0,0,msr
954,"@techalchemy - Parsing the `Pipfile.lock` to extract the existing pins: Haven't looked at that. Depends on how things are structured in `pipenv`. We need a set of `InstallRequirements` that represents the pins in the `Pipfile.lock`.
- Using `LocalRequirementsRepository`: Fairly easy: change our current `PyPIRepository` for a `LocalRequirementsRepository`.
---------------
But, as I'm looking into this, and following @brettdh comments, I realize a few things:
1. The current default `pipenv install` behavior doesn't match the `pipenv lock` behavior. Doing `pipenv install requests` alone won't update `requests` if a new version comes out (much like straight `pip install`). However, doing `pipenv lock` will update the `Pipfile.lock` with the latest version of `requests` that matches the `Pipfile` specifier, and the dependency constraints.
There's 2 main way to see this:
- A) The `Pipfile.lock` should stay as stable as possible by default, not changing pins unless required, in order to stay like the current environment, and only change in the event that we change the environment.
- B) The `Pipfile.lock` should get the newest versions that respect the environment constrains/dependencies in order to freely benefit from the open ranges in the `Pipfile` and lib dependencies, allowing to continuously acquire new compatible versions in your environment. You can then run `pipenv update` to benefit from the fresh lock.
IMHO, I would align the default behavior, which would be to go with A) by default. Because right now, everytime a lock is performed (i.e. after each installation), new versions can come in, which make the lockfile *drive the update of the environment*, which seems weird. But, this is arguable of course. While in development, I might want to continuously update my requirements to no get stale, like with B), so that should also be easily doable.
2. Even if we use `LocalRequirementsRepository` to avoid updating correct existing pins, and end up aligning the default behaviors, we then need to address the equivalent of `--upgrade` and `--upgrade-strategy` for the locking part. Currently, defining some environment variable (like `PIP_UPGRADE` and `PIP_UPGRADE_STRATEGY`) will affect the `pipenv install` behavior, but will not affect `pipenv lock`, as it doesn't affect the behavior of `pip-tools` (I confirmed that, as I was unsure at first).
Otherwise, there will be no way to update the environment without either deleting the `Pipfile.lock` (feels clunky, and ""all or nothing"") or *requiring* a newer version (I mean doing an explicit `pipenv install requests>2.18.4`, which requires you to *know* that a new version is out, and changes the specifier in the `Pipfile` itself, increasing the lower bound), which is wrong. As the ""original `pip-tools`"" doesn't deffer to `pip` to deal with this (as it's not related that what is currently installed), it offers an option to specify the dependencies to update in the lockfile, and simply remove the pins for these packages (or all) from the existing_pins list, effectively falling back to querying PyPI. I'm not sure how we can match the notion of ""--upgrade-strategy"" with this.
---------------
@techalchemy So while I was saying it was fairly easy to just ""align the default behavior"", I now realize that this would cause some major issue with being able to update the packages (as in: just fetch the latest version that match my current constraints).
If there's something unclear, ask away, a lot of editing went on when writing this.
(Dependency resolution is not easy. Good and practical dependency resolution is even worst 😄 )",0,0,msr
955,"@vphilippon that's exactly what I meant. Keeping the things that pip installs in sync with the things that pip-tools resolves is non-trivial unless you drive the process backwards, using the resolved lockfile to do the installation. I'm pretty sure that was why things were designed the way they were. > B) The Pipfile.lock should get the newest versions that respect the environment constrains/dependencies in order to freely benefit from the open ranges in the Pipfile and lib dependencies, allowing to continuously acquire new compatible versions in your environment. You can then run pipenv update to benefit from the fresh lock.
This workflow can possibly work with the current configuration. You can use `pipenv lock` to generate a lockfile, but `pipenv update` will reinstall the whole environment. I'm pretty sure we can use one of our various output formats to resolve the dependency graph (we already have a json format as you know) and only reinstall things that don't align to the lockfile. This might be more sensible, but I would be curious about the input of @nateprewitt or @erinxocon before making a decision",0,0,msr
956,"@vphilippon Totally agree that A and B are desirable workflows in different situations. Some of your phrasing around B confused me a bit, though, seeming to say that `pipenv lock` might result in a lockfile that doesn't actually match the environment - I particularly heard this in that one would need to ""run `pipenv update` to benefit from the fresh lock"" - as if the lock is ""ahead"" of the environment rather than matching it.
Regardless of whether you are in an A workflow or a B workflow, a few things seem constant to me, and I think this squares with what @techalchemy is saying as well:
* The result of `pipenv lock` should always be a lockfile that matches the environment.
* The result of `pipenv install` should always be an environment that matches the lockfile.
I'm ignoring implementation details, but that's kind of the baseline behavior I expect from a package manager with a lockfile feature.
Running `pipenv update` periodically allows you to stay in B mode as long as you want everything to be fresh, and having the ability to `pipenv install --upgrade requests` would allow specific updates of one package and its dependencies, without affecting packages that don't need to be upgraded unnecessarily.
Am I missing any use cases? I can think of optimizations for B - e.g. a flag or env var that tells it to always update eagerly - but I think that covers the basics. I also know I'm retreading ground you've already covered; it's just helpful for me to make sure I understand what you're talking about. :)",0,0,msr
957,"> Some of your phrasing around B confused me a bit, though, seeming to say that pipenv lock might result in a lockfile that doesn't actually match the environment
@brettdh this is correct -- the `pip-tools` resolver we use to generate `Pipfile.lock` doesn't ask the virtualenv for a list of which packages have been installed. Instead, it compiles a list of packages that meet the criteria specified in the list of pins from the `Pipfile`. Because the resolver itself runs using the system or outer python / pipenv / pip-tools install, we are doing some supreme fuckery to convince it to resolve packages with the same version of python used in the virtualenv. The assumption would be that `pip install` would resolve things similarly, but that isn't always the case, although even I'm not 100% sure about that. But yes, `pipenv lock` is not generated based on the virtualenv, it is generated based on the `Pipfile`. It is a dependency resolution lockfile, not an environment state pin.",0,0,msr
958,"As a potential resolution to this: something that pip itself currently supports, but `pip-compile` doesn't, is the notion of a constraints file.
A constraints file differs from a requirements file, in that it says ""*If* this component is installed, then it *must* meet this version constraint"". However, if a particular package in the constraints file doesn't show up in the dependency tree anywhere, it doesn't get added to the set of packages to be installed.
This is the feature that's currently missing from `pipenv`, as the desired inputs to the `Pipfile.lock` generation are:
1. The updated `Pipfile` contents as a new requirements input file
2. The full set of existing dependencies from `Pipfile.lock` as a constraints file, excluding the packages specifically named in the current command
Constraints file support at the pip-tools resolver level would then be enough for `pipenv` to support a mode where attempted implicit upgrades of dependencies would fail as a constraint violation, allowing the user to decide whether or not they wanted to add that package to the set being updated.",0,0,msr
959,"currently not supported, thanks for the feedback",0,0,msr
960,"@kennethreitz Do you mean:
1. This behavior should be changed, but it's not currently a priority,
2. This behavior should be added as something optional, but it's not currently a priority, or
3. This behavior should not be added?
This is a sufficient inconvenience given the inconsistency with how other similar locking package managers work that it would be good to keep this open as a solicitation for PRs.
If instead it's (3), and this will not be added, then I think a number of us on the issue will need to adjust our plans for our choice of Python package management tools.",0,0,msr
961,"I mean that this is currently not supported, and I appreciate the feedback.",1,0,msr
962,I understand that it's not supported. Are you also saying that you would not accept PRs either changing this behavior or adding this as an option?,0,0,msr
963,I have no idea.,0,0,msr
964,"@k4nar still interested in doing a PR for this? Specifically, something like `pipenv install --only <dep-to-update` which prevents unrelated deps from being updated. Since @kennethreitz seems uninterested in discussing further, it seems to me that that's the only way to find out whether that behavior addition/change could be acceptable (and, by extension, whether folks like @taion and I can continue using pipenv).",0,0,msr
965,"I'm interested but I'm not sure to know how would be the best way to implement this. There are a lot of components in action (pip, pip-tools, pipfile, pipenv…) and probably a lot of possible solutions.",0,0,msr
966,"Per https://github.com/kennethreitz/pipenv/issues/966#issuecomment-339707418, it should be relatively straightforward. That dep resolution logic is largely just from pip-tools. I was planning on submitting a PR, but I can't justify spending the work if we're not willing to talk about how we want the API to look before we spend time writing code.
I'm currently looking at taking an alternative approach – as Pipfile is a standard, interactions with it don't need to go through pipenv, and I'd like to work around some of the other odd semantics here like wiping existing virtualenvs per https://github.com/kennethreitz/pipenv/issues/997.",0,0,msr
967,"Sorry to comment on a closed issue, but I'd like to point out that, to my understanding, using pipenv in my projects currently requires a workflow like this:
```
pipenv install foo
vim Pipfile.lock # Manually remove all the unwanted updates
git add && git commit && git push
```
I find it really annoying having to communicate this to my team members. The alternative seems to be to pin everything to exact versions in `Pipfile`, but that defeats much of the purpose of using pipenv in the first place.
IIUC, this behavior is the equivalent of `apt` performing an implicit `apt dist-upgrade` whenever you run `apt install foo`.
This is made worse by the fact that `pipenv install` updates stuff in `Pipfile.lock`, but does not install the updates into the local virtualenv. If the developer does not carefully examine the diff of `Pipfile.lock`, they are still using the older versions locally, but once they share the code, all other environments see the surprising updates. People have a tendency to ignore the diff of `Pipfile.lock` because it's considered an auto-generated file.
I am strongly convinced that ""update everything to the latest version allowed by `Pipfile`"" should be an explicitly requested operation that is separate from ""install foo"".",0,0,msr
968,should be fixed in master,0,0,msr
969,"The behaviour is still present, I tested it in `pipenv 11.8.3`, @kennethreitz.",0,0,msr
970,"@marius92mc The ""fixed in master"" comment is referring to the `--selective-upgrade` and `--keep-outdated` options added in recent releases: https://docs.pipenv.org/#cmdoption-pipenv-install-keep-outdated
That allows folks that need or want more control over exactly when upgrades happen to opt in to that behaviour, while the default behaviour continues to respect [OWASP A9](https://www.owasp.org/index.php/Top_10-2017_A9-Using_Components_with_Known_Vulnerabilities) and push for eager upgrades at every opportunity.",0,0,msr
971,"@ncoghlan I think one thing that is needed (easy to ask for, not as easy to do) is an FAQ on _how_ those options behave (at least it's still confusing for me).
For instance: Using `--selective-upgrade` and `--keep-outdated` will still cause outdated libraries in the `Pipfile.lock` to be updated, if they're not directly related to the ""selected"" package to be updated.",0,0,msr
972,"It sounds like there may be implementation bugs, then.",0,0,msr
973,"They are intended to leave the pipfile.lock as-is, except for the new change.",0,0,msr
974,Let me know if it's helpful to provide a test Pipfile+.lock.,1,0,msr
975,I think you've provided enough information for us to investigate. I'll try to do that now.,1,0,msr
976,"Actually, your pipfile/lock would be great, if it contains outdated results.",0,0,msr
977,"@ncoghlan, thank you for providing the details. I tried again with your mentioned options and the result seems to be the same, it still updates the other packages as well, changing them in the `Pipfile.lock` file.",0,0,msr
978,@kennethreitz https://github.com/simonpercivall/pipenv-selective-upgrade-test,0,0,msr
979,"There are any updates about this issue, @kennethreitz?",0,0,msr
980,Sorry for the slow answers on this. We haven’t nailed down the root cause for the regression here yet (I know I personally have been handling a data center migration this weekend so I’ve been kinda slow) but we will get this sorted in the next few days. Contributions welcome as always!,0,0,msr
981,"I think there is a missing use case that can use this same change: when I am developing an application I often need to upgrade a single dependency's version. The steps I would like to follow are:
1. Update the version restriction for the dependency in setup.py
2. Run either `pipenv lock --selective-upgrade ; pipenv sync` or `pipenv install --selective-upgrade ""-e .""`",0,0,msr
982,"@wichert If `Pipfile` has been edited in a way that increases the minimum required version beyond what's in the current lock file, then `--keep-outdated` should already cover what you need. `--selective-upgrade` is for the case where `Pipfile` *hasn't* changed, but you want to update to a new pinned version anyway.",0,0,msr
983,"@ncoghlan `Pipfile` has not changed in this scenario, only `setup.py` by changing the minimum version requirement for a dependency, typically to something more recent and currently in `Pipfile.lock`.",0,0,msr
984,@wichert pipenv doesn't capture changes to your `setup.py` automatically because it isn't setuptools. You have to run `pipenv lock` if you want that to happen.,0,0,msr
985,"What's the current status of this? On March 25th someone said they thought implementation issues would be resolved ""in the next couple days"", and other bug reports have been closed due to being tracked here; but as of 2018.7.1 I still see the bug reported by Simon Percivall (indirect dependencies are always updated) and that bug hasn't been discussed since the original report. Is the problem still being tracked?
(I'm currently living in a second-tier city in Senegal so my Internet is terrible and it would be a game changer not to blow my data cap on updating indirect dependencies if possible :P )
PS: Thanks for making Pipenv, it's awesome <3",0,1,msr
986,Yes for sure. We are rewriting the resolver to support this right now. Whether it lands in this release or next release remains to be seen,0,0,msr
987,"I’m not that confident with my coding skill to estimate when the resolver would land :p Seriously, this is a completely volunteer project, and we don’t have a deadline mechanism as you would in commercial settings (we don’t even have a boss or a project manager or whatever you have in your company that decides when a thing needs to be done). If you want a thing to be done in a timeframe you desire, you need to do it yourself, or at least provide real motivation for others to do it.",0,1,msr
988,"@uranusjr FWIW, I didn't see any demands for expediency in @benkuhn 's comment above - just a question about where things are at; i.e. what work has been done, so that outside observers can make their own estimates/decisions.
I understand that pipenv is a volunteer project and that non-contributors cannot ask for a thing to be done by a date without signing up to make it happen. I do wonder, whether there is room for more transparency in the project's development process, or if I'm just not looking in the right places. Usually the answer is either ""if the issue hasn't been updated, there's been no movement"" or ""look at this WIP pull request,"" but this issue in particular seems to have triggered a much larger effort, so the dots can be difficult to connect for those not directly involved.
As always, much thanks to you and everyone who gives their valuable time towards the improvement of pipenv. 👏",0,0,msr
989,"For sure, this one doesn’t have activity or a work in progress PR because it is a lot more complicated than that. We are talking internally mostly about how we even want to structure this with respect to the larger project, and working iteratively to establish an approach that might even begin to work properly. Once we can sort that out we can build resolution logic. In the meantime the resolver stack in pipenv is super convoluted and I wouldn’t be comfortable asking people to invest too much effort trying to untangle it for this purpose. Even the simplest use case here will take a significant refactor. We’d be happy to review / discuss any proposed refactor if someone is interested in helping tackle this, but the two things are tightly coupled. If someone has expertise in dependency resolution and sat solving we would certainly be interested in input but there just isn’t a single concrete idea yet. We’ve been through several iterations that we never planned to carry forward as more than proof of concept. Not all code becomes a PR, and not all code organization decisions happen on the issue tracker. Sometimes we chat synchronously and propose and scrap ideas in real time.",0,0,msr
990,"Something I was _going_ to suggest as an alternative workflow that might address this is making it easy to pin to a specific version in the _Pipfile_ when installing.
I think it's slightly surprising but not completely unreasonable that pipenv interprets `foo = ""*""` to mean ""I just need to make sure _some_ version of foo is installed, the user doesn't care which"". To that end, having something like `pipenv install --pin foo` which results in `foo = ""==1.2.3""` instead of `foo = ""*""` in the Pipfile (where 1.2.3 is the current latest version of foo) seems like it might help.
The issue with this though is that the behavior of a lot of packages can change a lot based on their dependencies (e.g., the same version of pylint can do totally different things depending on what version of astroid it's using), and packages don't pin their own deps exactly. So I don't think this actually gets anyone very far. :/",0,0,msr
991,"(Just realised I have been commenting to the wrong issue. Sorry for the mess up, please ignore me) 😞",0,0,msr
992,"An actual use case that I've struggled with for some hours now: I want to measure test coverage in a Django 2.0 project. Even `pipenv install --keep-outdated --selective-upgrade --dev coverage` insists on updating the non-dev Django package to version 2.1, which because of breakage elsewhere I absolutely cannot use yet. **There really *must* be a way to change the set of installed packages without upgrading completely unrelated packages to possibly breaking versions.** The possibility of breakage in the latest version will always exist.
I'll try [@rfleschenberg's workaround](https://github.com/pypa/pipenv/issues/966#issuecomment-366333367), but I don't know whether having a presumably incorrect `_meta hash` property will break anything.",0,0,msr
993,"@l0b0 If your application genuinely cannot handle a particular version of Django, I think it makes sense to state that restriction in your Pipfile?",0,0,msr
994,@AlecBenzer That sounds like something for setup.py to me.,1,0,msr
995,"@wichert That might make sense too (I'm actually not totally clear on in what circumstances you'd want to have both a setup.py and a Pipfile), but if you have a line in your Pipfile that says:
```
Django = ""*""
```
you're telling pipenv that you want it to install _any_ version of Django. If what you really want it to do is install 2.0 or lower, tell it that instead:
```
Django = ""<=2.0.0""
```
While in this particular case pipenv is upgrading Django for no real reason, it could be that somewhere down the line you try to install a package that requires Django >2.0.0, at which point pipenv will happily install it if you haven't told it that you need <=2.0.0.",0,0,msr
996,"> If what you really want it to do is install 2.0 or lower, tell it that instead
@AlecBenzer on reflection, it now occurs to me that this is what npm/yarn do by default when you install a package; they find the latest major.minor version and specify `^major.minor.0` in package.json, which prevents unexpected major version upgrades, even when an upgrade-to-latest is explicitly requested. I wonder if Pipenv should do the same - but that would be a separate issue. Of course, their lock file also prevents accidental upgrades of even minor and patch versions, which is what's being requested here.",0,0,msr
997,"I think it's been discussed above and elsewhere, but there is a tension/tradeoff in the design space between npm/yarn and pipenv. Any package manager ostensibly has these goals, with some relative priority:
- Make it easy to install and upgrade packages
- Make it hard to accidentally break your app with an errant dependency upgrade
The trouble with pinning an exact version in the Pipfile is that it's then harder to upgrade packages; this is why [pip-tools](https://github.com/jazzband/pip-tools/) exists (though it's for requirements.txt).",0,0,msr
998,"The `--keep-outdated` flag does not seem to be working as intended, as was stated when the issue was re-opened. Whether that behavior should or should not be the default and how it aligns with other package managers is not really the central issue here. Let's fix the thing first.",0,0,msr
999,"@brettdh > on reflection, it now occurs to me that this is what npm/yarn do by default when you install a package; they find the latest major.minor version and specify ^major.minor.0 in package.json, which prevents unexpected major version upgrades, even when an upgrade-to-latest is explicitly requested. I wonder if Pipenv should do the same - but that would be a separate issue.
Yeah that's along the lines of what I was trying to suggest in https://github.com/pypa/pipenv/issues/966#issuecomment-408420493",0,0,msr
1000,"Really excited to hear this is being worked on!
In the mean time, does anyone have a suggested workaround that's less laborious and error-prone than running `pipenv lock` and hand-reverting the resulting lockfile changes that I don't want to apply?",0,0,msr
1001,@benkuhn Not that I know off - I do the same lock & revert dance all the time.,0,0,msr
1002,"Ah ok, you can at least sometimes avoid hand-reverting:
1. `pipenv lock`
2. `git commit -m ""FIXME: revert""`
3. `pipenv install packagename`
4. `git commit -m 'Add dependency on packagename'`
5. `git rebase -i`
6. Drop the `FIXME: revert` commit
Unfortunately it's still possible to create an inconsistent `Pipfile.lock` if your `Pipfile.lock` starts out containing a version of a package that's too old to satisfy the requirements of `packagename`, but perhaps pipenv will complain about this if it happens?",0,0,msr
1003,"`--keep-outdated` seems to *systematically* keep outdated only the explicit dependencies that are specified (unpinned) in Pipfile, while all the implicit dependencies are updated.",0,0,msr
1004,"Am I correct that it is not possible to update/install single dependency using `pipenv==2018.7.1` without updating other dependencies? I tried different combinations of `--selective-upgrade` and `--keep-outdated` with no success.
Editing Pipfile.lock manually is no fun...",0,0,msr
1005,"Same than @max-arnold , it's my first day using the tool in an existing project, and I have to say **I'm really disappointed**, before I started to use it, I checked the doc site and the video demo, it looked impressive to me, and now this: in real project, work with `pip` or `pipenv` is almost the same, i don't see the point, like many other said in the thread, if I have a lock file, why you are updating my other dependencies if there is no need to update them.
Of course, ### if the update is mandatory, it's OK to update all the necessary dependencies, but just those, not all the outdated instead.
Also the options `--selective-upgrade` and `--keep-outdated` are not clear for what are useful for, there is another issue highlighting this here #1554 , and nobody is able to respond what these options do, incredible.
But my **major disappointing** is why this package was **recommended by the Python official documentation** itself, these recommendations should be more careful conducted, I know this can be a great project in the feature, have a lot of potential, but simple things like this (we are not talking about a bug or a minor feature), make this project not eligible for production environments, but suddenly because it was recommended by the Python docs, everybody are trying to use it, instead of looking for other tools that maybe work better, or just stick with `pip`, that doesn't solve also these issues, but at least it's very minimalist and it's mostly included in any environment (does not add extra dependencies).",0,1,msr
1006,"@mrsarm Thank you for your opinion. Sorry things don’t work for you. I don’t understand where the disappointment comes from, however. Nobody is forcing Pipenv on anyone; if it doesn’t work for you, don’t use it. That is how recommendations work.
Your rant also has nothing particularly related to this issue. I understand it requires a little self-control to not dumping trash on people when things don’t go your way, but please show some respect, and refrain from doing so.",0,0,msr
1007,"@uranusjr it's not trash, it's an opinion, and some times it's not an option, like my case, where somebody chose pipenv to create a project where I started to work now, and I have to deal with this.
But things get worst just now, and what I going to say it's not an opinion, it's a fact.
After trying to add one dependency that just I dismissed to avoid to deal with this issue (because it's a dev dependency, so I created a second environment with `pip` and the old `requirements-dev.txt` approach, just with that tool), I needed to add another dependency.
The new dependency is PyYAML, let say the latest version. If you install it in any new environment with `pip`, you will see that the library does not add any dependency, so only PyYAML is installed, is that simple in these cases with Pip. But adding the dependency with Pipenv (because a project that I didn't create is managed with Pipenv) the same issue happened, despite PyYAML doesn't have any dependency, and it's not previously installed in the project (an older version), `pipenv` updates all my dependencies in the lock file and the virtual environment, but I don't want to update the others dependencies, I just one to add a single new module without any dependency.
So the conclusion (and again an opinion, not a fact like pipenv broke all my dependencies) it's that Pipenv instead of help me to deal with the dependencies management, it turn it into hell.",0,1,msr
1008,"I've followed this thread for months, and I think any real project will ultimately stumble upon this issue, because the behavior is unexpected, counter-intuitive, and yes: dangerous.
About a month ago I tried out a more-comprehensive alternative to `pipenv`, [`poetry`](https://github.com/sdispater/poetry); it solved the problems _I_ needed to solve:
1) managing one set of dependencies (setup.py, setup.cfg, pip, and pipfile -> pyproject.toml)
2) future oriented, backwards-compatible (again [pyproject.toml](https://www.python.org/dev/peps/pep-0518/))
3) fairly un-opinionated ([no i'm really not asking to install `redis`](https://github.com/pypa/pipenv/issues/1174))
4) and the solution to the classic Pipenv problem: ""Also, you have to explicitly tell it [pipenv] to not update the locked packages when you install new ones. This should be the default."" [[1](https://github.com/sdispater/poetry#what-about-pipenv)] [[2](https://github.com/pypa/pipenv/issues/966#issuecomment-339117289)]
I weighed sharing these thoughts on the `pipenv` issue, but as @uranusjr said, ""no one is forcing Pipenv on anyone"", and I'm not forcing Poetry. I like it, it works well, and it solves my problems, but I'm just sharing an alternative, more-comprehensive solution to the problem I was having. Just take all that as my 2¢.
* as a disclaimer, *I am not* a member of the Poetry team or affiliated with them.
p.s. I think the concern about Pipenv being the ""official"" solution is due to it's [first-class integrations](https://github.com/pypa/pipenv/blob/master/docs/advanced.rst#-community-integrations) – something that you, @uranusjr, might see it as a simple recommendation – the industry at large is taking it as the ""blessed approach going forward"". Frankly, that recommendation is more authoritative in the community than certain PEPs that have been around for more than a year.",0,0,msr
1009,"Nobody is forcing you to participate in our issue tracker; if you don’t have a productive comment please find a forum that is not for triaging issues.
For users who are interested in trying the alternative resolver @uranusjr and myself have been implementing for several weeks now, please try out https://github.com/sarugaku/passa which will generate compatible lockfiles. Poetry does a lot of different things, but it also has limitations and issues itself, and we have a design philosophy disagreement about scope. This is a project we manage in our spare time; if you want to see something fixed or you have a better approach, we are happy to accept contributions. If you are here to simply tell us we ruined your day and your project, I will ask you only once to see yourself out. We have not forgotten or ignored this issue, we have a full implementation of a fix in the resolver linked above. Have patience, be courteous, or find somewhere else to talk. To those who have been waiting patiently for a fix, please do try the resolver mentioned above— we are eager to see if it meets your needs. It implements proper backtracking and resolution and shouldn’t handle this upgrade strategy In the shorter term I think we can get a band aid for this into pipenv if we don’t wind up cutting over first.",0,0,msr
1010,"@dfee I am not really sure that blurring lines between applications and libraries is the correct answer to dependency management, so I don’t see poetry’s approach as an advantage. I wasn’t involved in whatever your issue was with the recommendation engine, but we took that out some time ago...",0,0,msr
1011,"@techalchemy
> I am not really sure that blurring lines between applications and libraries is the correct answer to dependency management, so I don’t see poetry’s approach as an advantage.
Why though? I never understood this idea that you should manage the dependencies of a library and an application differently. The only difference between the two is the lock file which is needed for an application to ensure a reproducible environment. Other than that it's the same thing. This is the standard in most other languages and Python seems the exception here for some reason and this is bad from a user experience standpoint since this is making things more complex than they should be.
> it also has limitations and issues itself
Which ones? I am really curious about the issues or limitations you encountered while using Poetry.",0,0,msr
1012,"My apologies to bean so rude. Now reading my comments I realize that despite the info I provided and some of my options are still valid (IMHO), it's wasn't appropriated the way I wrote what I wanted to say.
I understand that the issue tracker is most a place where to discuss bugs and improvements, and discuss whether this is a bug or an error by design is not clear in the thread, but again my apologies.
I thing there are two strong topics here:
- Should pipenv update all your outdated dependencies where you are trying just to install a new dependency: the ones that are not needed to update because the new package / version we are trying to install can works with the existent dependencies, and even the ones that aren't dependencies of the new package we are trying to install? Maybe this is out of scope of this ticket, but it's a really important topic to discuss.
- Do one of these parameters `--keep-outdated` `--selective-upgrade` allow us to avoid these behaviour? It's not clear what these options do, there is a lack of documentation about them, and even in the related issue (#1554) nobody is answering that.
In case it's a bug in on one of these params `--keep-outdated` `--selective-upgrade`, I still thinking that do not set whatever param solves the unnecessary update of the dependencies as default it's a really bad idea.
To compare with a similar scenario, imagine that you execute `apt-get install vim` to just install the `vim` tool in your system (and the necessary vim's dependencies or updates if apply), but imagine also that in this situation apt updates all the other dependencies of your system: python, the QT system, the Linux kernel... and so on. It's not that apt shouldn't allow us to updates other dependencies, but there is a clear command to do that: `apt-get upgrade`, while `apt-get install PACKAGE` just install / update PACKAGE and it's dependencies.",0,0,msr
1013,"@sdispater the distinction is at the heart of every disagreement we've ever had and it's incredibly subtle but I'd point you at https://caremad.io/posts/2013/07/setup-vs-requirement/ or a good article for the elixir use case: http://blog.plataformatec.com.br/2016/07/understanding-deps-and-applications-in-your-mixfile/
`pyproject.toml` isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file. `setup.cfg` exists for that purpose (the actual successor to `setup.py` ) and IMHO both of those should really be supported. A library is published and intended for consumption with abstract dependencies so that they can play nice in the sandbox with others; applications are usually large, complex beasts with sometimes hundreds of direct dependencies. So one of our main divergences is that when we design and build our tooling, we take this into account also
@mrsarm For your first question, the update behavior was intentional (and was discussed extensively at the time, /cc @ncoghlan and related to OWASP security concerns). On the second point, the behavior is currently not properly implemented which is why the issue is still opened, which led us to rewriting the backing resolver behind pipenv, which I mentioned above. It simply didn't support this behavior. `--selective-upgrade` is supposed to selectively upgrade only things that are dependencies of the new package, while `--keep-outdated` would hold back anything that satisfied the dependencies required by a new package. Slightly different, but I am fairly sure neither works correctly right now.",0,0,msr
1014,"> pyproject.toml isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file. setup.cfg exists for that purpose (the actual successor to setup.py ) and IMHO both of those should really be supported.
Well this is certainly off topic but it's an important discussion so I can't help myself.
There is actually no standard around `setup.cfg` right now other than the conventions established by distutils and setuptools. `pyproject.toml` is absolutely for library metadata as the successor to `setup.py` or the community would have placed build requirements in `setup.cfg` instead. `pyproject.toml` describes how to build a project (PEP 518), and part of building is describing metadata. I'm NOT saying that `pyproject.toml` needs a standard location for this metadata, but PEP 518 uses this file to install a build tool and from there it's very reasonable to expect that the build tool will use declarative configuration from somewhere else in the file to determine how to build the project.
Anyway, going back to pipenv vs poetry - there seems to be some idea floating around that applications don't need certain features that libraries get, like entry points, and this is just incorrect. It should be straightforward for an application to be a python package.
The only true difference between an application and a library in my experience with python and with other ecosystems is whether you're using a lockfile or not. Of course there's a third case where you really just want a `requirements.txt` or `Pipfile` and no actual code and that seems to be all that pipenv has focused on so far (`pipenv install -e .` falls into this category as pipenv is still afraid to try and support the package metadata). Unfortunately, while the design of pipenv is cleaner with this approach, it's also way less useful for most applications because PEP 518 decided to punt on how to install projects into editable mode so in order to continue using pipenv we will be stuck on setuptools quite a while longer as you cannot use `pyproject.toml` to switch away from setuptools and still use `pipenv install -e .`.",0,0,msr
1015,"> There is actually no standard around setup.cfg right now other than the conventions established by distutils and setuptools. pyproject.toml is absolutely for library metadata as the successor to setup.py or the community would have placed build requirements in setup.cfg instead.
Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly. Not to mention it uses the standard outlined in pep 345 for metadata, among others, and can also be used to specify build requirements.
> the community would have placed build requirements in setup.cfg instead.
Do you mean the pep authors? You can ask them why they made their decision, they outline it all in the pep.
> pyproject.toml describes how to build a project (PEP 518), and part of building is describing metadata. I'm NOT saying that pyproject.toml needs a standard location for this metadata, but PEP 518 uses this file to install a build tool and from there it's very reasonable to expect that the build tool will use declarative configuration from somewhere else in the file to determine how to build the project.
This came up on the mailing list recently -- nothing anywhere has declared a standard around `pyproject.toml` other than that it will be used to declare build system requirements. Anything else is an assumption; you can call that ""library definition metadata"", but it isn't. Try only defining a build system with no additional information about your project (i.e. no pep-345 compliant metadata) and upload it to pypi and let me know how that goes.
> Anyway, going back to pipenv vs poetry - there seems to be some idea floating around that applications don't need certain features that libraries get, like entry points, and this is just incorrect. It should be straightforward for an application to be a python package.
Who is saying that applications don't require entry points? Pipenv has an entire construct to handle this.
> so in order to continue using pipenv we will be stuck on setuptools quite a while longer as you cannot use pyproject.toml to switch away from setuptools and still use pipenv install -e .
Not following here... we are not going to leave pip vendored at version 10 forever, I've literally been describing our new resolver, and the actual installer just falls back to pip directly... how does this prevent people from using editable installs?",0,0,msr
1016,"> This came up on the mailing list recently -- nothing anywhere has declared a standard around `pyproject.toml`
That's correct, it is not a ""standard"", yet in that same thread recognise that by calling it `pyproject.toml` they likely asked for people to use this file for other project related settings/config.
So by the same logic you invoked here:
> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly.
`pyproject.toml` is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.",0,0,msr
1017,"> Not following here... we are not going to leave pip vendored at version 10 forever, I've literally been describing our new resolver, and the actual installer just falls back to pip directly... how does this prevent people from using editable installs?
PEP 517 punted on editable installs... which means there is no standard way to install a project in editable mode if you are not using setup tools (which has a concept known as develop mode which installs the project in editable mode).",0,0,msr
1018,"> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly. Not to mention it uses the standard outlined in pep 345 for metadata, among others, and can also be used to specify build requirements.
Yes, the build system is expected to output the PKG-INFO file described in PEP 345. This is a transfer format that goes in an sdist or wheel and is generated from a setup.py/setup.cfg, it is not a replacement as such for the user-facing metadata. PEP 518's usage of `pyproject.toml` is about supporting alternatives to distutils/setuptools as a build system, no one is trying to replace the sdist/wheel formats right now. Those replacement build systems need a place to put their metadata and fortunately PEP 517 reserved the `tool.` namespace for these systems to do so. It's not an assumption - **both flit and poetry have adopted this namespace for ""library definition metadata"".**
> Try only defining a build system with no additional information about your project (i.e. no pep-345 compliant metadata) and upload it to pypi and let me know how that goes.
How constructive.
> Who is saying that applications don't require entry points? Pipenv has an entire construct to handle this.
Where is this construct? I cannot even find the word ""entry"" on any page of the pipenv documentation at https://pipenv.readthedocs.io/en/latest/ so ""an entire construct"" sounds pretty far fetched? If you mean editable installs then we have reached the point I was making above - with pipenv deciding to couple itself to `pipenv install -e .` as the only way to hook into and develop an application as a package, for the foreseeable future pipenv's support here is coupled to setuptools. I think the entire controversy boils down to this point really and people (certainly me) are frustrated that we can now define libraries that don't use setuptools but can't develop on them with pipenv. To be perfectly clear this isn't strictly pipenv's fault (PEP 518 decided to punt on editable installs), but its refusal to acknowledge the issue has been frustrating in the discourse as poetry provides an alternative that does handle this issue in a way that's compliant with the `pyproject.toml` format. Pipenv keeps saying that poetry makes bad decisions but does not actually attempt to provide a path forward.",0,1,msr
1019,"https://pipenv.readthedocs.io/en/latest/advanced/#custom-script-shortcuts
Please read the documentation.",0,0,msr
1020,"@bertjwregeer:
> pyproject.toml is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.
Great, and we are happy to accommodate sdists and wheels built using this system and until there is a standard for editable installs we will continue to pursue using pip to build sdists and wheels and handle dependency resolution that way. Please read my responses in full. The authors and maintainers of pip, of the peps in question, and myself and @uranusjr are pretty well versed on the differences between editable installs and the implications of building them under the constraints of pep 517 and 518. So far All I'm seeing is that the peps in question didn't specifically address how to build them because they leave it up to the tooling, which for some reason everyone thinks means pipenv will never be able to use anything but setuptools? I've said already this is not correct. If you are actually interested in the implementation and having a productive conversation I'm happy to have that. If you are simply here to say that we don't know what we're doing, but not interested in first learning what it is we are doing, this is your only warning. We are volunteers with limited time and I am practicing a 0 tolerance policy for toxic engagements. I do not pretend my work is perfect and I don't pretend that pipenv is perfect. I will be happy to contribute my time and effort to these kinds of discussions; in exchange I ask that they be kept respectful, that they stick to facts, and that those who participate also be willing to learn, listen, and hear me out. If you are here just to soapbox you will have to find another platform; this is an issue tracker. I will moderate it as necessary.
This discussion is **wildly off topic**. If anyone has something constructive to say about the issue at hand, please feel free to continue that discussion. If anyone has issues or questions about our build system implementations, please open a new issue. If you have issues with our documentation, we accept many pull requests around documentation and we are aware it needs work. Please defer **all** of that discussion to new issues for those topics. And please note: the same rules will still apply -- this is not a soapbox, it is an issue tracker.",1,0,msr
1021,"> https://pipenv.readthedocs.io/en/latest/advanced/#custom-script-shortcuts
> Please read the documentation.
Entry points are a more general concept than just console scripts and this link is completely erroneous in addressing those concerns. `<soapbox>`Ban away - you're not the only maintainer of large open source projects on here and none of my comments have been a personal attack on you or the project. People commenting here are doing so because they want to use pipenv and appreciate a lot of what it does. My comment was not the first off topic post on this thread, yet is the only one marked. Your snarky comments indicating that you think I don't know what I'm talking about are embarrassing and toxic.",0,1,msr
1022,"In the project we maintain, we can soapbox. And yes, pip will support all compliant build systems which you both yourselves seem to full well understand will produce consumable metadata, and as pipenv uses pip as the backing tool to drive its installation process, as I described, yes, pipenv will support all compliant tooling. I already said this. So yeah, please take your toxicity somewhere else. Your attitude is not welcome here. Final warning. Persistent attempts to incite conflict won’t be tolerated.",0,1,msr
1023,"There is too much to parse here and I would just be echoing the sentiment that:
> There's a little thing to take into account here: Changing a single dependency could change the overall set of requirements.
Ex: Updating foo from 1.0 to 2.0 could require to update bar to >=2.0 (while it was <2.0 before), and so on.
or that
> upgrade a dependency that has cascading requirements obviously it will have consequences for other dependencies
In fact -- we have pretty great resolution of dependencies now that will update within the constraints you specify in your `Pipfile` to what is the possible allowed latest versions where all specifiers and constraints are met, or otherwise it will error out and let you know that you cannot do that. There is also the --skip-lock flag that might be helpful for some. Anyway I am closing this one down for the history books and If there is anything relevant in this thread that is not addressed or needs a new issue -- feel free to open one, but keep in mind that the current behavior is correct with respect to dependency resolution and keeping the constraints of the `Pipfile` all happy and satisfied.",0,0,msr
1024,"How I read PEP-8, it doesn't outlaw bare excepts, but merely recommends to catch more specific exceptions when possible. But what if there is no particular exception you want to catch, but when you just want to do some cleanup before propagating any exception?
```python
try:
self.connection.send(...)
except:
# We only close the connection on failure, otherwise we keep reusing it.
self.connection.close()
raise
```
Using `try...finally` isn't an option here, since we want to reuse the resource on success, and only clean up on failure. I could just explicitly catch `BaseException` instead, but there is no indication in PEP-8 that this is preferable (otherwise why would bare except be supported in the first place).
So how about suppressing E722 if there is a `raise` statement in the `except` block?",0,0,msr
1025,"Using bare except for resource closing is a common behavior, especially when I implement context manager. The check of `do not use bare except` should be changed to `do not use bare except without raise`.",0,0,msr
1026,The error code E722 was implemented in #592 / (#579) and I warned about it but sigmavirus24 wasn't interested and told that I did not read the whole thread....,0,1,msr
1027,"To be clear, pycodestyle doesn't do look-ahead's so we cannot silence this *if* there is a `raise` in the following block. That's just not how pycodestyle has ever worked and it would require a significant rewrite to enable that kind of behaviour.
----
Quoting the PEP
```
When catching exceptions, mention specific exceptions whenever possible instead of using a bare except: clause.
For example, use:
try:
import platform_specific_module
except ImportError:
platform_specific_module = None
A bare except: clause will catch SystemExit and KeyboardInterrupt exceptions, making it harder to interrupt a program with Control-C, and can disguise other problems. If you want to catch all exceptions that signal program errors, use except Exception: (bare except is equivalent to except BaseException:).
A good rule of thumb is to limit use of bare 'except' clauses to two cases:
1. If the exception handler will be printing out or logging the traceback; at least the user will be aware that an error has occurred.
2. If the code needs to do some cleanup work, but then lets the exception propagate upwards with raise. try...finally can be a better way to handle this case.
```
As a *general* rule, this new check is good. There are specific cases where people will need to do general clean-up work, as described above. Since we cannot check for people re-raising the exception in the except block, it truly is up to the user to determine whether:
1. This check is useful to them at all or whether they feel it should be ignored globally
2. Their particular use of a bare `except` is necessary and it should be ignored in that particular case (e.g., with `# noqa` or `# noqa: E722`).
I would hazard a guess that 90% of pycodestyle's users will find this check worthwhile, useful, and helpful. We can not ever satisfy 100% of our users so I am happy to settle for 90%.",0,1,msr
1028,"I have two (real world) scenarios:
1. Legacy code, initially written by novices, with inappropriate use of bare except all over the place. E722 is helpful there (somewhat since this usually isn't the only issue in such legacy code, and you end up ignoring most errors there anyway).
2. Code written by me (and similarly experienced Python developers), where bare except is used occasionally, but never without re-raising the exception. For reference, E722 is reported inappropriately three times in [this project](https://github.com/snoack/mypass) of mine, where the exception is re-raised.
If the design of pycodestyle doesn't allow considering the following block, in order to avoid inappropriate errors, my understanding is, that due to the potential of false positives this check should not be enabled by default, or such a check should rather be implemented in pyflakes which considers the AST and therefore is able to ignore bare excepts that re-raise the exception.
I'm not going to clutter my code with `#noqa` comments. IMO the purpose of a linter is to help you writing cleaner code, not to require additional boilerplate for legit practices. This isn't any better than using `except BaseException`.",0,0,msr
1029,"> I'm not going to clutter my code with #noqa comments. I'm not sure if you're intentionally ignoring my suggestion to include it in the `ignore` list for your projects or if you just want to try to argue. Either way, I'm not here to argue with you. Pycodestyle is a tool used by novices and experts alike. Novices will learn from this and so will some experts. Since this is a style tool, there will always be places where some people disagree with the checks and will disable them. That's *normal*. Just because a handful of folks object to a rule doesn't mean we will put it in the `DEFAULT_IGNORE` list even if generally speaking it has value for everyone else.
As for the suggestion that pyflakes entertain a *style* check, you can make that argument to them, but that is typically entirely against their philosophy.",0,1,msr
1030,"I just hit this as well. In my experience false positives can have a damaging effect. I've seen novices commit naive ""fixes"" for correct code. I believe this issue will cause novices to rewrite what I wrote
```python
try:
...
except:
destfile_tmp.unlink()
raise
```
to
```python
try:
...
except Exception:
destfile_tmp.unlink()
raise
```
which will of course leave temporary files around in the case of KeyboardInterrupt/SystemExit.
This check needs to see the raise, or be better worded, to steer people away from naive fixes.",0,0,msr
1031,"That's kind of a bad example though because you should be using `finally` to clean up resources. It could lead to novices getting a ""wrong"" idea, I agree, but they're already doing it wrong in the first place.",1,1,msr
1032,"No, as in @snoack's example, this is not an unconditional clean-up. In the happy path the file gets moved into a permanent location.",0,1,msr
1033,"![image](https://user-images.githubusercontent.com/13496612/36892494-2c2e0a5c-1e05-11e8-94c0-8daecf633955.png)
autopep8 said use `BaseException`.",0,0,msr
1034,"I'm with @sigmavirus24 on this one, but I'd take it a little further: a bare except is inherently unpythonic. Two important parts of python philosophy (`import this`) are 'Explicit is better than implicit' and 'Special cases aren't special enough to break the rules.'. I point these out because a bare except is a special, highly implicit case. Every other `except` has an 'argument', indicating the scope of the error to be handles. Even if that's 'everything', it's better to explicitly state that.
'There should be one-- and preferably only one --obvious way to do it.' also applies, since you can accomplish the same thing for the low-low price of 14 characters. That's simply not enough of a 'savings' to make it worth it.
There's also the fact that things like KeyboardInterrupt aren't caught by `Exception` for a reason. Those represent *very* exceptional cases - cases which shouldn't occur in normal (e.g. production) operation unless something has gone very wrong or the user is explicitly requesting an immediate shutdown.
In regards to @lordmauve's case: Why do you want to clean up those temporary files in every possible exception case? `except:` tells me that you don't know what exceptions may be raised. In those (hopefully rare) cases, your app will certainly crash (unless you have a bare except that silences every possible exception, but that's a whole new level of bad design), and you will probably need to investigate why. A temporary file is part of the state of the application at the point in time, and therefore will be forensically valuable.
If you know for sure that the temporary file will be forensically worthless, then `except Exception:` or `except BaseException:` makes that clear, and indicates that you're fully aware of the implications. If you're more worried about old temporary files accumulating in some way, consider having your app do cleanup as part of initialization. That idea comes from the [Crash-only software](https://lwn.net/Articles/191059/) pattern (It's a lot more reasonable than it sounds, I promise). Basically, it's a lot easier to assume that your app always terminates unexpectedly and code against that than to handle unexpected termination as a special case (i.e. Turning off the power is guaranteed to be an option, but a shut down command may fail or be unavailable).
As a last resort, there's always ` # noqa`. If you absolutely must break from best practices, then 8 characters is a very small price to pay.
edit: I r gud riter 😜",0,1,msr
1035,"@hoylemd I appreciate the support.
> Those represent very exceptional cases - cases which shouldn't occur in normal (e.g. production) operation unless something has gone very wrong or the user is explicitly requesting an immediate shutdown.
This sounds like you're assuming all development is on continuously running applications on a remote server. Things like pycodestyle and flake8 are ""production"" applications that should handle `KeyboardInterrupt`. That said, both projects handle it explicitly. I think I understand your point, but it feels like it's imposing a false dichotomy around what is ""production"".
> Why do you want to clean up those temporary files in every possible exception case?
I can imagine a few cases where @lordmauve would want to (and should!) clean up the temporary files. Perhaps those temporary files contain some sensitive information and cleaning them up is the secure thing to do. In reality, neither of us know the details and I think we should be assuming that they know their constraints better than us. Of course having some non-sensitive temporary files lying around can be useful for debugging, but there are certainly valid cases where those shouldn't be left around no matter what exception happens.",0,1,msr
1036,"@sigmavirus24 I think you're right re: imposing a false dichotomy - I'm primarily a web developer, where the dichotomy is pretty reliable.
In regards to temporary files containing sensititive information, Wouldn't it be a risk to write them to disk at all? If the power is shut off at the right moment, the data would still be there on disk, and no exception handling would have a chance to clean it up. So that makes me think that a temporary file is the wrong tool in the first place. I'd want to keep that in something volatile (like memory) so that it's sort of fail-safe.
You're more general point is a good one. - We can't know the real requirements on a project we're not a part of. But if those requirements do require a cleanup in all exception cases, I think we probably agree that it should be done explicitly.",0,1,msr
1037,"> I'd want to keep that in something volatile (like memory) so that it's sort of fail-safe.
Again, there are constraints where it makes sense. I agree with you in general, but there are always exceptions to the rule.
Also we're getting off topic :smile:",0,1,msr
1038,"`except BaseException` doesn't mean the same thing as `except:` in Python 2; in Python 2 you could raise old-style classes as exceptions, which do not inherit from BaseException.
Regarding ""I could never imagine a situation where I'd want to unconditionally clean up in the case of any exception"":
- In the case of temporary files, never leave them around on a production server because they can cause additional impact (eg. filling a disk volume) which can be much more serious than the original crash. Reproduce the issue in dev and diagnose.
- Just came across another example: in the case that my event loop crashes due to programming error, I must kill my subprocesses which are now in an unknown state ([code](https://github.com/lordmauve/chopsticks/blob/master/chopsticks/tunnel.py#L130)).",0,0,msr
1039,"@lordmauve I'll admit that there are some cases where `except:` is the right call, but they're cases that pycodestyle isn't intended to handle out of the box. Marking those with `# noqa` might seem a little cluttery, but it's much more helpful to those same junior devs who would be confused by the error. It explicitly indicates a code smell that is a necessary evil. Ideally, you'd document why it is necessary as well so that they understand why the exception had to be made. So to go back to your initial comment, Pycodestyle isn't designed in a way that it can detect the raise, so that's a non-starter. As for steering novices away from naive changes, that's the purpose of code review, not linting. A better solution is to prevent them from even knowing about it in the first place though, hence `# noqa` and documentation.",0,0,msr
1040,"> Pycodestyle isn't designed in a way that it can detect the raise, so that's a non-starter
Or evidence that this check needs to be moved to Pyflakes, which does consider the AST, in order to eliminate this false positive.
> As for steering novices away from naive changes, that's the purpose of code review, not linting. In a large organisation that is impossible. Junior people are often two or three steps removed from people who could confidently tell you what good practice looks like. We rely on linters to do this, and both the false positive and false negative rates are important. I'm not saying I'm not worried about the false negative. I'm arguing that this creates an undesirable false positive.
> A better solution is to prevent [novices] from even knowing about it in the first place though [by putting #noqa on that line]
You're assuming I'm writing code now for future novices to read. I'm concerned about the novices now maintaining the code I wrote 5 years ago when this check wasn't a thing.",0,1,msr
1041,"> Or evidence that this check needs to be moved to Pyflakes, which does consider the AST, in order to eliminate this false positive.
PyFlakes wouldn't accept this for the very reason that it's so controversial. They only accept obvious problems, e.g., unused imports. If this isn't satisfactory here, a different stylistic option for having it would be via a Flake8 AST plugin. Luckily, `flake8-bugbear` already provides this check using the AST but a quick check of the source indicates that it's as strict as this check is. I'd suggest either collaborating with [bugbear](/pycqa/flake8-bugbear) or creating a new plugin that meets the specific needs.
As it stands, this discussion seems to be getting heated. I'm going to lock the thread because it's devolved from constructive conversation to far less productive conversation.",0,0,msr
1042,"Trying to perform `brew bundle dump` and getting the following error: > sh: line 1: 18965 Segmentation fault: 11 mas list 2> /dev/null
Independently running `mas list` correctly outputs installed MAS apps.",0,0,msr
1043,I can't reproduce this locally I'm afraid. Try `brew reinstall mas`.,0,0,msr
1044,"Aww snap. Yea that fixed it. It looks like it was the High Sierra installer causing the issue as it doesn't have an ID. After updating mas it now correctly lists the installer (just minus an ID). Actually, this might be a different issue. Look at the line it adds to the Brewfile in this case:
`mas ""macOS High Sierra"", id: Install`
When I do `mas list`, this is how it appears within some other items:
```
595191960 CopyClip (1.9)
Install macOS High Sierra (13105)
803453959 Slack (2.9.0)
```",1,0,msr
1045,@jamesdh Ok. Would you consider submitting a PR to address that case?,1,0,msr
1046,@MikeMcQuaid will do,0,0,msr
1047,"Decided not to bother because A) the OSX installers aren't typically something you'd want to keep around B) am unable to find anything else that recreates this issue. Either way, thanks for the response @MikeMcQuaid!",0,0,msr
1048,"@jamesdh Cool, thanks for letting us know!",1,0,msr
1049,"For clarity in posterity, see mas-cli/mas#82 regarding the seg fault.",0,0,msr
1050,@paulp I cannot reproduce this locally with the installer installed. We will accept a pull request to resolve this (likely just not including anything in a dump missing an ID).,0,0,msr
1051,"Re #321, it's a different bug - as I said, ""the bug in bundle being masked by the bug in mas"" - regardless of your agreement, it's childish and hostile to immediately close and lock the ticket. Sorry to have bothered you with my effort.",0,1,msr
1052,"@paulp Calling someone who maintains software you use ""childish and hostile"" because they don't run their issue tracker in the way that you would like (not keeping issues open that we don't plan to work on) is rude. I've invested significantly more effort into this project (and Homebrew itself) over a long period of time than you or, at this point, frankly anyone. Would you consider apologising?",0,1,msr
1053,"@paulp I'll take that as a no, then.",0,1,msr
1054,Anyone else who hits this: we'll consider accepting a PR for fixing the case where `mas` does not output IDs.,0,0,msr
1055,"##### ISSUE TYPE
- Feature Idea
##### COMPONENT NAME
import_playbook
##### ANSIBLE VERSION
<!--- Paste verbatim output from ""ansible --version"" between quotes below -->
```
2.4.0.0
```
##### CONFIGURATION
<!---
If using Ansible 2.4 or above, paste the results of ""ansible-config dump --only-changed""
Otherwise, mention any settings you have changed/added/removed in ansible.cfg
(or using the ANSIBLE_* environment variables).
-->
n/a
##### OS / ENVIRONMENT
CentOS 6
##### SUMMARY
I need to conditionally import a playbook, which isn't possible. The keyword I'm missing is ""include_playbook"" which would allow a ""when"" to apply to it. Why?
I have a playbook that performs some maintenance, OS, Kernel, package, and firmware upgrades. For Major upgrades, we track progress in a ticketing system. So, during those runs, I'd like to conditionally import a playbook that updates the ticket information. In order for that to work, I need to pass the user's password to the ticketing system, so there's a `vars_prompt` in the ticket update playbook. If the user specifies which ticket they're working to the maintenance playbook, I'd like the ticket update playbook to be called after the maintenance is performed.
If there's another way for this to work, I'm open to alternate ideas, but I think conditionally playbook imports would be generically useful. I'm not understanding why tasks could be included dynamically, but a playbook wouldn't be.
##### STEPS TO REPRODUCE
<!---
For bugs, show exactly how to reproduce the problem, using a minimal test-case.
For new features, show how the feature would be used.
-->
<!--- Paste example playbooks or commands between quotes below -->
```yaml
# Maintenance Play Runs first, then conditionally import a second playbook
- import_playbook: update-ticket.yaml
when: ticket_id is defined
```
<!--- You can also paste gist.github.com links for larger files -->
##### EXPECTED RESULTS
<!--- What did you expect to happen when running the steps above? -->
I expect `update-ticket.yaml` to only import if the `ticket_id` is defined.
##### ACTUAL RESULTS
<!--- What actually happened? If possible run with extra verbosity (-vvvv) -->
no parse error, and playbook `update-ticket.yaml` is imported 100% of the time.",0,0,msr
1056,"Files identified in the description:
* [lib/ansible/modules/utilities/logic/import_playbook.py](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/utilities/logic/import_playbook.py)
If these files are inaccurate, please update the `component name` section of the description or use the `!component` bot command.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",0,0,msr
1057,"We have similar need. We have a main playbook that prepares the system, but we allow the user to provide some extra steps by creating a playbook files in a well known directories. We do not know what files will be present there in advance. For this we would really like to have include_playbook that supports with_fileglob (or with_items).",0,0,msr
1058,"Just to explain why include_tasks is not enough. The system is clustered and the tasks talk to localhost, other physical hosts and virtual machines. We could in theory use delegate_to if it supported host groups.",0,0,msr
1059,"I also have this need in order to import different playbooks to configure Vagrant guests differently depending on the active hypervisor.
Alternate to `when` is something like (this is also not supported):
```YAML
- import_playbook: ""install_{{ 'virtualbox' if ansible_product_name == 'VirtualBox' else 'vmware' if ansible_product_name = 'VMware Virtual Platform' else 'noop' }}_extensions.yml""
```",0,0,msr
1060,"Is this going to be fixed soon? I find it as a serious source of problems because the optional playbook code ca be huge when it comes to number of tasks, causing over **extensive console/log verbosity** of tasks that are never supposed to be loaded.
The `skip_reason"": ""Conditional result was False""` is not of much help either because the user will not see any condition on those tasks, the condition being few nested includes/imports away in another file.
It helps nobody that Ansible will list hundreds of lines of files that were never supposed to be run, making much harder to investigate them.",0,0,msr
1061,"As a workaround until this is possible, if the use case is to support different host groups it is possible to use an include_task with a conditional on group names, i.e. a task include for a group `foo` is included when a file `tasks_directory/foo.yml` is provided:
```
- include_tasks: ""{{item}}""
with_fileglob: ""tasks_directory/*.yml""
vars:
file_host_group: ""{{ (item | basename | splitext)[0]}}""
when: ""file_host_group in group_names""
```
The playbook including this construct has to run for all hosts.",0,0,msr
1062,"Is this still being worked at all? I'm kind of implementing something like an `ansible-galaxy` style method of ""installing"" playbooks into a playbooks sub-directory and then I want to `include_playbook` a playbook that was just ""installed"". Here is what I've got so far:
```yaml
- name: PLAY | Install other required playbooks
hosts: localhost
connection: local
tasks:
- name: INCLUDE_VARS | include variables to discover other needed playbooks
include_vars:
dir: playbooks/
files_matching: requirements.yml
depth: 1
- name: GIT | Clone playbooks
git:
repo: ""{{ item.src }}""
dest: ""playbooks/{{ item.src.split('/')[-1] }}""
version: ""{{ item.version }}""
loop: ""{{ elk_required_playbooks }}""
- name: SHELL | Install included playbooks roles
shell: ansible-galaxy install -r roles/requirements.yml -p roles/
args:
chdir: ""playbooks/{{ item.src.split('/')[-1] }}""
loop: ""{{ elk_required_playbooks }}""
- name: PLAY | Run the installed helloWorld playbook
import_playbook: ""playbooks/ap_hello_world/helloWorld.yml""
```
If I run this as-is, I get an import error because the playbook to be imported isn't there yet.
```bash
ERROR! Unable to retrieve file contents
Could not find or access '/path/to/playbooks/ap_hello_world/helloWorld.yml'
```
If I comment out the `import_playbook` play, the functionality above that works nearly like `ansible-galaxy` and ""installs"" the playbooks I need (and the roles they need). ```bash
PLAY [PLAY | Install other required playbooks] *************************************************************************************************************************************
TASK [Gathering Facts] *************************************************************************************************************************************************************
Friday 31 August 2018 13:54:13 -0500 (0:00:00.239) 0:00:00.240 ********* Friday 31 August 2018 13:54:13 -0500 (0:00:00.237) 0:00:00.237 ********* ok: [localhost]
TASK [INCLUDE_VARS | include variables to discover other needed playbooks] *********************************************************************************************************
Friday 31 August 2018 13:54:15 -0500 (0:00:01.366) 0:00:01.606 ********* Friday 31 August 2018 13:54:15 -0500 (0:00:01.366) 0:00:01.604 ********* ok: [localhost]
TASK [GIT | Clone playbooks] *******************************************************************************************************************************************************
Friday 31 August 2018 13:54:15 -0500 (0:00:00.124) 0:00:01.730 ********* Friday 31 August 2018 13:54:15 -0500 (0:00:00.124) 0:00:01.728 ********* changed: [localhost] => (item={u'src': u'<gir_url>/ap_hello_world', u'version': u'v0.3.0'})
TASK [SHELL | Install included playbooks roles] ************************************************************************************************************************************
Friday 31 August 2018 13:54:17 -0500 (0:00:02.591) 0:00:04.322 ********* Friday 31 August 2018 13:54:17 -0500 (0:00:02.591) 0:00:04.319 ********* changed: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})
PLAY RECAP *************************************************************************************************************************************************************************
localhost : ok=4 changed=2 unreachable=0 failed=0 ```
And now I can run the same playbook again, this time with the `import_playbook` play not commented and it works as I desire:
```bash
PLAY [PLAY | Install other required playbooks] *************************************************************************************************************************************
TASK [Gathering Facts] *************************************************************************************************************************************************************
Friday 31 August 2018 14:00:30 -0500 (0:00:00.244) 0:00:00.244 ********* Friday 31 August 2018 14:00:30 -0500 (0:00:00.241) 0:00:00.241 ********* ok: [localhost]
TASK [INCLUDE_VARS | include variables to discover other needed playbooks] *********************************************************************************************************
Friday 31 August 2018 14:00:32 -0500 (0:00:01.471) 0:00:01.716 ********* Friday 31 August 2018 14:00:32 -0500 (0:00:01.471) 0:00:01.713 ********* ok: [localhost]
TASK [GIT | Clone playbooks] *******************************************************************************************************************************************************
Friday 31 August 2018 14:00:32 -0500 (0:00:00.138) 0:00:01.854 ********* Friday 31 August 2018 14:00:32 -0500 (0:00:00.138) 0:00:01.852 ********* ok: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})
TASK [SHELL | Install included playbooks roles] ************************************************************************************************************************************
Friday 31 August 2018 14:00:35 -0500 (0:00:02.821) 0:00:04.675 ********* Friday 31 August 2018 14:00:35 -0500 (0:00:02.821) 0:00:04.673 ********* changed: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})
PLAY [PLAY | BEGIN Setup & Timing] *************************************************************************************************************************************************
TASK [set_fact] ********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:36 -0500 (0:00:01.323) 0:00:05.999 ********* Friday 31 August 2018 14:00:36 -0500 (0:00:01.323) 0:00:05.997 ********* ok: [localhost]
TASK [debug] ***********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:36 -0500 (0:00:00.155) 0:00:06.154 ********* Friday 31 August 2018 14:00:36 -0500 (0:00:00.155) 0:00:06.152 ********* ok: [localhost] => {
""msg"": ""Start Time - 2018-08-31 14:00:36""
}
PLAY [PLAY | Say Hello to My Little Friend] ****************************************************************************************************************************************
TASK [SHELL | echo something] ******************************************************************************************************************************************************
Friday 31 August 2018 14:00:36 -0500 (0:00:00.096) 0:00:06.251 ********* Friday 31 August 2018 14:00:36 -0500 (0:00:00.096) 0:00:06.249 ********* ok: [knebawils001]
TASK [DEBUG | debug host's standard output] ****************************************************************************************************************************************
Friday 31 August 2018 14:00:37 -0500 (0:00:00.714) 0:00:06.965 ********* Friday 31 August 2018 14:00:37 -0500 (0:00:00.714) 0:00:06.963 ********* skipping: [knebawils001]
PLAY [PLAY | Say Hello via an Ansible Role] ****************************************************************************************************************************************
TASK [ar_hello_world : SHELL | echo role's message on host] ************************************************************************************************************************
Friday 31 August 2018 14:00:37 -0500 (0:00:00.151) 0:00:07.117 ********* Friday 31 August 2018 14:00:37 -0500 (0:00:00.151) 0:00:07.115 ********* ok: [knebawils001]
TASK [ar_hello_world : DEBUG | debug host shell standard output] *******************************************************************************************************************
Friday 31 August 2018 14:00:37 -0500 (0:00:00.342) 0:00:07.459 ********* Friday 31 August 2018 14:00:37 -0500 (0:00:00.342) 0:00:07.457 ********* skipping: [knebawils001]
PLAY [PLAYBOOK | END Setup & Timing] ***********************************************************************************************************************************************
TASK [set_fact] ********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:38 -0500 (0:00:00.212) 0:00:07.672 ********* Friday 31 August 2018 14:00:38 -0500 (0:00:00.212) 0:00:07.669 ********* ok: [localhost]
TASK [debug] ***********************************************************************************************************************************************************************
Friday 31 August 2018 14:00:38 -0500 (0:00:00.179) 0:00:07.851 ********* Friday 31 August 2018 14:00:38 -0500 (0:00:00.178) 0:00:07.848 ********* ok: [localhost] => {
""msg"": ""Start Time - 2018-08-31 14:00:36, End Time - 2018-08-31 14:00:38, Elapsed Time - 0:00:02""
}
PLAY RECAP *************************************************************************************************************************************************************************
knebawils001 : ok=2 changed=0 unreachable=0 failed=0 localhost : ok=8 changed=1 unreachable=0 failed=0
```
I suppose this could be split up into two (2) separate playbooks in the same Git repo. The first would be called `prepare.yml` or maybe `prerequisites.yml` to ""install"" the other needed playbooks and the second main playbook (`playbook.yml`) will do the necessary imports, etc. I was really wanting to make this a ""one-shot"" playbook.",0,0,msr
1063,"Include conditionals are very useful for creating branches in our playbooks. Will it be ensured that import_playbook will support conditionals in the next release citing this issue?
If not, you are losing a lot of power and will end up creating a lot of hacks. Not to mention breaking a ton of include playbook conditionals in end user plays.",0,0,msr
1064,"+1 on implementing a ""when"" conditional.",0,0,msr
1065,soon-ish we approaching 1 y since this request was open and no progress so far ... any chance this get some attention @bcoca ? much thanks !,0,1,msr
1066,I would like to vote a '+1'as well.,0,0,msr
1067,1,0,0,msr
1068,"+1 as well, would love to see this feature",0,0,msr
1069,Please use the 👍 button to let the maintainers know you need this feature. Getting a ton of emails from these +1s.,0,0,msr
1070,1,0,0,msr
1071,"If someone is interested how to use play-level variables for conditional playbook-import:
1. Set up that variable as a fact in the play
2. Use `when` with `import_playbook` to check this variable (with full path, `hostvars.hostname.a_variable`)
If someone is interested, I managed to make import_playbook be conditional on `--limit` in the command line:
https://medium.com/opsops/import-playbook-with-play-level-condition-775122fe78ff
An example:
```
- hosts: all,localhost
gather_facts: no
run_once: True
tasks:
- set_fact:
full_run: '{{ play_hosts == groups.all }}'
delegate_to: localhost
delegate_facts: yes
- import_playbook: test.yaml
when: hostvars.localhost.full_run
```",0,0,msr
1072,is this issue/request still up to date in a more current version of ansible? Iam using ansible 2.6.1 and the when condition doesn't seem to work when I use the import_playbook function.,0,0,msr
1073,"@brotaxt You need to initialize variables before doing `when`. Just add some random task to random host (before doing first 'import_playbook'). F.e., do set_fact on localhost, as in example above.",0,0,msr
1074,"@amarao many thanks for the quick response. :+1: Unfortunately it doesn't seem to work for me. Even if answer the prompt with ""no"" the playbook ""vmware_createsnap.yml"" gets invoked. The other tasks are working as expected. What I am doing wrong? My playbook: ```
---
-
hosts: all
gather_facts: true
vars_prompt:
- name: ""snapshots_required""
prompt: ""Do you want to automatically create VMWare Snapshots? [yes/no]""
private: no
name: ""Install all available Updates""
tasks:
- name: Check for Updates
include: checkforupdates.yml
- name: setting fact for hosts which have outstanding updates
set_fact:
updates_available: ""yes""
when: ""yumoutput.changed or zypperoutput.changed""
- name: setting fact for hosts which have no outstanding updates
set_fact:
updates_available: ""false""
when: updates_available is not defined
- import_playbook: vmware_createsnap.yml
when: snapshots_required = ""yes""
```",0,0,msr
1075,"@brotaxt this is a feature request, you currently CANNOT conditionally import playbooks, the conditions above happen to skip all the tasks in one, but this is not a supported behaviour and not guaranteed to work across versions of Ansible.",0,0,msr
1076,"just to be clear:
import or include? because I think the current implementation or naming is actually wrong, based on the definition in https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_includes.html
Does import_playbook actually ""lazy load"" or does it get loaded & parsed with the yaml file?",0,0,msr
1077,"@Kriechi neither, it gets loaded at 'playbook compile time' which is before execution but not on file load
there is no include_playbook, that is the whole purpose of this feature request, to add one",0,0,msr
1078,"@bcoca mhm that sounds even more wrong - or am I missing the big picture here?
I would have expected that `import_task` and `include_task` have an `*_playbook` sibling...",0,0,msr
1079,"@Kriechi the engine never supported that, why `include:` was very misleading and we had to separate it into the different include_X/import_X options and make each behaviour explicit. So include_X is dynamic aka runtime, while import_X is 'static' aka 'compile time'.",0,0,msr
1080,"ok - so `include_playbook` would be a feature request? Or can we track it here?
E.g., I'm running a git-checkout task on localhost, and then want to `include_playbook: some/repo/foo.yml`
This should include (lazy-load) the updated playbook from that repository, AFTER pulling the latest commit from the remote. Currently, `import_playbook` imports the ""old"" playbook, then pulls, and then runs the outdated playbook.",0,0,msr
1081,"@Kriechi ... please read the subject of this ticket, that is EXACTLY what we are tracking here",1,1,msr
1082,"true - the part the confused me is ""conditionally import..."".
`import_playbook` and `include_playbook` are the feature we want.",0,0,msr
1083,"I'm trying to do what [watsonb](https://github.com/ansible/ansible/issues/34281#issuecomment-417762220) was, using ansible-galaxy to install roles, and then using the roles. I found a decent workaround for the all-in-one playbook, which was to place the playbooks, in the order that you want them to execute, on the command line. In Watsonb's case, that would look like
ansible-playbook ... prepare.yml playbook.yml
Any variables that you set on the command line are passed to the playbooks, sequentially.
This might also solve [MarSik's](https://github.com/ansible/ansible/issues/34281#issuecomment-361277446) problem, also, using file globbing on the command line instead of in the playbook.",0,0,msr
1084,"@decet It could, but that would basically mean using a top level bash script as the entrypoint and spliting the main ansible playbook into multiple stage files. Not too horrible, just ugly.",0,0,msr
1085,"I've overcome this in my own way as follows. First, my typical playbook directory structure:
```bash
.
├── .ansible-lint
├── .gitignore
├── .yamllint
├── ansible.cfg
├── callback_plugins
│   ├── junit.py
│   ├── log_plays.py
│   ├── profile_roles.py
│   ├── profile_tasks.py
│   ├── timer.py
├── check_ansible_lint.sh
├── check_syntax.sh
├── check_yaml_lint.sh
├── create.yml
├── destroy.yml
├── Jenkinsfile
├── localhost_inventory.yml
├── playbooks
│   ├── ap_linux_instance
│   └── requirements.yml
├── prerequisites.yml
├── README.md
├── reports
├── requirements.txt
├── roles
│   ├── ar_linux_ansible_venv
│   ├── ar_linux_cname
│   ├── config_encoder_filters
│   └── requirements.yml
└── VERSION.md
```
My .gitignore ignores most sane OS/language/IDE things, but also ignores everything in the `roles/` and `playbooks/` folders except for the `requirements.yml` files in each folder. The requirements.yml file within the playbooks folder is similar to your Galaxy-style requirements.yml, but rather than calling out dependent playbooks by Galaxy owner.name, I specify the full Git source (Galaxy supports this of course). The requirements.yml within the roles/ folder is just your traditional Galaxy-style requirements.
I have this `prerequisites.yml` playbook, that looks like this:
```yaml
---
- name: PLAY | Install other required playbooks
hosts: localhost
connection: local
tasks:
- name: INCLUDE_VARS | include variables to discover other needed playbooks
include_vars:
dir: playbooks/
files_matching: requirements.yml
depth: 1
- name: GIT | Clone playbooks
git:
repo: ""{{ item.src }}""
dest: ""playbooks/{{ item.src.split('/')[-1] }}""
version: ""{{ item.version }}""
loop: ""{{ required_playbooks }}""
- name: SHELL | Install included playbooks roles
shell: ansible-galaxy install -r roles/requirements.yml -p roles/ --force
args:
chdir: ""playbooks/{{ item.src.split('/')[-1] }}""
loop: ""{{ required_playbooks }}""
when: item.galaxy
changed_when: false
tags: [ skip_ansible_lint ]
```
And, assuming that my ""big bang"" create.yml depends on a playbook and its roles from another playbook project, I import it like this:
```yaml
# ~~~~~~~~~~
# Ensure that all of the host VMs in the inventory are up and running
# either on-prem or in Azure as specified in the inventory
#
- name: Ensure inventory hosts are present
import_playbook: ""playbooks/ap_linux_instance/create.yml""
tags: [ base_server, hosts ]
```
And so, the work-flow to run my ""big bang"" (e.g. create.yml) is a 3-liner:
```bash
ansible-playbook prerequisites.yml
ansible-galaxy install -r roles/requirements -p roles/
ansible-playbook create.yml -i <path_to_inventory>
```
You could, of course, wrap the above 3-liner in a `create.sh` shell script for convenience. This method has served me well for some fairly complex playbook projects that depend on other playbook projects. This forces us to keep roles and playbooks fairly self-contained and re-usable and factor variables out into their own inventory projects. When performed with discipline, it makes it really easy to migrate unaltered roles/playbooks to other environments, then just update inventory variables that are unique to that environment.
**This doesn't solve the conditional import problem**, mind you, but does help me use the `import_playbook` statement for something that may not exist just yet. It kind of gets around a conditional in my very specific use-case. I make use of tagging on the `import_playbook` to leverage the command-line `--tags` and `--skip-tags` features if I need scalpel-like precision at run-time. But if you had to make an import decision based on some other conditional logic (e.g., OS family), well, we still need that as a language feature I think. For now, I just handle those cases with sub-playbooks and chain them together ensuring I target the appropriate hosts/groups that should or should not be targeted based on how I've setup my inventory (yes, it can get messy).
This is all pretty wild and requires a high degree if what I commonly refer to as ""4th dimensional thinking"", especially when you consider branches/versions of things and running them from CI/CD platforms like Jenkins or even AWX. But I still find Ansible fascinating and use it daily.
HTH,
Ben",0,0,msr
1086,"I see that this issue now has a ""has_pr"" label. I searched a lot, but I can't find the PR. Can someone link it here?",0,0,msr
1087,"The bot added that label, and it looks like it was a false positive. No work is being done on this feature, nor are there any plans to work on it currently.",0,0,msr
1088,any plans for this one ?,1,0,msr
1089,"I'd just like to mention that there is a bit in the documentation that insinuates that the feature requested already exists. See:
https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#applying-when-to-roles-imports-and-includes
Specifically, the note mentions that Ansible allows `when` to work with playbook includes since version 2.0. Perhaps this documentation should also be amended.",0,0,msr
1090,"If only unfixed issues aged like fine wine instead of like bait fish....
But seriously is this ever going to either be fixed or closed as won't fix? The `has_pr` label typically indicates at fix is being gestated, but in this case there is no hint as to where we should be looking for the fix.",0,1,msr
1091,My previous comment still stands as an answer to these questions. https://github.com/ansible/ansible/issues/34281#issuecomment-517328598,0,0,msr
1092,1,1,0,msr
1093,"+1
This is a frustrating ""missing feature"" Since there is no plan to allow for conditional vars_prompt, the only other way to conditionally get user input at start of playbook would be with conditional ""import_playbook"" based on whether a tag was sent in on commandline.
`- import_playbook: playbook_choice_1.yml
when: ""'specific_tag' in ansible_run_tags""`
I understand that we want a playbook to require no user input, but this is just not a realistic scenario. When you have several users of a given playbook, then you need to ask for passwords. The other option is having dozens of ""ansible vault"" files (nightmare), or by implementing hashicorp vault (first get everything working as desired then implement another level of integration)...",0,0,msr
1094,"I (still) agree with @subcan that this feature should exist. This would be a great thing to get implemented while we're all on COVID-19 lockdown! ;)
As to options for managing multiple configurations, there is another option in between Ansible vault files and full-blown Hashicorp Vault deployment. I wrote a tool I named ""python_secrets"" and it works well with Ansible to manage multiple sets of variables outside of a Git repository (including one with Ansible playbooks). I document how to use it this way and have described it in several public talks listed on my home page (my talk at WSLConf from earlier this month will be added as soon as the videos are released).
https://pypi.org/project/python-secrets/
https://youtu.be/WD2Oqy2oc3A",0,0,msr
1095,"Files identified in the description:
* [`lib/ansible/modules/import_playbook.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/import_playbook.py)
If these files are incorrect, please update the `component name` section of the description or use the `!component` bot command.
[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",0,0,msr
1096,1,1,0,msr
1097,1,1,0,msr
1098,"I've locked this to contributors for now. Adding `+1` comments is too noisy. For future reference, add a reaction to the issue body, and don't comment.",0,0,msr
1099,"Thank you very much for your submission to Ansible. It means a lot to us that you've taken time to contribute.
Unfortunately, this issue has been open for some time while waiting for a contributor to take it up but there does not seem to have been anyone that did so. So we are going to close this issue to clear up the queues and make it easier for contributors to browse possible implementation targets.
However, we're absolutely always up for discussion. Because this project is very active, we're unlikely to see comments made on closed tickets and we lock them after some time. If you or anyone else has any further questions, please let us know by using any of the communication methods listed in the page below:
* https://docs.ansible.com/ansible/latest/community/communication.html
In the future, sometimes starting a discussion on the development list prior to proposing or implementing a feature can make getting things included a little easier, but it's not always necessary.
Thank you once again for this and your interest in Ansible!
[click here for bot help](https://github.com/ansible/ansibullbot/blob/devel/ISSUE_HELP.md)
<!--- boilerplate: waiting_on_contributor_close --->",0,0,msr
1100,"## Description
RetroArch builds debug builds by default after commit https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d
This is very bad and breaks ./configure && make which would explicitly need `DEBUG=0`.
## Related Issues
Debug support should not be enabled if `DEBUG` is undefined.
## Related Pull Requests
https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d
## Reviewers
@twinaphex, @alcaro, @bparker06",0,0,msr
1101,"Release builds are impossible to debug properly, therefore they're the broken ones. Debug builds work fine, they're a bit slower but are not broken by any plausible definition.
Fix your buildbots instead. They're probably already setting a dozen variables, it's easy to add another one.",0,1,msr
1102,"No, I'm not going to fix every distro package that follows correct behavior of not setting `DEBUG` for release builds.
You should just make this script.
```
#!/bin/sh
./configure && make DEBUG=1
```",0,1,msr
1103,"Can you please untag my PR with that ridiculous bs? Changes will not be made because this is the correct behavior and you just broke it, seriously why can't you just accept that you are wrong?",0,1,msr
1104,"For windows we even ship debug builds on every nightly.
I guess adding DEBUG=0 is feasible but I don't think it should be the default behavior",0,0,msr
1105,"I will admit I'm wrong when (if) I'm convinced I'm wrong. Right now, I believe you're just as wrong as you believe I am.
Who says not setting debug is correct? Especially considering we defaulted to debug=yes a while ago, this is a regression (though probably an ancient one).
Either you change your build scripts once and ignore it, or I have to remember RetroArch being _**SPECIAL**_ every single time I want to do anything. I don't see any advantages whatsoever to your approach.",0,1,msr
1106,"For the record, this probably breaks 95% of build scripts including the buildbot. Pretty much any build script that didn't set `DEBUG=0` explicitly.",0,0,msr
1107,"Debug behavior should be opt in, not opt out. This the standard pretty much everywhere for a damn good reason. The fact this has to be spelled out for you is not only insulting, its incredibly inane.",0,1,msr
1108,"If there is a ""damn good reason"" to default to undebuggable programs, post said reason and I'll consider it. Repeating variants of ""because I said so"" or ""because this person I'm conveniently not naming said so"" is not a valid argument.",0,1,msr
1109,"I'm merging this, it's the right thing to do. If you have a problem with it, take it up with @twinaphex privately, NOT HERE.",0,0,msr
1110,"## If you're having trouble installing Nokogiri ...
**Have you tried following [the installation tutorial][tutorial]?**
yes
**What is the output of `gem install`?**
```
PS C:\Users\replaced> gem install nokogiri -v 1.8.1
ERROR: Error installing nokogiri:
The last version of nokogiri (= 1.8.1) to support your Ruby & RubyGems was 1.8.1. Try installing it with `gem install nokogiri -v 1.8.1`
nokogiri requires Ruby version < 2.5, >= 2.2. The current ruby version is 2.5.0.
```
**What are the contents of the `mkmf.log` file?**
Didnt get so far
**What operating system are you using?**
Windows 10 x64",0,0,msr
1111,Same OS and same problem...,1,0,msr
1112,"Same here, and I really would like it if downgrading is not my only option",0,0,msr
1113,I am currently having the same issue,0,0,msr
1114,Same problem,0,0,msr
1115,"Windows 7 x64 same error msg.
```
>gem install nokogiri -v 1.8.1
ERROR: Error installing nokogiri:
The last version of nokogiri (= 1.8.1) to support your Ruby & RubyGems w
as 1.8.1. Try installing it with `gem install nokogiri -v 1.8.1`
nokogiri requires Ruby version < 2.5, >= 2.2. The current ruby version i
s 2.5.0.
```",0,0,msr
1116,"The fix is already merged, although a new release is pending. See: https://github.com/sparklemotion/nokogiri/pull/1704 & https://github.com/sparklemotion/nokogiri/issues/1706
Until then, it's better to stick with Ruby 2.4.2 on Windows.
The Nokogiri team removed the GEMSPEC from sources, to disallow people building from master branch, since it is unstable and probably would contain bugs. You can read more about it here:
https://github.com/sparklemotion/nokogiri/blob/master/Y_U_NO_GEMSPEC.md",0,0,msr
1117,Any word on support for 2.5.0 on OS X?,1,0,msr
1118,@leehanslim OSX should use the normal gems that also work on *nix. I had no problems with rbenv to install nokogiri on OSX.,0,0,msr
1119,Same problem.,0,0,msr
1120,any news?,0,0,msr
1121,I am wondering how this happens. Its kind of a tradition ruby makes a point release around christmas every year. the dev version was around for weeks. Ignoring Windows blames the users who are bound to this OS for what ever reason,0,0,msr
1122,"@SimonHoenscheid nokogiri-core is always welcoming of contributors who would like to help us support Windows. We've asked for help repeatedly over the years, as we've all got jobs that distract us from supporting open source software, despite our best efforts and intentions.
I'd like to ask that you pay attention to the tone of your message above. ""Ignoring Windows"" is clearly an untrue statement, given the number of hours I've personally spent, the number of hours @larskanis has spent, the amount of time we've spent providing [automated testing tooling][1] for [Nokogiri on Windows][2], etc., etc.
Windows support in Nokogiri is first class. We provide precompiled DLLs for users who want a fast, simple installation; and we provide support for users who want to compile Nokogiri and libxml using DevKit. Saying that we're ignoring Windows users is absurd and a little insulting.
Currently I'm blocking a release that supports Ruby 2.5 on windows on getting Ruby 2.5 supported in our test pipelines. I accept some fault for not being more transparent about this as the blocking factor. But now I'm trying to make amends by communicating exactly what's going on behind the scenes to properly support you and the platform that you use. Not ignoring you or blaming you, as you claim.
I'm going to lock this issue because I really don't want to do a whole back-and-forth. We'll release support for Ruby 2.5 on Windows as soon as we can, just as we always do, and we appreciate your patience and empathy as we work.
[1]: https://github.com/flavorjones/windows-ruby-dev-tools-release
[2]: https://ci.nokogiri.org/teams/nokogiri-core/pipelines/nokogiri/jobs/win-ruby-2.4-devkit/builds/10",0,0,msr
1123,"Remaining work to be done on our CI pipeline at https://ci.nokogiri.org/
* [x] [upgrade concourse to 3.8.0](https://concourse.ci/downloads.html#v380)
* [x] test Ruby 2.5 support just added to [windows-ruby-dev-tools-release](https://github.com/flavorjones/windows-ruby-dev-tools-release/tree/flavorjones-add-ruby-25) and cut a release
* [x] add a Ruby 2.5 test job to the pipeline (updating [concourse-gem)](https://github.com/flavorjones/concourse-gem/blob/master/lib/concourse.rb#L10)
* [x] get Ruby 2.5 tests to pass
* [ ] cut a Nokogiri release that adds Ruby 2.5
This is probably a good time to remind watchers that Ruby 2.2 is approaching EOL on 2018-03-31, and so the first release after that will likely remove Ruby 2.2 binaries from the fat gem.",0,0,msr
1124,"Windows Ruby 2.5 build is up and running:
> https://ci.nokogiri.org/teams/nokogiri-core/pipelines/nokogiri/jobs/win-ruby-2.5-devkit/builds/1",0,0,msr
1125,Note that commit bf94cf5 was added to master to make windows tests less brittle.,0,0,msr
1126,"Nokogiri 1.8.2 has shipped with Ruby 2.5 support in the ""fat binary"" windows gems. Thanks everyone for your patience.",0,0,msr
1127,"We need a written code of conduct. It should reference the relevant IETF code, and include anything we feel that we need here.
In particular, we need some thing around roles:
* Definition of what it means to be a project Member, and what behavioral standards Members are held to
* Definition of what it means to be a project Collaborator, and what behavioral standards Collaborators are held to
* What behavioral standards are expected of all participants
And some things around discussions:
* What sort of requirements exist for staying on-topic in an issue
* How to bring an end to discussions where it is clear that there will not be consensus (the IETF sets the goal of ""rough consensus"", acknowledging that sometimes you can't get everyone's buy-in)
I'm going to start this issue off locked until we figure out the best / least likely to explode way to take feedback on it.",0,1,msr
1128,Closing in favor of json-schema-org/community#162,1,0,msr
1129,"SS4.0
```php
TextField::create(<name>, <value>); // OK
HiddenField::create(<name>, <value>); // NOT OK. HiddenField::create(<name>)->setValue(<value>);
```
There is not reason to trip up new developers like this",0,1,msr
1130,"@worikgh Can you provide more info on what specifically you expect to happen vs what’s actually happening?
The second argument should be the title (that is, the “label” used when the field is displayed) - the third argument is for value - is that what’s causing your issue? It is a little strange to push a title to a `HiddenField` when that title will never be displayed, but it keeps that API (slightly) more consistent with other form field types :)",0,0,msr
1131,"Your examples are wrong. `FormField` objects are:
`FormField::create(<name>, <label>, <value>)`
see: https://github.com/silverstripe/silverstripe-framework/blob/4.0/src/Forms/FormField.php#L325",0,0,msr
1132,"No. FormField from http://api.silverstripe.org/4/SilverStripe/Forms/FormField.html#method_castingHelper
```
__construct(string $name, null|string $title = null, mixed $value = null)
Creates a new field.
```
Using that pattern for a hiddeen field does not work
```
$hftest = HiddenField::create(""HFieldTest"", ""Value"");
$hftest2 = HiddenField::create(""HFieldTest2"")->setValue(""Value"");
```
in the template as:
```
$Fields.fieldByName(HFieldTest)
$Fields.fieldByName(HFieldTest2)
```
Results in:
```
<input type=""hidden"" name=""HFieldTest"" class=""hidden"" id=""Form_ProfileEditForm_HFieldTest"" />
<input type=""hidden"" name=""HFieldTest2"" value=""Value"" class=""hidden"" id=""Form_ProfileEditForm_HFieldTest2"" />```
(edited for clarity)",0,0,msr
1133,"@worikgh that all looks as expected...
if you add:
```php
$hftest3 = HiddenField::create(""HFieldTest3"", ""Value"", ""actual value"");
```
and
```
$Fields.fieldByName('HFieldTest3')
```
you'll get output:
```html
<input type=""hidden"" name=""HFieldTest3"" value=""actual value"" class=""hidden"" id=""Form_ProfileEditForm_HFieldTest3"" />
```",0,0,msr
1134,The bug is that the interface for a text field and a hidden field are different. IMO They should be the same.,0,0,msr
1135,"can you explain in what way they are different, please?
---
edit:
to create a TextField: `TextField::create(<name>, <label>, <value>)`
to create a HiddenField: `HiddenField::create(<name>, <label>, <value>)`
they seem the same to me.",0,0,msr
1136,"```
TextField::create(<name>, <value>); // OK
HiddenField::create(<name>, <value>); // NOT OK. ````",0,0,msr
1137,"@worikgh - I'm sorry, you're not being clear.
both of those work, don't result in error, output expected HTML/fields.
From what I can tell you actually are asking for these APIs to be different, not the same... You'd like `HiddenField`'s second constructor argument to be the value and not the label, is that right?",0,0,msr
1138,"They produce different output.
See above where I created an example and cut and pasted resulting HTML.",0,0,msr
1139,"That is expected, the two examples you're giving are fundamentally different:
```php
$hftest = HiddenField::create(""HFieldTest"", ""Value"");
$hftest2 = HiddenField::create(""HFieldTest2"")->setValue(""Value"");
```
In the above code, `$hftest` has no value set and just a name and label, `$hftest2` has a name and no label and a value *is* set. Therefore you'd expect different output and the output you've shown looks expected. `$hftest` has no value and `$hftest2` does.",0,0,msr
1140,"No.
```
TextField::create(<name>, <value>);
```
produces
```
<input name=""<name>"" value=""<value>"" ..... type=""text"">
```
```
HiddenField::create(<name>, <value>);
```
produces
```
<input name=""<name>"" value="""" ..... type=""hidden"">
```
Those are different.",0,0,msr
1141,"Worik, sorry mate... I think you're confusing Title (ie. html `<label>Title`) and Value (ie. html attribute `value=""Value""`). This constructor format is true for virtually every form field.
Perhaps if you can provide a link to the documentation you're reading specifically, and what it is that has you turned around from it? ie. suggestions on how it could be clearer? Maybe the wording is a bit confusing.",0,0,msr
1142,"I am showing you code I have been implementing. On close examinatuion the second argument for TextField constructor chould be 'Title'
BUT, having missed that, I passed a value as the second argument to create(..) and hey presto it is the value. Doing *exactly* the same pattern for HiddenField resulte in a different output.
That is a bug. I have reproduced it Ad nauseam Perhaps the TextFiled interface is buggy, I do not know,. it is the way it is used in the lessons, perhaps they are buggy too. I do not know.
But I do know this: The interfaces to the HiddenField and TextField's create method are different when they should be the same.
I have made that clear in examples above. It is bnot a hypothesis but an observation.
It may not seem like a bug to have idiosyncratic interfaces, but it makes life very difficult for new comers to the platform",0,1,msr
1143,"# Test 1
```php
TextField::create('Test Name', 'Test Value')
```
Creates:
```html
<input type=""text"" name=""Test Name"" class=""text"" id=""Form_ItemEditForm_Test_Name"">
```
# Test 2
```php
TextField::create('Test Name', 'Test Label', 'Test Value'); // Shift 2nd param to 3rd
```
Creates:
```html
<input type=""text"" name=""Test Name"" value=""Test Value"" class=""text"" id=""Form_ItemEditForm_Test_Name"">
```",0,0,msr
1144,"@worikgh Is this SilverStripe 4.0?
Lets step through it together :)
`TextField` and `HiddenField` both inherit directly from `FormField`.
[`HiddenField`](https://github.com/silverstripe/silverstripe-framework/blob/4.0/src/Forms/HiddenField.php) does not define a constructor, thus uses `FormField`'s directly - ie. `($name, $title, value)` - this is 1. unique identification Name, 2. human Readable Title, 3. a preset Value.
While it is true that `TextField` implements a constructor, it simply extends `FormFields` to add two new parameters - at the _end_ of the list (maximum length and the form it belongs to):
`public function __construct($name, $title = null, $value = '', $maxLength = null, $form = null)`
Once these two extra parameters are set as fields on the object, the parent (ie. `FormField`) constructor is called to exact the same results as that of `HiddenField`.
You can see this here:
https://github.com/silverstripe/silverstripe-framework/blob/4.0/src/Forms/TextField.php#L40
This means that `TextField` also calls `FormField::__construct` with the parameter format `($name, $title, $value)`.",0,0,msr
1145,"Whatever. That is all nice theory. But the facts, here are that the interfaces differ. SS4.0",0,0,msr
1146,"I'm going to lock this topic, since it looks as if the original question has been answered adequately and the discussion seems to be degrading.
If you'd like to change the API of the FormFields, it might be worth raising an RFC issue for it with some proposals and reasons for the change so it can be discussed detail with the community and the core team. We can't change any APIs for SilverStripe 4 though without breaking semantic versioning.
Please keep in mind the [SilverStripe code of conduct](https://docs.silverstripe.org/en/contributing/code_of_conduct/) when contributing and responding to bug reports. Thanks =)",0,0,msr
1147,@worikgh how about we catch up on slack :),0,0,msr
1148,"> we have fixed the Marketplace instructions since a while
I've only recently noticed the change (from `ext-name` to `owner.ext-name`) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past?
> Since we now have URL handlers for Mac and Windows, those instructions became almost irrelevant
I would have replaced those `ext install (...)` instructions with URL handlers, but I've tried them once and after that, for a few days/weeks, a ""Do you want to install (...)"" message kept popping up every time I opened a new window. By the time that was fixed I guess I had forgotten about them.
> what if we showed in quick open a list of extensions which match that name? Why providing 2 different interfaces for discovering extensions?
Shouldn't searching ""just work""?",0,0,msr
1149,"> I've only recently noticed the change (from ext-name to owner.ext-name) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past?
I guess it wasn't, since it was a Markeplace change... Sorry about that.
> Shouldn't searching ""just work""?
It should. We've notified the Marketplace of the search issues.",0,0,msr
1150,"Just to reiterate on how bad the search engine works:
- I've just published an extension named `Open in node_modules`
- Searching for `open in node modules` (without the underscore) won't even show said extension within the first 60 results 🤷‍♂️",0,0,msr
1151,"@fabiospampinato, regarding the issue of searching your new extension:
While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching. Among the string matching, exact string matches carry a higher weight than prefix match. Since `node_modules` is one word in your extension name, word `node` in search text gets prefix match here. However, there are lot of extensions on Marketplace which have exact word 'node' in their extension name. So they carry a higher weight for matching the word 'node'. Additionally, since your extension is new, download count and ratings of other extensions are way higher, pushing their total score to the top.
One way to improve your extension rank is to break word `node_modules` into separate words `node modules`. This will increase the string matching score and push your extension up in the result.",0,0,msr
1152,"@gaurav42 Well, I would still consider whatever algorithm you guys are using broken because if I search for ""open in node modules"" I get these search results (some samples ~~randomly picked):
| Rank | Ext. Name | Ext. Description | My comments |
|--------|-------------|-----------------|----------------|
| `#6` | [Node Exec](https://marketplace.visualstudio.com/items?itemName=miramac.vscode-exec-node) | Execute the current file or your selected code with node.js. | 80k downloads, but no mention of ""open"" or ""module(s)"" anywhere.
| `#10` | [CSS Modules](https://marketplace.visualstudio.com/items?itemName=clinyong.vscode-css-modules) | Visual Studio Code extension for CSS Modules | 22k downloads, but no mention of ""open"" or ""node"" anywhere.
| `#17` | [Node TDD](https://marketplace.visualstudio.com/items?itemName=prashaantt.node-tdd) | Ease test-driven development in Node and JavaScript | 9k downloads, no mention of ""module(s)"", but at least it has a few ""opened"" in its readme.
| `#64` | [Open in Vim](https://marketplace.visualstudio.com/items?itemName=jonsmithers.open-in-vim) | Opens current file in vim | 2k downloads, this is not even mentioning ""node"" anywhere, let alone ""module(s)"". But at least it has ""open"" and ""in"" in it's title.
| `#65` | [Open in node_modules](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-open-in-node-modules) | Open the current selection or arbitrary string in node_modules. | 2 downloads, it matches all the provided keywords, **in its title**.
If I had to guess what's wrong with your approach I'd say:
- Lack of stopwords, kind of meaningless keywords like ""vscode"" or ""in"" are given too much weight.
- Poor query parsing, if keywords are joined in some sort or another the whole thing falls apart (vscode-foo-bar, foo_bar, GitLens etc.)
- Maybe you're giving too much weight to downloads and ratings, the first thing to sort for is relevancy.
> One way to improve your extension rank is to break word node_modules into separate words node modules. This will increase the string matching score and push your extension up in the result.
I'm not going to rename the extension to something wrong (`node_modules` is a folder, I'm not talking about `node modules`, as in ""NPM packages"", here) just to work around this.",0,1,msr
1153,"I agree, the search ranking seems wierd.
Some examples (when searching for ""python""):
Py Files generator is above autoDocString, despite having about 50k less installs and 0 reviews
Same thing with ladieratheme - it is above autoDocString despite having about 49k less installs and 3 less reviews
Trustcode odo snippets and kvlang is above docker linter, despite having 47k less installs
Python paste and indent has 3 stars but apparently it's 2k extra downloads trumps Python (pydev)'s 5 star rating. Seems like download count is weighted more heavily than rating in the ranking (or maybe python paste and indent has better keywords)
Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython (with 742 _thousand_ downloads and 3.5 stars) and several other extensions with far more downloads / good ratings. So you could have a very popular or well-rated extension but if you don't have the right keywords you will still be ranked down.
Wait..,. but looking at the Python Coding Conventions package.json, it doesn't even have any keywords!
https://github.com/harip/python-coding-conventions/blob/master/package.json 🤔🤔🤔
> While searching an extension, we also take into account the community inputs like number of downloads, You mean the download count that is _also_ the update count? This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased.
The update count should not effect the search ranking.
> number of ratings and average rating That's good, but what algorithim are you using to calculate the weighted rating? Not sure if that is open source but hopefully it isn't something like RatingA - RatingB or RatingA/RatingB
http://www.evanmiller.org/how-not-to-sort-by-average-rating.html",0,1,msr
1154,"> Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython This probably happens because of this:
> Poor query parsing, if keywords are joined in some sort of another the whole thing falls apart (vscode-foo-bar, foo_bar, GitLens etc.)
> You mean the download count that is also the update count? This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased.
The update count should not effect the search ranking.
Yeah that's another major problem, one could automatically push a new update every day and downloads will go through the roof even though the same number of people are using it.
I guess technically those users are _downloading_ the update, but once this things are counted the downloads counter becomes less meaningful. The download counter for extensions on Chrome's store decreases when somebody uninstalls your extension and I think it doesn't increase just because the extension gets updated.",0,0,msr
1155,"Hey guys, I want to thank you for all the feedback. Please keep them coming. We are discussing this issue internally and I'll update this thread as soon as we have something to share.",0,0,msr
1156,"Here there's another weird one, searching for ""monokai"" and ordering by downloads returns those language packs at the top. The only place where those language packs mention ""monokai"" is inside their `package.json` as the value of some `contributes.localizations[0].translations[x].id` keys, why are you guys even indexing those fields?
<img width=""1193"" alt=""screen shot 2018-07-18 at 13 09 36"" src=""https://user-images.githubusercontent.com/1812093/42878075-112f643c-8a8c-11e8-93a2-1c5f54f57589.png"">",0,0,msr
1157,"I've just realized that the readmes aren't indexed at all. I searched for ""cyclomatic"" in the marketplace, and I've got this result:
<img width=""674"" alt=""screen shot 2018-09-20 at 18 27 56"" src=""https://user-images.githubusercontent.com/1812093/45832685-ebe46c00-bd02-11e8-85ec-3cf9dc21ad82.png"">
Then I tried searching for ""marketplace cyclomatic"" on Google, and this is the result:
<img width=""846"" alt=""screen shot 2018-09-20 at 18 25 18"" src=""https://user-images.githubusercontent.com/1812093/45832583-95772d80-bd02-11e8-875b-473a56d814a1.png"">
This is a bit ridiculous. I'm not saying that the search functionalities in the marketplace should be as good as Google's under **all** circumstances, but that's a full-word match in the readme, this query just can't return 0 results.
@pkuma-msft do you have any updates for us?",0,0,msr
1158,"@fabiospampinato Yes, unfortunately we do not index readme.md.. sorry about that. We use SQL FTS in our backend today and that's kind of the limiting factor. There's only so much custom logic we can run over the results returned by FTS to make it 'more' relevant - and more importantly it doesn't scale in the long run.
We are exploring moving our search platform to Azure Search or Bing which are techologies that are being actively developed and should provide us with more features and capabilities.
I'm afraid we are not going to invest more in trying to optimize search with FTS.",0,0,msr
1159,@pkuma-msft thanks for the update. Any timeframe for when the move will happen?,0,0,msr
1160,"Holy crap changing my extension name to ""AREPL for python"" moved it to the *third* position in the search rankings when searching for python. Thanks for the tip @eamodio and @gaurav42 !",0,0,msr
1161,"@Almenon Sorry we are still in early exploration stages, cannot comment on a timeline yet.",0,0,msr
1162,"From @octref > Previously: https://github.com/Microsoft/vscode/issues/24511
> > Now if you search for Vue, Vetur doesn't even show up in the first page despite being the most popular Vue extension. The install count of the 24 Vue extensions in first page combined is not even half of Vetur's install count.
> > ![image](https://user-images.githubuserconten